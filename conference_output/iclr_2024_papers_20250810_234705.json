{
  "conference": "ICLR 2024",
  "papers_count": 586,
  "timestamp": "2025-08-10T23:47:05.714522",
  "papers": [
    {
      "id": "rhgIgTSSxW",
      "title": "TabR: Tabular Deep Learning Meets Nearest Neighbors",
      "abstract": "Deep learning (DL) models for tabular data problems (e.g. classification, regression) are currently receiving increasingly more attention from researchers.\nHowever, despite the recent efforts, the non-DL algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution for these problems.\nOne of the research directions aimed at improving the position of tabular DL involves designing so-called retrieval-augmented models.\nFor a target object, such models retrieve other objects (e.g. the nearest neighbors) from the available training data and use their features and labels to make a better prediction.\n\nIn this work, we present TabR -- essentially, a feed-forward network with a custom k-Nearest-Neighbors-like component in the middle.\nOn a set of public benchmarks with datasets up to several million objects, TabR marks a big step forward for tabular DL: it demonstrates the best average performance among tabular DL models, becomes the new state-of-the-art on several datasets, and even outperforms GBDT models on the recently proposed \"GBDT-friendly\" benchmark (see Figure 1).\nAmong the important findings and technical details powering TabR, the main ones lie in the attention-like mechanism that is responsible for retrieving the nearest neighbors and extracting valuable signal from them.\nIn addition to the higher performance, TabR is simple and significantly more efficient compared to prior retrieval-based tabular DL models.",
      "authors": [
        "Yury Gorishniy",
        "Ivan Rubachev",
        "Nikolay Kartashev",
        "Daniil Shlenskii",
        "Akim Kotelnikov",
        "Artem Babenko"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rhgIgTSSxW",
      "cdate": 1695558732991,
      "mdate": 1713672174575,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708201"
    },
    {
      "id": "eUgS9Ig8JG",
      "title": "SaNN: Simple Yet Powerful Simplicial-aware Neural Networks",
      "abstract": "Simplicial neural networks (SNNs) are deep models for higher-order graph representation learning. SNNs learn low-dimensional embeddings of simplices in a simplicial complex by aggregating features of their respective upper, lower, boundary, and coboundary adjacent simplices. The aggregation in SNNs is carried out during training. Since the number of simplices of various orders in a simplicial complex is significantly large, the memory and training-time requirement in SNNs is enormous. In this work, we propose a scalable simplicial-aware neural network (SaNN) model with a constant run-time and memory requirements independent of the size of the simplicial complex and the density of interactions in it. SaNN is based on pre-aggregated simplicial-aware features as inputs to a neural network, so it has a strong simplicial-structural inductive bias. We provide theoretical conditions under which SaNN is provably more powerful than the Weisfeiler-Lehman (WL) graph isomorphism test and as powerful as the simplicial Weisfeiler-Lehman (SWL) test. We also show that SaNN is permutation and orientation equivariant and satisfies simplicial-awareness of the highest order in a simplicial complex. We demonstrate via numerical experiments that despite being computationally economical, the proposed model achieves state-of-the-art performance in predicting trajectories,  simplicial closures, and classifying graphs.",
      "authors": [
        "Sravanthi Gurugubelli",
        "Sundeep Prabhakar Chepuri"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=eUgS9Ig8JG",
      "cdate": 1695557389659,
      "mdate": 1710599804238,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708221"
    },
    {
      "id": "qBL04XXex6",
      "title": "Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models",
      "abstract": "The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting, which involves providing a few chain of thought demonstrations as exemplars in prompts. Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in reasoning step selection for complex problem solving. In this paper, we present Boosting of Thoughts (BoT), an automated prompting framework for problem solving with LLMs by iteratively exploring and self-evaluating many trees of thoughts in order to acquire an ensemble of trial-and-error reasoning experiences, which will serve as a new form of prompting to solve the complex problem. Starting from a simple prompt without requiring examples, BoT iteratively explores and evaluates a large collection of reasoning steps, and more importantly, uses error analysis obtained from the LLM on them to explicitly revise prompting, which in turn enhances reasoning step generation, until a final answer is attained. Our experiments with GPT-4 and Llama2 across extensive complex mathematical problems demonstrate that BoT consistently achieves higher or comparable problem-solving rates than other advanced prompting approaches.",
      "authors": [
        "Sijia Chen",
        "Baochun Li",
        "Di Niu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qBL04XXex6",
      "cdate": 1695556700801,
      "mdate": 1710551113404,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708227"
    },
    {
      "id": "kmn0BhQk7p",
      "title": "Beyond Memorization: Violating Privacy via Inference with Large Language Models",
      "abstract": "Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models’ inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals’ privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to 85% top-1 and 95% top-3 accuracy at a fraction of the cost (100x) and time (240x) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for stronger and wider privacy protection.",
      "authors": [
        "Robin Staab",
        "Mark Vero",
        "Mislav Balunovic",
        "Martin Vechev"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kmn0BhQk7p",
      "cdate": 1695555710153,
      "mdate": 1710503155247,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708232"
    },
    {
      "id": "i8PjQT3Uig",
      "title": "Locality Sensitive Sparse Encoding for Learning World Models Online",
      "abstract": "Acquiring an accurate world model $\\textit{online}$ for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even with very high dimensional nonlinear features. We validate the representation power of our encoding and verify that it allows efficient online learning under data covariate shift. We also show, in the Dyna MBRL setting, that our world models learned online using a $\\textit{single pass}$ of trajectory data either surpass or match the performance of deep world models trained with replay and other continual learning methods.",
      "authors": [
        "Zichen Liu",
        "Chao Du",
        "Wee Sun Lee",
        "Min Lin"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=i8PjQT3Uig",
      "cdate": 1695555355042,
      "mdate": 1713340526679,
      "matched_keywords": [
        "reinforcement learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708237"
    },
    {
      "id": "eepoE7iLpL",
      "title": "Enhancing Neural Subset Selection: Integrating Background Information into Set Representations",
      "abstract": "Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an invariant sufficient statistic of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific superset from which the subset originated. Motivated by these insights, we propose a simple yet effective information aggregation module designed to merge the representations of subsets and supersets from a permutation invariance perspective. Comprehensive empirical evaluations across diverse tasks and datasets validate the enhanced efficacy of our approach over conventional methods, underscoring the practicality and potency of our proposed strategies in real-world contexts.",
      "authors": [
        "Binghui Xie",
        "Yatao Bian",
        "Kaiwen Zhou",
        "Yongqiang Chen",
        "Peilin Zhao",
        "Bo Han",
        "Wei Meng",
        "James Cheng"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=eepoE7iLpL",
      "cdate": 1695554227885,
      "mdate": 1709661554766,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708242"
    },
    {
      "id": "lK2V2E2MNv",
      "title": "Bridging Vision and Language Spaces with Assignment Prediction",
      "abstract": "This paper introduces VLAP, a novel approach that bridges pretrained vision models and large language models (LLMs) to make frozen LLMs understand the visual world. VLAP transforms the embedding space of pretrained vision models into the LLMs' word embedding space using a single linear layer for efficient and general-purpose visual and language understanding. Specifically, we harness well-established word embeddings to bridge two modality embedding spaces. The visual and text representations are simultaneously assigned to a set of word embeddings within pretrained LLMs by formulating the assigning procedure as an optimal transport problem. We predict the assignment of one modality from the representation of another modality data, enforcing consistent assignments for paired multimodal data. This allows vision and language representations to contain the same information, grounding the frozen LLMs' word embedding space in visual data. Moreover, a robust semantic taxonomy of LLMs can be preserved with visual data since the LLMs interpret and reason linguistic information from correlations between word embeddings. Experimental results show that VLAP achieves substantial improvements over the previous linear transformation-based approaches across a range of vision-language tasks, including image captioning, visual question answering, and cross-modal retrieval. We also demonstrate the learned visual representations hold a semantic taxonomy of LLMs, making visual semantic arithmetic possible.",
      "authors": [
        "Jungin Park",
        "Jiyoung Lee",
        "Kwanghoon Sohn"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lK2V2E2MNv",
      "cdate": 1695553760014,
      "mdate": 1713175069741,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.708247"
    },
    {
      "id": "gtkFw6sZGS",
      "title": "Generative Judge for Evaluating Alignment",
      "abstract": "The rapid development of Large Language Models (LLMs) has substantially expanded the range of tasks they can address. In the field of Natural Language Processing (NLP), researchers have shifted their focus from conventional NLP tasks (e.g., sequence tagging and parsing) towards tasks that revolve around aligning with human needs (e.g., brainstorming and email writing). This shift in task distribution imposes new requirements on evaluating these aligned models regarding *generality* (i.e., assessing performance across diverse scenarios), *flexibility* (i.e., examining under different protocols), and *interpretability* (i.e., scrutinizing models with explanations). In this paper, we propose a generative judge with 13B parameters, **Auto-J**, designed to address these challenges. Our model is trained on user queries and LLM-generated responses under massive real-world scenarios and accommodates diverse evaluation protocols (e.g., pairwise response comparison and single-response evaluation) with well-structured natural language critiques. To demonstrate the efficacy of our approach, we construct a new testbed covering 58 different scenarios. Experimentally, **Auto-J** outperforms a series of strong competitors, including both open-source and closed-source models, by a large margin. We also provide detailed analysis and case studies to further reveal the potential of our method and make a variety of resources public at https://github.com/GAIR-NLP/auto-j.",
      "authors": [
        "Junlong Li",
        "Shichao Sun",
        "Weizhe Yuan",
        "Run-Ze Fan",
        "hai zhao",
        "Pengfei Liu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=gtkFw6sZGS",
      "cdate": 1695553656358,
      "mdate": 1709661554684,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708257"
    },
    {
      "id": "7vVWiCrFnd",
      "title": "Rethinking and Extending the Probabilistic Inference Capacity of GNNs",
      "abstract": "Designing expressive Graph Neural Networks (GNNs) is an important topic in graph machine learning fields. Despite the existence of numerous approaches proposed to enhance GNNs based on Weisfeiler-Lehman (WL) tests, what GNNs can and cannot learn still lacks a deeper understanding. This paper adopts a fundamentally different approach to examine the expressive power of GNNs from a probabilistic perspective. By establishing connections between GNNs' predictions and the central inference problems of probabilistic graphical models (PGMs), we can analyze previous GNN variants with a novel hierarchical framework and gain new insights into their node-level and link-level behaviors. Additionally, we introduce novel methods that can provably enhance GNNs' ability to capture complex dependencies and make complex predictions. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of our approaches.",
      "authors": [
        "Tuo Xu",
        "Lei Zou"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7vVWiCrFnd",
      "cdate": 1695553568508,
      "mdate": 1713862850372,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708261"
    },
    {
      "id": "SLw9fp4yI6",
      "title": "Controlled Text Generation via Language Model Arithmetic",
      "abstract": "As Large Language Models (LLMs) are deployed more widely, customization with respect to vocabulary, style, and character becomes more important. In this work, we introduce model arithmetic, a novel inference framework for composing and biasing LLMs without the need for model (re)training or highly specific datasets. In addition, the framework allows for more precise control of generated text than direct prompting and prior controlled text generation (CTG) techniques. Using model arithmetic, we can express prior CTG techniques as simple formulas and naturally extend them to new and more effective formulations. Further, we show that speculative sampling, a technique for efficient LLM sampling, extends to our setting. This enables highly efficient text generation with multiple composed models with only marginal overhead over a single model. Our empirical evaluation demonstrates that model arithmetic allows fine-grained control of generated text while outperforming state-of-the-art on the task of toxicity reduction. We release an open source easy-to-use implementation of our framework at https://github.com/eth-sri/language-model-arithmetic.",
      "authors": [
        "Jasper Dekoninck",
        "Marc Fischer",
        "Luca Beurer-Kellner",
        "Martin Vechev"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=SLw9fp4yI6",
      "cdate": 1695553212922,
      "mdate": 1709663052044,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708267"
    },
    {
      "id": "My7lkRNnL9",
      "title": "Forward Learning with Top-Down Feedback: Empirical and Analytical Characterization",
      "abstract": "\"Forward-only\" algorithms, which train neural networks while avoiding a backward pass, have recently gained attention as a way of solving the biologically unrealistic aspects of backpropagation. Here, we first address compelling challenges related to the \"forward-only\" rules, which include reducing the performance gap with backpropagation and providing an analytical understanding of their dynamics. To this end, we show that the forward-only algorithm with top-down feedback is well-approximated by an \"adaptive-feedback-alignment\" algorithm, and we analytically track its performance during learning in a prototype high-dimensional setting. Then, we compare different versions of forward-only algorithms, focusing on the Forward-Forward and PEPITA frameworks, and we show that they share the same learning principles. Overall, our work unveils the connections between three key neuro-inspired learning rules, providing a link between \"forward-only\" algorithms, i.e., Forward-Forward and PEPITA, and an approximation of backpropagation, i.e., Feedback Alignment.",
      "authors": [
        "Ravi Francesco Srinivasan",
        "Francesca Mignacco",
        "Martino Sorbaro",
        "Maria Refinetti",
        "Avi Cooper",
        "Gabriel Kreiman",
        "Giorgia Dellaferrera"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=My7lkRNnL9",
      "cdate": 1695553023104,
      "mdate": 1709661554509,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708271"
    },
    {
      "id": "AZGIwqCyYY",
      "title": "Towards Cross Domain Generalization of Hamiltonian Representation via Meta Learning",
      "abstract": "Recent advances in deep learning for physics have focused on discovering shared representations of target systems by incorporating physics priors or inductive biases into neural networks. While effective, these methods are limited to the system domain, where the type of system remains consistent and thus cannot ensure the adaptation to new, or unseen physical systems governed by different laws. For instance, a neural network trained on a mass-spring system cannot guarantee accurate predictions for the behavior of a two-body system or any other system with different physical laws.\nIn this work, we take a significant leap forward by targeting cross domain generalization within the field of Hamiltonian dynamics. \nWe model our system with a graph neural network (GNN) and employ a meta learning algorithm to enable the model to gain experience over a distribution of systems and make it adapt to new physics. Our approach aims to learn a unified Hamiltonian representation that is generalizable across multiple system domains, thereby overcoming the limitations of system-specific models. \nWe demonstrate that the meta-trained model captures the generalized Hamiltonian representation that is consistent across different physical domains.\nOverall, through the use of meta learning, we offer a framework that achieves cross domain generalization, providing a step towards a unified model for understanding a wide array of dynamical systems via deep learning.",
      "authors": [
        "Yeongwoo Song",
        "Hawoong Jeong"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=AZGIwqCyYY",
      "cdate": 1695552644518,
      "mdate": 1710581713869,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708276"
    },
    {
      "id": "BTKAeLqLMw",
      "title": "What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning",
      "abstract": "Instruction tuning is a standard technique employed to align large language models to end tasks and user preferences after the initial pretraining phase. Recent research indicates the critical role of data engineering in instruction tuning -- when appropriately selected, only limited data is necessary to achieve superior performance. However, we still lack a principled understanding of what makes good instruction tuning data for alignment, and how we should select data automatically and effectively. In this work, we delve deeply into automatic data selection strategies for alignment. We start with controlled studies to measure data across three dimensions: complexity, quality, and diversity, along which we examine existing methods and introduce novel techniques for enhanced data measurement. Subsequently, we propose a simple strategy to select data samples based on the measurement. We present Deita (short for Data-Efficient Instruction Tuning for Alignment), a series of models fine-tuned from LLaMA models using data samples automatically selected with our proposed approach.  When assessed through both automatic metrics and human evaluation, Deita performs better or on par with the state-of-the-art open-source alignment models such as Vicuna and WizardLM with only 6K training data samples -- 10x less than the data used in the baselines. We anticipate this work to provide clear guidelines and tools on automatic data selection, aiding researchers and practitioners in achieving data-efficient alignment.",
      "authors": [
        "Wei Liu",
        "Weihao Zeng",
        "Keqing He",
        "Yong Jiang",
        "Junxian He"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BTKAeLqLMw",
      "cdate": 1695552169713,
      "mdate": 1713166048070,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708281"
    },
    {
      "id": "AJBkfwXh3u",
      "title": "Causality-Inspired Spatial-Temporal Explanations for Dynamic Graph Neural Networks",
      "abstract": "Dynamic Graph Neural Networks (DyGNNs) have gained significant popularity in the research of dynamic graphs, but are limited by the low transparency, such that human-understandable insights can hardly be drawn from their predictions. Although a number of existing research have been devoted to investigating the interpretability of graph neural networks (GNNs), achieving the interpretability of DyGNNs is pivotally challenging due to the complex spatial-temporal correlations in dynamic graphs. To this end, we propose an innovative causality-inspired generative model based on structural causal model (SCM), which explores the underlying philosophies of DyGNN predictions by identifying the trivial, static, and dynamic causal relationships. To reach this goal, two critical tasks need to be accomplished including (1) disentangling the complex causal relationships, and (2) fitting the spatial-temporal explanations of DyGNNs in the SCM architecture. To tackle these challenges, the proposed method incorporates a contrastive learning module to disentangle trivial and causal relationships, and a dynamic correlating module to disentangle dynamic and static causal relationships, respectively. A dynamic VGAE-based framework is further developed, which generates causal-and-dynamic masks for spatial interpretability, and recognizes dynamic relationships along the time horizon through causal invention for temporal interpretability. Comprehensive experiments have been conducted on both synthetic and real-world datasets, where our approach yields substantial improvements, thereby demonstrating significant superiority.",
      "authors": [
        "Kesen Zhao",
        "Liang Zhang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=AJBkfwXh3u",
      "cdate": 1695552059737,
      "mdate": 1710491992975,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708286"
    },
    {
      "id": "78iGZdqxYY",
      "title": "Mirage: Model-agnostic Graph Distillation for Graph Classification",
      "abstract": "GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called MIRAGE for graph classification. MIRAGE is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation trees. Furthermore, the frequency distribution of computation trees is often skewed in nature, enabling us to condense this data into a concise distilled summary. By compressing the computation data itself, as opposed to emulating gradient flows on the original training set—a prevalent approach to date—MIRAGE transforms into an unsupervised and architecture-agnostic distillation algorithm. Extensive benchmarking on real-world datasets underscores MIRAGE’s superiority, showcasing enhanced generalization accuracy, data compression, and distillation efficiency when compared to state-of-the-art baselines.",
      "authors": [
        "Mridul Gupta",
        "Sahil Manchanda",
        "HARIPRASAD KODAMANA",
        "Sayan Ranu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=78iGZdqxYY",
      "cdate": 1695551265394,
      "mdate": 1710056017506,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708291"
    },
    {
      "id": "lF2aip4Scn",
      "title": "Demonstration-Regularized RL",
      "abstract": "Incorporating expert demonstrations has empirically helped to improve the sample efficiency of reinforcement learning (RL). This paper quantifies theoretically to what extent this extra information reduces RL's sample complexity. In particular, we study the demonstration-regularized reinforcement learning framework that leverages the expert demonstrations by $\\mathrm{KL}$-regularization for a policy learned by behavior cloning. Our findings reveal that using $N^{\\mathrm{E}}$ expert demonstrations enables the identification of an optimal policy at a sample complexity of order $\\widetilde{\\mathcal{O}}(\\mathrm{Poly}(S,A,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$ in finite and $\\widetilde{\\mathcal{O}}(\\mathrm{Poly}(d,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$ in linear Markov decision processes, where $\\varepsilon$is the target precision, $H$ the horizon, $A$ the number of action, $S$ the number of states in the finite case and $d$ the dimension of the feature space in the linear case. As a by-product, we provide tight convergence guarantees for the behavior cloning procedure under general assumptions on the policy classes. Additionally, we establish that demonstration-regularized methods are provably efficient for reinforcement learning from human feedback (RLHF). In this respect, we provide theoretical evidence showing the benefits of KL-regularization for RLHF  in tabular and linear MDPs. \nInterestingly, we avoid pessimism injection by employing computationally feasible regularization to handle reward estimation uncertainty, thus setting our approach apart from the prior works.",
      "authors": [
        "Daniil Tiapkin",
        "Denis Belomestny",
        "Daniele Calandriello",
        "Eric Moulines",
        "Alexey Naumov",
        "Pierre Perrault",
        "Michal Valko",
        "Pierre Menard"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lF2aip4Scn",
      "cdate": 1695547931617,
      "mdate": 1710702621893,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708296"
    },
    {
      "id": "vESNKdEMGp",
      "title": "Multilingual Jailbreak Challenges in Large Language Models",
      "abstract": "While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low-resource languages exhibit about three times the likelihood of encountering harmful content compared to high-resource languages, with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts can exacerbate the negative impact of malicious instructions, with astonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for GPT-4. To handle such a challenge in the multilingual context, we propose a novel \\textsc{Self-Defense} framework that automatically generates multilingual training data for safety fine-tuning. Experimental results show that ChatGPT fine-tuned with such data can achieve a substantial reduction in unsafe content generation.  Data is available at \\url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}.",
      "authors": [
        "Yue Deng",
        "Wenxuan Zhang",
        "Sinno Jialin Pan",
        "Lidong Bing"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vESNKdEMGp",
      "cdate": 1695547633120,
      "mdate": 1709661553854,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708301"
    },
    {
      "id": "nTwb2vBLOV",
      "title": "Rethinking the Power of Graph Canonization in Graph Representation Learning with Stability",
      "abstract": "The expressivity of Graph Neural Networks (GNNs) has been studied broadly in recent years to reveal the design principles for more powerful GNNs. Graph canonization is known as a typical approach to distinguish non-isomorphic graphs, yet rarely adopted when developing expressive GNNs. This paper proposes to maximize the expressivity of GNNs by graph canonization, then the power of such GNNs is studies from the perspective of model stability. A stable GNN will map similar graphs to close graph representations in the vectorial space, and the stability of GNNs is critical to generalize their performance to unseen graphs. We theoretically reveal the trade-off of expressivity and stability in graph-canonization-enhanced GNNs. Then we introduce a notion of universal graph canonization as the general solution to address the trade-off and characterize a widely applicable sufficient condition to solve the universal graph canonization. A comprehensive set of experiments demonstrates the effectiveness of the proposed method. In many popular graph benchmark datasets, graph canonization successfully enhances GNNs and provides highly competitive performance, indicating the capability and great potential of proposed method in general graph representation learning. In graph datasets where the sufficient condition holds, GNNs enhanced by universal graph canonization consistently outperform GNN baselines and successfully improve the SOTA performance up to $31$%, providing the optimal solution to numerous challenging real-world graph analytical tasks like gene network representation learning in bioinformatics.",
      "authors": [
        "Zehao Dong",
        "Muhan Zhang",
        "Philip Payne",
        "Michael A Province",
        "Carlos Cruchaga",
        "Tianyu Zhao",
        "Fuhai Li",
        "Yixin Chen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nTwb2vBLOV",
      "cdate": 1695546910335,
      "mdate": 1710541796686,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708309"
    },
    {
      "id": "FMMF1a9ifL",
      "title": "Gradual Optimization Learning for Conformational Energy Minimization",
      "abstract": "Molecular conformation optimization is crucial to computer-aided drug discovery and materials design.\nTraditional energy minimization techniques rely on iterative optimization methods that use molecular forces calculated by a physical simulator (oracle) as anti-gradients.\nHowever, this is a computationally expensive approach that requires many interactions with a physical simulator.\nOne way to accelerate this procedure is to replace the physical simulator with a neural network.\nDespite recent progress in neural networks for molecular conformation energy prediction, such models are prone to errors due to distribution shift, leading to inaccurate energy minimization.\nWe find that the quality of energy minimization with neural networks can be improved by providing optimization trajectories as additional training data.\nStill, obtaining complete optimization trajectories demands a lot of additional computations.\nTo reduce the required additional data, we present the Gradual Optimization Learning Framework (GOLF) for energy minimization with neural networks.\nThe framework consists of an efficient data-collecting scheme and an external optimizer.\nThe external optimizer utilizes gradients from the energy prediction model to generate optimization trajectories, and the data-collecting scheme selects additional training data to be processed by the physical simulator. \nOur results demonstrate that the neural network trained with GOLF performs \\textit{on par} with the oracle on a benchmark of diverse drug-like molecules using significantly less additional data.",
      "authors": [
        "Artem Tsypin",
        "Leonid Anatolievich Ugadiarov",
        "Kuzma Khrabrov",
        "Alexander Telepov",
        "Egor Rumiantsev",
        "Alexey Skrynnik",
        "Aleksandr Panov",
        "Dmitry P. Vetrov",
        "Elena Tutubalina",
        "Artur Kadurin"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FMMF1a9ifL",
      "cdate": 1695546674929,
      "mdate": 1710234268962,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708314"
    },
    {
      "id": "uGtfk2OphU",
      "title": "Towards Faithful Explanations: Boosting Rationalization with Shortcuts Discovery",
      "abstract": "The remarkable success in neural networks provokes the selective rationalization. It explains the prediction results by identifying a small subset of the inputs sufficient to support them. Since existing methods still suffer from adopting the shortcuts in data to compose rationales and limited large-scale annotated rationales by human, in this paper, we propose a Shortcuts-fused Selective Rationalization (SSR) method, which boosts the rationalization by discovering and exploiting potential shortcuts. Specifically, SSR first designs a shortcuts discovery approach to detect several potential shortcuts. Then, by introducing the identified shortcuts, we propose two strategies to mitigate the problem of utilizing shortcuts to compose rationales. Finally, we develop two data augmentations methods to close the gap in the number of annotated rationales. Extensive experimental results on real-world datasets clearly validate the effectiveness of our proposed method.",
      "authors": [
        "Linan Yue",
        "Qi Liu",
        "Yichao Du",
        "Li Wang",
        "Weibo Gao",
        "Yanqing An"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=uGtfk2OphU",
      "cdate": 1695546281340,
      "mdate": 1713672124468,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708319"
    },
    {
      "id": "KTtEICH4TO",
      "title": "CORN: Contact-based Object Representation for Nonprehensile Manipulation of General Unseen Objects",
      "abstract": "Nonprehensile manipulation is essential for manipulating objects that are too thin, large, or otherwise ungraspable in the wild. To sidestep the difficulty of contact modeling in conventional modeling-based approaches, reinforcement learning (RL) has recently emerged as a promising alternative. However, previous RL approaches either lack the ability to generalize over diverse object shapes, or use simple action primitives that limit the diversity of robot motions. Furthermore, using RL over diverse object geometry is challenging due to the high cost of training a policy that takes in high-dimensional sensory inputs. We propose a novel contact-based object representation and pretraining pipeline to tackle this. To enable massively parallel training, we leverage a lightweight patch-based transformer architecture for our encoder that processes point clouds, thus scaling our training across thousands of environments. Compared to learning from scratch, or other shape representation baselines, our representation facilitates both time- and data-efficient learning. We validate the efficacy of our overall system by zero-shot transferring the trained policy to novel real-world objects. We highly recommend the video attached in the supplementary material. Code and videos are available at \\url{https://sites.google.com/view/contact-non-prehensile}.",
      "authors": [
        "Yoonyoung Cho",
        "Junhyek Han",
        "Yoontae Cho",
        "Beomjoon Kim"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=KTtEICH4TO",
      "cdate": 1695545917493,
      "mdate": 1710489237956,
      "matched_keywords": [
        "reinforcement learning",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708323"
    },
    {
      "id": "5JWAOLBxwp",
      "title": "An Intuitive Multi-Frequency Feature Representation for SO(3)-Equivariant Networks",
      "abstract": "The usage of 3D vision algorithms, such as shape reconstruction, remains limited because they require inputs to be at a fixed canonical rotation. Recently, a simple equivariant network, Vector Neuron (VN) has been proposed that can be easily used with the state-of-the-art 3D neural network (NN) architectures. However, its performance is limited because it is designed to use only three-dimensional features, which is insufficient to capture the details present in 3D data. In this paper, we introduce an equivariant feature representation for mapping a 3D point to a high-dimensional feature space. Our feature can discern multiple frequencies present in 3D data, which, as shown by Tancik et al. (2020), is the key to designing an expressive feature for 3D vision tasks. Our representation can be used as an input to VNs, and the results demonstrate that with our feature representation, VN captures more details, overcoming the limitation raised in its original paper.",
      "authors": [
        "Dongwon Son",
        "Jaehyung Kim",
        "Sanghyeon Son",
        "Beomjoon Kim"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5JWAOLBxwp",
      "cdate": 1695545864891,
      "mdate": 1710480520849,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708328"
    },
    {
      "id": "AqN23oqraW",
      "title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models",
      "abstract": "The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For ability modeling, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering 19 tasks. (2) For data, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For evaluation criteria, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models, and a unique self-contrast metric for automatically evaluating knowledge-creating ability. We evaluate 21 open-source and commercial LLMs and obtain some intriguing findings. The KoLA dataset will be updated every three months to provide timely references for developing LLMs and knowledge-related systems.",
      "authors": [
        "Jifan Yu",
        "Xiaozhi Wang",
        "Shangqing Tu",
        "Shulin Cao",
        "Daniel Zhang-Li",
        "Xin Lv",
        "Hao Peng",
        "Zijun Yao",
        "Xiaohan Zhang",
        "Hanming Li",
        "Chunyang Li",
        "Zheyuan Zhang",
        "Yushi Bai",
        "Yantao Liu",
        "Amy Xin",
        "Kaifeng Yun",
        "Linlu GONG",
        "Nianyi Lin",
        "Jianhui Chen",
        "Zhili Wu",
        "Yunjia Qi",
        "Weikai Li",
        "Yong Guan",
        "Kaisheng Zeng",
        "Ji Qi",
        "Hailong Jin",
        "Jinxin Liu",
        "Yu Gu",
        "Yuan Yao",
        "Ning Ding",
        "Lei Hou",
        "Zhiyuan Liu",
        "Xu Bin",
        "Jie Tang",
        "Juanzi Li"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=AqN23oqraW",
      "cdate": 1695545711513,
      "mdate": 1710475887703,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708332"
    },
    {
      "id": "N0nTk5BSvO",
      "title": "TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts",
      "abstract": "Accurate traffic forecasting is challenging due to the complex dependency on road networks, various types of roads, and the abrupt speed change due to the events. Recent works mainly focus on dynamic spatial modeling with adaptive graph embedding or graph attention having less consideration for temporal characteristics and in-situ modeling. In this paper, we propose a novel deep learning model named TESTAM, which individually models recurring and non-recurring traffic patterns by a mixture-of-experts model with three experts on temporal modeling, spatio-temporal modeling with static graph, and dynamic spatio-temporal dependency modeling with dynamic graph. By introducing different experts and properly routing them, TESTAM could better model various circumstances, including spatially isolated nodes, highly related nodes, and recurring and non-recurring events. For the proper routing, we reformulate a gating problem into a classification problem with pseudo labels. Experimental results on three public traffic network datasets, METR-LA, PEMS-BAY, and EXPY-TKY, demonstrate that TESTAM achieves a better indication and modeling of recurring and non-recurring traffic.",
      "authors": [
        "Hyunwook Lee",
        "Sungahn Ko"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=N0nTk5BSvO",
      "cdate": 1695545439600,
      "mdate": 1709661553293,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708337"
    },
    {
      "id": "OsGUnYOzii",
      "title": "Learning From Simplicial Data Based on Random Walks and 1D Convolutions",
      "abstract": "Triggered by limitations of graph-based deep learning methods in terms of computational expressivity and model flexibility, recent years have seen a surge of interest in computational models that operate on higher-order topological domains such as hypergraphs and simplicial complexes. While the increased expressivity of these models can indeed lead to a better classification performance and a more faithful representation of the underlying system, the computational cost of these higher-order models can increase dramatically. To this end, we here explore a simplicial complex neural network learning architecture based on random walks and fast 1D convolutions (SCRaWl), in which we can adjust the increase in computational cost by varying the length and number of random walks considered while accounting for higher-order relationships. Importantly, due to the random walk-based design, the expressivity of the proposed architecture is provably incomparable to that of existing message-passing simplicial neural networks. We empirically evaluate SCRaWl on real-world datasets and show that it outperforms other simplicial neural networks.",
      "authors": [
        "Florian Frantzen",
        "Michael T Schaub"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OsGUnYOzii",
      "cdate": 1695545230835,
      "mdate": 1712237333778,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708341"
    },
    {
      "id": "wkbeqr5XhC",
      "title": "LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition",
      "abstract": "Bandwidth constraints during signal acquisition frequently impede real-time detection applications. Hyperspectral data is a notable example, whose vast volume compromises real-time hyperspectral detection. To tackle this hurdle, we introduce a novel approach leveraging pre-acquisition modulation to reduce the acquisition volume. This modulation process is governed by a deep learning model, utilizing prior information. Central to our approach is LUM-ViT, a Vision Transformer variant. Uniquely, LUM-ViT incorporates a learnable under-sampling mask tailored for pre-acquisition modulation. To further optimize for optical calculations, we propose a kernel-level weight binarization technique and a three-stage fine-tuning strategy. Our evaluations reveal that, by sampling a mere 10\\% of the original image pixels, LUM-ViT maintains the accuracy loss within 1.8\\% on the ImageNet classification task. The method sustains near-original accuracy when implemented on real-world optical hardware, demonstrating its practicality. Code will be available at [https://github.com/MaxLLF/LUM-ViT](https://github.com/MaxLLF/LUM-ViT).",
      "authors": [
        "Lingfeng Liu",
        "Dong Ni",
        "Hangjie Yuan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=wkbeqr5XhC",
      "cdate": 1695544689958,
      "mdate": 1709661553267,
      "matched_keywords": [
        "transformer",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708346"
    },
    {
      "id": "SQpnEfv9WH",
      "title": "Social-Transmotion: Promptable Human Trajectory Prediction",
      "abstract": "Accurate human trajectory prediction is crucial for applications such as autonomous vehicles, robotics, and surveillance systems. Yet, existing models often fail to fully leverage the non-verbal social cues human subconsciously communicate when navigating the space.\nTo address this, we introduce *Social-Transmotion*, a generic Transformer-based model that exploits diverse and numerous visual cues to predict human behavior. We translate the idea of a prompt from Natural Language Processing (NLP) to the task of human trajectory prediction, where a prompt can be a sequence of x-y coordinates on the ground, bounding boxes in the image plane, or body pose keypoints in either 2D or 3D.  This, in turn, augments trajectory data, leading to enhanced human trajectory prediction.\nUsing masking technique, our model exhibits flexibility and adaptability by capturing spatiotemporal interactions between agents based on the available visual cues.\nWe delve into the merits of using 2D versus 3D poses, and a limited set of poses. Additionally, we investigate the spatial and temporal attention map to identify which keypoints and time-steps in the sequence are vital for optimizing human trajectory prediction.\nOur approach is validated on multiple datasets, including JTA, JRDB, Pedestrians and Cyclists in Road Traffic, and ETH-UCY.\nThe code is publicly available: [https://github.com/vita-epfl/social-transmotion](https://github.com/vita-epfl/social-transmotion).",
      "authors": [
        "Saeed Saadatnejad",
        "Yang Gao",
        "Kaouther Messaoud",
        "Alexandre Alahi"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=SQpnEfv9WH",
      "cdate": 1695543542128,
      "mdate": 1713133004754,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708351"
    },
    {
      "id": "wfgZc3IMqo",
      "title": "Robust Classification via Regression for Learning with Noisy Labels",
      "abstract": "Deep neural networks and large-scale datasets have revolutionized the field of machine learning. However, these large networks are susceptible to overfitting to label noise, resulting in reduced generalization. To address this challenge, two promising approaches have emerged: i) loss reweighting, which reduces the influence of noisy examples on the training loss, and ii) label correction that replaces noisy labels with estimated true labels. These directions have been pursued separately or combined as independent methods, lacking a unified approach. In this work, we present a unified method that seamlessly combines loss reweighting and label correction to enhance robustness against label noise in classification tasks. Specifically, by leveraging ideas from compositional data analysis in statistics, we frame the problem as a regression task, where loss reweighting and label correction can naturally be achieved with a shifted Gaussian label noise model. Our unified approach achieves strong performance compared to recent baselines on several noisy labelled datasets. We believe this work is a promising step towards robust deep learning in the presence of label noise. Our code is available at: https://github.com/ErikEnglesson/SGN.",
      "authors": [
        "Erik Englesson",
        "Hossein Azizpour"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=wfgZc3IMqo",
      "cdate": 1695543511256,
      "mdate": 1713672072558,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708359"
    },
    {
      "id": "3SJE1WLB4M",
      "title": "Generalization error of spectral algorithms",
      "abstract": "The asymptotically precise estimation of the generalization of kernel methods has recently received attention due to the parallels between neural networks and their associated kernels. However, prior works derive such estimates for training by kernel ridge regression (KRR), whereas neural networks are typically trained with gradient descent (GD). In the present work, we consider the training of kernels with a family of \\emph{spectral algorithms} specified by profile $h(\\lambda)$, and including KRR and GD as special cases. Then, we derive the generalization error as a functional of learning profile $h(\\lambda)$ for two data models: high-dimensional Gaussian and low-dimensional translation-invariant model. \nUnder power-law assumptions on the spectrum of the kernel and target, we use our framework to (i) give full loss asymptotics for both noisy and noiseless observations (ii) show that the loss localizes on certain spectral scales, giving a new perspective on the KRR saturation phenomenon (iii) conjecture, and demonstrate for the considered data models, the universality of the loss w.r.t. non-spectral details of the problem, but only in case of noisy observation.",
      "authors": [
        "Maksim Velikanov",
        "Maxim Panov",
        "Dmitry Yarotsky"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3SJE1WLB4M",
      "cdate": 1695543309509,
      "mdate": 1710753669608,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708364"
    },
    {
      "id": "jZPqf2G9Sw",
      "title": "Dynamics-Informed Protein Design with Structure Conditioning",
      "abstract": "Current protein generative models are able to design novel backbones with desired shapes or functional motifs. However, despite the importance of a protein’s dynamical properties for its function, conditioning on dynamical properties remains elusive. We present a new approach to protein generative modeling by leveraging Normal Mode Analysis that enables us to capture dynamical properties too. We introduce a method for conditioning the diffusion probabilistic models on protein dynamics, specifically on the lowest non-trivial normal mode of oscillation. Our method, similar to the classifier guidance conditioning, formulates the sampling process as being driven by conditional and unconditional terms. However, unlike previous works, we approximate the conditional term with a simple analytical function rather than an external neural network, thus making the eigenvector calculations approachable. We present the corresponding SDE theory as a formal justification of our approach. We extend our framework to conditioning on structure and dynamics at the same time, enabling scaffolding of the dynamical motifs. We demonstrate the empirical effectiveness of our method by turning the open-source unconditional protein diffusion model Genie into the conditional model with no retraining. Generated proteins exhibit the desired dynamical and structural properties while still being biologically plausible. Our work represents a first step towards incorporating dynamical behaviour in protein design and may open the door to designing more flexible and functional proteins in the future.",
      "authors": [
        "Urszula Julia Komorowska",
        "Simon V Mathis",
        "Kieran Didi",
        "Francisco Vargas",
        "Pietro Lio",
        "Mateja Jamnik"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jZPqf2G9Sw",
      "cdate": 1695541695645,
      "mdate": 1714607089029,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708372"
    },
    {
      "id": "tEgrUrUuwA",
      "title": "Partitioning Message Passing for Graph Fraud Detection",
      "abstract": "Label imbalance and homophily-heterophily mixture are the fundamental problems encountered when applying Graph Neural Networks (GNNs) to Graph Fraud Detection (GFD) tasks. Existing GNN-based GFD models are designed to augment graph structure to accommodate the inductive bias of GNNs towards homophily, by excluding heterophilic neighbors during message passing. In our work, we argue that the key to applying GNNs for GFD is not to exclude but to {\\em distinguish} neighbors with different labels. Grounded in this perspective, we introduce Partitioning Message Passing (PMP), an intuitive yet effective message passing paradigm expressly crafted for GFD. Specifically, in the neighbor aggregation stage of PMP, neighbors with different classes are aggregated with distinct node-specific aggregation functions. By this means, the center node can adaptively adjust the information aggregated from its heterophilic and homophilic neighbors, thus avoiding the model gradient being dominated by benign nodes which occupy the majority of the population. We theoretically establish a connection between the spatial formulation of PMP and spectral analysis to characterize that PMP operates an adaptive node-specific spectral graph filter, which demonstrates the capability of PMP to handle heterophily-homophily mixed graphs. Extensive experimental results show that PMP can significantly boost the performance on GFD tasks.",
      "authors": [
        "Wei Zhuo",
        "Zemin Liu",
        "Bryan Hooi",
        "Bingsheng He",
        "Guang Tan",
        "Rizal Fathony",
        "Jia Chen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tEgrUrUuwA",
      "cdate": 1695540898444,
      "mdate": 1710734361749,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708377"
    },
    {
      "id": "EmQSOi1X2f",
      "title": "Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation",
      "abstract": "Large language models (large LMs) are susceptible to producing text that contains hallucinated content. An important instance of this problem is self-contradiction, where the LM generates two contradictory sentences within the same context. In this work, we present a comprehensive investigation into self-contradiction for various instruction-tuned LMs, covering evaluation, detection, and mitigation. Our primary evaluation task is open-domain text generation, but we also demonstrate the applicability of our approach to shorter question answering. Our analysis reveals the prevalence of self-contradictions, e.g., in 17.7% of all sentences produced by ChatGPT. We then propose a novel prompting-based framework designed to effectively detect and mitigate self-contradictions. Our detector achieves high accuracy, e.g., around 80% F1 score when prompting ChatGPT. The mitigation algorithm iteratively refines the generated text to remove contradictory information while preserving text fluency and informativeness. Importantly, our entire framework is applicable to black-box LMs and does not require retrieval of external knowledge. Rather, our method complements retrieval-based methods, as a large portion of self-contradictions (e.g., 35.2% for ChatGPT) cannot be verified using online text. Our approach is practically effective and has been released as a push-button tool to benefit the public at https://chatprotect.ai/.",
      "authors": [
        "Niels Mündler",
        "Jingxuan He",
        "Slobodan Jenko",
        "Martin Vechev"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EmQSOi1X2f",
      "cdate": 1695540825331,
      "mdate": 1710535780023,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708381"
    },
    {
      "id": "327tbF3S65",
      "title": "DDMI: Domain-agnostic Latent Diffusion Models for Synthesizing High-Quality Implicit Neural Representations",
      "abstract": "Recent studies have introduced a new class of generative models for synthesizing implicit neural representations (INRs) that capture arbitrary continuous signals in various domains. These models opened the door for domain-agnostic generative models, but they often fail to achieve high-quality generation. We observed that the existing methods generate the weights of neural networks to parameterize INRs and evaluate the network with fixed positional embeddings (PEs). Arguably, this architecture limits the expressive power of generative models and results in low-quality INR generation. To address this limitation, we propose Domain-agnostic Latent Diffusion Model for INRs (DDMI) that generates adaptive positional embeddings instead of neural networks' weights. Specifically, we develop a Discrete-to-continuous space Variational AutoEncoder (D2C-VAE) that seamlessly connects discrete data and continuous signal functions in the shared latent space. Additionally, we introduce a novel conditioning mechanism for evaluating INRs with the hierarchically decomposed PEs to further enhance expressive power. Extensive experiments across four modalities, \\eg, 2D images, 3D shapes, Neural Radiance Fields, and videos, with seven benchmark datasets, demonstrate the versatility of DDMI and its superior performance compared to the existing INR generative models. Code is available at \\href{https://github.com/mlvlab/DDMI}{https://github.com/mlvlab/DDMI}.",
      "authors": [
        "Dogyun Park",
        "Sihyeon Kim",
        "Sojin Lee",
        "Hyunwoo J. Kim"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=327tbF3S65",
      "cdate": 1695537079601,
      "mdate": 1713673078690,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708386"
    },
    {
      "id": "uREj4ZuGJE",
      "title": "In-context Autoencoder for Context Compression in a Large Language Model",
      "abstract": "We propose the In-context Autoencoder (ICAE), leveraging the power of a large language model (LLM) to compress a long context into short compact memory slots that can be directly conditioned on by the LLM for various purposes. ICAE is first pretrained using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, it is fine-tuned on instruction data for producing desirable responses to various prompts. Experiments demonstrate that our lightweight ICAE, introducing about 1% additional parameters, effectively achieves $4\\times$ context compression based on Llama, offering advantages in both improved latency and GPU memory cost during inference, and showing an interesting insight in memorization as well as potential for scalability. These promising results imply a novel perspective on the connection between working memory in cognitive science and representation learning in LLMs, revealing ICAE's significant implications in addressing the long context problem and suggesting further research in LLM context management. Our data, code and models are available at https://github.com/getao/icae.",
      "authors": [
        "Tao Ge",
        "Hu Jing",
        "Lei Wang",
        "Xun Wang",
        "Si-Qing Chen",
        "Furu Wei"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=uREj4ZuGJE",
      "cdate": 1695535623476,
      "mdate": 1713059336016,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708391"
    },
    {
      "id": "KS8mIvetg2",
      "title": "Proving Test Set Contamination in Black-Box Language Models",
      "abstract": "Large language models are trained on vast amounts of internet data, prompting concerns that they have memorized public benchmarks. Detecting this type of contamination is challenging because the pretraining data used by proprietary models are often not publicly accessible.\n\nWe propose a procedure for detecting test set contamination of language models with exact false positive guarantees and without access to pretraining data or model weights. Our approach leverages the fact that when there is no data contamination, all orderings of an exchangeable benchmark should be equally likely. In contrast, the tendency for language models to memorize example order means that a contaminated language model will find certain canonical orderings to be much more likely than others. Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples.\n\nWe demonstrate that our procedure is sensitive enough to reliably detect contamination in challenging situations, including models as small as 1.4 billion parameters, on small test sets only 1000 examples, and datasets that appear only a few times in the pretraining corpus. Finally, we evaluate LLaMA-2 to apply our test in a realistic setting and find our results to be consistent with existing contamination evaluations.",
      "authors": [
        "Yonatan Oren",
        "Nicole Meister",
        "Niladri S. Chatterji",
        "Faisal Ladhak",
        "Tatsunori Hashimoto"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=KS8mIvetg2",
      "cdate": 1695534774121,
      "mdate": 1713672762124,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708396"
    },
    {
      "id": "HX5ujdsSon",
      "title": "In-Context Learning through the Bayesian Prism",
      "abstract": "In-context learning (ICL) is one of the surprising and useful features of large language models and subject of intense research. Recently, stylized meta-learning-like ICL setups have been devised that train transformers on sequences of input-output pairs $(x, f(x))$. The function $f$ comes from a function class and generalization is checked by evaluating on sequences generated from unseen functions from the same class. One of the main discoveries in this line of research has been that for several function classes, such as linear regression, transformers successfully generalize to new functions in the class. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution.\nIn this paper we empirically examine how far this Bayesian perspective can help us understand ICL. To this end, we generalize the previous meta-ICL setup to hierarchical meta-ICL setup which involve unions of multiple task families. We instantiate this setup on a diverse range of linear and nonlinear function families and find that transformers can do ICL in this setting as well. Where Bayesian inference is tractable, we find evidence that high-capacity transformers mimic the Bayesian predictor. The Bayesian perspective provides insights into the inductive bias of ICL and how transformers perform a particular task when they are trained on multiple tasks. We also find that transformers can learn to generalize to new function classes that were not seen during pretraining. This involves deviation from the Bayesian predictor. We examine these deviations in more depth offering new insights and hypotheses.",
      "authors": [
        "Madhur Panwar",
        "Kabir Ahuja",
        "Navin Goyal"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=HX5ujdsSon",
      "cdate": 1695532605311,
      "mdate": 1713058427229,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708401"
    },
    {
      "id": "WsRHpHH4s0",
      "title": "RingAttention with Blockwise Transformers for Near-Infinite Context",
      "abstract": "Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Blockwise RingAttention, which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.",
      "authors": [
        "Hao Liu",
        "Matei Zaharia",
        "Pieter Abbeel"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=WsRHpHH4s0",
      "cdate": 1695532532578,
      "mdate": 1713672538072,
      "matched_keywords": [
        "reinforcement learning",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708406"
    },
    {
      "id": "6xfe4IVcOu",
      "title": "Chain of Hindsight aligns Language Models with Feedback",
      "abstract": "Learning from human preferences is important for language models to match human needs and to align with human and social values. \nPrior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models.\nWe condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors.  Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations.",
      "authors": [
        "Hao Liu",
        "Carmelo Sferrazza",
        "Pieter Abbeel"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=6xfe4IVcOu",
      "cdate": 1695532275254,
      "mdate": 1713491921543,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708420"
    },
    {
      "id": "IjMUGuUmBI",
      "title": "GraphChef: Decision-Tree Recipes to Explain Graph Neural Networks",
      "abstract": "We propose a new self-explainable Graph Neural Network (GNN) model: GraphChef. GraphChef integrates decision trees into the GNN message passing framework. Given a dataset, GraphChef returns a set of rules (a recipe) that explains each class in the dataset unlike existing GNNs and explanation methods that reason on individual graphs. Thanks to the decision trees, GraphChef  recipes are human understandable.  We also present a new pruning method to produce small and easy to digest trees. Experiments demonstrate that GraphChef reaches comparable accuracy to not self-explainable GNNs and produced decision trees are indeed small. We further validate the correctness of the discovered recipes on datasets where explanation ground truth is available: Reddit-Binary, MUTAG, BA-2Motifs, BA-Shapes, Tree-Cycle, and Tree-Grid.",
      "authors": [
        "Peter Müller",
        "Lukas Faber",
        "Karolis Martinkus",
        "Roger Wattenhofer"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=IjMUGuUmBI",
      "cdate": 1695532036838,
      "mdate": 1710346798454,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708425"
    },
    {
      "id": "3K3s9qxSn7",
      "title": "On Representation Complexity of Model-based and Model-free Reinforcement Learning",
      "abstract": "We study the representation complexity of model-based and model-free reinforcement learning (RL) in the context of circuit complexity. We prove theoretically that there exists a broad class of MDPs such that their underlying transition and reward functions can be represented by constant depth circuits with polynomial size, while the optimal $Q$-function suffers an exponential circuit complexity in constant-depth circuits. By drawing attention to the approximation errors and building connections to complexity theory, our theory provides unique insights into why model-based algorithms usually enjoy better sample complexity than model-free algorithms from a novel representation complexity perspective: in some cases, the ground-truth rule (model) of the environment is simple to represent, while other quantities, such as $Q$-function, appear complex. We empirically corroborate our theory by comparing the approximation error of the transition kernel, reward function, and optimal $Q$-function in various Mujoco environments, which demonstrates that the approximation errors of the transition kernel and reward function are consistently lower than those of the optimal $Q$-function. To the best of our knowledge, this work is the first to study the circuit complexity of RL, which also provides a rigorous framework for future research.",
      "authors": [
        "Hanlin Zhu",
        "Baihe Huang",
        "Stuart Russell"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3K3s9qxSn7",
      "cdate": 1695531846898,
      "mdate": 1710126827147,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708429"
    },
    {
      "id": "sKPzAXoylB",
      "title": "Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning",
      "abstract": "Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.",
      "authors": [
        "Mohamed Elsayed",
        "A. Rupam Mahmood"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=sKPzAXoylB",
      "cdate": 1695531697040,
      "mdate": 1713672158534,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708434"
    },
    {
      "id": "k581sTMyPt",
      "title": "Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making",
      "abstract": "Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability (e.g. model suitability for a task, feature space evolution during fine-tuning, and interpretation of fine-tuned features and failure modes). We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1) while PubMedBERT, the domain-specific model, contains valuable information for fine-tuning, it can overfit to minority classes when class imbalances exist. In contrast, mixed-domain models exhibit greater resistance to overfitting, suggesting potential improvements in domain-specific model robustness; (2) in-domain pre-training accelerates feature disambiguation during fine-tuning; and (3) feature spaces undergo significant sparsification during this process, enabling clinicians to identify common outlier modes among fine-tuned models as demonstrated in this paper. These findings showcase the utility of SUFO in enhancing trust and safety when using transformers in medicine, and we believe SUFO can aid practitioners in evaluating fine-tuned language models (LMs) for other applications in medicine and in more critical domains.",
      "authors": [
        "Aliyah R. Hsu",
        "Yeshwanth Cherapanamjeri",
        "Briton Park",
        "Tristan Naumann",
        "Anobel Odisho",
        "Bin Yu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=k581sTMyPt",
      "cdate": 1695531351560,
      "mdate": 1709661552153,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708439"
    },
    {
      "id": "pPjZIOuQuF",
      "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems",
      "abstract": "Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems. RepoBench is actively maintained with the latest code, serving as a live benchmark publicly available at https://github.com/Leolty/repobench.",
      "authors": [
        "Tianyang Liu",
        "Canwen Xu",
        "Julian McAuley"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pPjZIOuQuF",
      "cdate": 1695530275561,
      "mdate": 1710317684224,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708443"
    },
    {
      "id": "mIEHIcHGOo",
      "title": "Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective",
      "abstract": "Large Language Models (LLMs) inherently encode a wealth of knowledge within their parameters through pre-training on extensive corpora. While prior research has delved into operations on these parameters to manipulate the underlying implicit knowledge — encompassing detection, editing, and merging — there remains an ambiguous understanding regarding their transferability across models with varying scales. In this paper, we seek to empirically investigate knowledge transfer from larger to smaller models through a parametric perspective. To achieve this, we employ sensitivity-based techniques to extract and align knowledge-specific parameters between different LLMs. Moreover, the LoRA module is used as the intermediary mechanism for injecting the extracted knowledge into smaller models. Evaluations across four benchmarks validate the efficacy of our proposed method. Our findings highlight the critical factors contributing to the process of parametric knowledge transfer, underscoring the transferability of model parameters across LLMs of different scales. Project website: https://maszhongming.github.io/ParaKnowTransfer.",
      "authors": [
        "Ming Zhong",
        "Chenxin An",
        "Weizhu Chen",
        "Jiawei Han",
        "Pengcheng He"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=mIEHIcHGOo",
      "cdate": 1695530065988,
      "mdate": 1713218944408,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708448"
    },
    {
      "id": "RXFVcynVe1",
      "title": "Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning",
      "abstract": "Representation learning on text-attributed graphs (TAGs) has become a critical research problem in recent years. A typical example of a TAG is a paper citation graph, where the text of each paper serves as node attributes. Initial graph neural network (GNN) pipelines handled these text attributes by transforming them into shallow or hand-crafted features, such as skip-gram or bag-of-words features. Recent efforts have focused on enhancing these pipelines with language models (LMs), which typically demand intricate designs and substantial computational resources. With the advent of powerful large language models (LLMs) such as GPT or Llama2, which demonstrate an ability to reason and to utilize general knowledge, there is a growing need for techniques which combine the textual modelling abilities of LLMs with the structural learning capabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to capture textual information as features, which can be used to boost GNN performance on downstream tasks. A key innovation is our use of \\emph{explanations as features}: we prompt an LLM to perform zero-shot classification, request textual explanations for its decision-making process, and design an \\emph{LLM-to-LM interpreter} to translate these explanations into informative features for downstream GNNs. Our experiments demonstrate that our method achieves state-of-the-art results on well-established TAG datasets, including \\texttt{Cora}, \\texttt{PubMed}, \\texttt{ogbn-arxiv}, as well as our newly introduced dataset, \\texttt{tape-arxiv23}. Furthermore, our method significantly speeds up training, achieving a 2.88 times improvement over the closest baseline on \\texttt{ogbn-arxiv}. Lastly, we believe the versatility of the proposed method extends beyond TAGs and holds the potential to enhance other tasks involving graph-text data~\\footnote{Our codes and datasets are available at: \\url{https://github.com/XiaoxinHe/TAPE}}.",
      "authors": [
        "Xiaoxin He",
        "Xavier Bresson",
        "Thomas Laurent",
        "Adam Perold",
        "Yann LeCun",
        "Bryan Hooi"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=RXFVcynVe1",
      "cdate": 1695530036004,
      "mdate": 1709779237698,
      "matched_keywords": [
        "large language model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708453"
    },
    {
      "id": "5ES5Hdlbxw",
      "title": "The Effective Horizon Explains Deep RL Performance in Stochastic Environments",
      "abstract": "Reinforcement learning (RL) theory has largely focused on proving minimax sample complexity bounds. These require strategic exploration algorithms that use relatively limited function classes for representing the policy or value function. Our goal is to explain why deep RL algorithms often perform well in practice, despite using random exploration and much more expressive function classes like neural networks. Our work arrives at an explanation by showing that many stochastic MDPs can be solved by performing only a few steps of value iteration on the random policy’s Q function and then acting greedily. When this is true, we find that it is possible to separate the exploration and learning components of RL, making it much easier to analyze. We introduce a new RL algorithm, SQIRL, that iteratively learns a near-optimal policy by exploring randomly to collect rollouts and then performing a limited number of steps of fitted-Q iteration over those roll- outs. We find that any regression algorithm that satisfies basic in-distribution generalization properties can be used in SQIRL to efficiently solve common MDPs. This can explain why deep RL works with complex function approximators like neural networks, since it is empirically established that neural networks generalize well in-distribution. Furthermore, SQIRL explains why random exploration works well in practice, since we show many environments can be solved by effectively estimating the random policy’s Q-function and then applying zero or a few steps of value iteration. We leverage SQIRL to derive instance-dependent sample complexity bounds for RL that are exponential only in an “effective horizon” of lookahead—which is typically much smaller than the full horizon—and on the complexity of the class used for function approximation. Empirically, we also find that SQIRL performance strongly correlates with PPO and DQN performance in a variety of stochastic environments, supporting that our theoretical analysis is predictive of practical performance. Our code and data are available at https://github.com/cassidylaidlaw/effective-horizon.",
      "authors": [
        "Cassidy Laidlaw",
        "Banghua Zhu",
        "Stuart Russell",
        "Anca Dragan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5ES5Hdlbxw",
      "cdate": 1695529830663,
      "mdate": 1713673025216,
      "matched_keywords": [
        "reinforcement learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708460"
    },
    {
      "id": "w4DW6qkRmt",
      "title": "SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs",
      "abstract": "Large language models (LLMs) have made significant advancements in various natural language processing tasks, including question answering (QA) tasks. While incorporating new information with the retrieval of relevant passages is a promising way to improve QA with LLMs, the existing methods often require additional fine-tuning which becomes infeasible with recent LLMs. Augmenting retrieved passages via prompting has the potential to address this limitation, but this direction has been limitedly explored. To this end, we design a simple yet effective framework to enhance open-domain QA (ODQA) with LLMs, based on the summarized retrieval (SuRe). SuRe helps LLMs predict more accurate answers for a given question, which are well-supported by the summarized retrieval that could be viewed as an explicit rationale extracted from the retrieved passages. Specifically, SuRe first constructs summaries of the retrieved passages for each of the multiple answer candidates. Then, SuRe confirms the most plausible answer from the candidate set by evaluating the validity and ranking of the generated summaries. Experimental results on diverse ODQA benchmarks demonstrate the superiority of SuRe, with improvements of up to 4.6\\% in exact match (EM) and 4.0\\% in F1 score over standard prompting approaches. SuRe also can be integrated with a broad range of retrieval methods and LLMs. Finally, the generated summaries from SuRe show additional advantages to measure the importance of retrieved passages and serve as more preferred rationales by models and humans.",
      "authors": [
        "Jaehyung Kim",
        "Jaehyun Nam",
        "Sangwoo Mo",
        "Jongjin Park",
        "Sang-Woo Lee",
        "Minjoon Seo",
        "Jung-Woo Ha",
        "Jinwoo Shin"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=w4DW6qkRmt",
      "cdate": 1695529572514,
      "mdate": 1713672084748,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708465"
    },
    {
      "id": "xw5nxFWMlo",
      "title": "Retrieval meets Long Context Large Language Models",
      "abstract": "Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.",
      "authors": [
        "Peng Xu",
        "Wei Ping",
        "Xianchao Wu",
        "Lawrence McAfee",
        "Chen Zhu",
        "Zihan Liu",
        "Sandeep Subramanian",
        "Evelina Bakhturina",
        "Mohammad Shoeybi",
        "Bryan Catanzaro"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xw5nxFWMlo",
      "cdate": 1695529436967,
      "mdate": 1709661551840,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708472"
    },
    {
      "id": "2DbVeuoa6a",
      "title": "Neural Spectral Methods: Self-supervised learning in the spectral domain",
      "abstract": "We present Neural Spectral Methods, a technique to solve parametric Partial Differential Equations (PDEs), grounded in classical spectral methods. Our method uses orthogonal bases to learn PDE solutions as mappings between spectral coefficients, instantiating a spectral-based neural operator. In contrast to current machine learning approaches which enforce PDE constraints by minimizing the numerical quadrature of the residuals in the spatiotemporal domain, we leverage Parseval's identity and introduce a new training strategy through a spectral loss. Our spectral loss enables more efficient differentiation through the neural network, and substantially reduces training complexity. At inference time, the computational cost of our method remains constant, regardless of the spatiotemporal resolution of the domain.  Our experimental results demonstrate that our method significantly outperforms previous machine learning approaches in terms of speed and accuracy by one to two orders of magnitude on multiple different problems, including reaction-diffusion, and forced and unforced Navier-Stokes equations. When compared to numerical solvers of the same accuracy, our method demonstrates a $10\\times$ increase in performance speed. Our source code is publicly available at https://github.com/ASK-Berkeley/Neural-Spectral-Methods.",
      "authors": [
        "Yiheng Du",
        "Nithin Chalapathi",
        "Aditi S. Krishnapriyan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=2DbVeuoa6a",
      "cdate": 1695529220502,
      "mdate": 1713673092053,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708477"
    },
    {
      "id": "he6mX9LTyE",
      "title": "Kosmos-G: Generating Images in Context with Multimodal Large Language Models",
      "abstract": "Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\"",
      "authors": [
        "Xichen Pan",
        "Li Dong",
        "Shaohan Huang",
        "Zhiliang Peng",
        "Wenhu Chen",
        "Furu Wei"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=he6mX9LTyE",
      "cdate": 1695528799406,
      "mdate": 1713672353698,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.708481"
    },
    {
      "id": "fwCoLe3TAX",
      "title": "Improving Generalization of Alignment with Human Preferences through Group Invariant Learning",
      "abstract": "The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. \nAs universal AI assistants, there's a growing expectation for them to perform consistently across various domains. \nHowever, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples.\nThis focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data.\nIn this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. \nGiven the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance.\nThen, we optimize the policy to perform well on challenging groups. \nLastly, leveraging the established groups, our approach adaptively adjusts the exploration space, allocating more learning capacity to more challenging data and preventing the model from over-optimizing on simpler data. Experimental results indicate that our approach significantly enhances training stability and model generalization.",
      "authors": [
        "Rui Zheng",
        "Wei Shen",
        "Yuan Hua",
        "Wenbin Lai",
        "Shihan Dou",
        "Yuhao Zhou",
        "Zhiheng Xi",
        "Xiao Wang",
        "Haoran Huang",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fwCoLe3TAX",
      "cdate": 1695528001020,
      "mdate": 1712984903949,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708486"
    },
    {
      "id": "cPgh4gWZlz",
      "title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources",
      "abstract": "We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. \nSpecifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. \nGiven a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains.\nIf there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains.\nThese corrected rationales can plausibly serve as a better foundation for the final answer consolidation.\nUnlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information.\nTo access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales.\nExtensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains.",
      "authors": [
        "Xingxuan Li",
        "Ruochen Zhao",
        "Yew Ken Chia",
        "Bosheng Ding",
        "Shafiq Joty",
        "Soujanya Poria",
        "Lidong Bing"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=cPgh4gWZlz",
      "cdate": 1695527756488,
      "mdate": 1709661551571,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708490"
    },
    {
      "id": "m3xVPaZp6Z",
      "title": "Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning",
      "abstract": "Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \\textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \\emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines.",
      "authors": [
        "Chengxing Jia",
        "Chenxiao Gao",
        "Hao Yin",
        "Fuxiang Zhang",
        "Xiong-Hui Chen",
        "Tian Xu",
        "Lei Yuan",
        "Zongzhang Zhang",
        "Zhi-Hua Zhou",
        "Yang Yu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=m3xVPaZp6Z",
      "cdate": 1695527592497,
      "mdate": 1713672267942,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708495"
    },
    {
      "id": "iS5ADHNg2A",
      "title": "Deceptive Fairness Attacks on Graphs via Meta Learning",
      "abstract": "We study deceptive fairness attacks on graphs to answer the following question: How can we achieve poisoning attacks on a graph learning model to exacerbate the bias deceptively? We answer this question via a bi-level optimization problem and propose a meta learning-based framework named FATE. FATE is broadly applicable with respect to various fairness definitions and graph learning models, as well as arbitrary choices of manipulation operations. We further instantiate FATE to attack statistical parity or individual fairness on graph neural networks. We conduct extensive experimental evaluations on real-world datasets in the task of semi-supervised node classification. The experimental results demonstrate that FATE could amplify the bias of graph neural networks with or without fairness consideration while maintaining the utility on the downstream task. We hope this paper provides insights into the adversarial robustness of fair graph learning and can shed light on designing robust and fair graph learning in future studies.",
      "authors": [
        "Jian Kang",
        "Yinglong Xia",
        "Ross Maciejewski",
        "Jiebo Luo",
        "Hanghang Tong"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iS5ADHNg2A",
      "cdate": 1695527305669,
      "mdate": 1710391204682,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708500"
    },
    {
      "id": "GzNaCp6Vcg",
      "title": "PINNACLE: PINN Adaptive ColLocation and Experimental points selection",
      "abstract": "Physics-Informed Neural Networks (PINNs), which incorporate PDEs as soft constraints, train with a composite loss function that contains multiple training point types: different types of collocation points chosen during training to enforce each PDE and initial/boundary conditions, and experimental points which are usually costly to obtain via experiments or simulations. Training PINNs using this loss function is challenging as it typically requires selecting large numbers of points of different types, each with different training dynamics. Unlike past works that focused on the selection of either collocation or experimental points, this work introduces PINN Adaptive ColLocation and Experimental points selection (PINNACLE), the first algorithm that jointly optimizes the selection of all training point types, while automatically adjusting the proportion of collocation point types as training progresses. PINNACLE uses information on the interactions among training point types, which had not been considered before, based on an analysis of PINN training dynamics via the Neural Tangent Kernel (NTK). We theoretically show that the criterion used by PINNACLE is related to the PINN generalization error, and empirically demonstrate that PINNACLE is able to outperform existing point selection methods for forward, inverse, and transfer learning problems.",
      "authors": [
        "Gregory Kang Ruey Lau",
        "Apivich Hemachandra",
        "See-Kiong Ng",
        "Bryan Kian Hsiang Low"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GzNaCp6Vcg",
      "cdate": 1695527036313,
      "mdate": 1710404052876,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708505"
    },
    {
      "id": "7Ttk3RzDeu",
      "title": "BooookScore: A systematic exploration of book-length summarization in the era of LLMs",
      "abstract": "Summarizing book-length documents ($>$100K tokens)  that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Because human evaluation is expensive and time-consuming, we develop an automatic metric, BooookScore, that measures the proportion of sentences in a summary that do not contain any of the identified error types. BooookScore has high agreement with human annotations and allows us to systematically evaluate the impact of many other critical parameters (e.g., chunk size, base LLM) while saving \\$15K USD and 500 hours in human evaluation costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce summaries with higher BooookScore than those generated by open-source models. While LLaMA 2 falls behind other models, Mixtral achieves performance on par with GPT-3.5-Turbo. Incremental updating yields lower BooookScore but higher level of detail than hierarchical merging, a trade-off sometimes preferred by annotators. We release code and annotations to spur more principled research on book-length summarization.",
      "authors": [
        "Yapei Chang",
        "Kyle Lo",
        "Tanya Goyal",
        "Mohit Iyyer"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7Ttk3RzDeu",
      "cdate": 1695526597280,
      "mdate": 1713045491214,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708510"
    },
    {
      "id": "qT7DXUmX7j",
      "title": "Extending Power of Nature from Binary to Real-Valued Graph Learning in Real World",
      "abstract": "Nature performs complex computations constantly at clearly lower cost and higher performance than digital computers. It is crucial to understand how to harness the unique computational power of nature in Machine Learning (ML). In the past decade, besides the development of Neural Networks (NNs), the community has also relentlessly explored nature-powered ML paradigms. Although most of them are still predominantly theoretical, a new practical paradigm enabled by the recent advent of CMOS-compatible room-temperature nature-based computers has emerged. By harnessing a dynamical system's intrinsic behavior of chasing the lowest energy state, this paradigm can solve some simple binary problems delivering considerable speedup and energy savings compared with NNs, while maintaining comparable accuracy. Regrettably, its values to the real world are highly constrained by its binary nature. A clear pathway to its extension to real-valued problems remains elusive. This paper aims to unleash this pathway by proposing a novel end-to-end Nature-Powered Graph Learning (NP-GL) framework. Specifically, through a three-dimensional co-design, NP-GL can leverage the spontaneous energy decrease in nature to efficiently solve real-valued graph learning problems. Experimental results across 4 real-world applications with 6 datasets demonstrate that NP-GL delivers, on average, $6.97\\times 10^3$ speedup and $10^5$ energy consumption reduction with comparable or even higher accuracy than Graph Neural Networks (GNNs).",
      "authors": [
        "Chunshu Wu",
        "Ruibing Song",
        "Chuan Liu",
        "Yunan Yang",
        "Ang Li",
        "Michael Huang",
        "Tong Geng"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qT7DXUmX7j",
      "cdate": 1695526505810,
      "mdate": 1713672192462,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708515"
    },
    {
      "id": "4kLVvIh8cp",
      "title": "Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning",
      "abstract": "Offline reinforcement learning (RL), where the agent aims to learn the optimal policy based on the data collected by a behavior policy, has attracted increasing attention in recent years. While offline RL with linear function approximation has been extensively studied with optimal results achieved under certain assumptions, many works shift their interest to offline RL with non-linear function approximation.\nHowever, limited works on offline RL with non-linear function approximation have instance-dependent regret guarantees.\n    In this paper, we propose an oracle-efficient algorithm, dubbed Pessimistic Nonlinear Least-Square Value Iteration (PNLSVI), for offline RL with non-linear function approximation. Our algorithmic design comprises three innovative components: (1) a variance-based weighted regression scheme that can be applied to a wide range of function classes, (2) a subroutine for variance estimation, and (3) a planning phase that utilizes a pessimistic value iteration approach. Our algorithm enjoys a regret bound that has a tight dependency on the function class complexity and achieves minimax optimal instance-dependent regret when specialized to linear function approximation. Our work extends the previous instance-dependent results within simpler function classes, such as linear and differentiable function to a more general framework. To the best of our knowledge, this is the first statistically optimal algorithm for nonlinear offline RL.",
      "authors": [
        "Qiwei Di",
        "Heyang Zhao",
        "Jiafan He",
        "Quanquan Gu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4kLVvIh8cp",
      "cdate": 1695525286533,
      "mdate": 1710562941386,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708522"
    },
    {
      "id": "yroyhkhWS6",
      "title": "A Quadratic Synchronization Rule for Distributed Deep Learning",
      "abstract": "In distributed deep learning with data parallelism, synchronizing gradients at each training step can cause a huge communication overhead, especially when many nodes work together to train large models.\n  Local gradient methods, such as Local SGD, address this issue by allowing workers to compute locally for $H$ steps without synchronizing with others, hence reducing communication frequency.\n  While $H$ has been viewed as a hyperparameter to trade optimization efficiency for communication cost, recent research indicates that setting a proper  $H$ value can lead to generalization improvement. Yet, selecting a proper $H$ is elusive. This work proposes a theory-grounded method for determining $H$, named the Quadratic Synchronization Rule (QSR), which recommends dynamically setting $H$ in proportion to $\\frac{1}{\\eta^2}$ as the learning rate $\\eta$ decays over time.\n  Extensive ImageNet experiments on ResNet and ViT show that local gradient methods with QSR consistently improve the test accuracy over other synchronization strategies. Compared to the standard data parallel training, QSR enables Local AdamW to cut the training time on 16 or 64 GPUs down from 26.7 to 20.2 hours or from 8.6 to 5.5 hours and, at the same time, achieves 1.16% or 0.84% higher top-1 validation accuracy.",
      "authors": [
        "Xinran Gu",
        "Kaifeng Lyu",
        "Sanjeev Arora",
        "Jingzhao Zhang",
        "Longbo Huang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=yroyhkhWS6",
      "cdate": 1695525170850,
      "mdate": 1710482917385,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708527"
    },
    {
      "id": "e2YOVTenU9",
      "title": "ArchLock: Locking DNN Transferability at the Architecture Level with a Zero-Cost Binary Predictor",
      "abstract": "Deep neural network (DNN) models, despite their impressive performance, are vulnerable to exploitation by attackers who attempt to transfer them to other tasks for their own benefit. Current defense strategies mainly address this vulnerability at the model parameter level, leaving the potential of architectural-level defense largely unexplored. This paper, for the first time, addresses the issue of model protection by reducing transferability at the architecture level. Specifically, we present a novel neural architecture search (NAS)-enabled algorithm that employs zero-cost proxies and evolutionary search, to explore model architectures with low transferability. Our method, namely ArchLock, aims to achieve high performance on the source task, while degrading the performance on potential target tasks, i.e., locking the transferability of a DNN model. To achieve efficient cross-task search without accurately knowing the training data owned by the attackers, we utilize zero-cost proxies to speed up architecture evaluation and simulate potential target task embeddings to assist cross-task search with a binary performance predictor. Extensive experiments on NAS-Bench-201 and TransNAS-Bench-101 demonstrate that ArchLock reduces transferability by up to 30% and 50%, respectively, with negligible performance degradation on source tasks (<2%). The code is available at https://github.com/Tongzhou0101/ArchLock.",
      "authors": [
        "Tong Zhou",
        "Shaolei Ren",
        "Xiaolin Xu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=e2YOVTenU9",
      "cdate": 1695525154769,
      "mdate": 1713217215478,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708532"
    },
    {
      "id": "ZDGKPbF0VQ",
      "title": "Leftover Lunch: Advantage-based Offline Reinforcement Learning for Language Models",
      "abstract": "Reinforcement Learning with Human Feedback (RLHF) is the most prominent method for Language Model (LM) alignment. However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LoL allows incorporating sequence-level classifiers or human-designed scoring functions as\nrewards. Subsequently, by using LM’s value estimate, A-LoL only trains on positive advantage (leftover) data points, making it resilient to noise. Overall, A-LoL is an easy-to-implement, sample-efficient, and stable LM training recipe.\n\nWe demonstrate the effectiveness of A-LoL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LoL methods achieve the highest diversity while also being rated more safe and helpful than the baselines according to humans. Additionally, in the remaining three tasks, A-LoL could optimize multiple distinct reward functions even when using noisy or suboptimal training data.",
      "authors": [
        "Ashutosh Baheti",
        "Ximing Lu",
        "Faeze Brahman",
        "Ronan Le Bras",
        "Maarten Sap",
        "Mark Riedl"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ZDGKPbF0VQ",
      "cdate": 1695523880919,
      "mdate": 1713672512001,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708537"
    },
    {
      "id": "62K7mALO2q",
      "title": "In-Context Learning Dynamics with Random Binary Sequences",
      "abstract": "Large language models (LLMs) trained on huge text datasets demonstrate intriguing capabilities, achieving state-of-the-art performance on tasks they were not explicitly trained for. The precise nature of LLM capabilities is often mysterious, and different prompts can elicit different capabilities through in-context learning. We propose a framework that enables us to analyze in-context learning dynamics to understand latent concepts underlying LLMs’ behavioral patterns. This provides a more nuanced understanding than success-or-failure evaluation benchmarks, but does not require observing internal activations as a mechanistic interpretation of circuits would. Inspired by the cognitive science of human randomness perception, we use random binary sequences as context and study dynamics of in-context learning by manipulating properties of context data, such as sequence length. In the latest GPT-3.5+ models, we find emergent abilities to generate seemingly random numbers and learn basic formal languages, with striking in-context learning dynamics where model outputs transition sharply from seemingly random behaviors to deterministic repetition.",
      "authors": [
        "Eric J Bigelow",
        "Ekdeep Singh Lubana",
        "Robert P. Dick",
        "Hidenori Tanaka",
        "Tomer Ullman"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=62K7mALO2q",
      "cdate": 1695523564712,
      "mdate": 1712774529974,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708541"
    },
    {
      "id": "22pyNMuIoa",
      "title": "PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization",
      "abstract": "Expert-level prompts, carefully engineered by human experts who have a deep understanding of both large language models (LLMs) and domain knowledge, are the future of prompting and pivotal to harnessing the full power of advanced LLMs. Discovering such prompts with an automated process remains a sought-after and unresolved challenge. Existing prompt optimization techniques, though automated through iterative sampling, often fall short in injecting domain knowledge and exploring the vast prompt space for complex expert-level prompts efficiently. To address this pressing need and achieve expert-level prompting, we introduce PromptAgent, which autonomously discovers prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm (rooted in Monte Carlo Tree Search) to strategically explore the vast expert-level prompt space. PromptAgent interacts with the LLM in a human-like trial-and-error manner during the planning, and injects expert-level knowledge by reflecting on model errors and generating insightful error feedback. This novel formulation allows it to iteratively evaluate intermediate prompts, refine them based on errors, simulate future rewards, and search for high-reward paths leading to expert-level prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), domain-expert, and general NLU tasks, showing PromptAgent consistently outperforms strong prompting and prompt optimization baselines by great margins. Our qualitative analysis further emphasizes PromptAgent's capability to distill insightful errors into expert-level prompts.",
      "authors": [
        "Xinyuan Wang",
        "Chenxi Li",
        "Zhen Wang",
        "Fan Bai",
        "Haotian Luo",
        "Jiayou Zhang",
        "Nebojsa Jojic",
        "Eric Xing",
        "Zhiting Hu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=22pyNMuIoa",
      "cdate": 1695523490224,
      "mdate": 1713368992051,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708546"
    },
    {
      "id": "JnRStoIuTe",
      "title": "Repeated Random Sampling for Minimizing the Time-to-Accuracy of Learning",
      "abstract": "Methods for carefully selecting or generating a small set of training data to learn from, i.e., data pruning, coreset selection, and dataset distillation, have been shown to be effective in reducing the ever-increasing cost of training neural networks. Behind this success are rigorously designed, yet expensive, strategies for identifying the most informative training examples out of large datasets. In this work, we revisit these methods to understand if the additional computational costs associated with such strategies are justified from the perspective of time-to-accuracy, which has become a critical efficiency measure of deep neural network training over large datasets. Surprisingly, we find that many of the recently proposed methods underperform what we call Repeated Sampling of Random Subsets (RSRS or RS2), a powerful yet overlooked extension of the standard random baseline that learns from repeatedly sampled data throughout training instead of a fixed random subset. We test RS2 against thirty-two state-of-the-art data pruning and distillation methods across four datasets including ImageNet. Our results demonstrate that RS2 significantly reduces time-to-accuracy, particularly in practical regimes where accuracy, but not runtime, is similar to that of training on full dataset. For example, when training ResNet-18 on ImageNet, with 10\\% of the dataset each epoch RS2 reaches an accuracy of 66\\% versus 69\\% when training with the full dataset. The best competing method achieves only 55\\% while training 1.6$\\times$ slower than RS2. Beyond the above meta-study, we discuss the theoretical properties of RS2 such as its convergence rate and generalization error. Our primary goal is to highlight that future works that aim to minimize total training cost by using subset selection, need to consider 1) the total computation cost (including preparing the subset) and 2) should aim to outperform a simple extension of random sampling (i.e., RS2).",
      "authors": [
        "Patrik Okanovic",
        "Roger Waleffe",
        "Vasilis Mageirakos",
        "Konstantinos Nikolakakis",
        "Amin Karbasi",
        "Dionysios Kalogerias",
        "Nezihe Merve Gürel",
        "Theodoros Rekatsinas"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=JnRStoIuTe",
      "cdate": 1695523401520,
      "mdate": 1713014620866,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708551"
    },
    {
      "id": "jTSKkcbEsj",
      "title": "Pushing Boundaries: Mixup's Influence on Neural Collapse",
      "abstract": "Mixup is a data augmentation strategy that employs convex combinations of training instances and their respective labels to improve the robustness and calibration of deep neural networks. Despite its widespread adoption, the nuanced mechanisms that underpin its success are not entirely understood. The observed phenomenon of Neural Collapse, where the last-layer activations and classifier of deep networks converge to a simplex equiangular tight frame (ETF), provides a compelling motivation to explore whether mixup induces alternative geometric configurations and whether those could explain its success. In this study, we delve into the last-layer activations of training data for deep networks subjected to mixup, aiming to uncover insights into its operational efficacy. Our investigation, spanning various architectures and dataset pairs, reveals that mixup's last-layer activations predominantly converge to a distinctive configuration different than one might expect. In this configuration, activations from mixed-up examples of identical classes align with the classifier, while those from different classes delineate channels along the decision boundary. These findings are unexpected, as mixed-up features are not simple convex combinations of feature class means (as one might get, for example, by training mixup with the mean squared error loss). By analyzing this distinctive geometric configuration, we elucidate the mechanisms by which mixup enhances model calibration. To further validate our empirical observations, we conduct a theoretical analysis under the assumption of an unconstrained features model, utilizing the mixup loss. Through this, we characterize and derive the optimal last-layer features under the assumption that the classifier forms a simplex ETF.",
      "authors": [
        "Quinn LeBlanc Fisher",
        "Haoming Meng",
        "Vardan Papyan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jTSKkcbEsj",
      "cdate": 1695523101246,
      "mdate": 1710539623719,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708556"
    },
    {
      "id": "2XBBumBGeP",
      "title": "sRGB Real Noise Modeling via Noise-Aware Sampling with Normalizing Flows",
      "abstract": "Noise poses a widespread challenge in signal processing, particularly when it comes to denoising images. Although convolutional neural networks (CNNs) have exhibited remarkable success in this field, they are predicated upon the belief that noise follows established distributions, which restricts their practicality when dealing with real-world noise. To overcome this limitation, several efforts have been taken to collect noisy image datasets from the real world. Generative methods, employing techniques such as generative adversarial networks (GANs) and normalizing flows (NFs), have emerged as a solution for generating realistic noisy images. Recent works model noise using camera metadata, however requiring metadata even for sampling phase. In contrast, in this work, we aim to estimate the underlying camera settings, enabling us to improve noise modeling and generate diverse noise distributions. To this end, we introduce a new NF framework that allows us to both classify noise based on camera settings and generate various noisy images. Through experimental results, our model demonstrates exceptional noise quality and leads in denoising performance on benchmark datasets.",
      "authors": [
        "Dongjin Kim",
        "Donggoo Jung",
        "Sungyong Baik",
        "Tae Hyun Kim"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=2XBBumBGeP",
      "cdate": 1695523083977,
      "mdate": 1711503426000,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708561"
    },
    {
      "id": "8dN7gApKm3",
      "title": "Uncertainty-aware Graph-based Hyperspectral Image Classification",
      "abstract": "Hyperspectral imaging (HSI) technology captures spectral information across a broad wavelength range, providing richer pixel features compared to traditional color images with only three channels. Although pixel classification in HSI  has been extensively studied, especially using graph convolution neural networks (GCNs), quantifying epistemic and aleatoric uncertainties associated with the HSI classification (HSIC) results remains an unexplored area. These two uncertainties are effective for out-of-distribution (OOD) and misclassification detection, respectively. In this paper, we adapt two advanced uncertainty quantification models, evidential GCNs (EGCN) and graph posterior networks (GPN), designed for node classifications in graphs, into the realm of HSIC. We first reveal theoretically that a popular uncertainty cross-entropy (UCE) loss function is insufficient to produce good epistemic uncertainty when learning EGCNs. To mitigate the limitations, we propose two regularization terms. One leverages the inherent property of HSI data where each feature vector is a linear combination of the spectra signatures of the confounding materials, while the other is the total variation (TV) regularization to enforce the spatial smoothness of the evidence with edge-preserving. We demonstrate the effectiveness of the proposed regularization terms on both EGCN and GPN on three real-world HSIC datasets for OOD and misclassification detection tasks. The code is available at GitHub.",
      "authors": [
        "Linlin Yu",
        "Yifei Lou",
        "Feng Chen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=8dN7gApKm3",
      "cdate": 1695522748814,
      "mdate": 1710542189639,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708565"
    },
    {
      "id": "TlyiaPXaVN",
      "title": "Generative Adversarial Equilibrium Solvers",
      "abstract": "We introduce the use of generative adversarial learning to compute equilibria in general game-theoretic settings, specifically the generalized Nash equilibrium (GNE) in pseudo-games, and its specific instantiation as the competitive equilibrium (CE) in Arrow-Debreu competitive economies. Pseudo-games are a generalization of games in which players' actions affect not only the payoffs of other players but also their feasible action spaces. Although the computation of GNE and CE is intractable in the worst-case, i.e., PPAD-hard, in practice, many applications only require solutions with high accuracy in expectation over a distribution of problem instances. We introduce Generative Adversarial Equilibrium Solvers (GAES): a family of generative adversarial neural networks that can learn GNE and CE from only a sample of problem instances. We provide computational and sample complexity bounds for Lipschitz-smooth function approximators in a large class of concave pseudo-games, and apply the framework to finding Nash equilibria in normal-form games, CE in Arrow-Debreu competitive economies, and GNE in an environmental economic model of the Kyoto mechanism.",
      "authors": [
        "Denizalp Goktas",
        "David C. Parkes",
        "Ian Gemp",
        "Luke Marris",
        "Georgios Piliouras",
        "Romuald Elie",
        "Guy Lever",
        "Andrea Tacchetti"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=TlyiaPXaVN",
      "cdate": 1695522706732,
      "mdate": 1713228035507,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708572"
    },
    {
      "id": "pe0Vdv7rsL",
      "title": "Graph Transformers on EHRs: Better Representation Improves Downstream Performance",
      "abstract": "Following the success of transformer-based methods across various machine learning applications, their adoption for healthcare predictive tasks using electronic health records (EHRs)  has also expanded extensively. Similarly, graph-based methods have been shown to be very effective in capturing inherent graph-type relationships in EHRs, leading to improved downstream performance. Although integrating these two families of approaches seems like a natural next step, in practice, creating such a design is challenging and has not been done. This is partly due to known EHR problems, such as high sparsity, making extracting meaningful temporal representations of medical visits challenging. In this study, we propose GT-BEHRT, a new approach that leverages temporal visit embeddings extracted from a graph transformer and uses a BERT-based model to obtain more robust patient representations, especially on longer EHR sequences. The graph-based approach allows GT-BEHRT to implicitly capture the intrinsic graphical relationships between medical observations, while the BERT model extracts the temporal relationships between visits, loosely mimicking the clinicians' decision-making process. As part of our method, we also present a two-step pre-training strategy for learning better graphical and temporal representations. Our proposed method achieves state-of-the-art performance in a variety of standard medical predictive tasks, demonstrating the versatility of our approach.",
      "authors": [
        "Raphael Poulain",
        "Rahmatollah Beheshti"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pe0Vdv7rsL",
      "cdate": 1695521971169,
      "mdate": 1712855245329,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708577"
    },
    {
      "id": "dwzLn78jq7",
      "title": "On the Scalability and Memory Efficiency of Semidefinite Programs  for Lipschitz Constant Estimation of Neural Networks",
      "abstract": "Lipschitz constant estimation plays an important role in understanding generalization, robustness, and fairness in deep learning. Unlike naive bounds based on the network weight norm product, semidefinite programs (SDPs) have shown great promise in providing less conservative Lipschitz bounds with polynomial-time complexity guarantees. However, due to the memory consumption and running speed, standard SDP algorithms cannot scale to modern neural network architectures. In this paper, we transform the SDPs for Lipschitz constant estimation into an eigenvalue optimization problem, which aligns with the modern large-scale optimization paradigms based on first-order methods. This is amenable to autodiff frameworks such as PyTorch and TensorFlow, requiring significantly less memory than standard SDP algorithms. The transformation also allows us to leverage various existing numerical techniques for eigenvalue optimization, opening the way for further memory improvement and computational speedup. The essential technique of our eigenvalue-problem transformation is to introduce redundant quadratic constraints and then utilize both Lagrangian and Shor's SDP relaxations under a certain trace constraint.  Notably, our numerical study successfully scales the SDP-based Lipschitz constant estimation to address large neural networks on ImageNet. Our numerical examples on CIFAR10 and ImageNet demonstrate that our technique is more scalable than existing approaches. Our code is available at https://github.com/z1w/LipDiff.",
      "authors": [
        "Zi Wang",
        "Bin Hu",
        "Aaron J Havens",
        "Alexandre Araujo",
        "Yang Zheng",
        "Yudong Chen",
        "Somesh Jha"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=dwzLn78jq7",
      "cdate": 1695521949303,
      "mdate": 1713672430031,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708582"
    },
    {
      "id": "kZEXgtMNNo",
      "title": "Large Language Models as Automated Aligners for  benchmarking  Vision-Language Models",
      "abstract": "With the advancements in Large Language Models (LLMs), Vision-Language Models (VLMs) have reached a new level of sophistication, showing notable competence in executing intricate cognition and reasoning tasks. However, existing evaluation benchmarks, primarily relying on rigid, hand-crafted datasets to measure task-specific performance, face significant limitations in assessing the alignment of these increasingly anthropomorphic models with human intelligence. In this work, we address the limitations via Auto-Bench, which delves into exploring LLMs as proficient aligners, measuring the alignment between VLMs and human intelligence and value through automatic data curation and assessment. Specifically, for data curation, Auto-Bench utilizes LLMs (e.g., GPT-4) to automatically generate a vast set of question-answer-reasoning triplets via prompting on visual symbolic representations (e.g., captions, object locations, instance relationships, and etc. The curated data closely matches human intent, owing to the extensive world knowledge embedded in LLMs. Through this pipeline, a total of 28.5K human-verified and 3,504K unfiltered question-answer-reasoning triplets have been curated, covering 4 primary abilities and 16 sub-abilities. We subsequently engage LLMs like GPT-3.5 to serve as judges, implementing the quantitative and qualitative automated assessments to facilitate a comprehensive evaluation of VLMs. Our validation results reveal that LLMs are proficient in both evaluation data curation and model assessment, achieving an average agreement rate of 85%. We envision Auto-Bench as a flexible, scalable, and comprehensive benchmark for evaluating the evolving sophisticated VLMs.",
      "authors": [
        "Yuanfeng Ji",
        "Chongjian GE",
        "Weikai Kong",
        "Enze Xie",
        "Zhengying Liu",
        "Zhenguo Li",
        "Ping Luo"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kZEXgtMNNo",
      "cdate": 1695521713473,
      "mdate": 1710826520088,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708587"
    },
    {
      "id": "ofzeypWosV",
      "title": "CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech",
      "abstract": "With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances.",
      "authors": [
        "Jaehyeon Kim",
        "Keon Lee",
        "Seungjun Chung",
        "Jaewoong Cho"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ofzeypWosV",
      "cdate": 1695521636069,
      "mdate": 1712155604196,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708594"
    },
    {
      "id": "4VgBjsOC8k",
      "title": "Unveiling the Unseen: Identifiable Clusters in Trained Depthwise Convolutional Kernels",
      "abstract": "Recent advances in depthwise-separable convolutional neural networks (DS-CNNs) have led to novel architectures, that surpass the performance of classical CNNs, by a considerable scalability and accuracy margin. This paper reveals another striking property of DS-CNN architectures: discernible and explainable patterns emerge in their trained depthwise convolutional kernels in all layers. Through an extensive analysis of millions of trained filters, with different sizes and from various models, we employed unsupervised clustering with autoencoders, to categorize these filters. Astonishingly, the patterns converged into a few main clusters, each resembling the difference of Gaussian (DoG) functions, and their first and second-order derivatives. Notably, we classify over 95\\% and 90\\% of the filters from state-of-the-art ConvNeXtV2 and ConvNeXt models, respectively. This finding is not merely a technological curiosity; it echoes the foundational models neuroscientists have long proposed for the vision systems of mammals. Our results thus deepen our understanding of the emergent properties of trained DS-CNNs and provide a bridge between artificial and biological visual processing systems. More broadly, they pave the way for more interpretable and biologically-inspired neural network designs in the future.",
      "authors": [
        "Zahra Babaiee",
        "Peyman Kiasari",
        "Daniela Rus",
        "Radu Grosu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4VgBjsOC8k",
      "cdate": 1695521493968,
      "mdate": 1712072990887,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708598"
    },
    {
      "id": "0j9ZDzMPqr",
      "title": "UNR-Explainer: Counterfactual Explanations for Unsupervised Node Representation Learning Models",
      "abstract": "Node representation learning, such as Graph Neural Networks (GNNs), has become one of the important learning methods in machine learning, and the demand for reliable explanation generation is growing. Despite extensive research on explanation generation for supervised node representation learning, explaining unsupervised models has been less explored. To address this gap, we propose a method for generating counterfactual (CF) explanations in unsupervised node representation learning, aiming to identify the most important subgraphs that cause a significant change in the $k$-nearest neighbors of a node of interest in the learned embedding space upon perturbation. The $k$-nearest neighbor-based CF explanation method provides simple, yet pivotal, information for understanding unsupervised downstream tasks, such as top-$k$ link prediction and clustering. Furthermore, we introduce a Monte Carlo Tree Search (MCTS)-based explainability method for generating expressive CF explanations for **U**nsupervised **N**ode **R**epresentation learning methods, which we call **UNR-Explainer**. The proposed method demonstrates improved performance on six datasets for both unsupervised GraphSAGE and DGI.",
      "authors": [
        "Hyunju Kang",
        "Geonhee Han",
        "Hogun Park"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0j9ZDzMPqr",
      "cdate": 1695521428100,
      "mdate": 1713437018909,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708603"
    },
    {
      "id": "x8VNtpCu1I",
      "title": "Are Bert Family Good Instruction Followers?  A Study on Their Potential And Limitations",
      "abstract": "Language modeling at scale has proven very effective and brought unprecedented success to natural language models. Many typical representatives, especially decoder-only models, e.g., BLOOM and LLaMA, and encoder-decoder models, e.g., Flan-T5 and AlexaTM, have exhibited incredible instruction-following capabilities while keeping strong task completion ability. These large language models can achieve superior performance in various tasks and even yield emergent capabilities, e.g., reasoning and universal generalization. Though the above two paradigms are mainstream and well explored, the potential of the BERT family, which are encoder-only based models and have ever been one of the most representative pre-trained models, also deserves attention, at least should be discussed. In this work, we adopt XML-R to explore the effectiveness of the BERT family for instruction following and zero-shot learning. We first design a simple yet effective strategy to utilize the encoder-only models for generation tasks and then conduct multi-task instruction tuning.  Experimental results demonstrate that our fine-tuned model, Instruct-XMLR, outperforms Bloomz on all evaluation tasks and achieves comparable performance with mT0 on most tasks. Surprisingly, Instruct-XMLR also possesses strong task and language generalization abilities, indicating that Instruct-XMLR can also serve as a good instruction follower and zero-shot learner. Besides, Instruct-XMLR can accelerate decoding due to its non-autoregressive generation manner, achieving around 3 times speedup compared with current autoregressive large language models. Although we also witnessed several limitations through our experiments, such as the performance decline in long-generation tasks and the shortcoming of length prediction, Instruct-XMLR can still become a good member of the family of current large language models.",
      "authors": [
        "yisheng xiao",
        "Juntao Li",
        "Zechen Sun",
        "Zechang Li",
        "Qingrong Xia",
        "Xinyu Duan",
        "Zhefeng Wang",
        "Min Zhang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=x8VNtpCu1I",
      "cdate": 1695521064585,
      "mdate": 1713672065361,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708607"
    },
    {
      "id": "V2cBKtdC3a",
      "title": "Exploring the Promise and Limits of Real-Time Recurrent Learning",
      "abstract": "Real-time recurrent learning (RTRL) for sequence-processing recurrent neural networks (RNNs) offers certain conceptual advantages over backpropagation through time (BPTT). RTRL requires neither caching past activations nor truncating context, and enables online learning. However, RTRL's time and space complexity make it impractical. To overcome this problem, most recent work on RTRL focuses on approximation theories, while experiments are often limited to diagnostic settings. Here we explore the practical promise of RTRL in more realistic settings. We study actor-critic methods that combine RTRL and policy gradients, and test them in several subsets of DMLab-30, ProcGen, and Atari-2600 environments. On DMLab memory tasks, our system trained on fewer than 1.2B environmental frames is competitive with or outperforms well-known IMPALA and R2D2 baselines trained on 10B frames. To scale to such challenging tasks, we focus on certain well-known neural architectures with element-wise recurrence, allowing for tractable RTRL without approximation. Importantly, we also discuss rarely addressed limitations of RTRL in real-world applications, such as its complexity in the multi-layer case.",
      "authors": [
        "Kazuki Irie",
        "Anand Gopalakrishnan",
        "Jürgen Schmidhuber"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=V2cBKtdC3a",
      "cdate": 1695520956457,
      "mdate": 1709661550206,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708612"
    },
    {
      "id": "YH5w12OUuU",
      "title": "TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting",
      "abstract": "The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO's potential to constitute a foundational model-building framework.",
      "authors": [
        "Defu Cao",
        "Furong Jia",
        "Sercan O Arik",
        "Tomas Pfister",
        "Yixiang Zheng",
        "Wen Ye",
        "Yan Liu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=YH5w12OUuU",
      "cdate": 1695520825744,
      "mdate": 1712162817704,
      "matched_keywords": [
        "transformer",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708617"
    },
    {
      "id": "u3dX2CEIZb",
      "title": "Scaling physics-informed hard constraints with mixture-of-experts",
      "abstract": "Imposing known physical constraints, such as conservation laws, during neural network training introduces an inductive bias that can improve accuracy, reliability, convergence, and data efficiency for modeling physical dynamics. While such constraints can be softly imposed via loss function penalties, recent advancements in differentiable physics and optimization improve performance by incorporating PDE-constrained optimization as individual layers in neural networks. This enables a stricter adherence to physical constraints. However, imposing hard constraints significantly increases computational and memory costs, especially for complex dynamical systems. This is because it requires solving an optimization problem over a large number of points in a mesh, representing spatial and temporal discretizations, which greatly increases the complexity of the constraint. To address this challenge, we develop a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE), which can be used with any neural network architecture. Our approach imposes the constraint over smaller decomposed domains, each of which is solved by an ``expert'' through differentiable optimization. During training, each expert independently performs a localized backpropagation step by leveraging the implicit function theorem; the independence of each expert allows for parallelization across multiple GPUs. Compared to standard differentiable optimization, our scalable approach achieves greater accuracy in the neural PDE solver setting for predicting the dynamics of challenging non-linear systems. We also improve training stability and require significantly less computation time during both training and inference stages.",
      "authors": [
        "Nithin Chalapathi",
        "Yiheng Du",
        "Aditi S. Krishnapriyan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=u3dX2CEIZb",
      "cdate": 1695520790211,
      "mdate": 1709661550077,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708625"
    },
    {
      "id": "ANvmVS2Yr0",
      "title": "Generalization in diffusion models arises from geometry-adaptive harmonic representations",
      "abstract": "Deep neural networks (DNNs) trained for image denoising are able to generate high-quality samples with score-based reverse diffusion algorithms. These impressive capabilities seem to imply an escape from the curse of dimensionality, but recent reports of memorization of the training set raise the question of whether these networks are learning the \"true\" continuous density of the data. Here, we show that two DNNs trained on non-overlapping subsets of a dataset learn nearly the same score function, and thus the same density, when the number of training images is large enough.  In this regime of strong generalization, diffusion-generated images are distinct from the training set, and are of high visual quality, suggesting that the inductive biases of the DNNs are well-aligned with the data density. We analyze the learned denoising functions and show that the inductive biases give rise to a shrinkage operation in a basis adapted to the underlying image. Examination of these bases reveals oscillating harmonic structures along contours and in homogeneous regions. We demonstrate that trained denoisers are inductively biased towards these geometry-adaptive harmonic bases since they arise not only when the network is trained on photographic images, but also when it is trained on image classes supported on low-dimensional manifolds for which the harmonic basis is suboptimal. Finally, we show that when trained on regular image classes for which the optimal basis is known to be geometry-adaptive and harmonic, the denoising performance of the networks is near-optimal.",
      "authors": [
        "Zahra Kadkhodaie",
        "Florentin Guth",
        "Eero P Simoncelli",
        "Stéphane Mallat"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ANvmVS2Yr0",
      "cdate": 1695520324208,
      "mdate": 1713672934776,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708629"
    },
    {
      "id": "bvjcMvMn7B",
      "title": "Structural Fairness-aware Active Learning for Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have seen significant achievements in semi-supervised node classification. Yet, their efficacy often hinges on access to high-quality labeled node samples, which may not always be available in real-world scenarios. While active learning is commonly employed across various domains to pinpoint and label high-quality samples based on data features, graph data present unique challenges due to their intrinsic structures that render nodes non-i.i.d. Furthermore, biases emerge from the positioning of labeled nodes; for instance, nodes closer to the labeled counterparts often yield better performance. To better leverage graph structure and mitigate structural bias in active learning, we present a unified optimization framework (SCARCE), which is also easily incorporated with node features.  Extensive experiments demonstrate that the proposed method not only improves the GNNs performance but also paves the way for more fair results.",
      "authors": [
        "Haoyu Han",
        "Xiaorui Liu",
        "Li Ma",
        "MohamadAli Torkamani",
        "Hui Liu",
        "Jiliang Tang",
        "Makoto Yamada"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=bvjcMvMn7B",
      "cdate": 1695520268147,
      "mdate": 1712850669121,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708634"
    },
    {
      "id": "3EWTEy9MTM",
      "title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems",
      "abstract": "Generating a sequence of intermediate steps, \\emph{a.k.a.}, a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. \nThis work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have constant-depth transformers with finite precision $\\mathsf{poly}(n)$ embedding size can only solve problems in $\\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\\mathsf{AC}^0$, a proper subset of $ \\mathsf{TC}^0$. However, with $T$ steps of CoT, constant-depth transformers using constant-bit precision and $O(\\log n)$ embedding size can solve any problem solvable by boolean circuits of size $T$. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers.",
      "authors": [
        "Zhiyuan Li",
        "Hong Liu",
        "Denny Zhou",
        "Tengyu Ma"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3EWTEy9MTM",
      "cdate": 1695519700217,
      "mdate": 1712197427018,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708639"
    },
    {
      "id": "kIPEyMSdFV",
      "title": "Reverse Diffusion Monte Carlo",
      "abstract": "We propose a Monte Carlo sampler from the reverse diffusion process. Unlike the practice of diffusion models, where the intermediary updates---the score functions---are learned with a neural network, we transform the score matching problem into a mean estimation one.\nBy estimating the means of the regularized posterior distributions, we derive a novel Monte Carlo sampling algorithm called reverse diffusion Monte Carlo (rdMC), which is distinct from the Markov chain Monte Carlo (MCMC) methods. We determine the sample size from the error tolerance and the properties of the posterior distribution to yield an algorithm that can approximately sample the target distribution with any desired accuracy. Additionally, we demonstrate and prove under suitable conditions that sampling with rdMC can be significantly faster than that with MCMC.  For multi-modal target distributions such as those in Gaussian mixture models, rdMC greatly improves over the Langevin-style MCMC sampling methods both theoretically and in practice. The proposed rdMC method offers a new perspective and solution beyond classical MCMC algorithms for the challenging complex distributions.",
      "authors": [
        "Xunpeng Huang",
        "Hanze Dong",
        "Yifan HAO",
        "Yian Ma",
        "Tong Zhang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kIPEyMSdFV",
      "cdate": 1695519252515,
      "mdate": 1709661549792,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708643"
    },
    {
      "id": "qaJxPhkYtD",
      "title": "Counting Graph Substructures with Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) are powerful representation learning tools that have achieved remarkable performance in various downstream tasks. However, there are still open questions regarding their ability to count and list substructures, which play a crucial role in biological and social networks. In this work, we fill this gap and characterize the representation {and generalization} power of GNNs in terms of their ability to produce powerful representations that count substructures. In particular, we study the message-passing operations of GNNs with random node input in a novel fashion, and show how they can produce equivariant representations that are associated with high-order statistical moments. Using these representations, we prove that GNNs can learn how to count cycles, {cliques}, quasi-cliques, and the number of connected components in a graph. We also provide new insights into the generalization capacity of GNNs. Our analysis is constructive and enables the design of a generic GNN architecture that shows remarkable performance in four distinct tasks: cycle detection, cycle counting, graph classification, and molecular property prediction.",
      "authors": [
        "Charilaos Kanatsoulis",
        "Alejandro Ribeiro"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qaJxPhkYtD",
      "cdate": 1695518830353,
      "mdate": 1713413474695,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708648"
    },
    {
      "id": "w1JanwReU6",
      "title": "Are Models Biased on Text without Gender-related Language?",
      "abstract": "Gender bias research has been pivotal in revealing undesirable behaviors in large language models, exposing serious gender stereotypes associated with occupations, and emotions. A key observation in prior work is that models reinforce stereotypes as a consequence of the gendered correlations that are present in the training data. In this paper, we focus on bias where the effect from training data is unclear, and instead address the question: *Do language models still exhibit gender bias in non-stereotypical settings?* To do so, we introduce **UnStereoEval (USE)**, a novel framework tailored for investigating gender bias in stereotype-free scenarios. USE defines a sentence-level score based on pretraining data statistics to determine if the sentence contain minimal word-gender associations. To systematically benchmark the fairness of popular language models in stereotype-free scenarios, we utilize USE to automatically generate benchmarks without any gender-related language.  By leveraging USE's sentence-level score, we also repurpose prior gender bias benchmarks (Winobias and Winogender) for non-stereotypical evaluation. Surprisingly, we find low fairness across all 28 tested models.  Concretely, models demonstrate fair behavior in only 9%-41% of  stereotype-free sentences, suggesting that bias does not solely stem from the presence of gender-related words. These results raise important questions about where underlying model biases come from and highlight the need for more systematic and comprehensive bias evaluation. We release the full dataset and code at [ucinlp.github.io/unstereo-eval](https://ucinlp.github.io/unstereo-eval).",
      "authors": [
        "Catarina G Belém",
        "Preethi Seshadri",
        "Yasaman Razeghi",
        "Sameer Singh"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=w1JanwReU6",
      "cdate": 1695517935031,
      "mdate": 1713224529450,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708652"
    },
    {
      "id": "dFcXJgnrGB",
      "title": "PlaSma: Procedural Knowledge Models for Language-based Planning and Re-Planning",
      "abstract": "Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex and often contextualized situations, e.g. ``scheduling a doctor's appointment without a phone''. While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (constrained) language-based planning capabilities. More concretely, we develop *symbolic procedural knowledge distillation* to enhance the commonsense knowledge in small language models and an *inference-time algorithm* to facilitate more structured and accurate reasoning. In addition, we introduce a new related task, *Replanning*, that requires a revision of a plan to cope with a constrained situation. In both the planning and replanning settings, we show that orders-of-magnitude smaller models (770M-11B parameters) can compete and often surpass their larger teacher models' capabilities. Finally, we showcase successful application of PlaSma in an embodied environment, VirtualHome.",
      "authors": [
        "Faeze Brahman",
        "Chandra Bhagavatula",
        "Valentina Pyatkin",
        "Jena D. Hwang",
        "Xiang Lorraine Li",
        "Hirona Jacqueline Arai",
        "Soumya Sanyal",
        "Keisuke Sakaguchi",
        "Xiang Ren",
        "Yejin Choi"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=dFcXJgnrGB",
      "cdate": 1695517527541,
      "mdate": 1710551144641,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708657"
    },
    {
      "id": "PfPnugdxup",
      "title": "From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction",
      "abstract": "Foundation models have been transformational in machine learning fields such as natural language processing and computer vision. Similar success in atomic property prediction has been limited due to the challenges of training effective models across multiple chemical domains. To address this, we introduce Joint Multi-domain Pre-training (JMP), a supervised pre-training strategy that simultaneously trains on multiple datasets from different chemical domains, treating each dataset as a unique pre-training task within a multi-task framework. Our combined training dataset consists of $\\sim$120M systems from OC20, OC22, ANI-1x, and Transition-1x. We evaluate performance and generalization by fine-tuning over a diverse set of downstream tasks and datasets including: QM9, rMD17, MatBench, QMOF, SPICE, and MD22. JMP demonstrates an average improvement of 59% over training from scratch and matches or sets state-of-the-art on 34 out of 40 tasks. Our work highlights the potential of pre-training strategies that utilize diverse data to advance property prediction across chemical domains, especially for low-data tasks.",
      "authors": [
        "Nima Shoghi",
        "Adeesh Kolluru",
        "John R. Kitchin",
        "Zachary Ward Ulissi",
        "C. Lawrence Zitnick",
        "Brandon M Wood"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=PfPnugdxup",
      "cdate": 1695517516805,
      "mdate": 1713135263243,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708662"
    },
    {
      "id": "M6XWoEdmwf",
      "title": "AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents",
      "abstract": "We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses sequence models to tackle the challenges of generalization, long-term memory, and meta-learning. Recent works have shown that off-policy learning can make in-context RL with recurrent policies viable. Nonetheless, these approaches require extensive tuning and limit scalability by creating key bottlenecks in agents' memory capacity, planning horizon, and model size. AMAGO revisits and redesigns the off-policy in-context approach to successfully train long-sequence Transformers over entire rollouts in parallel with end-to-end RL. Our agent is scalable and applicable to a wide range of problems, and we demonstrate its strong performance empirically in meta-RL and long-term memory domains. AMAGO's focus on sparse rewards and off-policy data also allows in-context learning to extend to goal-conditioned problems with challenging exploration. When combined with a multi-goal hindsight relabeling scheme, AMAGO can solve a previously difficult category of open-world domains, where agents complete many possible instructions in procedurally generated environments.",
      "authors": [
        "Jake Grigsby",
        "Linxi Fan",
        "Yuke Zhu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=M6XWoEdmwf",
      "cdate": 1695517213807,
      "mdate": 1709661549591,
      "matched_keywords": [
        "reinforcement learning",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708666"
    },
    {
      "id": "Zc2aIcucwc",
      "title": "Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets",
      "abstract": "Recently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models. In this work, we present seven novel datasets categorized by size into three distinct categories: ToyMix, LargeMix and UltraLarge. These datasets push the boundaries in both the scale and the diversity of supervised labels for molecular learning. They cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, our datasets contain 300 times more data points than the widely used OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In addition, to support the development of foundational models based on our proposed datasets, we present the Graphium graph machine learning library which simplifies the process of building and training molecular machine learning models for multi-task and multi-level molecular datasets. Finally, we present a range of baseline results as a starting point of multi-task and multi-level training on these datasets. Empirically, we observe that performance on low-resource biological datasets show improvement by also training on large amounts of quantum data. This indicates that there may be potential in multi-task and multi-level training of a foundation model and fine-tuning it to resource-constrained downstream tasks. The Graphium library is publicly available on Github and the dataset links are available in Part 1 and Part 2.",
      "authors": [
        "Dominique Beaini",
        "Shenyang Huang",
        "Joao Alex Cunha",
        "Zhiyi Li",
        "Gabriela Moisescu-Pareja",
        "Oleksandr Dymov",
        "Samuel Maddrell-Mander",
        "Callum McLean",
        "Frederik Wenkel",
        "Luis Müller",
        "Jama Hussein Mohamud",
        "Ali Parviz",
        "Michael Craig",
        "Michał Koziarski",
        "Jiarui Lu",
        "Zhaocheng Zhu",
        "Cristian Gabellini",
        "Kerstin Klaser",
        "Josef Dean",
        "Cas Wognum",
        "Maciej Sypetkowski",
        "Guillaume Rabusseau",
        "Reihaneh Rabbany",
        "Jian Tang",
        "Christopher Morris",
        "Mirco Ravanelli",
        "Guy Wolf",
        "Prudencio Tossou",
        "Hadrien Mary",
        "Therence Bois",
        "Andrew W Fitzgibbon",
        "Blazej Banaszewski",
        "Chad Martin",
        "Dominic Masters"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Zc2aIcucwc",
      "cdate": 1695517008517,
      "mdate": 1719496717883,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708673"
    },
    {
      "id": "gPKTTAfYBp",
      "title": "FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores",
      "abstract": "Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time.\nA major bottleneck is the Fast Fourier Transform (FFT)---which allows long convolutions to run in $O(N\\log N)$ time in sequence length $N$ but has poor hardware utilization.\nIn this paper, we study how to optimize the FFT convolution.\nWe find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy.\nIn response, we propose FlashFFTConv.\nFlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O.\nWe also present two sparse convolution algorithms---1) partial convolutions and 2) frequency-sparse convolutions---which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings.\nFlashFFTConv speeds up exact FFT convolutions by up to 8.7$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end.\nGiven the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity and M2-BERT-base to achieve 3.3 points higher GLUE score---matching models with twice the parameter count.\nFlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%.\nFurthermore, partial convolutions enable longer-sequence models---yielding the first DNA model that can process the longest human genes (2.3M base pairs)---and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.",
      "authors": [
        "Daniel Y Fu",
        "Hermann Kumbong",
        "Eric Nguyen",
        "Christopher Re"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=gPKTTAfYBp",
      "cdate": 1695516529598,
      "mdate": 1710643622792,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708678"
    },
    {
      "id": "oDdzXQzP2F",
      "title": "Transformer-VQ: Linear-Time Transformers via Vector Quantization",
      "abstract": "We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time.  Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. \nIn our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: \\url{https://github.com/transformer-vq/transformer_vq}",
      "authors": [
        "Lucas Dax Lingle"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=oDdzXQzP2F",
      "cdate": 1695516047186,
      "mdate": 1709661549403,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708683"
    },
    {
      "id": "ekeyCgeRfC",
      "title": "Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions",
      "abstract": "In order to understand the in-context learning phenomenon, recent works have adopted a stylized experimental framework and demonstrated that Transformers can match the performance of gradient-based learning algorithms for various classes of real-valued functions. However, the limitations of Transformers in implementing learning algorithms, and their ability to learn other forms of algorithms are not well understood. Additionally, the degree to which these capabilities are confined to attention-based models is unclear. Furthermore, it remains to be seen whether the insights derived from these stylized settings can be extrapolated to pretrained Large Language Models (LLMs). In this work, we take a step towards answering these questions by demonstrating the following: (a) On a test-bed with a variety of Boolean function classes, we find that Transformers can nearly match the optimal learning algorithm for 'simpler' tasks, while their performance deteriorates on more 'complex' tasks. Additionally, we find that certain attention-free models perform (almost) identically to Transformers on a range of tasks. (b) When provided a *teaching sequence*, i.e. a set of examples that uniquely identifies a function in a class, we show that Transformers learn more sample-efficiently. Interestingly, our results show that Transformers can learn to implement *two distinct* algorithms to solve a *single* task, and can adaptively select the more sample-efficient algorithm depending on the sequence of in-context examples. (c) Lastly, we show that extant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines on prediction tasks that are guaranteed to not be in their training set.",
      "authors": [
        "Satwik Bhattamishra",
        "Arkil Patel",
        "Phil Blunsom",
        "Varun Kanade"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ekeyCgeRfC",
      "cdate": 1695515867779,
      "mdate": 1710542360461,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708688"
    },
    {
      "id": "4g02l2N2Nx",
      "title": "The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry",
      "abstract": "Linear attentions have shown promise for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear Transformers from scratch, (2) `inetuned-conversion of task-specific Transformers into linear versions that recover task performance, and (3) pretrained-conversion of Transformers, such as language models, into linear versions readily finetunable on downstream tasks. However, linear attentions often underperform compared to standard softmax attention. To close this performance gap, we study the behaviors of softmax and linear attentions in various train-from-scratch and finetuned-conversion settings. We find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or spiky) weights and dot-product monotonicity. We further observe surprisingly simple feature maps that retain these properties match softmax performance, but are inefficient to compute in linear attention. We thus propose Hedgehog, a learnable linear attention that retains the spiky and monotonic properties of softmax attention while maintaining linear complexity. Hedgehog uses simple, trainable MLPs to produce attention weights mimicking softmax attention. Experiments show Hedgehog recovers over 99\\% of standard Transformer performance in train-from-scratch and finetuned-conversion settings, outperforming prior linear attentions by up to 6 perplexity points on WikiText-103 when training causal GPT models from scratch, and up to 8.7 GLUE score points when converting finetuned bidirectional BERT models. Hedgehog also enables pretrained-conversion. Converting a pretrained GPT-2 into a linear attention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for 125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into a viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama-2 7B achieves 28.1 higher ROUGE-1 points over the base standard attention model, where prior linear attentions lead to 16.5 point drops.",
      "authors": [
        "Michael Zhang",
        "Kush Bhatia",
        "Hermann Kumbong",
        "Christopher Re"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4g02l2N2Nx",
      "cdate": 1695515621460,
      "mdate": 1710713947666,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708692"
    },
    {
      "id": "XNa6r6ZjoB",
      "title": "Abstractors and relational cross-attention: An inductive bias for explicit relational reasoning in Transformers",
      "abstract": "An extension of Transformers is proposed that enables explicit relational reasoning through a novel module called the *Abstractor*. At the core of the Abstractor is a variant of attention called *relational cross-attention*. The approach is motivated by an architectural inductive bias for relational learning that disentangles relational information from object-level features. This enables explicit relational reasoning, supporting abstraction and generalization from limited data. The Abstractor is first evaluated on simple discriminative relational tasks and compared to existing relational architectures. Next, the Abstractor is evaluated on purely relational sequence-to-sequence tasks, where dramatic improvements are seen in sample efficiency compared to standard Transformers. Finally, Abstractors are evaluated on a collection of tasks based on mathematical problem solving, where consistent improvements in performance and sample efficiency are observed.",
      "authors": [
        "Awni Altabaa",
        "Taylor Whittington Webb",
        "Jonathan D. Cohen",
        "John Lafferty"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=XNa6r6ZjoB",
      "cdate": 1695515486447,
      "mdate": 1712958898030,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708697"
    },
    {
      "id": "YCWjhGrJFD",
      "title": "Training Diffusion Models with Reinforcement Learning",
      "abstract": "Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization ( DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO can adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation. The project’s website can be found at http://rl-diffusion.github.io.",
      "authors": [
        "Kevin Black",
        "Michael Janner",
        "Yilun Du",
        "Ilya Kostrikov",
        "Sergey Levine"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=YCWjhGrJFD",
      "cdate": 1695514938705,
      "mdate": 1712697859746,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708703"
    },
    {
      "id": "D2eOVqPX9g",
      "title": "Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement Learning",
      "abstract": "Federated reinforcement learning (FRL) has emerged as a promising paradigm for reducing the sample complexity of reinforcement learning tasks by exploiting information from different agents. However, when each agent interacts with a potentially different environment, little to nothing is known theoretically about the non-asymptotic performance of FRL algorithms. The lack of such results can be attributed to various technical challenges and their intricate interplay: Markovian sampling, linear function approximation, multiple local updates to save communication, heterogeneity in the reward functions and transition kernels of the agents' MDPs, and continuous state-action spaces.  Moreover, in the on-policy setting, the behavior policies vary with time, further complicating the analysis. In response, we introduce FedSARSA, a novel federated on-policy reinforcement learning scheme, equipped with linear function approximation, to address these challenges and provide a comprehensive finite-time error analysis. Notably, we establish that FedSARSA converges to a policy that is near-optimal for all agents, with the extent of near-optimality proportional to the level of heterogeneity. Furthermore, we prove that FedSARSA leverages agent collaboration to enable linear speedups as the number of agents increases, which holds for both fixed and adaptive step-size configurations.",
      "authors": [
        "Chenyu Zhang",
        "Han Wang",
        "Aritra Mitra",
        "James Anderson"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=D2eOVqPX9g",
      "cdate": 1695514737716,
      "mdate": 1712985907677,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708707"
    },
    {
      "id": "fe6ANBxcKM",
      "title": "Federated Q-Learning: Linear Regret Speedup with Low Communication Cost",
      "abstract": "In this paper, we consider federated reinforcement learning for tabular episodic Markov Decision Processes (MDP) where, under the coordination of a central server, multiple agents collaboratively explore the environment and learn an optimal policy without sharing their raw data.  While linear speedup in the number of agents has been achieved for some metrics, such as convergence rate and sample complexity, in similar settings, it is unclear whether it is possible to design a *model-free* algorithm to achieve linear *regret* speedup with low communication cost. We propose two federated Q-Learning algorithms termed as FedQ-Hoeffding and FedQ-Bernstein, respectively, and show that the corresponding total regrets achieve a linear speedup compared with their single-agent counterparts, while the communication cost scales logarithmically in the total number of time steps $T$. Those results rely on an event-triggered synchronization mechanism between the agents and the server, a novel step size selection when the server aggregates the local estimates of the state-action values to form the global estimates, and a set of new concentration inequalities to bound the sum of non-martingale differences. This is the first work showing that linear regret speedup and logarithmic communication cost can be achieved by model-free algorithms in federated reinforcement learning.",
      "authors": [
        "Zhong Zheng",
        "Fengyu Gao",
        "Lingzhou Xue",
        "Jing Yang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fe6ANBxcKM",
      "cdate": 1695514650730,
      "mdate": 1710526099590,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708712"
    },
    {
      "id": "MeHmwCDifc",
      "title": "The Trickle-down Impact of Reward Inconsistency on RLHF",
      "abstract": "Standard practice within Reinforcement Learning from Human Feedback (RLHF) involves optimizing against a Reward Model (RM), which itself is trained to reflect human preferences for desirable generations. A notable subject that is understudied is the (in-)consistency of RMs --- whether they can recognize the semantic changes to different prompts and \nappropriately adapt their reward assignments\n\n--- and their impact on the downstream RLHF model.\n\nIn this paper, we visit a series of research questions relevant to RM inconsistency:\n(1) How can we measure the consistency of reward models? \n(2) How consistent are the existing RMs and how can we improve them? \n(3) In what ways does reward inconsistency influence the chatbots resulting from the RLHF model training?\n\n\nWe propose **Contrast Instruction** -- a benchmarking strategy for the consistency of RM.  \nEach example in **Contrast Instruction** features a pair of lexically similar instructions with different ground truth responses. A consistent RM is expected to rank the corresponding instruction and response higher than other combinations. We observe that current RMs trained with the standard ranking objective fail miserably on \\contrast{} compared to average humans. To show that RM consistency can be improved efficiently without using extra training budget, we propose two techniques **ConvexDA** and **RewardFusion**, which enhance reward consistency \nthrough extrapolation during the RM training and inference stage, respectively.\nWe show that RLHF models trained with a more consistent RM yield more useful responses, suggesting that reward inconsistency exhibits a trickle-down effect on the downstream RLHF process.",
      "authors": [
        "Lingfeng Shen",
        "Sihao Chen",
        "Linfeng Song",
        "Lifeng Jin",
        "Baolin Peng",
        "Haitao Mi",
        "Daniel Khashabi",
        "Dong Yu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MeHmwCDifc",
      "cdate": 1695514558820,
      "mdate": 1711839021847,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708716"
    },
    {
      "id": "Eo7kv0sllr",
      "title": "An Emulator for Fine-tuning Large Language Models using Small Language Models",
      "abstract": "Widely used language models (LMs) are typically built by scaling up a two-stage training pipeline: a pre-training stage that uses a very large, diverse dataset of text and a fine-tuning (sometimes, 'alignment') stage that uses targeted examples or other specifications of desired behaviors. While it has been hypothesized that knowledge and skills come from pre-training, and fine-tuning mostly filters this knowledge and skillset, this intuition has not been extensively tested. To aid in doing so, we introduce a novel technique for decoupling the knowledge and skills gained in these two stages, enabling a direct answer to the question, *What would happen if we combined the knowledge learned by a large model during pre-training with the knowledge learned by a small model during fine-tuning (or vice versa)?* Using an RL-based framework derived from recent developments in learning from human preferences, we introduce *emulated fine-tuning (EFT)*, a principled and practical method for sampling from a distribution that approximates (or 'emulates') the result of pre-training and fine-tuning at different scales. Our experiments with EFT show that scaling up fine-tuning tends to improve helpfulness, while scaling up pre-training tends to improve factuality. Beyond decoupling scale, we show that EFT enables test-time adjustment of competing behavioral traits like helpfulness and harmlessness without additional training. Finally, a special case of emulated fine-tuning, which we call LM *up-scaling*, avoids resource-intensive fine-tuning of large pre-trained models by ensembling them with small fine-tuned models, essentially emulating the result of fine-tuning the large pre-trained model. Up-scaling consistently improves helpfulness and factuality of instruction-following models in the Llama, Llama-2, and Falcon families, without additional hyperparameters or training. For reference implementation, see [https://github.com/eric-mitchell/emulated-fine-tuning](https://github.com/eric-mitchell/emulated-fine-tuning).",
      "authors": [
        "Eric Mitchell",
        "Rafael Rafailov",
        "Archit Sharma",
        "Chelsea Finn",
        "Christopher D Manning"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Eo7kv0sllr",
      "cdate": 1695513487328,
      "mdate": 1713135518088,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708723"
    },
    {
      "id": "jE8xbmvFin",
      "title": "Language Models Represent Space and Time",
      "abstract": "The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual \"space neurons\" and \"time neurons\" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.",
      "authors": [
        "Wes Gurnee",
        "Max Tegmark"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jE8xbmvFin",
      "cdate": 1695513418084,
      "mdate": 1709661548940,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708728"
    },
    {
      "id": "pAoqRlTBtY",
      "title": "Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning",
      "abstract": "Scientific discovery hinges on the effective integration of metadata, which refers to a set of 'cognitive' operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA's performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer's Disease (AD). Our experimental results indicate that the CMA can outperform previous data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.",
      "authors": [
        "Ahmed Abdulaal",
        "adamos hadjivasiliou",
        "Nina Montana-Brown",
        "Tiantian He",
        "Ayodeji Ijishakin",
        "Ivana Drobnjak",
        "Daniel C. Castro",
        "Daniel C. Alexander"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pAoqRlTBtY",
      "cdate": 1695513408950,
      "mdate": 1709827690076,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708733"
    },
    {
      "id": "kNpSUN0uCc",
      "title": "Maximum Entropy Model Correction in Reinforcement Learning",
      "abstract": "We propose and theoretically analyze an approach for planning with an approximate model in reinforcement learning that can reduce the adverse impact of model error. If the model is accurate enough, it accelerates the convergence to the true value function too. One of its key components is the MaxEnt Model Correction (MoCo) procedure that corrects the model’s next-state distributions based on a Maximum Entropy density estimation formulation. Based on MoCo, we introduce the Model Correcting Value Iteration (MoCoVI) algorithm, and its sampled-based variant MoCoDyna. We show that MoCoVI and MoCoDyna’s convergence can be much faster than the conventional model-free algorithms. Unlike traditional model-based algorithms, MoCoVI and MoCoDyna effectively utilize an approximate model and still converge to the correct value function.",
      "authors": [
        "Amin Rakhsha",
        "Mete Kemertas",
        "Mohammad Ghavamzadeh",
        "Amir-massoud Farahmand"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kNpSUN0uCc",
      "cdate": 1695513167911,
      "mdate": 1713143205179,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708738"
    },
    {
      "id": "aN4Jf6Cx69",
      "title": "The mechanistic basis of data dependence and abrupt learning in an in-context classification task",
      "abstract": "Transformer models exhibit in-context learning: the ability to accurately predict the response to a novel query based on illustrative examples in the input sequence, which contrasts with traditional in-weights learning of query-output relationships. What aspects of the training data distribution and architecture favor in-context vs in-weights learning? Recent work has shown that specific distributional properties inherent in language, such as burstiness, large dictionaries and skewed rank-frequency distributions, control the trade-off or simultaneous appearance of these two forms of learning. We first show that these results are recapitulated in a minimal attention-only network trained on a simplified dataset. In-context learning (ICL) is driven by the abrupt emergence of an induction head, which subsequently competes with in-weights learning. By identifying progress measures that precede in-context learning and targeted experiments, we construct a two-parameter model of an induction head which emulates the full data distributional dependencies displayed by the attention-based network. A phenomenological model of induction head formation traces its abrupt emergence to the sequential learning of three nested logits enabled by an intrinsic curriculum. We propose that the sharp transitions in attention-based networks arise due to a specific chain of multi-layer operations necessary to achieve ICL, which is implemented by nested nonlinearities sequentially learned during training.",
      "authors": [
        "Gautam Reddy"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=aN4Jf6Cx69",
      "cdate": 1695512883715,
      "mdate": 1710344148402,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708742"
    },
    {
      "id": "OkHHJcMroY",
      "title": "PILOT: An $\\mathcal{O}(1/K)$-Convergent Approach for Policy Evaluation with Nonlinear Function Approximation",
      "abstract": "Learning an accurate value function for a given policy is a critical step in solving reinforcement learning (RL) problems. So far, however, the convergence speed and sample complexity performances of most existing policy evaluation algorithms remain unsatisfactory, particularly with non-linear function approximation. This challenge motivates us to develop a new path-integrated primal-dual stochastic gradient (PILOT) method, that is able to achieve a fast convergence speed for RL policy evaluation with nonlinear function approximation. To further alleviate the periodic full gradient evaluation requirement, we further propose an enhanced method with an adaptive-batch adjustment called PILOT$^+$. The main advantages of our methods include: i) PILOT allows the use of {\\em{constant}} step sizes and achieves the $\\mathcal{O}(1/K)$ convergence rate to first-order stationary points of non-convex policy evaluation problems; ii) PILOT is a generic {\\em{single}}-timescale algorithm that is also applicable for solving a large class of non-convex strongly-concave minimax optimization problems; iii) By adaptively adjusting the batch size via historical stochastic gradient information, PILOT$^+$ is more sample-efficient empirically without loss of theoretical convergence rate. Our extensive numerical experiments verify our theoretical findings and showcase the high efficiency of the proposed PILOT and PILOT$^+$ algorithms compared with the state-of-the-art methods.",
      "authors": [
        "Zhuqing Liu",
        "Xin Zhang",
        "Jia Liu",
        "Zhengyuan Zhu",
        "Songtao Lu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OkHHJcMroY",
      "cdate": 1695512820331,
      "mdate": 1713672668423,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708747"
    },
    {
      "id": "4eJDMjYZZG",
      "title": "Language Model Detectors Are Easily Optimized Against",
      "abstract": "The fluency and general applicability of large language models (LLMs) has motivated significant interest in detecting whether a piece of text was written by a language model. While both academic and commercial detectors have been deployed in some settings, particularly education, other research has highlighted the fragility of these systems. In this paper, we demonstrate a data-efficient attack that fine-tunes language models to confuse existing detectors, leveraging recent developments in reinforcement learning of language models. We use the `human-ness' score (often just a log probability) of various open-source and commercial detectors as a reward function for reinforcement learning, subject to a KL-divergence constraint that the resulting model does not differ significantly from the original. For a 7B parameter Llama-2 model, fine-tuning for under a day reduces the AUROC of the OpenAI RoBERTa-Large detector from 0.84 to 0.63, while perplexity on OpenWebText increases from 8.7 to only 9.0; with a larger perplexity budget, we can drive AUROC to 0.30 (worse than random). Similar to traditional adversarial attacks, we find that this increase in 'detector evasion' generalizes to other detectors not used during training. In light of our empirical results, we advise against continued reliance on LLM-generated text detectors. Models, datasets, and selected experiment code will be released at https://github.com/charlottttee/llm-detector-evasion.",
      "authors": [
        "Charlotte Nicks",
        "Eric Mitchell",
        "Rafael Rafailov",
        "Archit Sharma",
        "Christopher D Manning",
        "Chelsea Finn",
        "Stefano Ermon"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4eJDMjYZZG",
      "cdate": 1695512427095,
      "mdate": 1713232101491,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708752"
    },
    {
      "id": "kXHEBK9uAY",
      "title": "Simple Hierarchical Planning with Diffusion",
      "abstract": "Diffusion-based generative methods have proven effective in modeling trajectories with offline datasets. However, they often face computational challenges and can falter in generalization, especially in capturing temporal abstractions for long-horizon tasks. To overcome this, we introduce the Hierarchical Diffuser, a simple, fast, yet effective planning method combining the advantages of hierarchical and diffusion-based planning. Our model adopts a “jumpy” planning strategy at the high level, which allows it to have a larger receptive field but at a lower computational cost—a crucial factor for diffusion-based planning methods, as we have empirically verified. Additionally, the jumpy sub-goals guide our low-level planner, facilitating a fine-tuning stage and further improving our approach’s effectiveness. We conducted empirical evaluations on standard offline reinforcement learning benchmarks, demonstrating our method’s superior performance and efficiency in terms of training and planning speed compared to the non-hierarchical Diffuser as well as other hierarchical planning methods. Moreover, we explore our model’s generalization capability, particularly on how our method improves generalization capabilities on compositional out-of-distribution tasks.",
      "authors": [
        "Chang Chen",
        "Fei Deng",
        "Kenji Kawaguchi",
        "Caglar Gulcehre",
        "Sungjin Ahn"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kXHEBK9uAY",
      "cdate": 1695511537927,
      "mdate": 1713590796001,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708757"
    },
    {
      "id": "fj2E5OcLFn",
      "title": "Stochastic Gradient Descent for Gaussian Processes Done Right",
      "abstract": "As is well known, both sampling from the posterior and computing the mean of the posterior in Gaussian process regression reduces to solving a large linear system of equations. We study the use of stochastic gradient descent for solving this linear system, and show that when done right---by which we mean using specific insights from the optimisation and kernel communities---stochastic gradient descent is highly effective. To that end, we introduce a particularly simple stochastic dual descent algorithm, explain its design in an intuitive manner and illustrate the design choices through a series of ablation studies. Further experiments demonstrate that our new method is highly competitive. In particular, our evaluations on the UCI regression tasks and on Bayesian optimisation set our approach apart from preconditioned conjugate gradients and variational Gaussian process approximations. Moreover, our method places Gaussian process regression on par with state-of-the-art graph neural networks for molecular binding affinity prediction.",
      "authors": [
        "Jihao Andreas Lin",
        "Shreyas Padhy",
        "Javier Antoran",
        "Austin Tripp",
        "Alexander Terenin",
        "Csaba Szepesvari",
        "José Miguel Hernández-Lobato",
        "David Janz"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fj2E5OcLFn",
      "cdate": 1695511343689,
      "mdate": 1713119864958,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708761"
    },
    {
      "id": "gkfUvn0fLU",
      "title": "Confronting Reward Model Overoptimization with Constrained RLHF",
      "abstract": "Large language models are typically aligned with human preferences by optimizing reward models (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to *overoptimization*, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of preventing the agent from exceeding each RM's threshold of usefulness. Our method addresses the problem of weighting component RMs by learning dynamic weights, naturally given by the Lagrange multipliers. As a result, each RM stays within the range at which it is an effective proxy, improving evaluation performance. Finally, we introduce an adaptive method using gradient-free optimization to identify and optimize towards these points during a single run.",
      "authors": [
        "Ted Moskovitz",
        "Aaditya K Singh",
        "DJ Strouse",
        "Tuomas Sandholm",
        "Ruslan Salakhutdinov",
        "Anca Dragan",
        "Stephen Marcus McAleer"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=gkfUvn0fLU",
      "cdate": 1695511292202,
      "mdate": 1710463536664,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708766"
    },
    {
      "id": "c56TWtYp0W",
      "title": "GAFormer: Enhancing Timeseries Transformers Through Group-Aware Embeddings",
      "abstract": "Analyzing multivariate time series is important in many domains. However, it has been difficult to learn robust and generalizable representations within multivariate datasets due to complex inter-channel relationships and dynamic shifts. In this paper, we introduce a novel approach for learning spatiotemporal structure and using it to improve the application of transformers to timeseries datasets. Our framework learns a set of group tokens, and builds an instance-specific group embedding (GE) layer that assigns input tokens to a small number of group tokens to incorporate  structure into learning. We then introduce a novel architecture, Group-Aware transFormer (GAFormer), which incorporates both spatial and temporal group embeddings to achieve state-of-the-art performance on a number of time-series classification and regression tasks. In evaluations on a number of diverse timeseries datasets, we show that GE on its own can provide a nice enhancement to a number of backbones, and that by coupling spatial and temporal group embeddings, the GAFormer can outperform the existing baselines. Finally, we show how our approach discerns latent structures in data even without information about the spatial ordering of channels, and yields a more interpretable decomposition of spatial and temporal structure underlying complex multivariate datasets.",
      "authors": [
        "Jingyun Xiao",
        "Ran Liu",
        "Eva L Dyer"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=c56TWtYp0W",
      "cdate": 1695511281385,
      "mdate": 1713363592373,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708775"
    },
    {
      "id": "3aZCPl3ZvR",
      "title": "Why is SAM Robust to Label Noise?",
      "abstract": "Sharpness-Aware Minimization (SAM) is most known for achieving state-of the-art performances on natural image and language tasks. However, its most pronounced improvements (of tens of percent) is rather in the presence of label noise. Understanding SAM's label noise robustness requires a departure from characterizing the robustness of minimas lying in ``flatter'' regions of the loss landscape. In particular, the peak performance under label noise occurs with early stopping, far before the loss converges. We decompose SAM's robustness into two effects: one induced by changes to the logit term and the other induced by changes to the network Jacobian. The first can be observed in linear logistic regression where SAM provably up-weights the gradient contribution from clean examples. Although this explicit up-weighting is also observable in neural networks, when we intervene and modify SAM to remove this effect, surprisingly, we see no visible degradation in performance. We infer that SAM's effect in deeper networks is instead explained entirely by the effect SAM has on the network Jacobian. We theoretically derive the  implicit regularization induced by this Jacobian effect in two layer linear networks. Motivated by our analysis, we see that cheaper alternatives to SAM that explicitly induce these regularization effects largely recover the benefits in deep networks trained on real-world datasets.",
      "authors": [
        "Christina Baek",
        "J Zico Kolter",
        "Aditi Raghunathan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3aZCPl3ZvR",
      "cdate": 1695510842198,
      "mdate": 1714943157650,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708780"
    },
    {
      "id": "lOwkOIUJtx",
      "title": "Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling",
      "abstract": "High accuracy, low latency and high energy efficiency represent a set of contradictory goals when searching for  system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduce camera refresh rates, and increase the volume of data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, saccade mechanism observed in the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from foveal and peripheral vision, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90\\% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our study demonstrates a 5\\% higher image recognition accuracy compared to state-of-the-art foveal-peripheral vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches.",
      "authors": [
        "Jiayang Liu",
        "Yiming Bu",
        "Daniel Tso",
        "Qinru Qiu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lOwkOIUJtx",
      "cdate": 1695510038947,
      "mdate": 1710031100059,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708788"
    },
    {
      "id": "17pVDnpwwl",
      "title": "Tensor Programs VI: Feature Learning in Infinite Depth Neural Networks",
      "abstract": "Empirical studies have consistently demonstrated that increasing the size of neural networks often yields superior performance in practical applications. However, there is a lack of consensus regarding the appropriate scaling strategy, particularly when it comes to increasing the depth of neural networks. In practice, excessively large depths can lead to model performance degradation. In this paper, we introduce Depth-$\\mu$P, a principled approach for depth scaling, allowing for the training of arbitrarily deep architectures while maximizing feature learning and diversity among nearby layers. Our method involves dividing the contribution of each residual block and the parameter update by the square root of the depth. Through the use of Tensor Programs, we rigorously establish the existence of a limit for infinitely deep neural networks under the proposed scaling scheme. This scaling strategy ensures more stable training for deep neural networks and guarantees the transferability of hyperparameters from shallow to deep models. To substantiate the efficacy of our scaling method, we conduct empirical validation on neural networks with depths up to $2^{10}$.",
      "authors": [
        "Greg Yang",
        "Dingli Yu",
        "Chen Zhu",
        "Soufiane Hayou"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=17pVDnpwwl",
      "cdate": 1695509632961,
      "mdate": 1713673110583,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708793"
    },
    {
      "id": "BPHcEpGvF8",
      "title": "Demystifying Poisoning Backdoor Attacks from a Statistical Perspective",
      "abstract": "Backdoor attacks pose a significant security risk to machine learning applications due to their stealthy nature and potentially serious consequences. Such attacks involve embedding triggers within a learning model with the intention of causing malicious behavior when an active trigger is present while maintaining regular functionality without it. This paper derives a fundamental understanding of backdoor attacks that applies to both discriminative and generative models, including diffusion models and large language models. We evaluate the effectiveness of any backdoor attack incorporating a constant trigger, by establishing tight lower and upper boundaries for the performance of the compromised model on both clean and backdoor test data. The developed theory answers a series of fundamental but previously underexplored problems, including (1) what are the determining factors for a backdoor attack's success, (2) what is the direction of the most effective backdoor attack, and (3) when will a human-imperceptible trigger succeed. We demonstrate the theory by conducting experiments using benchmark datasets and state-of-the-art backdoor attack scenarios. Our code is available \\href{https://github.com/KeyWgh/DemystifyBackdoor}{here}.",
      "authors": [
        "Ganghua Wang",
        "Xun Xian",
        "Ashish Kundu",
        "Jayanth Srinivasa",
        "Xuan Bi",
        "Mingyi Hong",
        "Jie Ding"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BPHcEpGvF8",
      "cdate": 1695509024311,
      "mdate": 1710183520702,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708798"
    },
    {
      "id": "RgELE1dQXx",
      "title": "Learning to Make Adherence-aware Advice",
      "abstract": "As artificial intelligence (AI) systems play an increasingly prominent role in human decision-making, challenges surface in the realm of human-AI interactions. One challenge arises from the suboptimal AI policies due to the inadequate consideration of humans disregarding AI recommendations, as well as the need for AI to provide advice selectively when it is most pertinent. This paper presents a sequential decision-making model that (i) takes into account the human's adherence level (the probability that the human follows/rejects machine advice) and (ii) incorporates a defer option so that the machine can temporarily refrain from making advice. We provide learning algorithms that learn the optimal advice policy and make advice only at critical time stamps. Compared to problem-agnostic reinforcement learning algorithms, our specialized learning algorithms not only enjoy better theoretical convergence properties but also show strong empirical performance.",
      "authors": [
        "Guanting Chen",
        "Xiaocheng Li",
        "Chunlin Sun",
        "Hanzhao Wang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=RgELE1dQXx",
      "cdate": 1695509003047,
      "mdate": 1713672624904,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708805"
    },
    {
      "id": "f1xnBr4WD6",
      "title": "Cycle Consistency Driven Object Discovery",
      "abstract": "Developing deep learning models that effectively learn object-centric representations, akin to human cognition, remains a challenging task. Existing approaches facilitate object discovery by representing objects as fixed-size vectors, called ``slots'' or ``object files''. While these approaches have shown promise in certain scenarios, they still exhibit certain limitations. First, they rely on architectural priors which can be unreliable and usually require meticulous engineering to identify the correct objects. Second, there has been a notable gap in investigating the practical utility of these representations in downstream tasks. To address the first limitation, we introduce a method that explicitly optimizes the constraint that each object in a scene should be associated with a distinct slot. We formalize this constraint by introducing  consistency objectives which are cyclic in nature. By integrating these consistency objectives into various existing slot-based object-centric methods, we showcase substantial improvements in object-discovery performance. These enhancements consistently hold true across both synthetic and real-world scenes, underscoring the effectiveness and adaptability of the proposed approach. To tackle the second limitation, we apply the learned object-centric representations from the proposed method to two downstream reinforcement learning tasks, demonstrating considerable performance enhancements compared to conventional slot-based and monolithic representation learning methods. Our results suggest that the proposed approach not only improves object discovery, but also provides richer features for downstream tasks.",
      "authors": [
        "Aniket Rajiv Didolkar",
        "Anirudh Goyal",
        "Yoshua Bengio"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=f1xnBr4WD6",
      "cdate": 1695508566103,
      "mdate": 1710503052827,
      "matched_keywords": [
        "reinforcement learning",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708811"
    },
    {
      "id": "RVrINT6MT7",
      "title": "Sufficient conditions for offline reactivation in recurrent neural networks",
      "abstract": "During periods of quiescence, such as sleep, neural activity in many brain circuits resembles that observed during periods of task engagement. However, the precise conditions under which task-optimized networks can autonomously reactivate the same network states responsible for online behavior is poorly understood. In this study, we develop a mathematical framework that outlines sufficient conditions for the emergence of neural reactivation in circuits that encode features of smoothly varying stimuli. We demonstrate mathematically that noisy recurrent networks optimized to track environmental state variables using change-based sensory information naturally develop denoising dynamics, which, in the absence of input, cause the network to revisit state configurations observed during periods of online activity. We validate our findings using numerical experiments on two canonical neuroscience tasks: spatial position estimation based on self-motion cues, and head direction estimation based on angular velocity cues. Overall, our work provides theoretical support for modeling offline reactivation as an emergent consequence of task optimization in noisy neural circuits.",
      "authors": [
        "Nanda H Krishna",
        "Colin Bredenberg",
        "Daniel Levenstein",
        "Blake Aaron Richards",
        "Guillaume Lajoie"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=RVrINT6MT7",
      "cdate": 1695508203176,
      "mdate": 1713589211697,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708818"
    },
    {
      "id": "Abr7dU98ME",
      "title": "Forward Learning of Graph Neural Networks",
      "abstract": "Graph neural networks (GNNs) have achieved remarkable success across a wide range of applications, such as recommendation, drug discovery, and question answering. Behind the success of GNNs lies the backpropagation (BP) algorithm, which is the de facto standard for training deep neural networks (NNs). However, despite its effectiveness, BP imposes several constraints, which are not only biologically implausible, but also limit the scalability, parallelism, and flexibility in learning NNs. Examples of such constraints include storage of neural activities computed in the forward pass for use in the subsequent backward pass, and the dependence of parameter updates on non-local signals. To address these limitations, the forward-forward algorithm (FF) was recently proposed as an alternative to BP in the image classification domain, which trains NNs by performing two forward passes over positive and negative data. Inspired by this advance, we propose ForwardGNN in this work, a new forward learning procedure for GNNs, which avoids the constraints imposed by BP via an effective layer-wise local forward training. ForwardGNN extends the original FF to deal with graph data and GNNs, and makes it possible to operate without generating negative inputs (hence no longer forward-forward). Further, ForwardGNN enables each layer to learn from both the bottom-up and top-down signals without relying on the backpropagation of errors. Extensive experiments on real-world datasets show the effectiveness and generality of the proposed forward graph learning framework. We release our code at https://github.com/facebookresearch/forwardgnn.",
      "authors": [
        "Namyong Park",
        "Xing Wang",
        "Antoine Simoulin",
        "Shuai Yang",
        "Grey Yang",
        "Ryan A. Rossi",
        "Puja Trivedi",
        "Nesreen K. Ahmed"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Abr7dU98ME",
      "cdate": 1695508012700,
      "mdate": 1712966717049,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708823"
    },
    {
      "id": "rINBD8jPoP",
      "title": "Curriculum reinforcement learning for quantum architecture search under hardware errors",
      "abstract": "The key challenge in the noisy intermediate-scale quantum era is finding useful circuits compatible with current device limitations.\nVariational quantum algorithms (VQAs) offer a potential solution by fixing the circuit architecture and optimizing individual gate parameters in an external loop. However, parameter optimization can become intractable, and the overall performance of the algorithm depends heavily on the initially chosen circuit architecture. Several quantum architecture search (QAS) algorithms have been developed to design useful circuit architectures automatically. In the case of parameter optimization alone, noise effects have been observed to dramatically influence the performance of the optimizer and final outcomes, which is a key line of study. However, the effects of noise on the architecture search, which could be just as critical, are poorly understood. This work addresses this gap by introducing a curriculum-based reinforcement learning QAS (CRLQAS) algorithm designed to tackle challenges in realistic VQA deployment. The algorithm incorporates (i) a 3D architecture encoding and restrictions on environment dynamics to explore the search space of possible circuits efficiently, (ii) an episode halting scheme to steer the agent to find shorter circuits, and (iii) a novel variant of simultaneous perturbation stochastic approximation as an optimizer for faster convergence. To facilitate studies, we developed an optimized simulator for our algorithm, significantly improving computational efficiency in simulating noisy quantum circuits by employing the Pauli-transfer matrix formalism in the Pauli-Liouville basis. Numerical experiments focusing on quantum chemistry tasks demonstrate that CRLQAS outperforms existing QAS algorithms across several metrics in both noiseless and noisy environments.",
      "authors": [
        "Yash J. Patel",
        "Akash Kundu",
        "Mateusz Ostaszewski",
        "Xavier Bonet-Monroig",
        "Vedran Dunjko",
        "Onur Danaci"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rINBD8jPoP",
      "cdate": 1695507644607,
      "mdate": 1709661547973,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708827"
    },
    {
      "id": "tnBaiidobu",
      "title": "Does CLIP’s generalization performance mainly stem from high train-test similarity?",
      "abstract": "Foundation models like CLIP are trained on hundreds of millions of samples and effortlessly generalize to new tasks and inputs. Out of the box, CLIP shows stellar zero-shot and few-shot capabilities on a wide range of out-of-distribution (OOD) benchmarks, which prior works attribute mainly to today's large and comprehensive training dataset (like LAION). However, it is questionable how meaningful terms like out-of-distribution generalization are for CLIP as it seems likely that web-scale datasets like LAION simply contain many samples that are similar to common OOD benchmarks originally designed for ImageNet. To test this hypothesis, we retrain CLIP on pruned LAION splits that replicate ImageNet’s train-test similarity with respect to common OOD benchmarks. While we observe a performance drop on some benchmarks, surprisingly, CLIP’s overall performance remains high. This shows that high train-test similarity is insufficient to explain CLIP’s OOD performance, and other properties of the training data must drive CLIP to learn more generalizable representations. Additionally, by pruning data points that are dissimilar to the OOD benchmarks, we uncover a 100M split of LAION (¼ of its original size) on which CLIP can be trained to match its original OOD performance.",
      "authors": [
        "Prasanna Mayilvahanan",
        "Thaddäus Wiedemer",
        "Evgenia Rusak",
        "Matthias Bethge",
        "Wieland Brendel"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tnBaiidobu",
      "cdate": 1695507602250,
      "mdate": 1710440116318,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708835"
    },
    {
      "id": "5HCnKDeTws",
      "title": "When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method",
      "abstract": "While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning – full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative joint scaling law between finetuning data size and each other scaling factor; 2) LLM finetuning benefits more from LLM model scaling than pretraining data scaling, and PET parameter scaling is generally ineffective; and 3) the optimal finetuning method is highly task- and finetuning data-dependent. We hope our findings could shed light on understanding, selecting and developing LLM finetuning methods.",
      "authors": [
        "Biao Zhang",
        "Zhongtao Liu",
        "Colin Cherry",
        "Orhan Firat"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5HCnKDeTws",
      "cdate": 1695506447665,
      "mdate": 1709661547847,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708840"
    },
    {
      "id": "EhrzQwsV4K",
      "title": "L2MAC: Large Language Model Automatic Computer for Extensive Code Generation",
      "abstract": "Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and coherent outputs. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long output generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based general-purpose stored-program automatic computer (von Neumann architecture) framework, an LLM-based multi-agent system, for long and consistent output generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction in turn is executed by a separate LLM agent, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective interaction with the entire file store. These components enable L2MAC to generate extensive outputs, bypassing the constraints of the finite context window while producing outputs that fulfill a complex user-specified task. We empirically demonstrate that L2MAC achieves state-of-the-art performance in generating large codebases for system design tasks, significantly outperforming other coding methods in implementing the detailed user-specified task; we show that L2MAC works for general-purpose extensive text-based tasks, such as writing an entire book; and we provide valuable insights into L2MAC's performance improvement over existing methods.",
      "authors": [
        "Samuel Holt",
        "Max Ruiz Luyten",
        "Mihaela van der Schaar"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EhrzQwsV4K",
      "cdate": 1695505780327,
      "mdate": 1713672850593,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708844"
    },
    {
      "id": "c93SBwz1Ma",
      "title": "BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models",
      "abstract": "Large language models (LLMs) are shown to benefit from chain-of-thought (COT) prompting, particularly when tackling tasks that require systematic reasoning processes. On the other hand, COT prompting also poses new vulnerabilities in the form of backdoor attacks, wherein the model will output unintended malicious content under specific backdoor-triggered conditions during inference. Traditional methods for launching backdoor attacks involve either contaminating the training dataset with backdoored instances or directly manipulating the model parameters during deployment. However, these approaches are not practical for commercial LLMs that typically operate via API access. In this paper, we propose BadChain, the first backdoor attack against LLMs employing COT prompting, which does not require access to the training dataset or model parameters and imposes low computational overhead. BadChain leverages the inherent reasoning capabilities of LLMs by inserting a backdoor reasoning step into the sequence of reasoning steps of the model output, thereby altering the final response when a backdoor trigger is embedded in the query prompt. In particular, a subset of demonstrations will be manipulated to incorporate a backdoor reasoning step in COT prompting. Consequently, given any query prompt containing the backdoor trigger, the LLM will be misled to output unintended content. Empirically, we show the effectiveness of BadChain for two COT strategies across four LLMs (Llama2, GPT-3.5, PaLM2, and GPT-4) and six complex benchmark tasks encompassing arithmetic, commonsense, and symbolic reasoning. We show that the baseline backdoor attacks designed for simpler tasks such as semantic classification will fail on these complicated tasks. In addition, our findings reveal that LLMs endowed with stronger reasoning capabilities exhibit higher susceptibility to BadChain, exemplified by a high average attack success rate of 97.0\\% across the six benchmark tasks on GPT-4. We also demonstrate the interpretability of BadChain by showing that the relationship between the trigger and the backdoor reasoning step can be well-explained based on the output of the backdoored model. Finally, we propose two defenses based on shuffling and demonstrate their overall ineffectiveness against BadChain. Therefore, BadChain remains a severe threat to LLMs, underscoring the urgency for the development of robust and effective future defenses.",
      "authors": [
        "Zhen Xiang",
        "Fengqing Jiang",
        "Zidi Xiong",
        "Bhaskar Ramasubramanian",
        "Radha Poovendran",
        "Bo Li"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=c93SBwz1Ma",
      "cdate": 1695505607351,
      "mdate": 1709661547655,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708849"
    },
    {
      "id": "samyfu6G93",
      "title": "NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks",
      "abstract": "Propositional satisfiability (SAT) is an NP-complete problem that impacts many\nresearch fields, such as planning, verification, and security. Mainstream modern\nSAT solvers are based on the Conflict-Driven Clause Learning (CDCL) algorithm.\nRecent work aimed to enhance CDCL SAT solvers using Graph Neural Networks\n(GNNs). However, so far this approach either has not made solving more effective,\nor required substantial GPU resources for frequent online model inferences. Aiming\nto make GNN improvements practical, this paper proposes an approach called\nNeuroBack, which builds on two insights: (1) predicting phases (i.e., values) of\nvariables appearing in the majority (or even all) of the satisfying assignments are\nessential for CDCL SAT solving, and (2) it is sufficient to query the neural model\nonly once for the predictions before the SAT solving starts. Once trained, the\noffline model inference allows NeuroBack to execute exclusively on the CPU,\nremoving its reliance on GPU resources. To train NeuroBack, a new dataset called\nDataBack containing 120,286 data samples is created. Finally, NeuroBack is implemented\nas an enhancement to a state-of-the-art SAT solver called Kissat. As a result,\nit allowed Kissat to solve 5.2% more problems on the recent SAT competition\nproblem set, SATCOMP-2022. NeuroBack therefore shows how machine learning\ncan be harnessed to improve SAT solving in an effective and practical manner.",
      "authors": [
        "Wenxi Wang",
        "Yang Hu",
        "Mohit Tiwari",
        "Sarfraz Khurshid",
        "Kenneth McMillan",
        "Risto Miikkulainen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=samyfu6G93",
      "cdate": 1695505221832,
      "mdate": 1713153562047,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708853"
    },
    {
      "id": "DpFeMH4l8Q",
      "title": "Group Preference Optimization: Few-Shot Alignment of Large Language Models",
      "abstract": "Many applications of large language models (LLMs), ranging from chatbots to\ncreative writing, require nuanced subjective judgments that can differ significantly\nacross different groups. Existing alignment algorithms can be expensive to align\nfor each group, requiring prohibitive amounts of group-specific preference data\nand computation for real-world use cases. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the base\nLLM with an independent transformer module trained to predict the preferences\nof a group for the LLM generations. For few-shot learning, we parameterize this\nmodule as an in-context autoregressive transformer and train it via meta-learning\non several groups. We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. These tasks involve adapting to the preferences of US demographic\ngroups, global countries, and individual users. Our results demonstrate that GPO\nnot only aligns models more accurately but also requires fewer group-specific\npreferences and less training and inference computing resources, outperforming\nexisting strategies such as in-context steering and fine-tuning methods.",
      "authors": [
        "Siyan Zhao",
        "John Dang",
        "Aditya Grover"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DpFeMH4l8Q",
      "cdate": 1695505031049,
      "mdate": 1713156664963,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708858"
    },
    {
      "id": "jenyYQzue1",
      "title": "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning",
      "abstract": "While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our data instances are free text narratives corresponding to real-world domains of reasoning; this makes it simultaneously much more challenging than other synthetically-crafted benchmarks while remaining realistic and tractable for human annotators to solve with high accuracy. We evaluate a range of LLMs and prompting techniques on this dataset and characterize the gaps that remain for techniques like chain-of-thought to perform robust reasoning.",
      "authors": [
        "Zayne Rea Sprague",
        "Xi Ye",
        "Kaj Bostrom",
        "Swarat Chaudhuri",
        "Greg Durrett"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jenyYQzue1",
      "cdate": 1695503987987,
      "mdate": 1710532240758,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708862"
    },
    {
      "id": "THJEa8adBn",
      "title": "Harnessing Density Ratios for Online Reinforcement Learning",
      "abstract": "The theories of offline and online reinforcement learning, despite having evolved in parallel, have begun to show signs of the possibility for a unification, with algorithms and analysis techniques for one setting often having natural counterparts in the other. However, the notion of *density ratio modeling*, an emerging paradigm in offline RL, has been largely absent from online RL, perhaps for good reason: the very existence and boundedness of density ratios relies on access to an exploratory dataset with good coverage, but the core challenge in online RL is to collect such a dataset without having one to start.\n\nIn this work we show---perhaps surprisingly---that density ratio-based algorithms have online counterparts.  Assuming only the existence of an exploratory distribution with good coverage, a structural condition known as *coverability* (Xie et al., 2023), we give a new algorithm (GLOW) that uses density ratio realizability and value function realizability to perform sample-efficient online exploration. GLOW addresses unbounded density ratios via careful use of truncation, and combines this with optimism to guide exploration. GLOW is computationally inefficient; we complement it with a more efficient counterpart, HyGLOW, for the Hybrid RL setting (Song et al., 2023) wherein online RL is augmented with additional offline data. HyGLOW is derived as a special case of a more general meta-algorithm that provides a provable black-box reduction from hybrid RL to offline RL, which may be of independent interest.",
      "authors": [
        "Philip Amortila",
        "Dylan J Foster",
        "Nan Jiang",
        "Ayush Sekhari",
        "Tengyang Xie"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=THJEa8adBn",
      "cdate": 1695503627046,
      "mdate": 1710548057419,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708867"
    },
    {
      "id": "agPpmEgf8C",
      "title": "Predictive auxiliary objectives in deep RL mimic learning in the brain",
      "abstract": "The ability to predict upcoming events has been hypothesized to comprise a key aspect of natural and machine cognition. This is supported by trends in deep reinforcement learning (RL), where self-supervised auxiliary objectives such as prediction are widely used to support representation learning and improve task performance. Here, we study the effects predictive auxiliary objectives have on representation learning across different modules of an RL system and how these mimic representational changes observed in the brain. We find that predictive objectives improve and stabilize learning particularly in resource-limited architectures, and we identify settings where longer predictive horizons better support representational transfer. Furthermore, we find that representational changes in this RL system bear a striking resemblance to changes in neural activity observed in the brain across various experiments. Specifically, we draw a connection between the auxiliary predictive model of the RL system and hippocampus, an area thought to learn a predictive model to support memory-guided behavior. We also connect the encoder network and the value learning network of the RL system to visual cortex and striatum in the brain, respectively. This work demonstrates how representation learning in deep RL systems can provide an interpretable framework for modeling multi-region interactions in the brain. The deep RL perspective taken here also suggests an additional role of the hippocampus in the brain-- that of an auxiliary learning system that benefits representation learning in other regions.",
      "authors": [
        "Ching Fang",
        "Kim Stachenfeld"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=agPpmEgf8C",
      "cdate": 1695502361071,
      "mdate": 1713301733451,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708871"
    },
    {
      "id": "AcoXPIPh4A",
      "title": "Risk Bounds of Accelerated SGD for Overparameterized Linear Regression",
      "abstract": "Accelerated stochastic gradient descent (ASGD) is a workhorse in deep learning and often achieves better generalization performance than SGD. However, existing optimization theory can only explain the faster convergence of ASGD, but cannot explain its better generalization. In this paper, we study the generalization of ASGD for overparameterized linear regression, which is possibly the simplest setting of learning with overparameterization. We establish an instance-dependent excess risk bound for ASGD within each eigen-subspace of the data covariance matrix. Our analysis shows that (i) ASGD outperforms SGD in the subspace of small eigenvalues, exhibiting a faster rate of exponential decay for bias error, while in the subspace of large eigenvalues, its bias error decays slower than SGD; and (ii) the variance error of ASGD is always larger than that of SGD. Our result suggests that ASGD can outperform SGD when the difference between the initialization and the true weight vector is mostly confined to the subspace of small eigenvalues. Additionally, when our analysis is specialized to linear regression in the strongly convex setting, it yields a tighter bound for bias error than the best-known result.",
      "authors": [
        "Xuheng Li",
        "Yihe Deng",
        "Jingfeng Wu",
        "Dongruo Zhou",
        "Quanquan Gu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=AcoXPIPh4A",
      "cdate": 1695502194930,
      "mdate": 1710481576731,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708876"
    },
    {
      "id": "k9t8dQ30kU",
      "title": "Task structure and nonlinearity jointly determine learned representational geometry",
      "abstract": "The utility of a learned neural representation depends on how well its geometry supports performance in downstream tasks. This geometry depends on the structure of the inputs, the structure of the target outputs, and on the architecture of the network.  By studying the learning dynamics of networks with one hidden layer, we discovered that the network's activation function has an unexpectedly strong impact on the representational geometry: Tanh networks tend to learn representations that reflect the structure of the target outputs, while ReLU networks retain more information about the structure of the raw inputs. This difference is consistently observed across a broad class of parameterized tasks in which we modulated the degree of alignment between the geometry of the task inputs and that of the task labels. We analyzed the learning dynamics in weight space and show how the differences between the networks with Tanh and ReLU nonlinearities arise from the asymmetric saturation of ReLU, which leads feature neurons to specialize for different regions of input space. Feature neurons in Tanh networks, by contrast, tend to inherit the task label structure. Consequently, when the target outputs are low dimensional, Tanh networks generate neural representations that are more disentangled than those obtained with a ReLU nonlinearity. Our findings shed light on the interplay between input-output geometry, nonlinearity, and learned representations in neural networks.",
      "authors": [
        "Matteo Alleman",
        "Jack Lindsey",
        "Stefano Fusi"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=k9t8dQ30kU",
      "cdate": 1695502059841,
      "mdate": 1709661547257,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708883"
    },
    {
      "id": "4WnqRR915j",
      "title": "Llemma: An Open Language Model for Mathematics",
      "abstract": "We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known openly released models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.",
      "authors": [
        "Zhangir Azerbayev",
        "Hailey Schoelkopf",
        "Keiran Paster",
        "Marco Dos Santos",
        "Stephen Marcus McAleer",
        "Albert Q. Jiang",
        "Jia Deng",
        "Stella Biderman",
        "Sean Welleck"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4WnqRR915j",
      "cdate": 1695501968355,
      "mdate": 1710529746066,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708888"
    },
    {
      "id": "1vmSEVL19f",
      "title": "Directly Fine-Tuning Diffusion Models on Differentiable Rewards",
      "abstract": "We present Direct Reward Fine-Tuning (DRaFT), a simple and effective method for fine-tuning diffusion models to maximize differentiable reward functions, such as scores from human preference models. We first show that it is possible to backpropagate the reward function gradient through the full sampling procedure, and that doing so achieves strong performance on a variety of rewards, outperforming reinforcement learning-based approaches. We then propose more efficient variants of DRaFT: DRaFT-K, which truncates backpropagation to only the last K steps of sampling, and DRaFT-LV, which obtains lower-variance gradient estimates for the case when K=1. We show that our methods work well for a variety of reward functions and can be used to substantially improve the aesthetic quality of images generated by Stable Diffusion 1.4. Finally, we draw connections between our approach and prior work, providing a unifying perspective on the design space of gradient-based fine-tuning algorithms.",
      "authors": [
        "Kevin Clark",
        "Paul Vicol",
        "Kevin Swersky",
        "David J. Fleet"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=1vmSEVL19f",
      "cdate": 1695501638562,
      "mdate": 1710553431552,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708892"
    },
    {
      "id": "NgaLU2fP5D",
      "title": "Predictive, scalable and interpretable knowledge tracing on structured domains",
      "abstract": "Intelligent tutoring systems optimize the selection and timing of learning materials to enhance understanding and long-term retention. This requires estimates of both the learner's progress (\"knowledge tracing\"; KT), and the prerequisite structure of the learning domain (\"knowledge mapping\"). While recent deep learning models achieve high KT accuracy, they do so at the expense of the interpretability of psychologically-inspired models. In this work, we present a solution to this trade-off. PSI-KT is a hierarchical generative approach that explicitly models how both individual cognitive traits and the prerequisite structure of knowledge influence learning dynamics, thus achieving interpretability by design. Moreover, by using scalable Bayesian inference, PSI-KT targets the real-world need for efficient personalization even with a growing body of learners and interaction data. Evaluated on three datasets from online learning platforms, PSI-KT achieves superior multi-step **p**redictive accuracy and **s**calable inference in continual-learning settings, all while providing **i**nterpretable representations of learner-specific traits and the prerequisite structure of knowledge that causally supports learning. In sum, predictive, scalable and interpretable knowledge tracing with solid knowledge mapping lays a key foundation for effective personalized learning to make education accessible to a broad, global audience.",
      "authors": [
        "Hanqi Zhou",
        "Robert Bamler",
        "Charley M Wu",
        "Álvaro Tejero-Cantero"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NgaLU2fP5D",
      "cdate": 1695501550757,
      "mdate": 1710446811982,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708897"
    },
    {
      "id": "pAVJKp3Dvn",
      "title": "Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks",
      "abstract": "This paper investigates efficient deep neural networks (DNNs) to replace dense unstructured weight matrices with structured ones that possess desired properties. The challenge arises because the optimal weight matrix structure in popular neural network models is obscure in most cases and may vary from layer to layer even in the same network. Prior structured matrices proposed for efficient DNNs were mostly hand-crafted without a generalized framework to systematically learn them. To address this issue, we propose a generalized and differentiable framework to learn efficient structures of weight matrices by gradient descent. We first define a new class of structured matrices that covers a wide range of structured matrices in the literature by adjusting the structural parameters. Then, the frequency-domain differentiable parameterization scheme based on the Gaussian-Dirichlet kernel is adopted to learn the structural parameters by proximal gradient descent. On the image and language tasks, our method learns efficient DNNs with structured matrices, achieving lower complexity and/or higher performance than prior approaches that employ low-rank, block-sparse, or block-low-rank matrices.",
      "authors": [
        "Changwoo Lee",
        "Hun-Seok Kim"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pAVJKp3Dvn",
      "cdate": 1695501390544,
      "mdate": 1709862594150,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708902"
    },
    {
      "id": "lJYAkDVnRU",
      "title": "Context-Aware Meta-Learning",
      "abstract": "Large Language Models like ChatGPT demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or fine-tuning on similar objects. In this work, we propose a meta-learning algorithm that emulates Large Language Models by learning new visual concepts during inference without fine-tuning. Our approach leverages a frozen pre-trained feature extractor, and analogous to in-context learning, recasts meta-learning as sequence modeling over datapoints with known labels and a test datapoint with an unknown label. On 8 out of 11 meta-learning benchmarks, our approach---without meta-training or fine-tuning---exceeds or matches the state-of-the-art algorithm, P>M>F, which is meta-trained on these benchmarks.",
      "authors": [
        "Christopher Fifty",
        "Dennis Duan",
        "Ronald Guenther Junkins",
        "Ehsan Amid",
        "Jure Leskovec",
        "Christopher Re",
        "Sebastian Thrun"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lJYAkDVnRU",
      "cdate": 1695500893164,
      "mdate": 1711145871136,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708906"
    },
    {
      "id": "caW7LdAALh",
      "title": "Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain",
      "abstract": "Code Large Language Models (Code LLMs) are being increasingly employed in real-life applications, so evaluating them is critical. While the conventional accuracy evaluates the performance of Code LLMs on a set of individual tasks, their self-consistency across different tasks is overlooked. Intuitively, a trustworthy model should be self-consistent when generating natural language specifications for its own code and generating code for its own specifications. Failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model. In this paper, we first formally define the self-consistency of Code LLMs and then design a framework, IdentityChain, which effectively and efficiently evaluates the self-consistency and conventional accuracy of a model at the same time. We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indeed a distinct aspect from conventional accuracy. Furthermore, we show that IdentityChain can be used as a model debugging tool to expose weaknesses of Code LLMs by demonstrating three major weaknesses that we identify in current models using IdentityChain. Our code is available at https://github.com/marcusm117/IdentityChain.",
      "authors": [
        "Marcus J. Min",
        "Yangruibo Ding",
        "Luca Buratti",
        "Saurabh Pujar",
        "Gail Kaiser",
        "Suman Jana",
        "Baishakhi Ray"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=caW7LdAALh",
      "cdate": 1695500416674,
      "mdate": 1713672457558,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708911"
    },
    {
      "id": "vngVydDWft",
      "title": "From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication",
      "abstract": "It has been observed that representations learned by distinct neural networks conceal structural similarities when the models are trained under similar inductive biases. From a geometric perspective, identifying the classes of transformations and the related invariances that connect these representations is fundamental to unlocking applications, such as merging, stitching, and reusing different neural modules. However, estimating task-specific transformations a priori can be challenging and expensive due to several factors (e.g., weights initialization, training hyperparameters, or data modality). To this end, we introduce a versatile method to directly incorporate a set of invariances into the representations, constructing a product space of invariant components on top of the latent representations without requiring prior knowledge about the optimal invariance to infuse. We validate our solution on classification and reconstruction tasks, observing consistent latent similarity and downstream performance improvements in a zero-shot stitching setting. The experimental analysis comprises three modalities (vision, text, and graphs), twelve pretrained foundational models, nine benchmarks, and several architectures trained from scratch.",
      "authors": [
        "Irene Cannistraci",
        "Luca Moschella",
        "Marco Fumero",
        "Valentino Maiorca",
        "Emanuele Rodolà"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vngVydDWft",
      "cdate": 1695500299591,
      "mdate": 1712838253917,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708916"
    },
    {
      "id": "TFKIfhvdmZ",
      "title": "Proximal Policy Gradient Arborescence for Quality Diversity Reinforcement Learning",
      "abstract": "Training generally capable agents that thoroughly explore their environment and\nlearn new and diverse skills is a long-term goal of robot learning. Quality Diversity\nReinforcement Learning (QD-RL) is an emerging research area that blends the\nbest aspects of both fields – Quality Diversity (QD) provides a principled form\nof exploration and produces collections of behaviorally diverse agents, while\nReinforcement Learning (RL) provides a powerful performance improvement\noperator enabling generalization across tasks and dynamic environments. Existing\nQD-RL approaches have been constrained to sample efficient, deterministic off-\npolicy RL algorithms and/or evolution strategies and struggle with highly stochastic\nenvironments. In this work, we, for the first time, adapt on-policy RL, specifically\nProximal Policy Optimization (PPO), to the Differentiable Quality Diversity (DQD)\nframework and propose several changes that enable efficient optimization and\ndiscovery of novel skills on high-dimensional, stochastic robotics tasks. Our new\nalgorithm, Proximal Policy Gradient Arborescence (PPGA), achieves state-of-\nthe-art results, including a 4x improvement in best reward over baselines on the\nchallenging humanoid domain.",
      "authors": [
        "Sumeet Batra",
        "Bryon Tjanaka",
        "Matthew Christopher Fontaine",
        "Aleksei Petrenko",
        "Stefanos Nikolaidis",
        "Gaurav S. Sukhatme"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=TFKIfhvdmZ",
      "cdate": 1695500286488,
      "mdate": 1711144766046,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708921"
    },
    {
      "id": "o2IEmeLL9r",
      "title": "Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning",
      "abstract": "Pre-training on task-agnostic large datasets is a promising approach for enhancing the sample efficiency of reinforcement learning (RL) in solving complex tasks. We present PTGM, a novel method that pre-trains goal-based models to augment RL by providing temporal abstractions and behavior regularization. PTGM involves pre-training a low-level, goal-conditioned policy and training a high-level policy to generate goals for subsequent RL tasks. To address the challenges posed by the high-dimensional goal space, while simultaneously maintaining the agent's capability to accomplish various skills, we propose clustering goals in the dataset to form a discrete high-level action space. Additionally, we introduce a pre-trained goal prior model to regularize the behavior of the high-level policy in RL, enhancing sample efficiency and learning stability. Experimental results in a robotic simulation environment and the challenging open-world environment of Minecraft demonstrate PTGM’s superiority in sample efficiency and task performance compared to baselines. Moreover, PTGM exemplifies enhanced interpretability and generalization of the acquired low-level skills.",
      "authors": [
        "Haoqi Yuan",
        "Zhancun Mu",
        "Feiyang Xie",
        "Zongqing Lu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=o2IEmeLL9r",
      "cdate": 1695500211085,
      "mdate": 1709661546710,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708925"
    },
    {
      "id": "MrR3rMxqqv",
      "title": "Memorization Capacity of Multi-Head Attention in Transformers",
      "abstract": "Transformers have become the go-to architecture for language and vision tasks, yet their theoretical properties, especially memorization capacity, remain elusive. This paper investigates the memorization abilities of multi-head attention mechanisms, examining how many example sequences they can memorize, as a function of the number of heads and sequence length. Motivated by experimental findings on vision transformers, we introduce novel assumptions about the linear independence of input data, distinct from the commonly used general-position assumption. Under these assumptions, we demonstrate that an attention layer with $H$ heads, dimension $d$, and context size $n < d,$ featuring $\\Theta(Hd^2)$ parameters, can memorize $\\Omega(Hn)$ examples. Our analysis sheds light on how different attention heads handle various example sequences, aided by the softmax operator’s saturation property. We validate our findings through experiments on synthetic data.",
      "authors": [
        "Sadegh Mahdavi",
        "Renjie Liao",
        "Christos Thrampoulidis"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MrR3rMxqqv",
      "cdate": 1695500128975,
      "mdate": 1709661546680,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708932"
    },
    {
      "id": "MEGQGNUfPx",
      "title": "The Effectiveness of Random Forgetting for Robust Generalization",
      "abstract": "Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network's robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called \"Forget to Mitigate Overfitting (FOMO)\". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model's information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last robust test accuracy while improving the state-of-the-art robustness. Furthermore, FOMO provides a better trade-off between the standard and robust accuracy outperforming baseline adversarial methods. Finally, our framework is robust to AutoAttacks and increases generalization in many real-world scenarios.",
      "authors": [
        "Vijaya Raghavan T Ramkumar",
        "Bahram Zonooz",
        "Elahe Arani"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MEGQGNUfPx",
      "cdate": 1695499587541,
      "mdate": 1709933588378,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708937"
    },
    {
      "id": "gxhRR8vUQb",
      "title": "Diffeomorphic Mesh Deformation via Efficient Optimal Transport for Cortical Surface Reconstruction",
      "abstract": "Mesh deformation plays a pivotal role in many 3D vision tasks including dynamic simulations, rendering, and reconstruction. However, defining an efficient discrepancy between predicted and target meshes remains an open problem. A prevalent approach in current deep learning is the set-based approach which measures the discrepancy between two surfaces by comparing two randomly sampled point-clouds from the two meshes with Chamfer pseudo-distance. Nevertheless, the set-based approach still has limitations such as lacking a theoretical guarantee for choosing the number of points in sampled point-clouds, and the pseudo-metricity and the quadratic complexity of the Chamfer divergence. To address these issues, we propose a novel metric for learning mesh deformation. The metric is defined by sliced Wasserstein distance on meshes represented as probability measures that generalize the set-based approach. By leveraging probability measure space, we gain flexibility in encoding meshes using diverse forms of probability measures, such as continuous, empirical, and discrete measures via \\textit{varifold} representation. After having encoded probability measures, we can compare meshes by using the sliced Wasserstein distance which is an effective optimal transport distance with linear computational complexity and can provide a fast statistical rate for approximating the surface of meshes. To the end, we employ a neural ordinary differential equation (ODE) to deform the input surface into the target shape by modeling the trajectories of the points on the surface. Our experiments on cortical surface reconstruction demonstrate that our approach surpasses other competing methods in multiple datasets and metrics.",
      "authors": [
        "Thanh Tung Le",
        "Khai Nguyen",
        "shanlin sun",
        "Kun Han",
        "Nhat Ho",
        "Xiaohui Xie"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=gxhRR8vUQb",
      "cdate": 1695499527190,
      "mdate": 1710655845822,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708942"
    },
    {
      "id": "p34fRKp8qA",
      "title": "Lie Group Decompositions for Equivariant Neural Networks",
      "abstract": "Invariance and equivariance to geometrical transformations have proven to be very useful inductive biases when training (convolutional) neural network models, especially in the low-data regime.\nMuch work has focused on the case where the symmetry group employed is compact or abelian, or both.\nRecent work has explored enlarging the class of transformations used to the case of Lie groups, principally through the use of their Lie algebra, as well as the group exponential and logarithm maps.\nThe applicability of such methods to larger transformation groups is limited by the fact that depending on the group of interest $G$, the exponential map may not be surjective.\nFurther limitations are encountered when $G$ is neither compact nor abelian.\nUsing the structure and geometry of Lie groups and their homogeneous spaces, we present a framework by which it is possible to work with such groups primarily focusing on the Lie groups $G = \\textnormal{GL}^{+}(n, \\mathbb{R})$ and $G = \\textnormal{SL}(n, \\mathbb{R})$, as well as their representation as affine transformations $\\mathbb{R}^{n} \\rtimes G$.\nInvariant integration as well as a global parametrization is realized by decomposing the \"larger\" groups into subgroups and submanifolds which can be handled individually.\nUnder this framework, we show how convolution kernels can be parametrized to build models equivariant with respect to affine transformations.\nWe evaluate the robustness and out-of-distribution generalisation capability of our model on the standard affine-invariant benchmark classification task, where we outperform all previous equivariant models as well as all Capsule Network proposals.",
      "authors": [
        "Mircea Mironenco",
        "Patrick Forré"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=p34fRKp8qA",
      "cdate": 1695499485016,
      "mdate": 1713357037596,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708946"
    },
    {
      "id": "UHjE5v5MB7",
      "title": "To Grok or not to Grok: Disentangling Generalization and Memorization on Corrupted Algorithmic Datasets",
      "abstract": "Robust generalization is a major challenge in deep learning, particularly when the number of trainable parameters is very large. In general, it is very difficult to know if the network has memorized a particular set of examples or understood the underlying rule (or both). Motivated by this challenge, we study an interpretable model where generalizing representations are understood analytically, and are easily distinguishable from the memorizing ones. Namely, we consider multi-layer perceptron (MLP) and Transformer architectures trained on modular arithmetic tasks, where ($\\xi \\cdot 100\\\\%$) of labels are corrupted (*i.e.* some results of the modular operations in the training set are incorrect). We show that (i) it is possible for the network to memorize the corrupted labels *and* achieve $100\\\\%$ generalization at the same time; (ii) the memorizing neurons can be identified and pruned, lowering the accuracy on corrupted data and improving the accuracy on uncorrupted data; (iii) regularization methods such as weight decay, dropout and BatchNorm force the network to ignore the corrupted data during optimization, and achieve $100\\\\%$ accuracy on the uncorrupted dataset; and (iv) the effect of these regularization methods is (\"mechanistically\") interpretable: weight decay and dropout force all the neurons to learn generalizing representations, while BatchNorm de-amplifies the output of memorizing neurons and amplifies the output of the generalizing ones. Finally, we show that in the presence of regularization, the training dynamics involves two consecutive stages: first, the network undergoes *grokking* dynamics reaching high train *and* test accuracy; second, it unlearns the memorizing representations, where the train accuracy suddenly jumps from $100\\\\%$ to $100 (1-\\xi)\\\\%$.",
      "authors": [
        "Darshil Doshi",
        "Aritra Das",
        "Tianyu He",
        "Andrey Gromov"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=UHjE5v5MB7",
      "cdate": 1695499457255,
      "mdate": 1709661546520,
      "matched_keywords": [
        "transformer",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.708951"
    },
    {
      "id": "SUUrkC3STJ",
      "title": "VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections",
      "abstract": "Graph transformer has been proven as an effective graph learning method for its adoption of attention mechanism that is capable of capturing expressive representations from complex topological and feature information of graphs. Graph transformer conventionally performs dense attention (or global attention) for every pair of nodes to learn node representation vectors, resulting in quadratic computational costs that are unaffordable for large-scale graph data. Therefore, mini-batch training for graph transformers is a promising direction, but limited samples in each mini-batch can not support effective dense attention to encode informative representations. Facing this bottleneck, (1) we start by assigning each node a token list that is sampled by personalized PageRank (PPR) and then apply standard multi-head self-attention only on this list to compute its node representations. This PPR tokenization method decouples model training from complex graph topological information and makes heavy feature engineering offline and independent, such that mini-batch training of graph transformers is possible by loading each node's token list in batches. We further prove this PPR tokenization is viable as a graph convolution network with a fixed polynomial filter and jumping knowledge. However, only using personalized PageRank may limit information carried by a token list, which could not support different graph inductive biases for model training. To this end, (2) we rewire graphs by introducing multiple types of virtual connections through structure- and content-based super nodes that enable PPR tokenization to encode local and global contexts, long-range interaction, and heterophilous information into each node's token list, and then formalize our $\\underline{\\textbf{V}}$irtual $\\underline{\\textbf{C}}$onnection $\\underline{\\textbf{R}}$anking based $\\underline{\\textbf{Graph}}$ Trans$\\underline{\\textbf{former}}$ (VCR-Graphormer). Overall, VCR-Graphormer needs $O(m+klogk)$ complexity for graph tokenization as compared to $O(n^{3})$ of previous works. The [code](https://github.com/DongqiFu/VCR-Graphormer) is provided.",
      "authors": [
        "Dongqi Fu",
        "Zhigang Hua",
        "Yan Xie",
        "Jin Fang",
        "Si Zhang",
        "Kaan Sancak",
        "Hao Wu",
        "Andrey Malevich",
        "Jingrui He",
        "Bo Long"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=SUUrkC3STJ",
      "cdate": 1695499020263,
      "mdate": 1710564552644,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708955"
    },
    {
      "id": "pEGSdJu52I",
      "title": "On the Variance of Neural Network Training with respect to Test Sets and Distributions",
      "abstract": "Neural network trainings are stochastic, causing the performance of trained networks to vary across repeated runs of training.\nWe contribute the following results towards understanding this variation.\n(1) Despite having significant variance on their test-sets, we demonstrate that standard CIFAR-10 and ImageNet trainings have little variance in their performance on the test-distributions from which their test-sets are sampled.\n(2) We introduce the independent errors assumption and show that it suffices to recover the structure and variance of the empirical accuracy distribution across repeated runs of training.\n(3) We prove that test-set variance is unavoidable given the observation that ensembles of identically trained networks are calibrated (Jiang et al., 2021), and demonstrate that the variance of binary classification trainings closely follows a simple formula based on the error rate and number of test examples.\n(4) We conduct preliminary studies of data augmentation, learning rate, finetuning instability and distribution-shift through the lens of variance between runs.",
      "authors": [
        "Keller Jordan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pEGSdJu52I",
      "cdate": 1695498608181,
      "mdate": 1713672214106,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708960"
    },
    {
      "id": "fpoAYV6Wsk",
      "title": "Circuit Component Reuse Across Tasks in Transformer Language Models",
      "abstract": "Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in (Wang, 2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito & Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment, in which we adjust four attention heads in middle layers in order to ‘repair’ the Colored Objects circuit and make it behave like the IOI circuit. In doing so, we boost accuracy from 49.6% to 93.7% on the Colored Objects task and explain most sources of error. The intervention affects downstream attention heads in specific ways predicted by their interactions in the IOI circuit, indicating that this subcircuit behavior is invariant to the different task inputs. Overall, our results provide evidence that it may yet be possible to explain large language models' behavior in terms of a relatively small number of interpretable task-general algorithmic building blocks and computational components.",
      "authors": [
        "Jack Merullo",
        "Carsten Eickhoff",
        "Ellie Pavlick"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fpoAYV6Wsk",
      "cdate": 1695498403101,
      "mdate": 1710549167531,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.708964"
    },
    {
      "id": "OOxotBmGol",
      "title": "Large Language Models to Enhance Bayesian Optimization",
      "abstract": "Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance remains a delicate process. In this light, we present \\texttt{LLAMBO}, a novel approach that integrates the capabilities of Large Language Models (LLM) within BO. At a high level, we frame the BO problem in natural language, enabling LLMs to iteratively \\emph{propose} and \\emph{evaluate} promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can improve model-based BO. Our findings illustrate that \\texttt{LLAMBO} is effective at zero-shot warmstarting, and enhances surrogate modeling and candidate sampling, especially in the early stages of search when observations are sparse. Our approach is performed in context and does not require LLM finetuning. Additionally, it is modular by design, allowing individual components to be integrated into existing BO frameworks, or function cohesively as an end-to-end method. We empirically validate \\texttt{LLAMBO}'s efficacy on the problem of hyperparameter tuning, highlighting strong empirical performance across a range of diverse benchmarks, proprietary, and synthetic tasks.",
      "authors": [
        "Tennison Liu",
        "Nicolás Astorga",
        "Nabeel Seedat",
        "Mihaela van der Schaar"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OOxotBmGol",
      "cdate": 1695498093945,
      "mdate": 1710413250732,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708969"
    },
    {
      "id": "LSYhE2hLWG",
      "title": "SineNet: Learning Temporal Dynamics in Time-Dependent Partial Differential Equations",
      "abstract": "We consider using deep neural networks to solve time-dependent partial differential equations (PDEs), where multi-scale processing is crucial for modeling complex, time-evolving dynamics. While the U-Net architecture with skip connections is commonly used by prior studies to enable multi-scale processing, our analysis shows that the need for features to evolve across layers results in temporally misaligned features in skip connections, which limits the model’s performance. To address this limitation, we propose SineNet, consisting of multiple sequentially connected U-shaped network blocks, referred to as waves. In SineNet, high-resolution features are evolved progressively through multiple stages, thereby reducing the amount of misalignment within each stage. We furthermore analyze the role of skip connections in enabling both parallel and sequential processing of multi-scale information. Our method is rigorously tested on multiple PDE datasets, including the Navier-Stokes equations and shallow water equations, showcasing the advantages of our proposed approach over conventional U-Nets with a comparable parameter budget. We further demonstrate that increasing the number of waves in SineNet while maintaining the same number of parameters leads to a monotonically improved performance. The results highlight the effectiveness of SineNet and the potential of our approach in advancing the state-of-the-art in neural PDE solver design. Our code is available as part of AIRS (https://github.com/divelab/AIRS).",
      "authors": [
        "Xuan Zhang",
        "Jacob Helwig",
        "Yuchao Lin",
        "Yaochen Xie",
        "Cong Fu",
        "Stephan Wojtowytsch",
        "Shuiwang Ji"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LSYhE2hLWG",
      "cdate": 1695497134947,
      "mdate": 1711480645541,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708973"
    },
    {
      "id": "WIzzXCVYiH",
      "title": "GNNBoundary: Towards Explaining Graph Neural Networks through the Lens of Decision Boundaries",
      "abstract": "While Graph Neural Networks (GNNs) have achieved remarkable performance on various machine learning tasks on graph data, they also raised questions regarding their transparency and interpretability. Recently, there have been extensive research efforts to explain the decision-making process of GNNs. These efforts often focus on explaining why a certain prediction is made for a particular instance, or what discriminative features the GNNs try to detect for each class. However, to the best of our knowledge, there is no existing study on understanding the decision boundaries of GNNs, even though the decision-making process of GNNs is directly determined by the decision boundaries. To bridge this research gap, we propose a model-level explainability method called GNNBoundary, which attempts to gain deeper insights into the decision boundaries of graph classifiers. Specifically, we first develop an algorithm to identify the pairs of classes whose decision regions are adjacent. For an adjacent class pair, the near-boundary graphs between them are effectively generated by optimizing a novel objective function specifically designed for boundary graph generation. Thus, by analyzing the nearboundary graphs, the important characteristics of decision boundaries can be uncovered. To evaluate the efficacy of GNNBoundary, we conduct experiments on both synthetic and public real-world datasets. The results demonstrate that, via the analysis of faithful near-boundary graphs generated by GNNBoundary, we can thoroughly assess the robustness and generalizability of the explained GNNs. The official implementation can be found at https://github.com/yolandalalala/GNNBoundary.",
      "authors": [
        "Xiaoqi Wang",
        "Han Wei Shen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=WIzzXCVYiH",
      "cdate": 1695496983024,
      "mdate": 1710651893072,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708980"
    },
    {
      "id": "0jsfesDZDq",
      "title": "Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN",
      "abstract": "Recurrent Spiking Neural Networks (RSNNs) have emerged as a computationally efficient and brain-inspired machine learning model. The design of sparse RSNNs with fewer neurons and synapses helps reduce the computational complexity of RSNNs. Traditionally, sparse SNNs are obtained by first training a dense and complex SNN for a target task and, next, eliminating neurons with low activity (activity-based pruning) while maintaining task performance. In contrast, this paper presents a task-agnostic methodology for designing sparse RSNNs by pruning an untrained (arbitrarily initialized) large model. \nWe introduce a novel Lyapunov Noise Pruning (LNP) algorithm that uses graph sparsification methods and utilizes Lyapunov exponents to design a stable sparse RSNN from an untrained RSNN. We show that the LNP can leverage diversity in neuronal timescales to design a sparse Heterogeneous RSNN (HRSNN). Further, we show that the same sparse HRSNN model can be trained for different tasks, such as image classification and time-series prediction. The experimental results show that, in spite of being task-agnostic, LNP increases computational efficiency (fewer neurons and synapses) and prediction performance of RSNNs compared to traditional activity-based pruning of trained dense models.",
      "authors": [
        "Biswadeep Chakraborty",
        "Beomseok Kang",
        "Harshit Kumar",
        "Saibal Mukhopadhyay"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0jsfesDZDq",
      "cdate": 1695496666852,
      "mdate": 1709661545861,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708985"
    },
    {
      "id": "1YO4EE3SPB",
      "title": "A Variational Perspective on Solving Inverse Problems with Diffusion Models",
      "abstract": "Diffusion models have emerged as a key pillar of foundation models in visual domains. One of their critical applications is to universally solve different downstream inverse tasks via a single diffusion prior without re-training for each task. Most inverse tasks can be formulated as inferring a posterior distribution over data (e.g., a full image) given a measurement (e.g., a masked image). This is however challenging in diffusion models since the nonlinear and iterative nature of the diffusion process renders the posterior intractable. To cope with this challenge, we propose a variational approach that by design seeks to approximate the true posterior distribution. We show that our approach naturally leads to regularization by denoising diffusion process (RED-diff) where denoisers at different timesteps concurrently impose different structural constraints over the image. To gauge the contribution of denoisers from different timesteps, we propose a weighting mechanism based on signal-to-noise-ratio (SNR). Our approach provides a new variational perspective for solving inverse problems with diffusion models, allowing us to formulate sampling as stochastic optimization, where one can simply apply off-the-shelf solvers with lightweight iterates. Our experiments for various linear and nonlinear image restoration tasks demonstrate the strengths of our method compared with state-of-the-art sampling-based diffusion models. The code is available online \\footnote{\\url{https://github.com/NVlabs/RED-diff}}.",
      "authors": [
        "Morteza Mardani",
        "Jiaming Song",
        "Jan Kautz",
        "Arash Vahdat"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=1YO4EE3SPB",
      "cdate": 1695496338348,
      "mdate": 1713147164535,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708990"
    },
    {
      "id": "vqIH0ObdqL",
      "title": "Can Large Language Models Infer Causation from Correlation?",
      "abstract": "Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 200K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize – they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. Corr2Cause is a challenging task for LLMs, and can be helpful in guiding future research on improving LLMs’ pure reasoning skills and generalizability. Our data is at https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at https://github.com/causalNLP/corr2cause.",
      "authors": [
        "Zhijing Jin",
        "Jiarui Liu",
        "Zhiheng LYU",
        "Spencer Poff",
        "Mrinmaya Sachan",
        "Rada Mihalcea",
        "Mona T. Diab",
        "Bernhard Schölkopf"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vqIH0ObdqL",
      "cdate": 1695496293454,
      "mdate": 1713162676710,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.708994"
    },
    {
      "id": "Of2nEDc4s7",
      "title": "Improved statistical and computational complexity of the mean-field Langevin dynamics under structured data",
      "abstract": "Recent works have shown that neural networks optimized by gradient-based methods can adapt to sparse or low-dimensional target functions through feature learning; an often studied target is the sparse parity function on the unit hypercube. However, such isotropic data setting does not capture the anisotropy and low intrinsic dimensionality exhibited in realistic datasets. In this work, we address this shortcoming by studying how gradient-based feature learning interacts with structured (anisotropic) input data: we consider the classification of $k$-sparse parity on high-dimensional orthotope where the feature coordinates have varying magnitudes, and analyze the learning complexity of the mean-field Langevin dynamics (MFLD), which describes the noisy gradient descent update on two-layer neural network. We show that the statistical complexity (i.e. sample size) and computational complexity (i.e. network width) of MFLD can both be improved when prominent directions of the anisotropic input data align with the support of the target function. Moreover, by employing a coordinate transform determined by the gradient covariance, the width can be made independent of the target degree $k$. Lastly, we demonstrate the benefit of feature learning by establishing a kernel lower bound on the classification error, which applies to neural networks in the lazy regime.",
      "authors": [
        "Atsushi Nitanda",
        "Kazusato Oko",
        "Taiji Suzuki",
        "Denny Wu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Of2nEDc4s7",
      "cdate": 1695496052320,
      "mdate": 1713672669651,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.708999"
    },
    {
      "id": "jX2DT7qDam",
      "title": "Jointly-Learned Exit and Inference for a Dynamic Neural Network",
      "abstract": "Large pretrained models, coupled with fine-tuning, are slowly becoming established as the dominant architecture in machine learning. Even though these models offer impressive performance, their practical application is often limited by the prohibitive amount of resources required for $\\textit{every}$ inference. Early-exiting dynamic neural networks (EDNN) circumvent this issue by allowing a model to make some of its predictions from intermediate layers (i.e., early-exit). Training an EDNN architecture is challenging as it consists of two intertwined components: the gating mechanism (GM) that controls early-exiting decisions and the intermediate inference modules (IMs) that perform inference from intermediate representations. As a result, most existing approaches rely on thresholding confidence metrics for the gating mechanism and strive to improve the underlying backbone network and the inference modules. Although successful, this approach has two fundamental shortcomings: 1) the GMs and the IMs are decoupled during training, leading to a train-test mismatch; and 2) the thresholding gating mechanism introduces a positive bias into the predictive probabilities, making it difficult to readily extract uncertainty information. We propose a novel architecture that connects these two modules. This leads to significant performance improvements on classification datasets and enables better uncertainty characterization capabilities.",
      "authors": [
        "florence regol",
        "Joud Chataoui",
        "Mark Coates"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jX2DT7qDam",
      "cdate": 1695495477158,
      "mdate": 1710792201855,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709004"
    },
    {
      "id": "rtl4XnJYBh",
      "title": "Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift",
      "abstract": "Recently, multimodal contrastive learning (MMCL) approaches, such as CLIP, have achieved a remarkable success in learning representations that are robust against distribution shift and generalize to new domains. Despite the empirical success, the mechanism behind learning such generalizable representations is not understood. In this work, we rigorously analyze this problem and \nuncover two mechanisms behind MMCL's robustness: \\emph{intra-class contrasting}, which allows the model to learn features with a high variance, and \\emph{inter-class feature sharing}, where annotated details in one class help learning other classes better. Both mechanisms prevent spurious features that are over-represented in the training data to overshadow the generalizable core features. This yields superior zero-shot classification accuracy under distribution shift. Furthermore, we theoretically demonstrate the benefits of using rich captions on robustness and explore the effect of annotating different types of details in the captions. We validate our theoretical findings through experiments, including a well-designed synthetic experiment and an experiment involving training CLIP models on MSCOCO/Conceptual Captions and evaluating them on shifted ImageNets.",
      "authors": [
        "Yihao Xue",
        "Siddharth Joshi",
        "Dang Nguyen",
        "Baharan Mirzasoleiman"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rtl4XnJYBh",
      "cdate": 1695495340902,
      "mdate": 1710571536476,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.709008"
    },
    {
      "id": "LfmZh91tDI",
      "title": "Layer-wise linear mode connectivity",
      "abstract": "Averaging neural network parameters is an intuitive method for fusing the knowledge of two independent models. It is most prominently used in federated learning. If models are averaged at the end of training, this can only lead to a good performing model if the loss surface of interest is very particular, i.e., the loss in the midpoint between the two models needs to be sufficiently low. This is impossible to guarantee for the non-convex losses of state-of-the-art networks. For averaging models trained on vastly different datasets, it was proposed to average only the parameters of particular layers or combinations of layers, resulting in better performing models. To get a better understanding of the effect of layer-wise averaging, we analyse the performance of the models that result from averaging single layers, or groups of layers. Based on our empirical and theoretical investigation, we introduce a novel notion of the layer-wise linear connectivity, and show that deep networks do not have layer-wise barriers between them.",
      "authors": [
        "Linara Adilova",
        "Maksym Andriushchenko",
        "Michael Kamp",
        "Asja Fischer",
        "Martin Jaggi"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LfmZh91tDI",
      "cdate": 1695495229128,
      "mdate": 1710851522100,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709013"
    },
    {
      "id": "h05eQniJsQ",
      "title": "Understanding Certified Training with Interval Bound Propagation",
      "abstract": "As robustness verification methods are becoming more precise, training certifiably robust neural networks is becoming ever more relevant. To this end, certified training methods compute and then optimize an upper bound on the worst-case loss over a robustness specification. Curiously, training methods based on the imprecise interval bound propagation (IBP) consistently outperform those leveraging more precise bounds. Still, we lack a theoretical understanding of the mechanisms making IBP so successful. In this work, we investigate these mechanisms by leveraging a novel metric measuring the tightness of IBP bounds. We first show theoretically that, for deep linear models (DLNs), tightness decreases with width and depth at initialization, but improves with IBP training. We, then, derive sufficient and necessary conditions on weight matrices for IBP bounds to become exact and demonstrate that these impose strong regularization, providing an explanation for the observed robustness-accuracy trade-off. Finally, we show how these results on DLNs transfer to ReLU networks, before conducting an extensive empirical study, (i) confirming this transferability and yielding state-of-the-art certified accuracy, (ii) finding that while all IBP-based training methods lead to high tightness, this increase is dominated by the size of the propagated input regions rather than the robustness specification, and finally (iii) observing that non-IBP-based methods do not increase tightness. Together, these results help explain the success of recent certified training methods and may guide the development of new ones.",
      "authors": [
        "Yuhao Mao",
        "Mark Niklas Mueller",
        "Marc Fischer",
        "Martin Vechev"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=h05eQniJsQ",
      "cdate": 1695495162964,
      "mdate": 1709661545389,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709017"
    },
    {
      "id": "GnOLWS4Llt",
      "title": "Offline RL with Observation Histories: Analyzing and Improving Sample Complexity",
      "abstract": "Offline reinforcement learning (RL) can in principle synthesize more optimal behavior from a dataset consisting only of suboptimal trials. One way that this can happen is by \"stitching\" together the best parts of otherwise suboptimal trajectories that overlap on similar states, to create new behaviors where each individual state is in-distribution, but the overall returns are higher. However, in many interesting and complex applications, such as autonomous navigation and dialogue systems, the state is partially observed. Even worse, the state representation is unknown or not easy to define. In such cases, policies and value functions are often conditioned on observation histories instead of states. In these cases, it is not clear if the same kind of \"stitching\" is feasible at the level of observation histories, since two different trajectories would always have different histories, and thus \"similar states\" that might lead to effective stitching cannot be leveraged.  Theoretically, we show that standard offline RL algorithms conditioned on observation histories suffer from poor sample complexity, in accordance with the above intuition. We then identify sufficient conditions under which offline RL can still be efficient -- intuitively, it needs to learn a compact representation of history comprising only features relevant for action selection. We introduce a bisimulation loss that captures the extent to which this happens, and propose that offline RL can explicitly optimize this loss to aid worst-case sample complexity. Empirically, we show that across a variety of tasks either our proposed loss improves performance, or the value of this loss is already minimized as a consequence of standard offline RL, indicating that it correlates well with good performance.",
      "authors": [
        "Joey Hong",
        "Anca Dragan",
        "Sergey Levine"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GnOLWS4Llt",
      "cdate": 1695495078001,
      "mdate": 1710563298161,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709022"
    },
    {
      "id": "CAqdG2dy5s",
      "title": "Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations",
      "abstract": "Virtual sensing techniques allow for inferring signals at new unmonitored locations by exploiting spatio-temporal measurements coming from physical sensors at different locations. However, as the sensor coverage becomes sparse due to costs or other constraints, physical proximity cannot be used to support interpolation. In this paper, we overcome this challenge by leveraging dependencies between the target variable and a set of correlated variables (covariates) that can frequently be associated with each location of interest. From this viewpoint, covariates provide partial observability, and the problem consists of inferring values for unobserved channels by exploiting observations at other locations to learn how such variables can correlate. We introduce a novel graph-based methodology to exploit such relationships and design a graph deep learning architecture, named GgNet, implementing the framework. The proposed approach relies on propagating information over a nested graph structure that is used to learn dependencies between variables as well as locations. GgNet is extensively evaluated under different virtual sensing scenarios, demonstrating higher reconstruction accuracy compared to the state-of-the-art.",
      "authors": [
        "Giovanni De Felice",
        "Andrea Cini",
        "Daniele Zambon",
        "Vladimir Gusev",
        "Cesare Alippi"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=CAqdG2dy5s",
      "cdate": 1695494838895,
      "mdate": 1709661545302,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709032"
    },
    {
      "id": "pCEgna6Qco",
      "title": "Two-stage LLM Fine-tuning with Less Specialization and More Generalization",
      "abstract": "Pretrained large language models (LLMs) are general purpose problem solvers applicable to a diverse set of tasks with prompts. They can be further improved towards a specific task by fine-tuning on a specialized dataset. However, fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances, which is undesirable whenever the fine-tuned model needs to handle additional tasks where no fine-tuning data is available. \nIn this work, we first demonstrate that fine-tuning on a single task indeed decreases LLMs' general in-context learning performance. We discover one important cause of such forgetting, format specialization, where the model overfits to the format of the fine-tuned task.We further show that format specialization happens at the very beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that reduces format specialization and improves generalization.ProMoT offloads task-specific format learning into additional and removable parameters by first doing prompt tuning and then fine-tuning the model itself with this soft prompt attached. \nWith experiments on several fine-tuning tasks and 8 in-context evaluation tasks, we show that ProMoT achieves comparable performance on fine-tuned tasks to standard fine-tuning, but with much less loss of in-context learning performances across a board range of  out-of-domain evaluation tasks. More importantly, ProMoT can even enhance generalization on in-context learning tasks that are semantically related to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly improves performance on other language pairs, and ProMoT on NLI improves performance on summarization.\nExperiments also show that ProMoT can improve the generalization performance of  multi-task training.",
      "authors": [
        "Yihan Wang",
        "Si Si",
        "Daliang Li",
        "Michal Lukasik",
        "Felix Yu",
        "Cho-Jui Hsieh",
        "Inderjit S Dhillon",
        "Sanjiv Kumar"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pCEgna6Qco",
      "cdate": 1695494655424,
      "mdate": 1710280324163,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709037"
    },
    {
      "id": "5RielfrDkP",
      "title": "Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network",
      "abstract": "Graph Neural Networks are popular tools in graph representation learning that capture the graph structural properties. However, most GNNs employ single-resolution graph feature extraction, thereby failing to capture micro-level local patterns (high resolution) and macro-level graph cluster and community patterns (low resolution) simultaneously. Many multiresolution methods have been developed to capture graph patterns at multiple scales, but most of them depend on predefined and handcrafted multiresolution transforms that remain fixed throughout the training process once formulated. Due to variations in graph instances and distributions, fixed handcrafted transforms can not effectively tailor multiresolution representations to each graph instance. To acquire multiresolution representation suited to different graph instances and distributions, we introduce the Multiresolution Meta-Framelet-based Graph Convolutional Network (MM-FGCN), facilitating comprehensive and adaptive multiresolution analysis across diverse graphs. Extensive experiments demonstrate that our MM-FGCN achieves SOTA performance on various graph learning tasks.",
      "authors": [
        "Tianze Luo",
        "Zhanfeng Mo",
        "Sinno Jialin Pan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5RielfrDkP",
      "cdate": 1695493914551,
      "mdate": 1710323520866,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709042"
    },
    {
      "id": "fgKjiVrm6u",
      "title": "REFACTOR: Learning to Extract Theorems from Proofs",
      "abstract": "Human mathematicians are often good at recognizing modular and reusable theorems that make complex mathematical results within reach. In this paper, we propose a novel method called theoREm-from-prooF extrACTOR (REFACTOR) for training neural networks to mimic this ability in formal mathematical theorem proving. We show on a set of unseen proofs, REFACTOR is able to extract 19.6\\% of the theorems that humans would use to write the proofs. When applying the model to the existing Metamath library, REFACTOR extracted 16 new theorems. With newly extracted theorems, we show that the existing proofs in the MetaMath database can be refactored. The new theorems are used very frequently after refactoring, with an average usage of 733.5 times, and help shorten the proof lengths. Lastly, we demonstrate that the prover trained on the new-theorem refactored dataset proves more test theorems and outperforms state-of-the-art baselines by frequently leveraging a diverse set of newly extracted theorems. Code can be found at https://github.com/jinpz/refactor.",
      "authors": [
        "Jin Peng Zhou",
        "Yuhuai Wu",
        "Qiyang Li",
        "Roger Baker Grosse"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fgKjiVrm6u",
      "cdate": 1695493599730,
      "mdate": 1710469046882,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709046"
    },
    {
      "id": "zWqr3MQuNs",
      "title": "Detecting Pretraining Data from Large Language Models",
      "abstract": "Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method MIN-K PROB based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. MIN-K PROB can be applied without any knowledge about the pretrainig corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that MIN-K PROB achieves a 7.4% improvement on WIKIMIA over these previous methods. We apply MIN-K PROB to two real-world scenarios, copyrighted book detection and contaminated downstream example detection, and find that it to be a consistently effective solution.",
      "authors": [
        "Weijia Shi",
        "Anirudh Ajith",
        "Mengzhou Xia",
        "Yangsibo Huang",
        "Daogao Liu",
        "Terra Blevins",
        "Danqi Chen",
        "Luke Zettlemoyer"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=zWqr3MQuNs",
      "cdate": 1695493332007,
      "mdate": 1710474796746,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709051"
    },
    {
      "id": "V5tdi14ple",
      "title": "Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization",
      "abstract": "Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code --- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting --- the previously best method to identify correct answers, by more than 12\\% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes. The code can be found at https://github.com/jinpz/dtv.",
      "authors": [
        "Jin Peng Zhou",
        "Charles E Staats",
        "Wenda Li",
        "Christian Szegedy",
        "Kilian Q Weinberger",
        "Yuhuai Wu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=V5tdi14ple",
      "cdate": 1695493299993,
      "mdate": 1710468538276,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709056"
    },
    {
      "id": "lR3rk7ysXz",
      "title": "On Diffusion Modeling for Anomaly Detection",
      "abstract": "Known for their impressive performance in generative modeling, diffusion models are attractive candidates for density-based anomaly detection. This paper investigates different variations of diffusion modeling for unsupervised and semi-supervised anomaly detection. In particular, we find that Denoising Diffusion Probability Models (DDPM) are performant on anomaly detection benchmarks yet computationally expensive. By simplifying DDPM in application to anomaly detection, we are naturally led to an alternative approach called Diffusion Time Estimation (DTE). DTE estimates the distribution over diffusion time for a given input and uses the mode or mean of this distribution as the anomaly score. We derive an analytical form for this density and leverage a deep neural network to improve inference efficiency. Through empirical evaluations on the ADBench benchmark, we demonstrate that all diffusion-based anomaly detection methods perform competitively for both semi-supervised and unsupervised settings. Notably, DTE achieves orders of magnitude faster inference time than DDPM, while outperforming it on this benchmark. These results establish diffusion-based anomaly detection as a scalable alternative to traditional methods and recent deep-learning techniques for standard unsupervised and semi-supervised anomaly detection settings.",
      "authors": [
        "Victor Livernoche",
        "Vineet Jain",
        "Yashar Hezaveh",
        "Siamak Ravanbakhsh"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lR3rk7ysXz",
      "cdate": 1695493283945,
      "mdate": 1710453603103,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709060"
    },
    {
      "id": "v3K5TVP8kZ",
      "title": "AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ",
      "abstract": "Generating bitmap graphics from text has gained considerable attention, yet for scientific figures, vector graphics are often preferred. Given that vector graphics are typically encoded using low-level graphics primitives, generating them directly is difficult. To address this, we propose the use of TikZ, a well-known abstract graphics language that can be compiled to vector graphics, as an intermediate representation of scientific figures. TikZ offers human-oriented, high-level commands, thereby facilitating conditional language modeling with any large language model. To this end, we introduce DaTikZ the first large-scale TikZ dataset, consisting of 120k TikZ drawings aligned with captions. We fine-tune LLaMA on DaTikZ, as well as our new model CLiMA, which augments LLaMA with multimodal CLIP embeddings. In both human and automatic evaluation, CLiMA and LLaMA outperform commercial GPT-4 and Claude 2 in terms of similarity to human-created figures, with CLiMA additionally improving text-image alignment. Our detailed analysis shows that all models generalize well and are not susceptible to memorization. GPT-4 and Claude 2, however, tend to generate more simplistic figures compared to both humans and our models. We make our framework, AutomaTikZ, along with model weights and datasets, publicly available.",
      "authors": [
        "Jonas Belouadi",
        "Anne Lauscher",
        "Steffen Eger"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=v3K5TVP8kZ",
      "cdate": 1695492955383,
      "mdate": 1710501687091,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.709067"
    },
    {
      "id": "3xDaj4pRna",
      "title": "Sharpness-Aware Minimization Enhances Feature Quality via Balanced Learning",
      "abstract": "Sharpness-Aware Minimization (SAM) has emerged as a promising alternative optimizer to stochastic gradient descent (SGD). The originally-proposed motivation behind SAM was to bias neural networks towards flatter minima that are believed to generalize better. However, recent studies have shown conflicting evidence on the relationship between flatness and generalization, suggesting that flatness does fully explain SAM's success. Sidestepping this debate, we identify an orthogonal effect of SAM that is beneficial out-of-distribution: we argue that SAM implicitly balances the quality of diverse features. SAM achieves this effect by adaptively suppressing well-learned features which gives remaining features opportunity to be learned. We show that this mechanism is beneficial in datasets that contain redundant or spurious features where SGD falls for the simplicity bias and would not otherwise learn all available features. Our insights are supported by experiments on real data: we demonstrate that SAM improves the quality of features in datasets containing redundant or spurious features, including CelebA, Waterbirds, CIFAR-MNIST, and DomainBed.",
      "authors": [
        "Jacob Mitchell Springer",
        "Vaishnavh Nagarajan",
        "Aditi Raghunathan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3xDaj4pRna",
      "cdate": 1695492783951,
      "mdate": 1713278101248,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709072"
    },
    {
      "id": "ccxD4mtkTU",
      "title": "Can LLM-Generated Misinformation Be Detected?",
      "abstract": "The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures.",
      "authors": [
        "Canyu Chen",
        "Kai Shu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ccxD4mtkTU",
      "cdate": 1695492699508,
      "mdate": 1713912789627,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709077"
    },
    {
      "id": "bkdWThqE6q",
      "title": "A Simple Interpretable Transformer for Fine-Grained Image Classification and Analysis",
      "abstract": "We present a novel usage of Transformers to make image classification interpretable. Unlike mainstream classifiers that wait until the last fully connected layer to incorporate class information to make predictions, we investigate a proactive approach, asking each class to search for itself in an image. We realize this idea via a Transformer encoder-decoder inspired by DEtection TRansformer (DETR). We learn ''class-specific'' queries (one for each class) as input to the decoder, enabling each class to localize its patterns in an image via cross-attention. We name our approach INterpretable TRansformer (INTR), which is fairly easy to implement and exhibits several compelling properties. We show that INTR intrinsically encourages each class to attend distinctively; the cross-attention weights thus provide a faithful interpretation of the prediction. Interestingly, via ''multi-head'' cross-attention, INTR could identify different ''attributes'' of a class, making it particularly suitable for fine-grained classification and analysis, which we demonstrate on eight datasets. Our code and pre-trained models are publicly accessible at the Imageomics Institute GitHub site: https://github.com/Imageomics/INTR.",
      "authors": [
        "DIPANJYOTI PAUL",
        "Arpita Chowdhury",
        "Xinqi Xiong",
        "Feng-Ju Chang",
        "David Edward Carlyn",
        "Samuel Stevens",
        "Kaiya L Provost",
        "Anuj Karpatne",
        "Bryan Carstens",
        "Daniel Rubenstein",
        "Charles Stewart",
        "Tanya Berger-Wolf",
        "Yu Su",
        "Wei-Lun Chao"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=bkdWThqE6q",
      "cdate": 1695492636310,
      "mdate": 1713928102418,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709084"
    },
    {
      "id": "hTEGyKf0dZ",
      "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
      "abstract": "Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open-source release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on customized datasets accelerate this trend. But, what are the safety costs associated with such customized fine-tuning? While existing safety alignment techniques restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing --- even if a model's initial safety alignment is impeccable, how can it be maintained after customized fine-tuning? We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the customized fine-tuning of aligned LLMs.  (This paper contains red-teaming data and model-generated content that can be offensive in nature.)",
      "authors": [
        "Xiangyu Qi",
        "Yi Zeng",
        "Tinghao Xie",
        "Pin-Yu Chen",
        "Ruoxi Jia",
        "Prateek Mittal",
        "Peter Henderson"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hTEGyKf0dZ",
      "cdate": 1695492383773,
      "mdate": 1710514437125,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709088"
    },
    {
      "id": "AfnsTnYphT",
      "title": "Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs",
      "abstract": "Vision tasks are characterized by the properties of locality and translation invariance. \n    The superior performance of convolutional neural networks (CNNs) on these tasks is widely attributed to the inductive bias of locality and weight sharing baked into their architecture.\n    Existing attempts to quantify the statistical benefits of these biases in CNNs over locally connected convolutional neural networks (LCNs) and fully connected neural networks (FCNs) fall into one of the following categories: either they disregard the optimizer and only provide uniform convergence upper bounds with no separating lower bounds, \n    or they consider simplistic tasks that do not truly mirror the locality and translation invariance as found in real-world vision tasks.\n    To address these deficiencies, we introduce the Dynamic Signal Distribution (DSD) classification task that models an image as consisting of $k$ patches, each of dimension $d$, and the label is determined by a $d$-sparse signal vector that can freely appear in any one of the $k$ patches. \n    On this task, for any orthogonally equivariant algorithm like gradient descent, we prove that CNNs require $\\tilde{O}(k+d)$ samples, whereas LCNs require $\\Omega(kd)$ samples, establishing the statistical advantages of weight sharing in translation invariant tasks. \n    Furthermore, LCNs need $\\tilde{O}(k(k+d))$ samples, compared to $\\Omega(k^2d)$ samples for FCNs, showcasing the benefits of locality in local tasks.\n    Additionally, we develop information theoretic tools for analyzing randomized algorithms, which may be of interest for statistical research.",
      "authors": [
        "Aakash Lahoti",
        "Stefani Karp",
        "Ezra Winston",
        "Aarti Singh",
        "Yuanzhi Li"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=AfnsTnYphT",
      "cdate": 1695492120030,
      "mdate": 1710558168859,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709093"
    },
    {
      "id": "0tWTxYYPnW",
      "title": "Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF",
      "abstract": "In practice, preference learning from human feedback depends on incomplete data with hidden context. Hidden context refers to data that affects the feedback received, but which is not represented in the data used to train a preference model. This captures common issues of data collection, such as having human annotators with varied preferences, cognitive processes that result in seemingly irrational behavior, and combining data labeled according to different criteria. We prove that standard applications of preference learning, including reinforcement learning from human feedback (RLHF), implicitly aggregate over hidden contexts according to a well-known voting rule called *Borda count*. We show this can produce counter-intuitive results that are very different from other methods which implicitly aggregate via expected utility. Furthermore, our analysis formalizes the way that preference learning from users with diverse values tacitly implements a social choice function. A key implication of this result is that annotators have an incentive to misreport their preferences in order to influence the learned model, leading to vulnerabilities in the deployment of RLHF. As a step towards mitigating these problems, we introduce a class of methods called *distributional preference learning* (DPL). DPL methods estimate a distribution of possible score values for each alternative in order to better account for hidden context. Experimental results indicate that applying DPL to RLHF for LLM chatbots identifies hidden context in the data and significantly reduces subsequent jailbreak vulnerability.",
      "authors": [
        "Anand Siththaranjan",
        "Cassidy Laidlaw",
        "Dylan Hadfield-Menell"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0tWTxYYPnW",
      "cdate": 1695491922409,
      "mdate": 1713673113043,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709098"
    },
    {
      "id": "kvByNnMERu",
      "title": "Estimating Shape Distances on Neural Representations with Limited Samples",
      "abstract": "Measuring geometric similarity between high-dimensional network representations is a topic of longstanding interest to neuroscience and deep learning. Although many methods have been proposed, only a few works have rigorously analyzed their statistical efficiency or quantified estimator uncertainty in data-limited regimes. Here, we derive upper and lower bounds on the worst-case convergence\nof standard estimators of shape distance—a measure of representational dissimilarity proposed by Williams et al. (2021). These bounds reveal the challenging nature of the problem in high-dimensional feature spaces. To overcome these challenges, we introduce a novel method-of-moments estimator with a tunable bias-variance tradeoff parameterized by an upper bound on bias. We show that this estimator achieves superior performance to standard estimators in simulation and on neural data, particularly in high-dimensional settings. Our theoretical work and estimator thus respectively define and dramatically expand the scope of neural data for which geometric similarity can be accurately measured.",
      "authors": [
        "Dean A Pospisil",
        "Brett W. Larsen",
        "Sarah E Harvey",
        "Alex H Williams"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kvByNnMERu",
      "cdate": 1695491790845,
      "mdate": 1710553053718,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709103"
    },
    {
      "id": "IEduRUO55F",
      "title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
      "abstract": "Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.",
      "authors": [
        "Yecheng Jason Ma",
        "William Liang",
        "Guanzhi Wang",
        "De-An Huang",
        "Osbert Bastani",
        "Dinesh Jayaraman",
        "Yuke Zhu",
        "Linxi Fan",
        "Anima Anandkumar"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=IEduRUO55F",
      "cdate": 1695491091791,
      "mdate": 1711583882575,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709110"
    },
    {
      "id": "dONpC9GL1o",
      "title": "Closing the Curious Case of Neural Text Degeneration",
      "abstract": "Despite their ubiquity in language generation, it remains unknown why truncation sampling heuristics like nucleus sampling are so effective. We provide a theoretical explanation for the effectiveness of the truncation sampling by proving that truncation methods that discard tokens below some probability threshold (the most common type of truncation) can guarantee that all sampled tokens have nonzero true probability. However, thresholds are a coarse heuristic, and necessarily discard some tokens with nonzero true probability as well. In pursuit of a more precise sampling strategy, we show that we can leverage a known source of model errors, the softmax bottleneck, to prove that certain tokens have nonzero true probability, without relying on a threshold. Based on our findings, we develop an experimental truncation strategy and the present pilot studies demonstrating the promise of this type of algorithm. Our evaluations show that our method outperforms its threshold-based counterparts under automatic and human evaluation metrics for low-entropy (i.e., close to greedy) open-ended text generation. Our theoretical findings and pilot experiments provide both insight into why truncation sampling works, and make progress toward more expressive sampling algorithms that better surface the generative capabilities of large language models.",
      "authors": [
        "Matthew Finlayson",
        "John Hewitt",
        "Alexander Koller",
        "Swabha Swayamdipta",
        "Ashish Sabharwal"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=dONpC9GL1o",
      "cdate": 1695490613268,
      "mdate": 1710518192298,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709115"
    },
    {
      "id": "VrHiF2hsrm",
      "title": "Understanding Catastrophic Forgetting in Language Models via Implicit Inference",
      "abstract": "We lack a systematic understanding of the effects of fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback), particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of capabilities on other tasks. We hypothesize that language models implicitly infer the task of the prompt and that fine-tuning skews this inference towards tasks in the fine-tuning distribution. To test this, we propose Conjugate Prompting, which artificially makes the task look farther from the fine-tuning distribution while requiring the same capability, and we find that this recovers some of the pretraining capabilities in our synthetic setup. Since real-world fine-tuning distributions are predominantly English, we apply conjugate prompting to recover pretrained capabilities in LLMs by simply translating the prompts to different languages. This allows us to recover in-context learning abilities lost via instruction tuning, natural reasoning capability lost during code fine-tuning, and, more concerningly, harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT.",
      "authors": [
        "Suhas Kotha",
        "Jacob Mitchell Springer",
        "Aditi Raghunathan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=VrHiF2hsrm",
      "cdate": 1695490389339,
      "mdate": 1713056885243,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709119"
    },
    {
      "id": "tbVWug9f2h",
      "title": "A Benchmark for Learning to Translate a New Language from One Grammar Book",
      "abstract": "Large language models (LLMs) can perform impressive feats with in-context learning or lightweight finetuning. It is natural to wonder how well these models adapt to genuinely new tasks, but how does one find tasks that are unseen in internet-scale training sets? We turn to a field that is explicitly motivated and bottlenecked by a scarcity of web data: low-resource languages. In this paper, we introduce MTOB (Machine Translation from One Book), a benchmark for learning to translate between English and Kalamang—a language with less than 200 speakers and therefore virtually no presence on the web—using several hundred pages of field linguistics reference materials. This task framing is novel in that it asks a model to learn a language from a single human-readable book of grammar explanations, rather than a large mined corpus of in-domain data, more akin to L2 language learning than L1 language acquisition. We demonstrate that baselines using current LLMs are promising but fall short of human performance, achieving 44.7 chrF on Kalamang to English translation and 45.8 chrF on English to Kalamang translation, compared to 51.6 and 57.0 chrF by a human who learned Kalamang from the same reference materials. We hope that MTOB will help measure LLM capabilities along a new dimension, and that the methods developed to solve it could help expand access to language technology for underserved communities by leveraging qualitatively different kinds of data than traditional machine translation.",
      "authors": [
        "Garrett Tanzer",
        "Mirac Suzgun",
        "Eline Visser",
        "Dan Jurafsky",
        "Luke Melas-Kyriazi"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tbVWug9f2h",
      "cdate": 1695488647415,
      "mdate": 1710501852937,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709124"
    },
    {
      "id": "HfXDrAzFvG",
      "title": "Novel Quadratic Constraints for Extending LipSDP beyond Slope-Restricted Activations",
      "abstract": "Recently, semidefinite programming (SDP) techniques have shown great promise in providing accurate Lipschitz bounds for neural networks. Specifically, the LipSDP approach (Fazlyab et al., 2019) has received much attention and provides the least conservative Lipschitz upper bounds that can be computed with polynomial time guarantees. However, one main restriction of LipSDP is that its formulation requires the activation functions to be slope-restricted on $[0,1]$, preventing its further use for more general activation functions such as GroupSort, MaxMin, and Householder. One can rewrite MaxMin activations for example as residual ReLU networks. However, a direct application of LipSDP to the resultant residual ReLU networks is conservative and even fails in recovering the well-known fact that the MaxMin activation is 1-Lipschitz. Our paper bridges this gap and extends LipSDP  beyond slope-restricted activation functions. To this end, we provide novel quadratic constraints for GroupSort, MaxMin, and Householder activations via leveraging their underlying properties such as sum preservation. Our proposed analysis is general and provides a unified approach for estimating $\\ell_2$ and $\\ell_\\infty$ Lipschitz bounds for a rich class of neural network architectures, including non-residual and residual neural networks and implicit models,  with GroupSort, MaxMin, and HouseHolder activations. Finally, we illustrate the utility of our approach with a variety of experiments and show that our proposed SDPs generate less conservative Lipschitz bounds in comparison to existing approaches.",
      "authors": [
        "Patricia Pauli",
        "Aaron J Havens",
        "Alexandre Araujo",
        "Siddharth Garg",
        "Farshad Khorrami",
        "Frank Allgöwer",
        "Bin Hu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=HfXDrAzFvG",
      "cdate": 1695488636128,
      "mdate": 1710450079101,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709128"
    },
    {
      "id": "HYyRwm367m",
      "title": "Neural Language of Thought Models",
      "abstract": "The Language of Thought Hypothesis suggests that human cognition operates on a structured, language-like system of mental representations. While neural language models can naturally benefit from the compositional structure inherently and explicitly expressed in language data, learning such representations from non-linguistic general observations, like images, remains a challenge. In this work, we introduce the Neural Language of Thought Model (NLoTM), a novel approach for unsupervised learning of LoTH-inspired representation and generation. NLoTM comprises two key components: (1) the Semantic Vector-Quantized Variational Autoencoder, which learns hierarchical, composable discrete representations aligned with objects and their properties, and (2) the Autoregressive LoT Prior, an autoregressive transformer that learns to generate semantic concept tokens compositionally, capturing the underlying data distribution. We evaluate NLoTM on several 2D and 3D image datasets, demonstrating superior performance in downstream tasks, out-of-distribution generalization, and image generation quality compared to patch-based VQ-VAE and continuous object-centric representations. Our work presents a significant step towards creating neural networks exhibiting more human-like understanding by developing LoT-like representations and offers insights into the intersection of cognitive science and machine learning.",
      "authors": [
        "Yi-Fu Wu",
        "Minseung Lee",
        "Sungjin Ahn"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=HYyRwm367m",
      "cdate": 1695487029042,
      "mdate": 1713672799301,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709137"
    },
    {
      "id": "0t1O8ziRZp",
      "title": "Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization",
      "abstract": "Logic synthesis, a pivotal stage in chip design, entails optimizing chip specifications encoded in hardware description languages like Verilog into highly efficient implementations using Boolean logic gates. The process involves a sequential application of logic minimization heuristics (``synthesis recipe\"), with their arrangement significantly impacting crucial metrics such as area and delay. Addressing the challenge posed by the broad spectrum of hardware design complexities — from variations of past designs (e.g., adders and multipliers) to entirely novel configurations (e.g., innovative processor instructions) — requires a nuanced 'synthesis recipe' guided by human expertise and intuition. This study conducts a thorough examination of learning and search techniques for logic synthesis, unearthing a surprising revelation: pre-trained agents, when confronted with entirely novel designs, may veer off course, detrimentally affecting the search trajectory. We present ABC-RL, a meticulously tuned $\\alpha$ parameter that adeptly adjusts recommendations from pre-trained agents during the search process. Computed based on similarity scores through nearest neighbor retrieval from the training dataset, ABC-RL yields superior synthesis recipes tailored for a wide array of hardware designs. Our findings showcase substantial enhancements in the Quality of Result (QoR) of synthesized circuits, boasting improvements of up to 24.8\\% compared to state-of-the-art techniques. Furthermore, ABC-RL achieves an impressive up to 9x reduction in runtime (iso-QoR) when compared to current state-of-the-art methodologies.",
      "authors": [
        "Animesh Basak Chowdhury",
        "Marco Romanelli",
        "Benjamin Tan",
        "Ramesh Karri",
        "Siddharth Garg"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0t1O8ziRZp",
      "cdate": 1695487002693,
      "mdate": 1710540982066,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709142"
    },
    {
      "id": "ztpy1gsUpT",
      "title": "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
      "abstract": "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
      "authors": [
        "Xinlu Zhang",
        "Shiyang Li",
        "Xianjun Yang",
        "Chenxin Tian",
        "Yao Qin",
        "Linda Ruth Petzold"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ztpy1gsUpT",
      "cdate": 1695486586687,
      "mdate": 1709661543228,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709146"
    },
    {
      "id": "jsvvPVVzwf",
      "title": "What Makes a Good Prune? Maximal Unstructured Pruning for Maximal Cosine Similarity",
      "abstract": "Pruning is an effective method to reduce the size of deep neural network models, maintain accuracy, and, in some cases, improve the network's overall performance. However, the mechanisms underpinning pruning remain unclear. Why can different methods prune by different percentages yet achieve similar performance? Why can we not prune at the start of training? Why are some models more amenable to being pruned than others? Given a model, what is the maximum amount it can be pruned before significantly affecting the performance? This paper explores and answers these questions from the global unstructured magnitude pruning perspective with one epoch of fine-tuning. We develop the idea that cosine similarity is an effective proxy measure for functional similarity between the parent and the pruned network. We prove that the L1 pruning method is optimal when pruning by cosine similarity. We show that the higher the kurtosis of a model's parameter distribution, the more it can be pruned while maintaining performance. Finally, we present a simple method to determine the optimal amount by which a network can be L1-pruned based on its parameter distribution. The code demonstrating the method is available at https://github.com/gmw99/what_makes_a_good_prune",
      "authors": [
        "Gabryel Mason-Williams",
        "Fredrik Dahlqvist"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jsvvPVVzwf",
      "cdate": 1695486433706,
      "mdate": 1713672311881,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709151"
    },
    {
      "id": "BqEvdOS1Hs",
      "title": "Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain",
      "abstract": "Existing game AI research mainly focuses on enhancing agents' abilities to win games, but this does not inherently make humans have a better experience when collaborating with these agents. For example, agents may dominate the collaboration and exhibit unintended or detrimental behaviors, leading to poor experiences for their human partners. In other words, most game AI agents are modeled in a \"self-centered\" manner. In this paper, we propose a \"human-centered\" modeling scheme for collaborative agents that aims to enhance the experience of humans. Specifically, we model the experience of humans as the goals they expect to achieve during the task. We expect that agents should learn to enhance the extent to which humans achieve these goals while maintaining agents' original abilities (e.g., winning games). To achieve this, we propose the Reinforcement Learning from Human Gain (RLHG) approach. The RLHG approach introduces a \"baseline\", which corresponds to the extent to which humans primitively achieve their goals, and encourages agents to learn behaviors that can effectively enhance humans in achieving their goals better. We evaluate the RLHG agent in the popular Multi-player Online Battle Arena (MOBA) game, Honor of Kings, by conducting real-world human-agent tests. Both objective performance and subjective preference results show that the RLHG agent provides participants better gaming experience.",
      "authors": [
        "Yiming Gao",
        "Feiyu Liu",
        "Liang Wang",
        "Dehua Zheng",
        "Zhenjie Lian",
        "Weixuan Wang",
        "Wenjin Yang",
        "Siqin Li",
        "Xianliang Wang",
        "Wenhui Chen",
        "Jing Dai",
        "QIANG FU",
        "Yang Wei",
        "Lanxiao Huang",
        "Wei Liu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BqEvdOS1Hs",
      "cdate": 1695486352627,
      "mdate": 1709661543199,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709156"
    },
    {
      "id": "PdaPky8MUn",
      "title": "Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors",
      "abstract": "Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, *using only the downstream task data*, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 absolute points. Subsequently, we analyze the utility of previously-proposed structured parameterizations for SSMs and show they become mostly redundant in the presence of data-driven initialization obtained through pretraining. Our work shows that, when evaluating different architectures on supervised tasks, incorporation of data-driven priors via pretraining is essential for reliable performance estimation, and can be done efficiently.",
      "authors": [
        "Ido Amos",
        "Jonathan Berant",
        "Ankit Gupta"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=PdaPky8MUn",
      "cdate": 1695485756547,
      "mdate": 1710436135709,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709161"
    },
    {
      "id": "U6Qulbv2qT",
      "title": "Provable Benefits of Multi-task RL under Non-Markovian Decision Making Processes",
      "abstract": "In multi-task reinforcement learning (RL) under Markov decision processes (MDPs), the presence of shared latent structures among multiple MDPs has been shown to yield significant benefits to the sample efficiency compared to single-task RL. In this paper, we investigate whether such a benefit can extend to more general sequential decision making problems such as predictive state representations (PSRs). The main challenge here is that the large and complex model space makes it hard to identify what types of common latent structure of multi-task PSRs can reduce the model complexity and improve sample efficiency.\nTo this end, we posit a  joint model class for tasks and use the notion of $\\eta$-bracketing number to quantify its complexity; this number also serves as a general metric  to capture the similarity of tasks and thus determines the benefit of multi-task over single-task RL. We first study  upstream multi-task learning over PSRs, in which all tasks share the same observation and action spaces. We propose a provably efficient algorithm  UMT-PSR for finding near-optimal policies for all PSRs, and demonstrate that the advantage of multi-task learning manifests if the joint model class of PSRs has a smaller $\\eta$-bracketing number compared to that of individual single-task learning. We further investigate downstream learning, in which the agent needs to learn a new target task that shares some commonalities with the upstream tasks via a similarity constraint. By exploiting the learned PSRs from the upstream, we develop a sample-efficient algorithm that provably finds a near-optimal policy. \nUpon specialization to some examples with small $\\eta$-bracketing numbers, our results further highlight the benefit compared to directly learning a single-task PSR.",
      "authors": [
        "Ruiquan Huang",
        "Yuan Cheng",
        "Jing Yang",
        "Vincent Tan",
        "Yingbin Liang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=U6Qulbv2qT",
      "cdate": 1695484935692,
      "mdate": 1710380540117,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709165"
    },
    {
      "id": "4r2ybzJnmN",
      "title": "Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings",
      "abstract": "Spiking Neural Networks (SNNs) are a promising research direction for building power-efficient information processing systems, especially for temporal tasks such as speech recognition. In SNNs, delays refer to the time needed for one spike to travel from one neuron to another. These delays matter because they influence the spike arrival times, and it is well-known that spiking neurons respond more strongly to coincident input spikes. More formally, it has been shown theoretically that plastic delays greatly increase the expressivity in SNNs. Yet, efficient algorithms to learn these delays have been lacking. Here, we propose a new discrete-time algorithm that addresses this issue in deep feedforward SNNs using backpropagation, in an offline manner. To simulate delays between consecutive layers, we use 1D convolutions across time. The kernels contain only a few non-zero weights – one per synapse – whose positions correspond to the delays. These positions are learned together with the weights using the recently proposed Dilated Convolution with Learnable Spacings (DCLS). We evaluated our method on three datasets: the Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands (SSC) and its non spiking version Google Speech Commands v0.02 (GSC) benchmarks, which require detecting temporal patterns. We used feedforward SNNs with two or three hidden fully connected layers, and vanilla leaky integrate-and-fire neurons. We showed that fixed random delays help and that learning them helps even more. Furthermore, our method outperformed the state-of-the-art in the three datasets without using recurrent connections and with substantially fewer parameters. Our work demonstrates the potential of delay learning in developing accurate and precise models for temporal data processing. Our code is based on PyTorch / SpikingJelly and available at: https://github.com/Thvnvtos/SNN-delays",
      "authors": [
        "Ilyass Hammouamri",
        "Ismail Khalfaoui-Hassani",
        "Timothée Masquelier"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4r2ybzJnmN",
      "cdate": 1695484891913,
      "mdate": 1709661542908,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709170"
    },
    {
      "id": "GPKTIktA0k",
      "title": "The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”",
      "abstract": "We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form ''_A_ is _B_'', it will not automatically generalize to the reverse direction ''_B_ is _A_''. This is the **Reversal Curse**. For instance, if a model is trained on ''Valentina Tereshkova was the first woman to travel to space'', it will not automatically be able to answer the question, ''Who was the first woman to travel to space?''. Moreover, the likelihood of the correct answer (''Valentina Tershkova'') will not be higher than for a random name. Thus, models do not generalize a prevalent pattern in their training set: if ''_A_ is _B_'' occurs, ''_B_ is _A_'' is more likely to occur. It is worth noting, however, that if ''_A_ is _B_'' appears _in-context_, models can deduce the reverse relationship. \n\nWe provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as ''Uriah Hawthorne is the composer of _Abyssal Melodies_'' and showing that they fail to correctly answer ''Who composed _Abyssal Melodies?_''. The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation.\n\nWe also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about real-world celebrities, such as ''Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]'' and the reverse ''Who is Mary Lee Pfeiffer's son?''. GPT-4 correctly answers questions like the former 79\\% of the time, compared to 33\\% for the latter.\n\n Code available at: https://github.com/lukasberglund/reversal_curse.",
      "authors": [
        "Lukas Berglund",
        "Meg Tong",
        "Maximilian Kaufmann",
        "Mikita Balesni",
        "Asa Cooper Stickland",
        "Tomasz Korbak",
        "Owain Evans"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GPKTIktA0k",
      "cdate": 1695484844648,
      "mdate": 1712265187562,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709174"
    },
    {
      "id": "7Jwpw4qKkb",
      "title": "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models",
      "abstract": "The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. Moreover, we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively. Code is available at https://github.com/SheltonLiu-N/AutoDAN.",
      "authors": [
        "Xiaogeng Liu",
        "Nan Xu",
        "Muhao Chen",
        "Chaowei Xiao"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7Jwpw4qKkb",
      "cdate": 1695484446326,
      "mdate": 1713672988734,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709179"
    },
    {
      "id": "af2c8EaKl8",
      "title": "Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making",
      "abstract": "The recent success of Transformer in natural language processing has sparked its use in various domains. In offline reinforcement learning (RL), Decision Transformer (DT) is emerging as a promising model based on Transformer. However, we discovered that the attention module of DT is not appropriate to capture the inherent local dependence pattern in trajectories of RL modeled as a Markov decision process. To overcome the limitations of DT, we propose a novel action sequence predictor, named Decision ConvFormer (DC), based on the architecture of MetaFormer, which is a general structure to process multiple entities in parallel and understand the interrelationship among the multiple entities. DC employs local convolution filtering as the token mixer and can effectively capture the inherent local associations of the RL dataset. In extensive experiments, DC achieved state-of-the-art performance across various standard RL benchmarks while requiring fewer resources. Furthermore, we show that DC better understands the underlying meaning in data and exhibits enhanced generalization capability.",
      "authors": [
        "Jeonghye Kim",
        "Suyoung Lee",
        "Woojun Kim",
        "Youngchul Sung"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=af2c8EaKl8",
      "cdate": 1695484341270,
      "mdate": 1710517965420,
      "matched_keywords": [
        "reinforcement learning",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709186"
    },
    {
      "id": "vSh5ePa0ph",
      "title": "How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?",
      "abstract": "Transformers pretrained on diverse tasks exhibit remarkable in-context learning (ICL) capabilities, enabling them to solve unseen tasks solely based on input contexts without adjusting model parameters. In this paper, we study ICL in one of its simplest setups: pretraining a single-layer linear attention model for linear regression with a Gaussian prior. We establish a statistical task complexity bound for the attention model pretraining, showing that effective pretraining only requires a small number of independent tasks. Furthermore, we prove that the pretrained model closely matches the Bayes optimal algorithm, i.e., optimally tuned ridge regression, by achieving nearly Bayes optimal risk on unseen tasks under a fixed context length. These theoretical findings complement prior experimental research and shed light on the statistical foundations of ICL.",
      "authors": [
        "Jingfeng Wu",
        "Difan Zou",
        "Zixiang Chen",
        "Vladimir Braverman",
        "Quanquan Gu",
        "Peter Bartlett"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vSh5ePa0ph",
      "cdate": 1695483910120,
      "mdate": 1710467009155,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709190"
    },
    {
      "id": "DjIsNDEOYX",
      "title": "Scalable Monotonic Neural Networks",
      "abstract": "In this research, we focus on the problem of learning monotonic neural networks, as preserving the monotonicity of a model with respect to a subset of inputs is crucial for practical applications across various domains. Although several methods have recently been proposed to address this problem, they have limitations such as not guaranteeing monotonicity in certain cases, requiring additional inference time, lacking scalability with increasing network size and number of monotonic inputs, and manipulating network weights during training. To overcome these limitations, we introduce a simple but novel architecture of the partially connected network which incorporates a 'scalable monotonic hidden layer' comprising three units: the exponentiated unit, ReLU unit, and confluence unit. This allows for the repetitive integration of the scalable monotonic hidden layers without other structural constraints. Consequently, our method offers ease of implementation and rapid training through the conventional error-backpropagation algorithm. We accordingly term this method as Scalable Monotonic Neural Networks (SMNN). Numerical experiments demonstrated that our method achieved comparable prediction accuracy to the state-of-the-art approaches while effectively addressing the aforementioned weaknesses.",
      "authors": [
        "Hyunho Kim",
        "Jong-Seok Lee"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DjIsNDEOYX",
      "cdate": 1695483864579,
      "mdate": 1710420225413,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709195"
    },
    {
      "id": "s2NjWfaYdZ",
      "title": "Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models",
      "abstract": "Given a pretrained encoder-based language model, how can we accurately compress it without retraining? Retraining-free structured pruning algorithms are crucial in pretrained language model compression due to their significantly reduced pruning cost and capability to prune large language models. However, existing retraining-free algorithms encounter severe accuracy degradation, as they fail to handle pruning errors, especially at high compression rates. In this paper, we propose KPrune (Knowledge-preserving pruning), an accurate retraining-free structured pruning algorithm for pretrained encoder-based language models.\nKPrune focuses on preserving the useful knowledge of the pretrained model to minimize pruning errors through a carefully designed iterative pruning process composed of knowledge measurement, knowledge-preserving mask search, and knowledge-preserving weight-tuning. As a result, KPrune shows significant accuracy improvements up to 58.02%p higher F1 score compared to existing retraining-free pruning algorithms under a high compression rate of 80% on the SQuAD benchmark without any retraining process.",
      "authors": [
        "Seungcheol Park",
        "Hojun Choi",
        "U Kang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=s2NjWfaYdZ",
      "cdate": 1695483853468,
      "mdate": 1709661542462,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709199"
    },
    {
      "id": "d94x0gWTUX",
      "title": "Tool-Augmented Reward Modeling",
      "abstract": "Reward modeling (*a.k.a.*, preference modeling) is instrumental for aligning large language models with human preferences, particularly within the context of reinforcement learning from human feedback (RLHF). While conventional reward models (RMs) have exhibited remarkable scalability, they oft struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup. In this paper, we propose a tool-augmented preference modeling approach, named Themis, to address these limitations by empowering RMs with access to external environments, including calculators and search engines. This approach not only fosters synergy between tool utilization and reward grading but also enhances interpretive capacity and scoring reliability. Our study delves into the integration of external tools into RMs, enabling them to interact with diverse external sources and construct task-specific tool engagement and reasoning traces in an autoregressive manner. We validate our approach across a wide range of domains, incorporating seven distinct external tools. Our experimental results demonstrate a noteworthy overall improvement of 17.7% across eight tasks in preference ranking. Furthermore, our approach outperforms Gopher 280B by 7.3% on TruthfulQA task in zero-shot evaluation. In human evaluations, RLHF trained with Themis attains an average win rate of 32% when compared to baselines across four distinct tasks. Additionally, we provide a comprehensive collection of tool-related RM datasets, incorporating data from seven distinct tool APIs, totaling 15,000 instances. We have made the code, data, and model checkpoints publicly available to facilitate and inspire further research advancements (https://github.com/ernie-research/Tool-Augmented-Reward-Model).",
      "authors": [
        "Lei Li",
        "Yekun Chai",
        "Shuohuan Wang",
        "Yu Sun",
        "Hao Tian",
        "Ningyu Zhang",
        "Hua Wu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=d94x0gWTUX",
      "cdate": 1695483315652,
      "mdate": 1709661542216,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709204"
    },
    {
      "id": "EW8ZExRZkJ",
      "title": "Minimax optimality of convolutional neural networks for infinite dimensional input-output problems and separation from kernel methods",
      "abstract": "Recent deep learning applications, exemplified by text-to-image tasks, often involve high-dimensional inputs and outputs. While several studies have investigated the function estimation capabilities of deep learning, research on dilated convolutional neural networks (CNNs) has mainly focused on cases where input dimensions are infinite but output dimensions are one-dimensional, similar to many other studies. However, many practical deep learning tasks involve high-dimensional (or even infinite dimensional) inputs and outputs.\nIn this paper, we investigate the optimality of dilated CNNs for estimating a map between infinite-dimensional input and output spaces \nby analyzing their approximation and estimation abilities. \nFor that purpose, we first show that approximation and estimation errors depend only on the smoothness and decay rate with respect to the infinity norm of the output, and their estimation accuracy actually achieve the {\\it minimax optimal} rate of convergence.\nSecond, we demonstrate that the dilated CNNs outperform {\\it any} linear estimators including kernel ridge regression and $k$-NN estimators in a minimax error sense, highlighting the usefulness of feature learning realized by deep neural networks.\nOur theoretical analysis particularly explains the success of deep learning in recent high-dimensional input-output tasks.",
      "authors": [
        "Yuto Nishimura",
        "Taiji Suzuki"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EW8ZExRZkJ",
      "cdate": 1695483201598,
      "mdate": 1710956350292,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709208"
    },
    {
      "id": "d4uL2MSe0z",
      "title": "Dynamic Layer Tying for Parameter-Efficient Transformers",
      "abstract": "In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer $i$ independently or to copy the weights of a previous layer $j<i$. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method.",
      "authors": [
        "Tamir David Hay",
        "Lior Wolf"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=d4uL2MSe0z",
      "cdate": 1695483133125,
      "mdate": 1709661542083,
      "matched_keywords": [
        "reinforcement learning",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709213"
    },
    {
      "id": "GSBHKiw19c",
      "title": "Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning",
      "abstract": "Learning a precise dynamics model can be crucial for offline reinforcement learning, which, unfortunately, has been found to be quite challenging. Dynamics models that are learned by fitting historical transitions often struggle to generalize to unseen transitions. In this study, we identify a hidden but pivotal factor termed dynamics reward that remains consistent across transitions, offering a pathway to better generalization. Therefore, we propose the idea of reward-consistent dynamics models: any trajectory generated by the dynamics model should maximize the dynamics reward derived from the data. We implement this idea as the MOREC (Model-based Offline reinforcement learning with Reward Consistency) method, which can be seamlessly integrated into previous offline model-based reinforcement learning (MBRL) methods. MOREC learns a generalizable dynamics reward function from offline data, which is subsequently employed as a transition filter in any offline MBRL method: when generating transitions, the dynamics model generates a batch of transitions and selects the one with the highest dynamics reward value. On a synthetic task, we visualize that MOREC has a strong generalization ability and can surprisingly recover some distant unseen transitions. On 21 offline tasks in D4RL and NeoRL benchmarks, MOREC improves the previous state-of-the-art performance by a significant margin, i.e., 4.6\\% on D4RL tasks and 25.9\\% on NeoRL tasks. Notably, MOREC is the first method that can achieve above 95\\% online RL performance in 6 out of 12 D4RL tasks and 3 out of 9 NeoRL tasks. Code is available at https://github.com/polixir/morec.",
      "authors": [
        "Fan-Ming Luo",
        "Tian Xu",
        "Xingchen Cao",
        "Yang Yu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GSBHKiw19c",
      "cdate": 1695483069336,
      "mdate": 1710573337035,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709217"
    },
    {
      "id": "ymjI8feDTD",
      "title": "Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion",
      "abstract": "Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 1.92). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE solution trajectories. It consistently improves sample quality as computational budgets increase, avoiding the degradation seen in CM. Furthermore, unlike CM, CTM's access to the score function can streamline the adoption of established controllable/conditional generation methods from the diffusion community. This access also enables the computation of likelihood. The code is available at https://github.com/sony/ctm.",
      "authors": [
        "Dongjun Kim",
        "Chieh-Hsin Lai",
        "Wei-Hsiang Liao",
        "Naoki Murata",
        "Yuhta Takida",
        "Toshimitsu Uesaka",
        "Yutong He",
        "Yuki Mitsufuji",
        "Stefano Ermon"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ymjI8feDTD",
      "cdate": 1695482803454,
      "mdate": 1711748990522,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709222"
    },
    {
      "id": "Q3YaCghZNt",
      "title": "Lemur: Integrating Large Language Models in Automated Program Verification",
      "abstract": "The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.",
      "authors": [
        "Haoze Wu",
        "Clark Barrett",
        "Nina Narodytska"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Q3YaCghZNt",
      "cdate": 1695481853819,
      "mdate": 1713570105214,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709227"
    },
    {
      "id": "6yv8UHVJn4",
      "title": "Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback",
      "abstract": "We study online reinforcement learning in linear Markov decision processes with adversarial losses and bandit feedback. We introduce two algorithms that achieve improved regret performance compared to existing approaches. The first algorithm, although computationally inefficient, achieves a regret of $\\widetilde{O}(\\sqrt{K})$ without relying on simulators, where $K$ is the number of episodes. This is the first rate-optimal result in the considered setting. The second algorithm is computationally efficient and achieves a regret of  $\\widetilde{O}(K^{\\frac{3}{4}})$ . These results significantly improve over the prior state-of-the-art: a computationally inefficient algorithm by Kong et al. (2023) with $\\widetilde{O}(K^{\\frac{4}{5}}+1/\\lambda_{\\min})$ regret, and a computationally efficient algorithm by Sherman et al. (2023b) with $\\widetilde{O}(K^{\\frac{6}{7}})$ regret.",
      "authors": [
        "Haolin Liu",
        "Chen-Yu Wei",
        "Julian Zimmert"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=6yv8UHVJn4",
      "cdate": 1695481699696,
      "mdate": 1712967794155,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709236"
    },
    {
      "id": "UMOlFJzLfL",
      "title": "A Precise Characterization of SGD Stability Using Loss Surface Geometry",
      "abstract": "Stochastic Gradient Descent (SGD) stands as a cornerstone optimization algorithm with proven real-world empirical successes but relatively limited theoretical understanding. Recent research has illuminated a key factor contributing to its practical efficacy: the implicit regularization it instigates. Several studies have investigated the linear stability property of SGD in the vicinity of a stationary point as a predictive proxy for sharpness and generalization error in overparameterized neural networks (Wu et al., 2022; Jastrzebski et al., 2019; Cohen et al., 2021). In this paper, we delve deeper into the relationship between linear stability and sharpness. More specifically, we meticulously delineate the necessary and sufficient conditions for linear stability, contingent on hyperparameters of SGD and the sharpness at the optimum. Towards this end, we introduce a novel coherence measure of the loss Hessian that encapsulates pertinent geometric properties of the loss function that are relevant to the linear stability of SGD. It enables us to provide a simplified sufficient condition for identifying linear instability at an optimum. Notably, compared to previous works, our analysis relies on significantly milder assumptions and is applicable for a broader class of loss functions than known before, encompassing not only mean-squared error but also cross-entropy loss.",
      "authors": [
        "Gregory Dexter",
        "Borja Ocejo",
        "Sathiya Keerthi",
        "Aman Gupta",
        "Ayan Acharya",
        "Rajiv Khanna"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=UMOlFJzLfL",
      "cdate": 1695481656426,
      "mdate": 1713273947195,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709241"
    },
    {
      "id": "xt9Bu66rqv",
      "title": "Dual RL: Unification and New Methods for Reinforcement and Imitation Learning",
      "abstract": "The goal of reinforcement learning (RL) is to find a policy that maximizes the expected cumulative return. It has been shown that this objective can be represented as an optimization problem of state-action visitation distribution under linear constraints. The dual problem of this formulation, which we refer to as *dual RL*, is unconstrained and easier to optimize. In this work, we first cast several state-of-the-art offline RL and offline imitation learning (IL) algorithms as instances of dual RL approaches with shared structures. Such unification allows us to identify the root cause of the shortcomings of prior methods. For offline IL, our analysis shows that prior methods are based on a restrictive coverage assumption that greatly limits their performance in practice. To fix this limitation, we propose a new discriminator-free method ReCOIL that learns to imitate from arbitrary off-policy data to obtain near-expert performance. For offline RL, our analysis frames a recent offline RL method XQL in the dual framework, and we further propose a new method $f$-DVL that provides alternative choices to the Gumbel regression loss that fixes the known training instability issue of XQL. The performance improvements by both of our proposed methods, ReCOIL and $f$-DVL, in IL and RL are validated on an extensive suite of simulated robot locomotion and manipulation tasks.",
      "authors": [
        "Harshit Sikchi",
        "Qinqing Zheng",
        "Amy Zhang",
        "Scott Niekum"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xt9Bu66rqv",
      "cdate": 1695481445327,
      "mdate": 1709661541700,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709249"
    },
    {
      "id": "ulaUJFd96G",
      "title": "Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs",
      "abstract": "Large language models (LLMs) have shown remarkable performance in various natural language processing tasks. However, a primary constraint they face is the context limit, i.e., the maximum number of tokens they can process. Previous works have explored architectural changes and modifications in positional encoding to relax the constraint, but they often require expensive training or do not address the computational demands of self-attention. In this paper, we present Hierarchical cOntext MERging (HOMER), a new training-free scheme designed to overcome the limitations. HOMER uses a divide-and-conquer algorithm, dividing long inputs into manageable chunks. Each chunk is then processed collectively, employing a hierarchical strategy that merges adjacent chunks at progressive transformer layers. A token reduction technique precedes each merging, ensuring memory usage efficiency. We also propose an optimized computational order reducing the memory requirement to logarithmically scale with respect to input length, making it especially favorable for environments with tight memory restrictions.  Our experiments demonstrate the proposed method's superior performance and memory efficiency, enabling the broader use of LLMs in contexts requiring extended context. Code is available at https://github.com/alinlab/HOMER.",
      "authors": [
        "Woomin Song",
        "Seunghyuk Oh",
        "Sangwoo Mo",
        "Jaehyung Kim",
        "Sukmin Yun",
        "Jung-Woo Ha",
        "Jinwoo Shin"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ulaUJFd96G",
      "cdate": 1695481225606,
      "mdate": 1713179984629,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709253"
    },
    {
      "id": "DfPtC8uSot",
      "title": "Bounding the Expected Robustness of Graph Neural Networks Subject to Node Feature Attacks",
      "abstract": "Graph Neural Networks (GNNs) have demonstrated state-of-the-art performance in various graph representation learning tasks. Recently, studies revealed their vulnerability to adversarial attacks. In this work, we theoretically define the concept of expected robustness in the context of attributed graphs and relate it to the classical definition of adversarial robustness in the graph representation learning literature. Our definition allows us to derive an upper bound of the expected robustness of Graph Convolutional Networks (GCNs) and Graph Isomorphism Networks subject to node feature attacks. Building on these findings, we connect the expected robustness of GNNs to the orthonormality of their weight matrices and consequently propose an attack-independent, more robust variant of the GCN, called the Graph Convolutional Orthonormal Robust Networks (GCORNs). We further introduce a probabilistic method to estimate the expected robustness, which allows us to evaluate the effectiveness of GCORN on several real-world datasets. Experimental experiments showed that GCORN outperforms available defense methods. Our code is publicly available at: https://github.com/Sennadir/GCORN .",
      "authors": [
        "Yassine ABBAHADDOU",
        "Sofiane ENNADIR",
        "Johannes F. Lutzeyer",
        "Michalis Vazirgiannis",
        "Henrik Boström"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DfPtC8uSot",
      "cdate": 1695481005110,
      "mdate": 1713360588430,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709258"
    },
    {
      "id": "oXjnwQLcTA",
      "title": "Score Models for Offline Goal-Conditioned Reinforcement Learning",
      "abstract": "Offline Goal-Conditioned Reinforcement Learning (GCRL) is tasked with learning to achieve multiple goals in an environment purely from offline datasets using sparse reward functions. Offline GCRL is pivotal for developing generalist agents capable of leveraging pre-existing datasets to learn diverse and reusable skills without hand-engineering reward functions. However, contemporary approaches to GCRL based on supervised learning and contrastive learning are often suboptimal in the offline setting. An alternative perspective on GCRL optimizes for occupancy matching, but necessitates learning a discriminator, which subsequently serves as a pseudo-reward for downstream RL. Inaccuracies in the learned discriminator can cascade, negatively influencing the resulting policy. We present a novel approach to GCRL under a new lens of mixture-distribution matching, leading to our discriminator-free method: SMORe. The key insight is combining the occupancy matching perspective of GCRL with a convex dual formulation to derive a learning objective that can better leverage suboptimal offline data. SMORe learns *scores* or unnormalized densities representing the importance of taking an action at a state for reaching a particular goal. SMORe is principled and our extensive experiments on the fully offline GCRL benchmark composed of robot manipulation and locomotion tasks, including high-dimensional observations, show that SMORe can outperform state-of-the-art baselines by a significant margin.",
      "authors": [
        "Harshit Sikchi",
        "Rohan Chitnis",
        "Ahmed Touati",
        "Alborz Geramifard",
        "Amy Zhang",
        "Scott Niekum"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=oXjnwQLcTA",
      "cdate": 1695480911444,
      "mdate": 1709661541454,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709262"
    },
    {
      "id": "bAMPOUF227",
      "title": "Supervised Knowledge Makes Large Language Models Better In-context Learners",
      "abstract": "Large Language Models (LLMs) exhibit emerging in-context learning abilities through prompt engineering. The recent progress in large-scale generative models has further expanded their use in real-world language applications. However, the critical challenge of improving the generalizability and factuality of LLMs in natural language understanding and question answering remains under-explored. While previous in-context learning research has focused on enhancing models to adhere to users' specific instructions and quality expectations, and to avoid undesired outputs, little to no work has explored the use of task-specific fine-tuned Language Models (SLMs) to improve LLMs' in-context learning during the inference stage. Our primary contribution is the establishment of a simple yet effective framework that enhances the reliability of LLMs as it: 1) generalizes out-of-distribution data, 2) elucidates how LLMs benefit from discriminative models, and 3) minimizes hallucinations in generative tasks. Using our proposed plug-in method, enhanced versions of Llama 2 and ChatGPT surpass their original versions regarding generalizability and factuality. We offer a comprehensive suite of resources, including 16 curated datasets, prompts, model checkpoints, and LLM outputs across 9 distinct tasks. Our empirical analysis sheds light on the advantages of incorporating discriminative models into LLMs and highlights the potential of our methodology in fostering more reliable LLMs.",
      "authors": [
        "Linyi Yang",
        "Shuibai Zhang",
        "Zhuohao Yu",
        "Guangsheng Bao",
        "Yidong Wang",
        "Jindong Wang",
        "Ruochen Xu",
        "Wei Ye",
        "Xing Xie",
        "Weizhu Chen",
        "Yue Zhang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=bAMPOUF227",
      "cdate": 1695480507702,
      "mdate": 1711516569925,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709267"
    },
    {
      "id": "0akLDTFR9x",
      "title": "Contrastive Difference Predictive Coding",
      "abstract": "Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \\times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20\\times$ more sample efficient than the successor representation and $1500 \\times$ more sample efficient than the standard (Monte Carlo) version of contrastive predictive coding.",
      "authors": [
        "Chongyi Zheng",
        "Ruslan Salakhutdinov",
        "Benjamin Eysenbach"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0akLDTFR9x",
      "cdate": 1695480434284,
      "mdate": 1710103871152,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709272"
    },
    {
      "id": "LNLjU5C5dK",
      "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment",
      "abstract": "Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which can't fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named $\\textbf{FIGA}$. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quailty signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines.",
      "authors": [
        "Geyang Guo",
        "Ranchi Zhao",
        "Tianyi Tang",
        "Xin Zhao",
        "Ji-Rong Wen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LNLjU5C5dK",
      "cdate": 1695480153291,
      "mdate": 1713013335671,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709277"
    },
    {
      "id": "ZWzUA9zeAg",
      "title": "Effective Data Augmentation With Diffusion Models",
      "abstract": "Data augmentation is one of the most prevalent tools in deep learning, underpinning many recent advances, including those from classification, generative models, and representation learning. The standard approach to data augmentation combines simple transformations like rotations and flips to generate new images from existing ones. However, these new images lack diversity along key semantic axes present in the data. Current augmentations cannot alter the high-level semantic attributes, such as animal species present in a scene, to enhance the diversity of data. We address the lack of diversity in data augmentation with image-to-image transformations parameterized by pre-trained text-to-image diffusion models. Our method edits images to change their semantics using an off-the-shelf diffusion model, and generalizes to novel visual concepts from a few labelled examples. We evaluate our approach on few-shot image classification tasks, and on a real-world weed recognition task, and observe an improvement in accuracy in tested domains.",
      "authors": [
        "Brandon Trabucco",
        "Kyle Doherty",
        "Max A Gurinas",
        "Ruslan Salakhutdinov"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ZWzUA9zeAg",
      "cdate": 1695480130681,
      "mdate": 1713039924109,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709282"
    },
    {
      "id": "KNvubydSB5",
      "title": "HiGen: Hierarchical Graph Generative Networks",
      "abstract": "Most real-world graphs exhibit a hierarchical structure, which is often overlooked by existing graph generation methods. To address this limitation, we propose a novel graph generative network that captures the hierarchical nature of graphs and successively generates the graph sub-structures in a coarse-to-fine fashion. At each level of hierarchy, this model generates communities in parallel, followed by the prediction of cross-edges between communities using separate neural networks. This modular approach enables scalable graph generation for large and complex graphs.  Moreover, we model the output distribution of edges in the hierarchical graph with a multinomial distribution and derive a recursive factorization for this distribution. This enables us to generate  community graphs with integer-valued edge weights in an autoregressive manner. Empirical studies demonstrate the effectiveness and scalability of our proposed generative model, achieving state-of-the-art performance in terms of graph quality across various benchmark datasets. \nCode available at https://github.com/Karami-m/HiGen_main.",
      "authors": [
        "Mahdi Karami"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=KNvubydSB5",
      "cdate": 1695479222928,
      "mdate": 1710517678350,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709289"
    },
    {
      "id": "yoVq2BGQdP",
      "title": "Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning",
      "abstract": "Fairness plays a crucial role in various multi-agent systems (e.g., communication networks, financial markets, etc.). Many multi-agent dynamical interactions can be cast as Markov Decision Processes (MDPs). While existing research has focused on studying fairness in known environments, the exploration of fairness in such systems for unknown environments remains open. In this paper, we propose a  Reinforcement Learning (RL) approach to achieve fairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing the sum of individual agents' value functions, we introduce a fairness function that ensures equitable rewards across agents. Since the classical Bellman's equation does not hold when the sum of individual value functions is not maximized, we cannot use traditional approaches. Instead, in order to explore, we maintain a confidence bound of the unknown environment and then propose an online convex optimization based approach to obtain a policy constrained to this confidence region. We show that such an approach achieves sub-linear regret in terms of the number of episodes. Additionally, we provide a probably approximately correct (PAC) guarantee based on the obtained regret bound. We also propose an offline RL algorithm and bound the optimality gap with respect to the optimal fair solution. To mitigate computational complexity, we introduce a policy-gradient type method for the fair objective. Simulation experiments also demonstrate the efficacy of our approach.",
      "authors": [
        "Peizhong Ju",
        "Arnob Ghosh",
        "Ness Shroff"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=yoVq2BGQdP",
      "cdate": 1695479108675,
      "mdate": 1710182829027,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709294"
    },
    {
      "id": "LzPWWPAdY4",
      "title": "LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models",
      "abstract": "Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning (Dettmers et al., 2023). In this work we focus on the scenario where quantization and LoRA fine- tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrep- ancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural lan- guage understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and out- performs existing quantization methods, especially in the challenging 2-bit and 2/4-bit mixed precision regimes. We will release our code.",
      "authors": [
        "Yixiao Li",
        "Yifan Yu",
        "Chen Liang",
        "Nikos Karampatziakis",
        "Pengcheng He",
        "Weizhu Chen",
        "Tuo Zhao"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LzPWWPAdY4",
      "cdate": 1695478136846,
      "mdate": 1710583312042,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709299"
    },
    {
      "id": "ikmuHqugN7",
      "title": "Scaling Convex Neural Networks with Burer-Monteiro Factorization",
      "abstract": "It has been demonstrated that the training problem for a variety of (non) linear two-layer neural networks (such as two-layer perceptrons, convolutional networks, and self-attention) can be posed as equivalent convex optimization problems, with an induced regularizer which encourages low rank. However, this regularizer becomes prohibitively expensive to compute at moderate scales, impeding training convex neural networks. To this end, we propose applying the Burer-Monteiro factorization to convex neural networks, which for the first time enables a Burer-Monteiro perspective on neural networks with non-linearities. This factorization leads to an equivalent yet computationally tractable non-convex alternative with no spurious local minima. We develop a novel relative optimality bound of stationary points of the Burer-Monteiro factorization, providing verifiable conditions under which any stationary point is a global optimum. Further, for the first time, we show that linear self-attention with sufficiently many heads has no spurious local minima. Our experiments validate the novel relative optimality bound and the utility of the Burer-Monteiro factorization for scaling convex neural networks.",
      "authors": [
        "Arda Sahiner",
        "Tolga Ergen",
        "Batu Ozturkler",
        "John M. Pauly",
        "Morteza Mardani",
        "Mert Pilanci"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ikmuHqugN7",
      "cdate": 1695478132877,
      "mdate": 1710556343617,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709303"
    },
    {
      "id": "r42tSSCHPh",
      "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation",
      "abstract": "The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as ``jailbreaks\". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the attack success rate from $0\\%$ to more than $95\\%$ across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with $30\\times$ lower computational cost. Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the attack success rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs, strongly advocating for more comprehensive red teaming and better alignment before releasing such models.",
      "authors": [
        "Yangsibo Huang",
        "Samyak Gupta",
        "Mengzhou Xia",
        "Kai Li",
        "Danqi Chen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=r42tSSCHPh",
      "cdate": 1695478063666,
      "mdate": 1710551587824,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709308"
    },
    {
      "id": "6LLho5X6xV",
      "title": "UniTabE: A Universal Pretraining Protocol for Tabular Foundation  Model in Data Science",
      "abstract": "Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks. This study seeks to extend the power of pretraining methodologies to facilitating the prediction over tables in data science, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks. The primary research questions underpinning this work revolve around the establishment of a universal pretraining protocol for tables with varied structures, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time. In response to these challenges, we introduce UniTabE, a straightforward yet effective method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures. UniTabE's core concept relies on representing each basic table element with a module, termed TabUnit. This is subsequently followed by a Transformer encoder to refine the representation. Moreover, our model is designed to facilitate pretraining and finetuning through the utilization of free-form prompts. In order to implement the pretraining phase, we curated an expansive tabular dataset comprising approximately 13 billion samples, meticulously gathered from the Kaggle platform. This research primarily centers on classification and regression tasks involving tabular data, and conducts rigorous experimental testing and analyses to validate the effectiveness of our methodology. The experimental results demonstrate UniTabE's superior performance against several baseline models across a multitude of benchmark datasets. This, therefore, underscores UniTabE's potential to significantly enhance the semantic representation of tabular data, thereby marking a significant stride for tabular data analysis.",
      "authors": [
        "Yazheng Yang",
        "Yuqi Wang",
        "Guang Liu",
        "Ledell Wu",
        "Qi Liu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=6LLho5X6xV",
      "cdate": 1695477814811,
      "mdate": 1710317326661,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709313"
    },
    {
      "id": "IOEEDkla96",
      "title": "Adversarial Feature Map Pruning for Backdoor",
      "abstract": "Deep neural networks have been widely used in many critical applications, such as autonomous vehicles and medical diagnosis. However, their security is threatened by backdoor attacks, which are achieved by adding artificial patterns to specific training data. Existing defense strategies primarily focus on using reverse engineering to reproduce the backdoor trigger generated by attackers and subsequently repair the DNN model by adding the trigger into inputs and fine-tuning the model with ground truth labels. However, once the trigger generated by the attackers is complex and invisible, the defender cannot reproduce the trigger successfully then the DNN model will not be repaired, as the trigger is not effectively removed. \n\nIn this work, we propose Adversarial Feature Map Pruning for Backdoor (FMP) to mitigate backdoor from the DNN. Unlike existing defense strategies, which focus on reproducing backdoor triggers, FMP attempts to prune backdoor feature maps, which are trained to extract backdoor information from inputs. After pruning these backdoor feature maps, FMP will fine-tune the model with a secure subset of training data. Our experiments demonstrate that, compared to existing defense strategies, FMP can effectively reduce the Attack Success Rate (ASR) even against the most complex and invisible attack triggers (e.g., FMP decreases the ASR to 2.86% in CIFAR10, which is 19.2% to 65.41% lower than baselines). Second, unlike conventional defense methods that tend to exhibit low robust accuracy (that is, the accuracy of the model on poisoned data), FMP achieves a higher RA, indicating its superiority in maintaining model performance while mitigating the effects of backdoor attacks (e.g., FMP obtains 87.40% RA in CIFAR10). Our code is publicly available at: https://github.com/hku-systems/FMP.",
      "authors": [
        "Dong HUANG",
        "Qingwen Bu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=IOEEDkla96",
      "cdate": 1695477261138,
      "mdate": 1713672790079,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709317"
    },
    {
      "id": "awHTL3Hpto",
      "title": "Expressivity of ReLU-Networks under Convex Relaxations",
      "abstract": "Convex relaxations are a key component of training and certifying provably safe neural networks. However, despite substantial progress, a wide and poorly understood accuracy gap to standard networks remains, raising the question of whether this is due to fundamental limitations of convex relaxations. Initial work investigating this question focused on the simple and widely used IBP relaxation. It revealed that some univariate, convex, continuous piecewise linear (CPWL) functions cannot be encoded by any ReLU network such that its IBP-analysis is precise.\nTo explore whether this limitation is shared by more advanced convex relaxations, we conduct the first in-depth study on the expressive power of ReLU networks across all commonly used convex relaxations. We show that: (i) more advanced relaxations allow a larger class of univariate functions to be expressed as precisely analyzable ReLU networks, (ii) more precise relaxations can allow exponentially larger solution spaces of ReLU networks encoding the same functions, and (iii) even using the most precise single-neuron relaxations, it is impossible to construct precisely analyzable ReLU networks that express multivariate, convex, monotone CPWL functions.",
      "authors": [
        "Maximilian Baader",
        "Mark Niklas Mueller",
        "Yuhao Mao",
        "Martin Vechev"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=awHTL3Hpto",
      "cdate": 1695476993080,
      "mdate": 1710172447623,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709325"
    },
    {
      "id": "oO6FsMyDBt",
      "title": "Graph Neural Networks for Learning Equivariant Representations of Neural Networks",
      "abstract": "Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalization performance, and learning to optimize, while consistently outperforming state-of-the-art methods. The source code is open-sourced at https://github.com/mkofinas/neural-graphs.",
      "authors": [
        "Miltiadis Kofinas",
        "Boris Knyazev",
        "Yan Zhang",
        "Yunlu Chen",
        "Gertjan J. Burghouts",
        "Efstratios Gavves",
        "Cees G. M. Snoek",
        "David W. Zhang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=oO6FsMyDBt",
      "cdate": 1695476894164,
      "mdate": 1710763567050,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709329"
    },
    {
      "id": "wXpSidPpc5",
      "title": "CLEX: Continuous  Length Extrapolation for Large Language Models",
      "abstract": "Transformer-based Large Language Models (LLMs) are pioneering advances in many natural language processing tasks, however, their exceptional capabilities are restricted within the preset context window of Transformer. Position Embedding (PE) scaling methods, while effective in extending the context window to a specific length, demonstrate either notable limitations in their extrapolation abilities or sacrificing partial performance within the context window. Length extrapolation methods, although theoretically capable of extending the context window beyond the training sequence length, often underperform in practical long-context applications. To address these challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We generalise the PE scaling approaches to model the continuous dynamics by ordinary differential equations over the length scaling factor, thereby overcoming the constraints of current PE scaling methods designed for specific lengths. Moreover, by extending the dynamics to desired context lengths beyond the training sequence length, CLEX facilitates the length extrapolation with impressive performance in practical tasks. We demonstrate that CLEX can be seamlessly incorporated into LLMs equipped with Rotary Position Embedding, such as LLaMA and GPT-NeoX, with negligible impact on training and inference latency. Experimental results reveal that CLEX can effectively extend the context window to over 4× or almost 8× training length, with no deterioration in performance. Furthermore, when evaluated on the practical LongBench benchmark, our model trained on a 4k length exhibits competitive performance against state-of-the-art open-source models trained on context lengths up to 32k. Our code is available at https://github.com/DAMO-NLP-SG/CLEX.",
      "authors": [
        "Guanzheng Chen",
        "Xin Li",
        "Zaiqiao Meng",
        "Shangsong Liang",
        "Lidong Bing"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=wXpSidPpc5",
      "cdate": 1695476414490,
      "mdate": 1711300318384,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709334"
    },
    {
      "id": "IGzaH538fz",
      "title": "GNNCert: Deterministic Certification of Graph Neural Networks against Adversarial Perturbations",
      "abstract": "Graph classification, which aims to predict a label for a graph, has many real-world applications such as malware detection, fraud detection, and healthcare. However, many studies show an attacker could carefully perturb the structure and/or node features in a graph such that a graph classifier misclassifies the perturbed graph. Such vulnerability impedes the deployment of graph classification in security/safety-critical applications. Existing empirical defenses lack formal robustness guarantees and could be broken by adaptive or unknown attacks. Existing provable defenses have the following limitations: 1)  they achieve sub-optimal robustness guarantees for graph structure perturbation, 2) they cannot provide robustness guarantees for arbitrarily node feature perturbations, 3) their robustness guarantees are probabilistic, meaning they could be incorrect with a non-zero probability, and 4) they incur large computation costs. We aim to address those limitations in this work. We propose GNNCert, a certified defense against both graph structure and node feature perturbations for graph classification. Our GNNCert provably predicts the same label for a graph when the number of perturbed edges and the number of nodes with perturbed features are bounded. Our results on 8 benchmark datasets show that GNNCert outperforms three state-of-the-art methods.",
      "authors": [
        "zaishuo xia",
        "Han Yang",
        "Binghui Wang",
        "Jinyuan Jia"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=IGzaH538fz",
      "cdate": 1695475919263,
      "mdate": 1713672791712,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709341"
    },
    {
      "id": "gbrHZq07mq",
      "title": "Logical Languages Accepted by Transformer Encoders with Hard Attention",
      "abstract": "We contribute to the study of formal languages that can be recognized by transformer encoders. We focus on two self-attention mechanisms: (1) UHAT (Unique Hard Attention Transformers) and (2) AHAT (Average Hard Attention Transformers). UHAT encoders are known to  recognize only languages inside the circuit complexity class ${\\sf AC}^0$, i.e., accepted by a family of poly-sized and depth-bounded boolean circuits with unbounded fan-ins. On the other hand, AHAT encoders can recognize languages outside ${\\sf AC}^0$), but their expressive power still lies within the bigger circuit complexity class ${\\sf TC}^0$, i.e., ${\\sf AC}^0$-circuits extended by majority gates.\nWe first show a negative result that there is an  ${\\sf AC}^0$-language that cannot be recognized by an UHAT encoder. On the positive side, we show that UHAT encoders can recognize a rich fragment of ${\\sf AC}^0$-languages, namely, all languages definable in first-order logic with arbitrary unary numerical predicates. This logic, includes, for example, all regular languages from  ${\\sf AC}^0$. We then show that AHAT encoders can recognize all languages of our logic even when we enrich it with counting terms. Using these results, we obtain a characterization of which counting properties are expressible by UHAT and AHAT, in relation to regular languages.",
      "authors": [
        "Pablo Barcelo",
        "Alexander Kozachinskiy",
        "Anthony Widjaja Lin",
        "Vladimir Podolskii"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=gbrHZq07mq",
      "cdate": 1695475188635,
      "mdate": 1710515452219,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709347"
    },
    {
      "id": "G1Hlubz1fR",
      "title": "Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning",
      "abstract": "Modular and composable transfer learning is an emerging direction in the field of Parameter Efficient Fine-Tuning, as it enables neural networks to better organize various aspects of knowledge, leading to improved cross-task generalization.\nIn this paper, we introduce a novel approach Customized Polytropon ($\\texttt{C-Poly}$) that combines task-common skills and task-specific skills, while the skill parameters being highly parameterized using low-rank techniques.\nEach task is associated with a customizable number of exclusive specialized skills and also benefits from skills shared with peer tasks. A skill assignment matrix is jointly learned. To evaluate our approach, we conducted extensive experiments on the Super-NaturalInstructions and the SuperGLUE benchmarks.\nOur findings demonstrate that $\\texttt{C-Poly}$ outperforms fully-shared, task-specific, and skill-indistinguishable baselines, significantly enhancing the sample efficiency in multi-task learning scenarios.",
      "authors": [
        "Haowen Wang",
        "Tao Sun",
        "Congyun Jin",
        "Yingbo Wang",
        "Yibo Fan",
        "Yunqi Xu",
        "Yuliang Du",
        "Cong Fan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=G1Hlubz1fR",
      "cdate": 1695474776570,
      "mdate": 1713277014992,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709351"
    },
    {
      "id": "Nu9mOSq7eH",
      "title": "InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists",
      "abstract": "Recent advances in generative diffusion models have enabled text-controlled synthesis of realistic and diverse images with impressive quality. Despite these remarkable advances, the application of text-to-image generative models in computer vision for standard visual recognition tasks remains limited. The current de facto approach for these tasks is to design model architectures and loss functions that are tailored to the task at hand. In this paper, we develop a unified language interface for computer vision tasks that abstracts away task specific design choices and enables task execution by following natural language instructions. Our approach involves casting multiple computer vision tasks as text-to-image generation problems. Here, the text represents an instruction describing the task, and the resulting image is a visually-encoded task output. To train our model, we pool commonly-used computer vision datasets covering a range of tasks, including segmentation, object detection, depth estimation, and classification. We then use a large language model to paraphrase prompt templates that convey the specific tasks to be conducted on each image, and through this process, we create a multi-modal and multi-task training dataset comprising input and output images along with annotated instructions. Following the InstructPix2Pix architecture, we apply instruction-tuning to a text-to-image diffusion model using our constructed dataset, steering its functionality from a generative model to an instruction-guided multi-task vision learner. Experiments demonstrate that our model, dubbed InstructCV, performs competitively compared to other generalist and task-specific vision models. Moreover, it exhibits compelling generalization capabilities to unseen data, categories, and user instructions.",
      "authors": [
        "Yulu Gan",
        "Sungwoo Park",
        "Alexander Marcel Schubert",
        "Anthony Philippakis",
        "Ahmed Alaa"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Nu9mOSq7eH",
      "cdate": 1695474312459,
      "mdate": 1712351352458,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709356"
    },
    {
      "id": "Koh0i2u8qX",
      "title": "Mitigating Emergent Robustness Degradation while Scaling Graph Learning",
      "abstract": "Although graph neural networks have exhibited remarkable performance in various graph tasks, a significant concern is their vulnerability to adversarial attacks. Consequently, many defense methods have been proposed to alleviate the deleterious effects of adversarial attacks and learn robust graph representations. However, most of them are difficult to *simultaneously* avoid two major limitations: (i) an emergent and severe degradation in robustness when exposed to very intense attacks, and (ii) heavy computation complexity hinders them from scaling to large graphs. In response to these challenges, we introduce an innovative graph defense method for unpredictable real-world scenarios by *designing a graph robust learning framework that is resistant to robustness degradation* and *refraining from unscalable designs with heavy computation*: specifically, our method employs a denoising module, which eliminates edges that are associated with attacked nodes to reconstruct a cleaner graph; Then, it applies Mixture-of-Experts to select differentially private noises with varying magnitudes to counteract the hidden features attacked at different intensities toward robust predictions; Moreover, our overall design avoids the reliance on heavy adjacency matrix computations, such as SVD, thus facilitating its applicability even on large graphs. Comprehensive experiments have been conducted to demonstrate the anti-degraded robustness and scalability of our method, as compared to popular graph adversarial learning methods, under diverse attack intensities and various datasets of different sizes.",
      "authors": [
        "Xiangchi Yuan",
        "Chunhui Zhang",
        "Yijun Tian",
        "Yanfang Ye",
        "Chuxu Zhang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Koh0i2u8qX",
      "cdate": 1695474195371,
      "mdate": 1713672750395,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709361"
    },
    {
      "id": "p4S5Z6Sah4",
      "title": "Traveling Waves Encode The Recent Past and Enhance Sequence Learning",
      "abstract": "Traveling waves of neural activity have been observed throughout the brain at a diversity of regions and scales; however, their precise computational role is still debated. One physically inspired hypothesis suggests that the cortical sheet may act like a wave-propagating system capable of invertibly storing a short-term memory of sequential stimuli through induced waves traveling across the cortical surface, and indeed many experimental results from neuroscience correlate wave activity with memory tasks. To date, however, the computational implications of this idea have remained hypothetical due to the lack of a simple recurrent neural network architecture capable of exhibiting such waves. In this work, we introduce a model to fill this gap, which we denote the Wave-RNN (wRNN), and demonstrate how such an architecture indeed efficiently encodes the recent past through a suite of synthetic memory tasks where wRNNs learn faster and reach significantly lower error than wave-free counterparts. We further explore the implications of this memory storage system on more complex sequence modeling tasks such as sequential image classification and find that wave-based models not only again outperform comparable wave-free RNNs while using significantly fewer parameters, but additionally perform comparably to more complex gated architectures such as LSTMs and GRUs.",
      "authors": [
        "T. Anderson Keller",
        "Lyle Muller",
        "Terrence Sejnowski",
        "Max Welling"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=p4S5Z6Sah4",
      "cdate": 1695474134796,
      "mdate": 1710467521714,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709365"
    },
    {
      "id": "6IjN7oxjXt",
      "title": "Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training",
      "abstract": "Adversarial training improves the robustness of neural networks against adversarial attacks, albeit at the expense of the trade-off between standard and robust generalization. To unveil the underlying factors driving this phenomenon, we examine the layer-wise learning capabilities of neural networks during the transition from a standard to an adversarial setting. Our empirical findings demonstrate that selectively updating specific layers while preserving others can substantially enhance the network's learning capacity. We, therefore, propose CURE, a novel training framework that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights. Importantly, CURE is designed to be dataset- and architecture-agnostic, ensuring its applicability across various scenarios. It effectively tackles both memorization and overfitting issues, thus enhancing the trade-off between robustness and generalization and additionally, this training approach also aids in mitigating \"robust overfitting\". Furthermore, our study provides valuable insights into the mechanisms of selective adversarial training and offers a promising avenue for future research.",
      "authors": [
        "Shruthi Gowda",
        "Bahram Zonooz",
        "Elahe Arani"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=6IjN7oxjXt",
      "cdate": 1695473863409,
      "mdate": 1709933310479,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709370"
    },
    {
      "id": "hB2hXtxIPH",
      "title": "Solving Homogeneous and Heterogeneous Cooperative Tasks with Greedy Sequential Execution",
      "abstract": "Cooperative multi-agent reinforcement learning (MARL) is extensively used for solving complex cooperative tasks, and value decomposition methods are a prevalent approach for this domain. However, these methods have not been successful in addressing both homogeneous and heterogeneous tasks simultaneously which is a crucial aspect for the practical application of cooperative agents. \nOn one hand, value decomposition methods demonstrate superior performance in homogeneous tasks. Nevertheless, they tend to produce agents with similar policies, which is unsuitable for heterogeneous tasks. On the other hand, solutions based on personalized observation or assigned roles are well-suited for heterogeneous tasks. However, they often lead to a trade-off situation where the agent's performance in homogeneous scenarios is negatively affected due to the aggregation of distinct policies. An alternative approach is to adopt sequential execution policies, which offer a flexible form for learning both types of tasks. However, learning sequential execution policies poses challenges in terms of credit assignment, and the limited information about subsequently executed agents can lead to sub-optimal solutions, which is known as the relative over-generalization problem. To tackle these issues, this paper proposes Greedy Sequential Execution (GSE) as a solution to learn the optimal policy that covers both scenarios. In the proposed GSE framework, we introduce an individual utility function into the framework of value decomposition to consider the complex interactions between agents. \nThis function is capable of representing both the homogeneous and heterogeneous optimal policies. Furthermore, we utilize greedy marginal contribution calculated by the utility function as the credit value of the sequential execution policy to address the credit assignment and relative over-generalization problem. We evaluated GSE in both homogeneous and heterogeneous scenarios. The results demonstrate that GSE achieves significant improvement in performance across multiple domains, especially in scenarios involving both homogeneous and heterogeneous tasks.",
      "authors": [
        "Shanqi Liu",
        "Dong Xing",
        "Pengjie Gu",
        "Xinrun Wang",
        "Bo An",
        "Yong Liu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hB2hXtxIPH",
      "cdate": 1695473426000,
      "mdate": 1713672360377,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709374"
    },
    {
      "id": "NLevOah0CJ",
      "title": "Hindsight PRIORs for Reward Learning from Human Preferences",
      "abstract": "Preference based Reinforcement Learning (PbRL) removes the need to hand specify a reward function by learning one from preference feedback over policy behaviors. Current approaches to PbRL do not address the credit assignment problem inherent in determining which parts of a behavior most contributed to a preference resulting in data intensive approaches and subpar reward models. We address such limitations by introducing a credit assignment strategy (PRIOR) that uses a forward dynamics world model to approximate state importance within a trajectory and then guides rewards to be proportional to state importance through an auxiliary predicted return redistribution objective. Incorporating state importance into reward learning improves the speed of policy learning, overall policy performance, and reward recovery on both locomotion and manipulation tasks. For example, PRIOR achieves 80% success rate with half the amount of data compared to baselines. The performance gains and our ablations demonstrate the benefits even a simple credit assignment strategy can have on reward learning and that state importance in forward dynamics prediction is a strong proxy for a state's contribution to a preference decision.",
      "authors": [
        "Mudit Verma",
        "Katherine Metcalf"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NLevOah0CJ",
      "cdate": 1695473383126,
      "mdate": 1710570617708,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709379"
    },
    {
      "id": "Xkf2EBj4w3",
      "title": "Stabilizing Contrastive RL: Techniques for Robotic Goal Reaching from Offline Data",
      "abstract": "Robotic systems that rely primarily on self-supervised learning have the potential to decrease the amount of human annotation and engineering effort required to learn control strategies. In the same way that prior robotic systems have leveraged self-supervised techniques from computer vision (CV) and natural language processing (NLP), our work builds on prior work showing that the reinforcement learning (RL) itself can be cast as a self-supervised problem: learning to reach any goal without human-specified rewards or labels. Despite the seeming appeal, little (if any) prior work has demonstrated how self-supervised RL methods can be practically deployed on robotic systems. By first studying a challenging simulated version of this task, we discover design decisions about architectures and hyperparameters that increase the success rate by $2 \\times$. These findings lay the groundwork for our main result: we demonstrate that a self-supervised RL algorithm based on contrastive learning can solve real-world, image-based robotic manipulation tasks, with tasks being specified by a single goal image provided after training.",
      "authors": [
        "Chongyi Zheng",
        "Benjamin Eysenbach",
        "Homer Rich Walke",
        "Patrick Yin",
        "Kuan Fang",
        "Ruslan Salakhutdinov",
        "Sergey Levine"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Xkf2EBj4w3",
      "cdate": 1695472665146,
      "mdate": 1710103610706,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709384"
    },
    {
      "id": "BqHaLnans2",
      "title": "LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation",
      "abstract": "Following the impressive development of LLMs, vision-language alignment in LLMs is actively being researched to enable multimodal reasoning and visual input/output. This direction of research is particularly relevant to medical imaging because accurate medical image analysis and generation consist of a combination of reasoning based on visual features and prior knowledge. Many recent works have focused on training adapter networks that serve as an information bridge between image processing (encoding or generating) networks and LLMs; but presumably, in order to achieve maximum reasoning potential of LLMs on visual information as well, visual and language features should be allowed to interact more freely. This is especially important in the medical domain because understanding and generating medical images such as chest X-rays (CXR) require not only accurate visual and language-based reasoning but also a more intimate mapping between the two modalities. Thus, taking inspiration from previous work on the transformer and VQ-GAN combination for bidirectional image and text generation, we build upon this approach and develop a method for instruction-tuning an LLM pre-trained only on text to gain vision-language capabilities for medical images. Specifically, we leverage a pretrained LLM’s existing question-answering and instruction-following abilities to teach it to understand visual inputs by instructing it to answer questions about image inputs and, symmetrically, output both text and image responses appropriate to a given query by tuning the LLM with diverse tasks that encompass image-based text-generation and text-based image-generation. We show that our LLM-CXR trained in this approach shows better image-text alignment in both CXR understanding and generation tasks while being smaller in size compared to previously developed models that perform a narrower range of tasks.",
      "authors": [
        "Suhyeon Lee",
        "Won Jun Kim",
        "Jinho Chang",
        "Jong Chul Ye"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BqHaLnans2",
      "cdate": 1695472568972,
      "mdate": 1710488694895,
      "matched_keywords": [
        "multimodal",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709391"
    },
    {
      "id": "iriEqxFB4y",
      "title": "DOS: Diverse Outlier Sampling for Out-of-Distribution Detection",
      "abstract": "Modern neural networks are known to give overconfident predictions for out-of-distribution inputs when deployed in the open world. It is common practice to leverage a surrogate outlier dataset to regularize the model during training, and recent studies emphasize the role of uncertainty in designing the sampling strategy for outlier datasets. However, the OOD samples selected solely based on predictive uncertainty can be biased towards certain types, which may fail to capture the full outlier distribution. In this work, we empirically show that diversity is critical in sampling outliers for OOD detection performance. Motivated by the observation, we propose a straightforward and novel sampling strategy named DOS (Diverse Outlier Sampling) to select diverse and informative outliers. Specifically, we cluster the normalized features at each iteration, and the most informative outlier from each cluster is selected for model training with absent category loss. With DOS, the sampled outliers efficiently shape a globally compact decision boundary between ID and OOD data. Extensive experiments demonstrate the superiority of DOS, reducing the average FPR95 by up to 25.79% on CIFAR-100 with TI-300K.",
      "authors": [
        "Wenyu Jiang",
        "Hao Cheng",
        "MingCai Chen",
        "Chongjun Wang",
        "Hongxin Wei"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iriEqxFB4y",
      "cdate": 1695470980396,
      "mdate": 1709661539475,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709396"
    },
    {
      "id": "SQrHpTllXa",
      "title": "CABINET: Content Relevance-based Noise Reduction for Table Question Answering",
      "abstract": "Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) – a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question answering LLM (QA LLM). To further aid the relevance scorer, CABINET employs a weakly supervised module that generates a parsing statement describing the criteria of rows and columns relevant to the question and highlights the content of corresponding table cells. CABINET significantly outperforms various tabular LLM baselines, as well as GPT3-based in-context learning methods, is more robust to noise, maintains outperformance on tables of varying sizes, and establishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We release our code and datasets here.",
      "authors": [
        "Sohan Patnaik",
        "Heril Changwal",
        "Milan Aggarwal",
        "Sumit Bhatia",
        "Yaman Kumar",
        "Balaji Krishnamurthy"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=SQrHpTllXa",
      "cdate": 1695470637604,
      "mdate": 1709840163776,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709400"
    },
    {
      "id": "dcjtMYkpXx",
      "title": "Reward Model Ensembles Help Mitigate Overoptimization",
      "abstract": "Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the “true” reward, these learned reward models are susceptible to overoptimization. Gao et al. (2023) studied this phenomenon in a synthetic human feedback setup with a significantly larger “gold” reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. Using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sampling (BoN) (b) proximal policy optimization (PPO). We additionally extend the setup of Gao et al. (2023) to include 25% label noise to better mirror real-world conditions. Both with and without label noise we find that conservative optimization practically eliminates overoptimization and improves performance by up to 70% for BoN sampling. For PPO, ensemble-based conservative optimization always reduces overoptimization and outperforms single reward model optimization. Moreover, combining it with a small KL penalty successfully prevents overoptimization at no performance cost. Overall, our results demonstrate that ensemble-based conservative optimization can effectively counter overoptimization.",
      "authors": [
        "Thomas Coste",
        "Usman Anwar",
        "Robert Kirk",
        "David Krueger"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=dcjtMYkpXx",
      "cdate": 1695470511815,
      "mdate": 1710033982826,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709405"
    },
    {
      "id": "TyFrPOKYXw",
      "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback",
      "abstract": "With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowd workers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms. Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.\n\nCode is available at https://github.com/PKU-Alignment/safe-rlhf.\n\n\nWarning: This paper contains example data that may be offensive or harmful.",
      "authors": [
        "Josef Dai",
        "Xuehai Pan",
        "Ruiyang Sun",
        "Jiaming Ji",
        "Xinbo Xu",
        "Mickel Liu",
        "Yizhou Wang",
        "Yaodong Yang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=TyFrPOKYXw",
      "cdate": 1695470439334,
      "mdate": 1713328885039,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709410"
    },
    {
      "id": "HKGQDDTuvZ",
      "title": "Frequency-Aware Transformer for Learned  Image Compression",
      "abstract": "Learned image compression (LIC) has gained traction as an effective solution for image storage and transmission in recent years. However, existing LIC methods are redundant in latent representation due to limitations in capturing anisotropic frequency components and preserving directional details. To overcome these challenges, we propose a novel frequency-aware transformer (FAT) block that for the first time achieves multiscale directional ananlysis for LIC. The FAT block comprises frequency-decomposition window attention (FDWA) modules to capture multiscale and directional frequency components of natural images. Additionally, we introduce frequency-modulation feed-forward network (FMFFN) to adaptively modulate different frequency components, improving rate-distortion performance. Furthermore, we present a transformer-based channel-wise autoregressive (T-CA) model that effectively exploits channel dependencies. Experiments show that our method achieves state-of-the-art rate-distortion performance compared to existing LIC methods, and evidently outperforms latest standardized codec VTM-12.1 by 14.5\\%, 15.1\\%, 13.0\\% in BD-rate on the Kodak, Tecnick, and CLIC datasets.",
      "authors": [
        "Han Li",
        "Shaohui Li",
        "Wenrui Dai",
        "Chenglin Li",
        "Junni Zou",
        "Hongkai Xiong"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=HKGQDDTuvZ",
      "cdate": 1695470191477,
      "mdate": 1710996286176,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709414"
    },
    {
      "id": "Y3wpuxd7u9",
      "title": "GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction",
      "abstract": "Large Language Models (LLMs) combined with instruction tuning have made significant progress when generalizing to unseen tasks. However, they have been less successful in Information Extraction (IE), lagging behind task-specific models. Typically, IE tasks are characterized by complex annotation guidelines which describe the task and give examples to humans. Previous attempts to leverage such information have failed, even with the largest models, as they are not able to follow the guidelines out-of-the-box. In this paper we propose GoLLIE (Guideline-following Large Language Model for IE), a model able to improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to comply with annotation guidelines. Comprehensive evaluation empirically demonstrates that GoLLIE is able to generalize to and follow unseen guidelines, outperforming previous attempts at zero-shot information extraction. The ablation study shows that detailed guidelines is key for good results. Code, data and models will be made publicly available.",
      "authors": [
        "Oscar Sainz",
        "Iker García-Ferrero",
        "Rodrigo Agerri",
        "Oier Lopez de Lacalle",
        "German Rigau",
        "Eneko Agirre"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Y3wpuxd7u9",
      "cdate": 1695468337292,
      "mdate": 1709742841417,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709419"
    },
    {
      "id": "PczQtTsTIX",
      "title": "CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity",
      "abstract": "Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample.\nHowever, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce CrossQ:\nA lightweight algorithm for continuous control tasks that makes careful use of Batch Normalization and removes target networks to surpass the current state-of-the-art in sample efficiency while maintaining a low UTD ratio of 1. Notably, CrossQ does not rely on advanced bias-reduction schemes used in current methods. CrossQ's contributions are threefold: (1) it matches or surpasses current state-of-the-art methods in terms of sample efficiency, (2) it substantially reduces the computational cost compared to REDQ and DroQ, (3) it is easy to implement, requiring just a few lines of code on top of SAC.",
      "authors": [
        "Aditya Bhatt",
        "Daniel Palenicek",
        "Boris Belousov",
        "Max Argus",
        "Artemij Amiranashvili",
        "Thomas Brox",
        "Jan Peters"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=PczQtTsTIX",
      "cdate": 1695468251623,
      "mdate": 1713672652962,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709424"
    },
    {
      "id": "LjivA1SLZ6",
      "title": "Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning",
      "abstract": "In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD target in Q-learning and acts as an additional incentive for desirable transitions. We provide theoretical support for the proposed incentive and demonstrate the effectiveness of EMU compared to conventional episodic control. The proposed method is evaluated in StarCraft II and Google Research Football, and empirical results indicate further performance improvement over state-of-the-art methods.",
      "authors": [
        "Hyungho Na",
        "Yunkyeong Seo",
        "Il-chul Moon"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LjivA1SLZ6",
      "cdate": 1695467628362,
      "mdate": 1709790543033,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709428"
    },
    {
      "id": "vZ6r9GMT1n",
      "title": "Understanding the Robustness of Randomized Feature Defense Against Query-Based Adversarial Attacks",
      "abstract": "Recent works have shown that deep neural networks are vulnerable to adversarial examples that find samples close to the original image but can make the model misclassify. Even with access only to the model's output, an attacker can employ black-box attacks to generate such adversarial examples. In this work, we propose a simple and lightweight defense against black-box attacks by adding random noise to hidden features at intermediate layers of the model at inference time. Our theoretical analysis confirms that this method effectively enhances the model's resilience against both score-based and decision-based black-box attacks. Importantly, our defense does not necessitate adversarial training and has minimal impact on accuracy, rendering it applicable to any pre-trained model. Our analysis also reveals the significance of selectively adding noise to different parts of the model based on the gradient of the adversarial objective function, which can be varied during the attack. We demonstrate the robustness of our defense against multiple black-box attacks through extensive empirical experiments involving diverse models with various architectures.",
      "authors": [
        "Nguyen Hung-Quang",
        "Yingjie Lao",
        "Tung Pham",
        "Kok-Seng Wong",
        "Khoa D Doan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vZ6r9GMT1n",
      "cdate": 1695467361428,
      "mdate": 1713174991335,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709435"
    },
    {
      "id": "02f3mUtqnM",
      "title": "Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing",
      "abstract": "Large language models (LLMs) excel in most NLP tasks but also require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of response quality. Therefore in this work we propose a hybrid inference approach which combines their respective strengths to save cost and maintain quality. Our approach uses a router that assigns queries to the small or large model based on the predicted query difficulty and the desired quality level. The desired quality level can be tuned dynamically at test time to seamlessly trade  quality for cost as per the scenario requirements. In experiments our approach allows us to make up to 40% fewer calls to the large model, with no drop in response quality.",
      "authors": [
        "Dujian Ding",
        "Ankur Mallick",
        "Chi Wang",
        "Robert Sim",
        "Subhabrata Mukherjee",
        "Victor Rühle",
        "Laks V. S. Lakshmanan",
        "Ahmed Hassan Awadallah"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=02f3mUtqnM",
      "cdate": 1695467332860,
      "mdate": 1713575519314,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709442"
    },
    {
      "id": "j5JvZCaDM0",
      "title": "Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model",
      "abstract": "Safe offline reinforcement learning is a promising way to bypass risky online interactions towards safe policy learning. Most existing methods only enforce soft constraints, i.e., constraining safety violations in expectation below thresholds predetermined. This can lead to potentially unsafe outcomes, thus unacceptable in safety-critical scenarios. An alternative is to enforce the hard constraint of zero violation. However, this can be challenging in offline setting, as it needs to strike the right balance among three highly intricate and correlated aspects: safety constraint satisfaction, reward maximization, and behavior regularization imposed by offline datasets. Interestingly, we discover that via reachability analysis of safe-control theory, the hard safety constraint can be equivalently translated to identifying the largest feasible region given the offline dataset. This seamlessly converts the original trilogy problem to a feasibility-dependent objective, i.e., maximizing reward value within the feasible region while minimizing safety risks in the infeasible region. Inspired by these, we propose FISOR (FeasIbility-guided Safe Offline RL), which allows safety constraint adherence, reward maximization, and offline policy learning to be realized via three decoupled processes, while offering strong safety performance and stability. In FISOR, the optimal policy for the translated optimization problem can be derived in a special form of weighted behavior cloning, which can be effectively extracted with a guided diffusion model thanks to its expressiveness.  We compare FISOR against baselines on DSRL benchmark for safe offline RL. Evaluation results show that FISOR is the only method that can guarantee safety satisfaction in all tasks, while achieving top returns in most tasks. Code: https://github.com/ZhengYinan-AIR/FISOR.",
      "authors": [
        "Yinan Zheng",
        "Jianxiong Li",
        "Dongjie Yu",
        "Yujie Yang",
        "Shengbo Eben Li",
        "Xianyuan Zhan",
        "Jingjing Liu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=j5JvZCaDM0",
      "cdate": 1695466861202,
      "mdate": 1713672328248,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709447"
    },
    {
      "id": "eJ0dzPJq1F",
      "title": "Blending Imitation and Reinforcement Learning for Robust Policy Improvement",
      "abstract": "While reinforcement learning (RL) has shown promising performance, its sample complexity continues to be a substantial hurdle, restricting its broader application across a variety of domains. Imitation learning (IL) utilizes oracles to improve sample efficiency, yet it is often constrained by the quality of the oracles deployed. To address the demand for robust policy improvement in real-world scenarios, we introduce a novel algorithm, Robust Policy Improvement (RPI), which actively interleaves between IL and RL based on an online estimate of their performance. RPI draws on the strengths of IL, using oracle queries to facilitate exploration—an aspect that is notably challenging in sparse-reward RL—particularly during the early stages of learning. As learning unfolds, RPI gradually transitions to RL, effectively treating the learned policy as an improved oracle. This algorithm is capable of learning from and improving upon a diverse set of black-box oracles. Integral to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient (RPG), both of which reason over whether to perform state-wise imitation from the oracles or learn from its own value function when the learner’s performance surpasses that of the oracles in a specific state. Empirical evaluations and theoretical analysis validate that RPI excels in comparison to existing state-of-the-art methodologies, demonstrating superior performance across various benchmark domains.",
      "authors": [
        "Xuefeng Liu",
        "Takuma Yoneda",
        "Rick Stevens",
        "Matthew Walter",
        "Yuxin Chen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=eJ0dzPJq1F",
      "cdate": 1695465823358,
      "mdate": 1713121697472,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709452"
    },
    {
      "id": "vtyasLn4RM",
      "title": "CoRe-GD: A Hierarchical Framework for Scalable Graph Visualization with GNNs",
      "abstract": "Graph Visualization, also known as Graph Drawing, aims to find geometric embeddings of graphs that optimize certain criteria. Stress is a widely used metric; stress is minimized when every pair of nodes is positioned at their shortest path distance. However, stress optimization presents computational challenges due to its inherent complexity and is usually solved using heuristics in practice. We introduce a scalable Graph Neural Network (GNN) based Graph Drawing framework with sub-quadratic runtime that can learn to optimize stress. Inspired by classical stress optimization techniques and force-directed layout algorithms, we create a coarsening hierarchy for the input graph. Beginning at the coarsest level, we iteratively refine and un-coarsen the layout, until we generate an embedding for the original graph. To enhance information propagation within the network, we propose a novel positional rewiring technique based on intermediate node positions. Our empirical evaluation demonstrates that the framework achieves state-of-the-art performance while remaining scalable.",
      "authors": [
        "Florian Grötschla",
        "Joël Mathys",
        "Robert Veres",
        "Roger Wattenhofer"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vtyasLn4RM",
      "cdate": 1695464122177,
      "mdate": 1710264612287,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709457"
    },
    {
      "id": "HRkyLbBRHI",
      "title": "Compositional Conservatism: A Transductive Approach in Offline Reinforcement Learning",
      "abstract": "Offline reinforcement learning (RL) is a compelling framework for learning optimal policies from past experiences without additional interaction with the environment. Nevertheless, offline RL inevitably faces the problem of distributional shifts, where the states and actions encountered during policy execution may not be in the training dataset distribution. A common solution involves incorporating conservatism into the policy or the value function to safeguard against uncertainties and unknowns. In this work, we focus on achieving the same objectives of conservatism but from a different perspective. We propose COmpositional COnservatism with Anchor-seeking (COCOA) for offline RL, an approach that pursues conservatism in a _compositional_ manner on top of the transductive reparameterization (Netanyahu et al., 2023), which decomposes the input variable (the state in our case) into an anchor and its difference from the original input. Our COCOA seeks both in-distribution anchors and differences by utilizing the learned reverse dynamics model, encouraging conservatism in the compositional input space for the policy or value function. Such compositional conservatism is independent of and agnostic to the prevalent _behavioral_ conservatism in offline RL. We apply COCOA to four state-of-the-art offline RL algorithms and evaluate them on the D4RL benchmark, where COCOA generally improves the performance of each algorithm. The code is available at https://github.com/runamu/compositional-conservatism.",
      "authors": [
        "Yeda Song",
        "Dongwook Lee",
        "Gunhee Kim"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=HRkyLbBRHI",
      "cdate": 1695463939115,
      "mdate": 1710589878816,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709464"
    },
    {
      "id": "OwtMhMSybu",
      "title": "Unlocking the Power of Representations in Long-term Novelty-based Exploration",
      "abstract": "We introduce Robust Exploration via Clustering-based Online Density Estimation (RECODE), a non-parametric method for novelty-based exploration that estimates visitation counts for clusters of states based on their similarity in a chosen embedding space. By adapting classical clustering to the nonstationary setting of Deep RL, RECODE can efficiently track state visitation counts over thousands of episodes. We further propose a novel generalization of the inverse dynamics loss, which leverages masked transformer architectures for multi-step prediction; which in conjunction with \\DETOCS achieves a new state-of-the-art in a suite of challenging 3D-exploration tasks in DM-Hard-8. RECODE also sets new state-of-the-art in hard exploration Atari games, and is the first agent to reach the end screen in \"Pitfall!\"",
      "authors": [
        "Alaa Saade",
        "Steven Kapturowski",
        "Daniele Calandriello",
        "Charles Blundell",
        "Pablo Sprechmann",
        "Leopoldo Sarra",
        "Oliver Groth",
        "Michal Valko",
        "Bilal Piot"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OwtMhMSybu",
      "cdate": 1695463508006,
      "mdate": 1711062954017,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709469"
    },
    {
      "id": "EG68RSznLT",
      "title": "Flow to Better: Offline Preference-based Reinforcement Learning via Preferred Trajectory Generation",
      "abstract": "Offline preference-based reinforcement learning (PbRL) offers an effective solution to overcome the challenges associated with designing rewards and the high costs of online interactions. In offline PbRL, agents are provided with a fixed dataset containing human preferences between pairs of trajectories. Previous studies mainly focus on recovering the rewards from the preferences, followed by policy optimization with an off-the-shelf offline RL algorithm. However, given that preference label in PbRL is inherently trajectory-based, accurately learning transition-wise rewards from such label can be challenging, potentially leading to misguidance during subsequent offline RL training. To address this issue, we introduce our method named $\\textit{Flow-to-Better (FTB)}$, which leverages the pairwise preference relationship to guide a generative model in producing preferred trajectories, avoiding Temporal Difference (TD) learning with inaccurate rewards. Conditioning on a low-preference trajectory, $\\textit{FTB}$ uses a diffusion model to generate a better one with a higher preference, achieving high-fidelity full-horizon trajectory improvement. During diffusion training, we propose a technique called $\\textit{Preference Augmentation}$ to alleviate the problem of insufficient preference data. As a result, we surprisingly find that the model-generated trajectories not only exhibit increased preference and consistency with the real transition but also introduce elements of $\\textit{novelty}$ and $\\textit{diversity}$, from which we can derive a desirable policy through imitation learning. Experimental results on D4RL benchmarks demonstrate that FTB achieves a remarkable improvement compared to state-of-the-art offline PbRL methods. Furthermore, we show that FTB can also serve as an effective data augmentation method for offline RL.",
      "authors": [
        "Zhilong Zhang",
        "Yihao Sun",
        "Junyin Ye",
        "Tian-Shuo Liu",
        "Jiaji Zhang",
        "Yang Yu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EG68RSznLT",
      "cdate": 1695462996246,
      "mdate": 1710076083499,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709474"
    },
    {
      "id": "iTFdNLHE7k",
      "title": "Kernelised Normalising Flows",
      "abstract": "Normalising Flows are non-parametric statistical models known for their dual capabilities of density estimation and generation. They are distinguished by their inherently invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve satisfactory outcomes. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency. Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability.",
      "authors": [
        "Eshant English",
        "Matthias Kirchler",
        "Christoph Lippert"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iTFdNLHE7k",
      "cdate": 1695462847152,
      "mdate": 1710429531618,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709478"
    },
    {
      "id": "R7rZUSGOPD",
      "title": "PAE: Reinforcement Learning from External Knowledge for Efficient Exploration",
      "abstract": "Human intelligence is adept at absorbing valuable insights from external knowledge.\nThis capability is equally crucial for artificial intelligence. \nIn contrast, classical reinforcement learning agents lack such capabilities and often resort to extensive trial and error to explore the environment. \nThis paper introduces $\\textbf{PAE}$: $\\textbf{P}$lanner-$\\textbf{A}$ctor-$\\textbf{E}$valuator, a novel framework for teaching agents to $\\textit{learn to absorb external knowledge}$. \nPAE integrates the Planner's knowledge-state alignment mechanism, the Actor's mutual information skill control, and the Evaluator's adaptive intrinsic exploration reward to achieve 1) effective cross-modal information fusion, 2) enhanced linkage between knowledge and state, and 3) hierarchical mastery of complex tasks.\nComprehensive experiments across\n 11 challenging tasks from the BabyAI and MiniHack environment suites demonstrate PAE's superior exploration efficiency with good interpretability.",
      "authors": [
        "Zhe Wu",
        "Haofei Lu",
        "Junliang Xing",
        "You Wu",
        "Renye Yan",
        "Yaozhong Gan",
        "Yuanchun Shi"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=R7rZUSGOPD",
      "cdate": 1695461906953,
      "mdate": 1710296500377,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709483"
    },
    {
      "id": "8p3fu56lKc",
      "title": "One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention",
      "abstract": "Recent works have empirically analyzed in-context learning and shown that transformers trained on synthetic linear regression tasks can learn to implement ridge regression, which is the Bayes-optimal predictor, given sufficient capacity (Akyurek et al., 2023), while one-layer transformers with linear self-attention and no MLP layer will learn to implement one step of gradient descent (GD) on a least-squares linear regression objective (von Oswald et al., 2022). However, the theory behind these observations remains poorly understood. We theoretically study transformers with a single layer of linear self-attention, trained on synthetic noisy linear regression data. First, we mathematically show that when the covariates are drawn from a standard Gaussian distribution, the one-layer transformer which minimizes the pre-training loss will implement a single step of GD on the least-squares linear regression objective. Then, we find that changing the distribution of the covariates and weight vector to a non-isotropic Gaussian distribution has a strong impact on the learned algorithm: the global minimizer of the pre-training loss now implements a single step of $\\textit{pre-conditioned}$ GD. However, if only the distribution of the responses is changed, then this does not have a large effect on the learned algorithm: even when the response comes from a more general family of $\\textit{nonlinear}$ functions, the global minimizer of the pre-training loss still implements a single step of GD on a least-squares linear regression objective.",
      "authors": [
        "Arvind V. Mahankali",
        "Tatsunori Hashimoto",
        "Tengyu Ma"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=8p3fu56lKc",
      "cdate": 1695460535763,
      "mdate": 1710567873830,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709487"
    },
    {
      "id": "8euJaTveKw",
      "title": "Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models",
      "abstract": "Recently, GPT-4 has become the de facto evaluator for long-form text generated by large language models (LLMs). However, for practitioners and researchers with large and custom evaluation tasks, GPT-4 is unreliable due to its closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose PROMETHEUS a fully open-source LLM that is on par with GPT-4’s evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. For this purpose, we construct a new dataset – FEEDBACK COLLECTION – that consists of 1K fine-grained score rubrics, 20K instructions, and 100K natural language feedback generated by GPT-4. Using the FEEDBACK COLLECTION, we train PROMETHEUS, a 13B evaluation-specific LLM that can assess any given response based on novel and unseen score rubrics and reference materials provided by the user. Our dataset’s versatility and diversity make our model generalize to challenging real-world criteria, such as prioritizing conciseness, child-readability, or varying levels of formality. We show that PROMETHEUS shows a stronger correlation with GPT-4 evaluation compared to ChatGPT on seven evaluation benchmarks (Two Feedback Collection testsets, MT Bench, Vicuna Bench, Flask Eval, MT Bench Human Judgment, and HHH Alignment), showing the efficacy of our model and dataset design. During human evaluation with hand-crafted score rubrics, PROMETHEUS shows a Pearson correlation of 0.897 with human evaluators, which is on par with GPT-4-0613 (0.882), and greatly outperforms ChatGPT (0.392). Remarkably, when assessing the quality of the generated feedback, PROMETHEUS demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. Our findings suggests that by adding reference materials and training on GPT-4 feedback, we can obtain effective open-source evaluator LMs.",
      "authors": [
        "Seungone Kim",
        "Jamin Shin",
        "Yejin Cho",
        "Joel Jang",
        "Shayne Longpre",
        "Hwaran Lee",
        "Sangdoo Yun",
        "Seongjin Shin",
        "Sungdong Kim",
        "James Thorne",
        "Minjoon Seo"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=8euJaTveKw",
      "cdate": 1695459491424,
      "mdate": 1713672967492,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709495"
    },
    {
      "id": "xuY33XhEGR",
      "title": "ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs",
      "abstract": "Climate and weather prediction traditionally relies on complex numerical simulations of atmospheric physics. Deep learning approaches, such as transformers, have recently challenged the simulation paradigm with complex network forecasts. However, they often act as data-driven black-box models that neglect the underlying physics and lack uncertainty quantification. We address these limitations with ClimODE, a  spatiotemporal continuous-time process that implements a key principle of advection from statistical mechanics, namely, weather changes due to a spatial movement of quantities over time. ClimODE models precise weather evolution with value-conserving dynamics, learning global weather transport as a neural flow, which also enables estimating the uncertainty in predictions. Our approach outperforms existing data-driven methods in global and regional forecasting with an order of magnitude smaller parameterization, establishing a new state of the art.",
      "authors": [
        "Yogesh Verma",
        "Markus Heinonen",
        "Vikas Garg"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xuY33XhEGR",
      "cdate": 1695459335703,
      "mdate": 1713672052718,
      "matched_keywords": [
        "transformer",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709500"
    },
    {
      "id": "THUBTfSAS2",
      "title": "Querying Easily Flip-flopped Samples for Deep Active Learning",
      "abstract": "Active learning, a paradigm within machine learning, aims to select and query unlabeled data to enhance model performance strategically. A crucial selection strategy leverages the model's predictive uncertainty, reflecting the informativeness of a data point. While the sample's distance to the decision boundary intuitively measures predictive uncertainty, its computation becomes intractable for complex decision boundaries formed in multiclass classification tasks. This paper introduces the *least disagree metric* (LDM), the smallest probability of predicted label disagreement. We propose an asymptotically consistent estimator for LDM under mild assumptions. The estimator boasts computational efficiency and straightforward implementation for deep learning models using parameter perturbation. The LDM-based active learning algorithm queries unlabeled data with the smallest LDM, achieving state-of-the-art *overall* performance across various datasets and deep architectures, as demonstrated by the experimental results.",
      "authors": [
        "Seong Jin Cho",
        "Gwangsu Kim",
        "Junghyun Lee",
        "Jinwoo Shin",
        "Chang D. Yoo"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=THUBTfSAS2",
      "cdate": 1695459298144,
      "mdate": 1713147561968,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709505"
    },
    {
      "id": "FDb2JQZsFH",
      "title": "Attention-based Iterative Decomposition for Tensor Product Representation",
      "abstract": "In recent research, Tensor Product Representation (TPR) is applied for the systematic generalization task of deep neural networks by learning the compositional structure of data. However, such prior works show limited performance in discovering and representing the symbolic structure from unseen test data because their decomposition to the structural representations was incomplete. In this work, we propose an Attention-based Iterative Decomposition (AID) module designed to enhance the decomposition operations for the structured representations encoded from the sequential input data with TPR. Our AID can be easily adapted to any TPR-based model and provides enhanced systematic decomposition through a competitive attention mechanism between input features and structured representations. In our experiments, AID shows effectiveness by significantly improving the performance of TPR-based prior works on the series of systematic generalization tasks. Moreover, in the quantitative and qualitative evaluations, AID produces more compositional and well-bound structural representations than other works.",
      "authors": [
        "Taewon Park",
        "Inchul Choi",
        "Minho Lee"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FDb2JQZsFH",
      "cdate": 1695459055806,
      "mdate": 1712115401563,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709509"
    },
    {
      "id": "2iGiSHmeAN",
      "title": "BroGNet: Momentum-Conserving Graph Neural Stochastic Differential Equation for Learning Brownian Dynamics",
      "abstract": "Neural networks (NNs) that exploit strong inductive biases based on physical laws and symmetries have shown remarkable success in learning the dynamics of physical systems directly from their trajectory. However, these works focus only on the systems that follow deterministic dynamics, such as Newtonian or Hamiltonian. Here, we propose a framework, namely Brownian graph neural networks (BroGNet), combining stochastic differential equations (SDEs) and GNNs to learn Brownian dynamics directly from the trajectory. We modify the architecture of BroGNet to enforce linear momentum conservation of the system, which, in turn, provides superior performance on learning dynamics as revealed empirically. We demonstrate this approach on several systems, namely, linear spring, linear spring with binary particle types, and non-linear spring systems, all following Brownian dynamics at finite temperatures. We show that BroGNet significantly outperforms proposed baselines across all the benchmarked Brownian systems. In addition, we demonstrate zero-shot generalizability of BroGNet to simulate unseen system sizes that are two orders of magnitude larger and to different temperatures than those used during training. Finally, we show that BroGNet conserves the momentum of the system resulting in superior performance and data efficiency. Altogether, our study contributes to advancing the understanding of the intricate dynamics of Brownian motion and demonstrates the effectiveness of graph neural networks in modeling such complex systems.",
      "authors": [
        "Suresh Bishnoi",
        "Jayadeva Jayadeva",
        "Sayan Ranu",
        "N M Anoop Krishnan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=2iGiSHmeAN",
      "cdate": 1695458666120,
      "mdate": 1710493272597,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709513"
    },
    {
      "id": "5jWsW08zUh",
      "title": "Some Fundamental Aspects about Lipschitz Continuity of Neural Networks",
      "abstract": "Lipschitz continuity is a crucial functional property of any predictive model, that naturally governs its robustness, generalisation, as well as adversarial vulnerability. Contrary to other works that focus on obtaining tighter bounds and developing different practical strategies to enforce certain Lipschitz properties, we aim to thoroughly examine and characterise the Lipschitz behaviour of Neural Networks. Thus, we carry out an empirical investigation in a range of different settings (namely, architectures, datasets, label noise, and more) by exhausting the limits of the simplest and the most general lower and upper bounds. As a highlight of this investigation, we showcase a remarkable fidelity of the lower Lipschitz bound, identify a striking Double Descent trend in both upper and lower bounds to the Lipschitz and explain the intriguing effects of label noise on function smoothness and generalisation.",
      "authors": [
        "Grigory Khromov",
        "Sidak Pal Singh"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5jWsW08zUh",
      "cdate": 1695458252877,
      "mdate": 1713673016218,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709518"
    },
    {
      "id": "JSS9rKHySk",
      "title": "On the Role of General Function Approximation in Offline Reinforcement Learning",
      "abstract": "We study offline reinforcement learning (RL) with general function approximation. General function approximation is a powerful tool for algorithm design and analysis, but its adaptation to offline RL encounters several challenges due to varying approximation targets and assumptions that blur the real meanings of function assumptions. In this paper, we try to formulate and clarify the treatment of general function approximation in offline RL in two aspects: (1) analyzing different types of assumptions and their practical usage, and (2) understanding its role as a restriction on underlying MDPs from information-theoretic perspectives. Additionally, we introduce a new insight for lower bound establishing: one can exploit model-realizability to establish general-purpose lower bounds that can be generalized into other functions. Building upon this insight, we propose two generic lower bounds that contribute to a better understanding of offline RL with general function approximation.",
      "authors": [
        "Chenjie Mao",
        "Qiaosheng Zhang",
        "Zhen Wang",
        "Xuelong Li"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=JSS9rKHySk",
      "cdate": 1695457613799,
      "mdate": 1712985838781,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709522"
    },
    {
      "id": "mYWsyTuiRp",
      "title": "Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Maps",
      "abstract": "Transformers are ubiquitous in wide tasks.\nInterpreting their internals is a pivotal goal. \nNevertheless, their particular components, feed-forward (FF) blocks, have typically been less analyzed despite their substantial parameter amounts.\nWe analyze the input contextualization effects of FF blocks by rendering them in the attention maps as a human-friendly visualization scheme.\nOur experiments with both masked- and causal-language models reveal that FF networks modify the input contextualization to emphasize specific types of linguistic compositions. \nIn addition, FF and its surrounding components tend to cancel out each other's effects, suggesting potential redundancy in the processing of the Transformer layer.",
      "authors": [
        "Goro Kobayashi",
        "Tatsuki Kuribayashi",
        "Sho Yokoi",
        "Kentaro Inui"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=mYWsyTuiRp",
      "cdate": 1695456913252,
      "mdate": 1713672261281,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709527"
    },
    {
      "id": "ox2ATRM90I",
      "title": "Yet Another ICU Benchmark: A Flexible Multi-Center Framework for Clinical ML",
      "abstract": "Medical applications of machine learning (ML) have experienced a surge in popularity in recent years. Given the abundance of available data from electronic health records, the intensive care unit (ICU) is a natural habitat for ML. Models have been proposed to address numerous ICU prediction tasks like the early detection of complications. While authors frequently report state-of-the-art performance, it is challenging to verify claims of superiority. Datasets and code are not always published, and cohort definitions, preprocessing pipelines, and training setups are difficult to reproduce. This work introduces Yet Another ICU Benchmark (YAIB), a modular framework that allows researchers to define reproducible and comparable clinical ML experiments; we offer an end-to-end solution from cohort definition to model evaluation. The framework natively supports most open-access ICU datasets (MIMIC III/IV, eICU, HiRID, AUMCdb) and is easily adaptable to future ICU datasets. Combined with a transparent preprocessing pipeline and extensible training code for multiple ML and deep learning models, YAIB enables unified model development, transfer, and evaluation. Our benchmark comes with five predefined established prediction tasks (mortality, acute kidney injury, sepsis, kidney function, and length of stay) developed in collaboration with clinicians. Adding further tasks is straightforward by design. Using YAIB, we demonstrate that the choice of dataset, cohort definition, and preprocessing have a major impact on the prediction performance — often more so than model class — indicating an urgent need for YAIB as a holistic benchmarking tool. We provide our work to the clinical ML community to accelerate method development and enable real-world clinical implementations.",
      "authors": [
        "Robin van de Water",
        "Hendrik Nils Aurel Schmidt",
        "Paul Elbers",
        "Patrick Thoral",
        "Bert Arnrich",
        "Patrick Rockenschaub"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ox2ATRM90I",
      "cdate": 1695456571924,
      "mdate": 1710499356442,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709531"
    },
    {
      "id": "iPWxqnt2ke",
      "title": "Identifying Policy Gradient Subspaces",
      "abstract": "Policy gradient methods hold great potential for solving complex continuous control tasks. Still, their training efficiency can be improved by exploiting structure within the optimization problem. Recent work indicates that supervised learning can be accelerated by leveraging the fact that gradients lie in a low-dimensional and slowly-changing subspace. In this paper, we conduct a thorough evaluation of this phenomenon for two popular deep policy gradient methods on various simulated benchmark tasks. Our results demonstrate the existence of such gradient subspaces despite the continuously changing data distribution inherent to reinforcement learning. These findings reveal promising directions for future work on more efficient reinforcement learning, e.g., through improving parameter-space exploration or enabling second-order optimization.",
      "authors": [
        "Jan Schneider",
        "Pierre Schumacher",
        "Simon Guist",
        "Le Chen",
        "Daniel Haeufle",
        "Bernhard Schölkopf",
        "Dieter Büchler"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iPWxqnt2ke",
      "cdate": 1695456061732,
      "mdate": 1710585183725,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709536"
    },
    {
      "id": "j4VMrwgn1M",
      "title": "Training Graph Transformers via Curriculum-Enhanced Attention Distillation",
      "abstract": "Recent studies have shown that Graph Transformers (GTs) can be effective for specific graph-level tasks. However, when it comes to node classification, training GTs remains challenging, especially in semi-supervised settings with a severe scarcity of labeled data. Our paper aims to address this research gap by focusing on semi-supervised node classification. To accomplish this, we develop a curriculum-enhanced attention distillation method that involves utilizing a Local GT teacher and a Global GT student. Additionally, we introduce the concepts of in-class and out-of-class and then propose two improvements, out-of-class entropy and top-k pruning, to facilitate the student's out-of-class exploration under the teacher's in-class guidance. Taking inspiration from human learning, our method involves a curriculum mechanism for distillation that initially provides strict guidance to the student and gradually allows for more out-of-class exploration by a dynamic balance. Extensive experiments show that our method outperforms many state-of-the-art approaches on seven public graph benchmarks, proving its effectiveness.",
      "authors": [
        "Yisong Huang",
        "Jin Li",
        "Xinlong Chen",
        "Yang-Geng Fu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=j4VMrwgn1M",
      "cdate": 1695455854090,
      "mdate": 1710327888371,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709543"
    },
    {
      "id": "bm1JVsVZVu",
      "title": "Adaptive Stochastic Gradient Algorithm for Black-box Multi-Objective Learning",
      "abstract": "Multi-objective optimization (MOO) has become an influential framework for various machine learning problems, including reinforcement learning and multi-task learning. In this paper, we study the black-box multi-objective optimization problem, where we aim to optimize multiple potentially conflicting objectives with function queries only. To address this challenging problem and find a Pareto optimal solution or the Pareto stationary solution, \nwe propose a novel adaptive stochastic gradient algorithm for black-box MOO, called ASMG. \nSpecifically, we use the stochastic gradient approximation method to obtain the gradient for the distribution parameters of the Gaussian smoothed MOO with function queries only. Subsequently, an adaptive weight is employed to aggregate all stochastic gradients to optimize all objective functions effectively. \nTheoretically, we explicitly provide the connection between the original MOO problem and the corresponding Gaussian smoothed MOO problem and prove the convergence rate for the proposed ASMG algorithm in both convex and non-convex scenarios.\nEmpirically, the proposed ASMG method achieves competitive performance on multiple numerical benchmark problems. Additionally, the state-of-the-art performance on the black-box multi-task learning problem demonstrates the effectiveness of the proposed ASMG method.",
      "authors": [
        "Feiyang Ye",
        "Yueming Lyu",
        "Xuehao Wang",
        "Yu Zhang",
        "Ivor Tsang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=bm1JVsVZVu",
      "cdate": 1695455725780,
      "mdate": 1717092774435,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709548"
    },
    {
      "id": "zAdUB0aCTQ",
      "title": "AgentBench: Evaluating LLMs as Agents",
      "abstract": "The potential of Large Language Model (LLM) as agents has been widely acknowledged recently.\nThus, there is an urgent need to quantitatively evaluate LLMs as agents on challenging tasks in interactive environments.\nWe present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities.\nOur extensive test over 29 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B.\nWe identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents.\nImproving instruction following and training on high quality multi-round alignment data could improve agent performance.\nAnd different from existing assumptions, training on code present ambivalent impacts on different agent tasks.\nDatasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.",
      "authors": [
        "Xiao Liu",
        "Hao Yu",
        "Hanchen Zhang",
        "Yifan Xu",
        "Xuanyu Lei",
        "Hanyu Lai",
        "Yu Gu",
        "Hangliang Ding",
        "Kaiwen Men",
        "Kejuan Yang",
        "Shudan Zhang",
        "Xiang Deng",
        "Aohan Zeng",
        "Zhengxiao Du",
        "Chenhui Zhang",
        "Sheng Shen",
        "Tianjun Zhang",
        "Yu Su",
        "Huan Sun",
        "Minlie Huang",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=zAdUB0aCTQ",
      "cdate": 1695455338027,
      "mdate": 1713363290705,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709552"
    },
    {
      "id": "YEhQs8POIo",
      "title": "Differentially Private Synthetic Data via Foundation Model APIs 1: Images",
      "abstract": "Generating differentially private (DP) synthetic data that closely resembles the original private data is a scalable way to mitigate privacy concerns in the current data-driven world. In contrast to current practices that train customized models for this task, we aim to generate DP Synthetic Data via APIs (DPSDA), where we treat foundation models as blackboxes and only utilize their inference APIs. Such API-based, training-free approaches are easier to deploy as exemplified by the recent surge in the number of API-based apps. These approaches can also leverage the power of large foundation models which are only accessible via their inference APIs. However, this comes with greater challenges due to strictly more restrictive model access and the need to protect privacy from the API provider. \n\nIn this paper, we present a new framework called Private Evolution (PE) to solve this problem and show its initial promise on synthetic images. Surprisingly, PE can match or even outperform state-of-the-art (SOTA) methods without any model training. For example, on CIFAR10 (with ImageNet as the public data), we achieve FID ≤ 7.9 with privacy cost ε = 0.67, significantly improving the previous SOTA from ε = 32. We further demonstrate the promise of applying PE on large foundation models such as Stable Diffusion to tackle challenging private datasets with a small number of high-resolution images. The code and data are released at https://github.com/microsoft/DPSDA.",
      "authors": [
        "Zinan Lin",
        "Sivakanth Gopi",
        "Janardhan Kulkarni",
        "Harsha Nori",
        "Sergey Yekhanin"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=YEhQs8POIo",
      "cdate": 1695454652451,
      "mdate": 1711489659145,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709557"
    },
    {
      "id": "pdJXYfJjz9",
      "title": "Harnessing Joint Rain-/Detail-aware Representations to Eliminate Intricate Rains",
      "abstract": "Recent advances in image deraining have focused on training powerful models on mixed multiple datasets comprising diverse rain types and backgrounds. However, this approach tends to overlook the inherent differences among rainy images, leading to suboptimal results. To overcome this limitation, we focus on addressing various rainy images by delving into meaningful representations that encapsulate both the rain and background components. Leveraging these representations as instructive guidance, we put forth a Context-based Instance-level Modulation (CoI-M) mechanism adept at efficiently modulating CNN- or Transformer-based models. Furthermore, we devise a rain-/detail-aware contrastive learning strategy to help extract joint rain-/detail-aware representations. By integrating CoI-M with the rain-/detail-aware Contrastive learning, we develop [CoIC](https://github.com/Schizophreni/CoIC), an innovative and potent algorithm tailored for training models on mixed datasets. Moreover, CoIC offers insight into modeling relationships of datasets, quantitatively assessing the impact of rain and details on restoration, and unveiling distinct behaviors of models given diverse inputs. Extensive experiments validate the efficacy of CoIC in boosting the deraining ability of CNN and Transformer models. CoIC also enhances the deraining prowess remarkably when real-world dataset is included.",
      "authors": [
        "Wu Ran",
        "Peirong Ma",
        "Zhiquan He",
        "Hao Ren",
        "Hong Lu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pdJXYfJjz9",
      "cdate": 1695454342976,
      "mdate": 1713672209847,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709561"
    },
    {
      "id": "1Wi0Ys33Nm",
      "title": "Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes",
      "abstract": "The infinitely wide neural network has been proven a useful and manageable mathematical model that enables the understanding of many phenomena appearing in deep learning. One example is the convergence of random deep networks to Gaussian processes that enables a rigorous analysis of the way the choice of activation function and network weights impacts the training dynamics. In this paper, we extend the seminal proof of Matthews et al., 2018 to a larger class of initial weight distributions (which we call pseudo-iid), including the established cases of iid and orthogonal weights, as well as the emerging low-rank and structured sparse settings celebrated for their computational speed-up benefits. We show that fully-connected and convolutional networks initialised with pseudo-iid distributions are all effectively equivalent up to their variance. Using our results, one can identify the Edge of Chaos for a broader class of neural networks and tune them at criticality in order to enhance their training. Moreover, they enable the posterior distribution of Bayesian Neural Networks to be tractable across these various initialization schemes.",
      "authors": [
        "Thiziri Nait Saada",
        "Alireza Naderi",
        "Jared Tanner"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=1Wi0Ys33Nm",
      "cdate": 1695453986706,
      "mdate": 1710779566748,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709566"
    },
    {
      "id": "rIx1YXVWZb",
      "title": "Understanding Addition in Transformers",
      "abstract": "Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper provides a comprehensive analysis of a one-layer Transformer model trained to perform n-digit integer addition. Our findings suggests that the model dissects the task into parallel streams dedicated to individual digits, employing varied algorithms tailored to different positions within the digits. Furthermore, we identify a rare scenario characterized by high loss, which we explain. By thoroughly elucidating the model’s algorithm, we provide new insights into its functioning. These findings are validated through rigorous testing and mathematical modeling, thereby contributing to the broader fields of model understanding and interpretability. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.",
      "authors": [
        "Philip Quirke",
        "Fazl Barez"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rIx1YXVWZb",
      "cdate": 1695453637820,
      "mdate": 1710621021332,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709571"
    },
    {
      "id": "9OevMUdods",
      "title": "Towards Understanding Factual Knowledge of Large Language Models",
      "abstract": "Large language models (LLMs) have recently driven striking performance improvements across a range of natural language processing tasks. The factual knowledge acquired during pretraining and instruction tuning can be useful in various downstream tasks, such as question answering, and language generation. Unlike conventional Knowledge Bases (KBs) that explicitly store factual knowledge, LLMs implicitly store facts in their parameters. Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time. To this end, we aim to explore the extent and scope of factual knowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains 20K diverse factual questions that span different sources, timelines, domains, regions, and languages. Furthermore, we investigate whether LLMs can compose multiple facts, update factual knowledge temporally, reason over multiple pieces of facts, identify subtle factual differences, and resist adversarial examples. Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing trustworthy artificial intelligence. The dataset Pinocchio and our codes are publicly available at: https://github.com/THU-BPM/Pinocchio.",
      "authors": [
        "Xuming Hu",
        "Junzhe Chen",
        "Xiaochuan Li",
        "Yufei Guo",
        "Lijie Wen",
        "Philip S. Yu",
        "Zhijiang Guo"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9OevMUdods",
      "cdate": 1695453247533,
      "mdate": 1713672956312,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709576"
    },
    {
      "id": "RzY9qQHUXy",
      "title": "Kill Two Birds with One Stone: Rethinking Data Augmentation for Deep Long-tailed Learning",
      "abstract": "Real-world tasks are universally associated with training samples that exhibit a long-tailed class distribution, and traditional deep learning models are not suitable for fitting this distribution, thus resulting in a biased trained model. To surmount this dilemma, massive deep long-tailed learning studies have been proposed to achieve inter-class fairness models by designing sophisticated sampling strategies or improving existing model structures and loss functions. Habitually, these studies tend to apply data augmentation strategies to improve the generalization performance of their models. However, this augmentation strategy applied to balanced distributions may not be the best option for long-tailed distributions. For a profound understanding of data augmentation, we first theoretically analyze the gains of traditional augmentation strategies in long-tailed learning, and observe that augmentation methods cause the long-tailed distribution to be imbalanced again, resulting in an intertwined imbalance: inherent data-wise imbalance and extrinsic augmentation-wise imbalance, i.e., two 'birds' co-exist in long-tailed learning. Motivated by this observation, we propose an adaptive Dynamic Optional Data Augmentation (DODA) to address this intertwined imbalance, i.e., one 'stone' simultaneously 'kills' two 'birds', which allows each class to choose appropriate augmentation methods by maintaining a corresponding augmentation probability distribution for each class during training. Extensive experiments across mainstream long-tailed recognition benchmarks (e.g., CIFAR-100-LT, ImageNet-LT, and iNaturalist 2018) prove the effectiveness and flexibility of the DODA in overcoming the intertwined imbalance.",
      "authors": [
        "Binwu Wang",
        "Pengkun Wang",
        "Wei Xu",
        "Xu Wang",
        "Yudong Zhang",
        "Kun Wang",
        "Yang Wang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=RzY9qQHUXy",
      "cdate": 1695452645918,
      "mdate": 1709661537218,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709583"
    },
    {
      "id": "MCNqgUFTHI",
      "title": "Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents",
      "abstract": "Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues.",
      "authors": [
        "Yang Deng",
        "Wenxuan Zhang",
        "Wai Lam",
        "See-Kiong Ng",
        "Tat-Seng Chua"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MCNqgUFTHI",
      "cdate": 1695452594816,
      "mdate": 1710145889242,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709588"
    },
    {
      "id": "9bmTbVaA2A",
      "title": "Bootstrapping Variational Information Pursuit with Large Language and Vision Models for Interpretable Image Classification",
      "abstract": "Variational Information Pursuit (V-IP) is an interpretable-by-design framework that makes predictions by sequentially selecting a short chain of user-defined, interpretable queries about the data that are most informative for the task. The prediction is based solely on the obtained query answers, which also serve as a faithful explanation for the prediction. Applying the framework to any task requires (i) specification of a query set, and (ii) densely annotated data with query answers to train classifiers to answer queries at test time. This limits V-IP's application to small-scale tasks where manual data annotation is feasible. In this work, we focus on image classification tasks and propose to relieve this bottleneck by leveraging pretrained language and vision models. Specifically, following recent work, we propose to use GPT, a Large Language Model, to propose semantic concepts as queries for a given classification task. To answer these queries, we propose a light-weight Concept Question-Answering network (Concept-QA) which learns to answer binary queries about semantic concepts in images. We design pseudo-labels to train our Concept-QA model using GPT and CLIP (a Vision-Language Model). Empirically, we find our Concept-QA model to be competitive with state-of-the-art VQA models in terms of answering accuracy but with an order of magnitude fewer parameters. This allows for seamless integration of Concept-QA into the V-IP framework as a fast-answering mechanism. We name this method Concept-QA+V-IP. Finally, we show on several datasets that Concept-QA+V-IP produces shorter, interpretable query chains which are more accurate than V-IP trained with CLIP-based answering systems. Code available at https://github.com/adityac94/conceptqa_vip.",
      "authors": [
        "Aditya Chattopadhyay",
        "Kwan Ho Ryan Chan",
        "Rene Vidal"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9bmTbVaA2A",
      "cdate": 1695452528460,
      "mdate": 1713672952283,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709595"
    },
    {
      "id": "5ep85sakT3",
      "title": "Contextual Bandits with Online Neural Regression",
      "abstract": "Recent works have shown a reduction from contextual bandits to online regression under a realizability assumption (Foster and Rakhlin, 2020; Foster and Krishnamurthy, 2021). In this work, we investigate the use of neural networks for such online regression and associated Neural Contextual Bandits (NeuCBs). Using existing results for wide networks, one can readily show a  ${\\mathcal{O}}(\\sqrt{T})$ regret for online regression with square loss, which via the reduction implies a ${\\mathcal{O}}(\\sqrt{K} T^{3/4})$ regret for NeuCBs. Departing from this standard approach, we first show a $\\mathcal{O}(\\log T)$ regret for online regression with almost convex losses that satisfy QG (Quadratic Growth) condition, a generalization of the PL (Polyak-\\L ojasiewicz) condition, and that have a unique minima. Although not directly applicable to wide networks since they do not have unique minima, we show that adding a suitable small random perturbation to the network predictions surprisingly makes the loss satisfy QG with unique minima. Based on such a perturbed prediction, we show a ${\\mathcal{O}}(\\log T)$ regret for online regression with both squared loss and KL loss, and subsequently convert these respectively to $\\tilde{\\mathcal{O}}(\\sqrt{KT})$ and $\\tilde{\\mathcal{O}}(\\sqrt{KL^*} + K)$ regret for NeuCB, where $L^*$ is the loss of the best policy. Separately, we also show that existing regret bounds for NeuCBs are $\\Omega(T)$ or assume i.i.d. contexts, unlike this work. Finally, our experimental results on various datasets demonstrate that our algorithms, especially the one based on KL loss, persistently outperform existing algorithms.",
      "authors": [
        "Rohan Deb",
        "Yikun Ban",
        "Shiliang Zuo",
        "Jingrui He",
        "Arindam Banerjee"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5ep85sakT3",
      "cdate": 1695452320737,
      "mdate": 1713082710827,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709599"
    },
    {
      "id": "tr0KidwPLc",
      "title": "Evaluating Large Language Models at Evaluating Instruction Following",
      "abstract": "As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these “LLM evaluators”, particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM evaluator in discerning instruction-following outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBar, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.",
      "authors": [
        "Zhiyuan Zeng",
        "Jiatong Yu",
        "Tianyu Gao",
        "Yu Meng",
        "Tanya Goyal",
        "Danqi Chen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tr0KidwPLc",
      "cdate": 1695452302329,
      "mdate": 1713159693767,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709604"
    },
    {
      "id": "ZTssMmhC2X",
      "title": "How to Fine-Tune Vision Models with SGD",
      "abstract": "SGD and AdamW are the two most used optimizers for fine-tuning large neural networks in computer vision. When the two methods perform the same, SGD is preferable because it uses less memory (12 bytes/parameter with momentum and 8 bytes/parameter without) than AdamW (16 bytes/parameter). However, on a suite of downstream tasks, especially those with distribution shifts, we find that fine-tuning with AdamW performs substantially better than SGD on modern Vision Transformer and ConvNeXt models. We find that large gaps in performance between SGD and AdamW occur when the fine-tuning gradients in the first \"embedding\" layer are much larger than in the rest of the model. Our analysis suggests an easy fix that works consistently across datasets and models: freezing the embedding layer (less than 1% of the parameters) leads to SGD with or without momentum performing slightly better than AdamW while using less memory (e.g., on ViT-L, SGD uses 33% less GPU memory). Our insights result in state-of-the-art accuracies on five popular distribution shift benchmarks: WILDS-FMoW, WILDS-Camelyon, BREEDS-Living-17, Waterbirds, and DomainNet.",
      "authors": [
        "Ananya Kumar",
        "Ruoqi Shen",
        "Sebastien Bubeck",
        "Suriya Gunasekar"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ZTssMmhC2X",
      "cdate": 1695452177952,
      "mdate": 1710487469213,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709609"
    },
    {
      "id": "ByR3NdDSZB",
      "title": "PARL: A Unified Framework for Policy Alignment in Reinforcement Learning from Human Feedback",
      "abstract": "We present a novel unified bilevel optimization-based framework, \\textsf{PARL}, formulated to address the recently highlighted critical issue of policy alignment in reinforcement learning using utility or preference-based feedback. We identify a major gap within current algorithmic designs for solving policy alignment due to a lack of precise characterization of the dependence of the alignment objective on the data generated by policy trajectories. This shortfall contributes to the sub-optimal performance observed in contemporary algorithms. Our framework addressed these concerns by explicitly parameterizing the distribution of the upper alignment objective (reward design)  by the lower optimal variable (optimal policy for the designed reward). Interestingly, from an optimization perspective, our formulation leads to a new class of stochastic bilevel problems where the stochasticity at the upper objective depends upon the lower-level variable. {True to our best knowledge, this work presents the first formulation of the RLHF as a bilevel optimization problem which generalizes the existing RLHF formulations and addresses the existing distribution shift issues in RLHF formulations.} To demonstrate the efficacy of our formulation in resolving alignment issues in RL, we devised an algorithm named \\textsf{A-PARL} to solve PARL problem, establishing sample complexity bounds of order $\\mathcal{O}(1/T)$. Our empirical results substantiate that the proposed \\textsf{PARL} can address the alignment concerns in RL by showing significant improvements (up to 63\\% in terms of required samples) for policy alignment in large-scale environments of the Deepmind control suite and Meta world tasks.",
      "authors": [
        "Souradip Chakraborty",
        "Amrit Bedi",
        "Alec Koppel",
        "Huazheng Wang",
        "Dinesh Manocha",
        "Mengdi Wang",
        "Furong Huang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ByR3NdDSZB",
      "cdate": 1695451669195,
      "mdate": 1713672895452,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709614"
    },
    {
      "id": "tsE5HLYtYg",
      "title": "SafeDreamer: Safe Reinforcement Learning with World Models",
      "abstract": "The deployment of Reinforcement Learning (RL) in real-world applications is constrained by its failure to satisfy safety criteria.\nExisting Safe Reinforcement Learning (SafeRL) methods, which rely on cost functions to enforce safety, often fail to achieve zero-cost performance in complex scenarios, especially vision-only tasks. These limitations are primarily due to model inaccuracies and inadequate sample efficiency. The integration of the world model has proven effective in mitigating these shortcomings. In this work, we introduce SafeDreamer, a novel algorithm incorporating Lagrangian-based methods into world model planning processes within the superior Dreamer framework. Our method achieves nearly zero-cost performance on various tasks, spanning low-dimensional and vision-only input, within the Safety-Gymnasium benchmark, showcasing its efficacy in balancing performance and safety in RL tasks. \nFurther details can be found in the code repository: https://github.com/PKU-Alignment/SafeDreamer.",
      "authors": [
        "Weidong Huang",
        "Jiaming Ji",
        "Chunhe Xia",
        "Borong Zhang",
        "Yaodong Yang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tsE5HLYtYg",
      "cdate": 1695451668179,
      "mdate": 1716275492106,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709618"
    },
    {
      "id": "DiWRG9JTWZ",
      "title": "MetaCoCo: A New Few-Shot Classification Benchmark with Spurious Correlation",
      "abstract": "Out-of-distribution (OOD) problems in few-shot classification (FSC) occur when novel classes sampled from testing distributions differ from base classes drawn from training distributions, which considerably degrades the performance of deep learning models deployed in real-world applications. Recent studies suggest that the OOD problems in FSC mainly including: (a) cross-domain few-shot classification (CD-FSC) and (b) spurious-correlation few-shot classification (SC-FSC). Specifically, CD-FSC occurs when a classifier learns transferring knowledge from base classes drawn from \\underline{seen} training distributions but recognizes novel classes sampled from unseen testing distributions. In contrast, SC-FSC arises when a classifier relies on non-causal features (or contexts) that happen to be correlated with the labels (or concepts) in base classes but such relationships no longer hold during the model deployment. Despite CD-FSC has been extensively studied, SC-FSC remains understudied due to lack of the corresponding evaluation benchmarks. To this end, we present Meta Concept Context (MetaCoCo), a benchmark with spurious-correlation shifts collected from real-world scenarios. Moreover, to quantify the extent of spurious-correlation shifts of the presented MetaCoCo, we further propose a metric by using CLIP as a pre-trained vision-language model. Extensive experiments on the proposed benchmark are performed to evaluate the state-of-the-art methods in FSC, cross-domain shifts, and self-supervised learning. The experimental results show that the performance of the existing methods degrades significantly in the presence of spurious-correlation shifts. We open-source all codes of our benchmark and hope that the proposed MetaCoCo can facilitate future research on spurious-correlation shifts problems in FSC.",
      "authors": [
        "Min Zhang",
        "Haoxuan Li",
        "Fei Wu",
        "Kun Kuang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DiWRG9JTWZ",
      "cdate": 1695451450691,
      "mdate": 1713882369935,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709623"
    },
    {
      "id": "5sixirvG0I",
      "title": "Whittle Index with Multiple Actions and State Constraint for Inventory Management",
      "abstract": "Whittle index is a heuristic tool that leads to good performance for the restless bandits problem. In this paper, we extend Whittle index to a new multi-agent reinforcement learning (MARL) setting with multiple discrete actions and a possibly changing constraint on the state space, resulting in WIMS (Whittle Index with Multiple actions and State constraint). This setting is common for inventory management where each agent chooses a replenishing quantity level for the corresponding stock-keeping-unit (SKU) such that the total profit is maximized while the total inventory does not exceed a certain limit. Accordingly, we propose a deep MARL algorithm based on WIMS for inventory management. Empirically, our algorithm is evaluated on real large-scale inventory management problems with up to 2307 SKUs and outperforms operation-research-based methods and baseline MARL algorithms.",
      "authors": [
        "Chuheng Zhang",
        "Xiangsen Wang",
        "Wei Jiang",
        "Xianliang Yang",
        "Siwei Wang",
        "Lei Song",
        "Jiang Bian"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5sixirvG0I",
      "cdate": 1695451259681,
      "mdate": 1709661536802,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709627"
    },
    {
      "id": "HHbRxoDTxE",
      "title": "Looped Transformers are Better at Learning Learning Algorithms",
      "abstract": "Transformers have demonstrated effectiveness in in-context solving data-fitting problems from various (latent) models, as reported by Garg et al. (2022). However, the absence of an inherent iterative structure in the transformer architecture presents a challenge in emulating the iterative algorithms, which are commonly employed in traditional machine learning methods. To address this, we propose the utilization of looped transformer architecture and its associated training methodology, with the aim of incorporating iterative characteristics into the transformer architectures. Experimental results suggest that the looped transformer achieves performance comparable to the standard transformer in solving various data-fitting problems, while utilizing less than 10% of the parameter count.",
      "authors": [
        "Liu Yang",
        "Kangwook Lee",
        "Robert D Nowak",
        "Dimitris Papailiopoulos"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=HHbRxoDTxE",
      "cdate": 1695450873525,
      "mdate": 1710271255421,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709632"
    },
    {
      "id": "mF3cTns4pe",
      "title": "Sum-Product-Set Networks: Deep Tractable Models for Tree-Structured Graphs",
      "abstract": "Daily internet communication relies heavily on tree-structured graphs, embodied by popular data formats such as XML and JSON. However, many recent generative (probabilistic) models utilize neural networks to learn a probability distribution over undirected cyclic graphs. This assumption of a generic graph structure brings various computational challenges, and, more importantly, the presence of non-linearities in neural networks does not permit tractable probabilistic inference. We address these problems by proposing sum-product-set networks, an extension of probabilistic circuits from unstructured tensor data to tree-structured graph data. To this end, we use random finite sets to reflect a variable number of nodes and edges in the graph and to allow for exact and efficient inference. We demonstrate that our tractable model performs comparably to various intractable models based on neural networks.",
      "authors": [
        "Milan Papez",
        "Martin Rektoris",
        "Vaclav Smidl",
        "Tomáš Pevný"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=mF3cTns4pe",
      "cdate": 1695450856461,
      "mdate": 1713672265208,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709636"
    },
    {
      "id": "Qwq4cpLtoX",
      "title": "Is attention required for ICL? Exploring the Relationship Between Model Architecture and In-Context Learning Ability",
      "abstract": "What is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps toward answering this question. We evaluate thirteen model architectures capable of causal language modeling across a suite of synthetic in-context learning tasks. These selected architectures represent a broad range of paradigms, including recurrent and convolution-based neural networks, transformers, state space model inspired, and other emerging attention alternatives. We discover that all the considered architectures can perform in-context learning under a wider range of conditions than previously documented. Additionally, we observe stark differences in statistical efficiency and consistency by varying the number of in-context examples and task difficulty. We also measure each architecture's predisposition towards in-context learning when presented with the option to memorize rather than leverage in-context examples. Finally, and somewhat surprisingly, we find that several attention alternatives are sometimes competitive with or better in-context learners than transformers. However, no single architecture demonstrates consistency across all tasks, with performance either plateauing or declining when confronted with a significantly larger number of in-context examples than those encountered during gradient-based training.",
      "authors": [
        "Ivan Lee",
        "Nan Jiang",
        "Taylor Berg-Kirkpatrick"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Qwq4cpLtoX",
      "cdate": 1695450252317,
      "mdate": 1713672636571,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709643"
    },
    {
      "id": "nJnky5K944",
      "title": "Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?",
      "abstract": "Existing analyses of the expressive capacity of Transformer models have required excessively deep layers for data memorization, leading to a discrepancy with the Transformers actually used in practice. \nThis is primarily due to the interpretation of the softmax function as an approximation of the hardmax function.\nBy clarifying the connection between the softmax function and the Boltzmann operator, we prove that a single layer of self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence.\nAs a consequence, we show that one-layer and single-head Transformers have a memorization capacity for finite samples, and that Transformers consisting of one self-attention layer with two feed-forward neural networks are universal approximators for continuous functions on a compact domain.",
      "authors": [
        "Tokio Kajitsuka",
        "Issei Sato"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nJnky5K944",
      "cdate": 1695449558707,
      "mdate": 1710171685505,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709648"
    },
    {
      "id": "5bNYf0CqxY",
      "title": "Certified Adversarial Robustness for Rate Encoded Spiking Neural Networks",
      "abstract": "The spiking neural networks are inspired by the biological neurons that employ binary spikes to propagate information in the neural network. It has garnered considerable attention as the next-generation neural network, as the spiking activity simplifies the computation burden of the network to a large extent and is known for its low energy deployment enabled by specialized neuromorphic hardware. One popular technique to feed a static image to such a network is rate encoding, where each pixel is encoded into random binary spikes, following a Bernoulli distribution that uses the pixel intensity as bias. By establishing a novel connection between rate-encoding and randomized smoothing, we give the first provable robustness guarantee for spiking neural networks against adversarial perturbation of inputs bounded under $l_1$-norm. We introduce novel adversarial training algorithms for rate-encoded models that significantly improve the state-of-the-art empirical robust accuracy result. Experimental validation of the method is performed across various static image datasets, including CIFAR-10, CIFAR-100 and ImageNet-100. The code is available at \\url{https://github.com/BhaskarMukhoty/CertifiedSNN}.",
      "authors": [
        "Bhaskar Mukhoty",
        "Hilal AlQuabeh",
        "Giulia De Masi",
        "Huan Xiong",
        "Bin Gu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5bNYf0CqxY",
      "cdate": 1695449463582,
      "mdate": 1710932016863,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709653"
    },
    {
      "id": "o8tjamaJ80",
      "title": "Adversarial AutoMixup",
      "abstract": "Data mixing augmentation has been widely applied to improve the generalization ability of deep neural networks. Recently, offline data mixing augmentation, e.g. handcrafted and saliency information-based mixup, has been gradually replaced by automatic mixing approaches. Through minimizing two sub-tasks, namely, mixed sample generation and mixup classification in an end-to-end way, AutoMix significantly improves accuracy on image classification tasks. However, as the optimization objective is consistent for the two sub-tasks, this approach is prone to generating consistent instead of diverse mixed samples, which results in overfitting for target task training. In this paper, we propose AdAutomixup, an adversarial automatic mixup augmentation approach that generates challenging samples to train a robust classifier for image classification, by alternatively optimizing the classifier and the mixup sample generator. AdAutomixup comprises two modules, a mixed example generator, and a target classifier. The mixed sample generator aims to produce hard mixed examples to challenge the target classifier, while the target classifier's aim is to learn robust features from hard mixed examples to improve generalization. To prevent the collapse of the inherent meanings of images, we further introduce an exponential moving average (EMA) teacher and cosine similarity to train AdAutomixup in an end-to-end way. Extensive experiments on seven image benchmarks consistently prove that our approach outperforms the state of the art in various classification scenarios. The source code is available at \nhttps://github.com/JinXins/Adversarial-AutoMixup.",
      "authors": [
        "Huafeng Qin",
        "Xin Jin",
        "Yun Jiang",
        "Mounîm El-Yacoubi",
        "Xinbo Gao"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=o8tjamaJ80",
      "cdate": 1695447438799,
      "mdate": 1709661536192,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709658"
    },
    {
      "id": "Ts95eXsPBc",
      "title": "Spatially-Aware Transformers for Embodied Agents",
      "abstract": "Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanced accuracy in various place-centric downstream tasks. Additionally, we propose the Adaptive Memory Allocator, a memory management method based on reinforcement learning that aims to optimize efficiency of memory utilization. Our experiments demonstrate the advantages of our proposed model in various environments and across multiple downstream tasks, including prediction, generation, reasoning, and reinforcement learning. The source code for our models and experiments will be available at \\href{https://github.com/spatially_aware_transformer}{https://github.com/spatially_aware_transformer}.",
      "authors": [
        "Junmo Cho",
        "Jaesik Yoon",
        "Sungjin Ahn"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Ts95eXsPBc",
      "cdate": 1695446964971,
      "mdate": 1712303192278,
      "matched_keywords": [
        "reinforcement learning",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709662"
    },
    {
      "id": "xAqcJ9XoTf",
      "title": "On the Stability of Expressive Positional Encodings for Graphs",
      "abstract": "Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) *Non-uniqueness*: there are many different eigendecompositions of the same Laplacian, and (2) *Instability*: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding.  Despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be the use of \"hard partition'' of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to ``softly partition'' eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally expressive for basis invariant functions whilst respecting all symmetries of eigenvectors. Besides guaranteed stability, we prove that SPE is at least as expressive as existing methods, and highly capable of counting graph structures. Finally, we evaluate the effectiveness of our method on molecular property prediction, and out-of-distribution generalization tasks, finding improved generalization compared to existing positional encoding methods. Our code is available\nat https://github.com/Graph-COM/SPE.",
      "authors": [
        "Yinan Huang",
        "William Lu",
        "Joshua Robinson",
        "Yu Yang",
        "Muhan Zhang",
        "Stefanie Jegelka",
        "Pan Li"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xAqcJ9XoTf",
      "cdate": 1695446894256,
      "mdate": 1713672064938,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709667"
    },
    {
      "id": "HiTg16qhxp",
      "title": "Dynamic Neural Response Tuning",
      "abstract": "Artificial Neural Networks (ANNs) have gained widespread applications across various areas in recent years. The ANN design was initially inspired by principles of biology. The biological neural network's fundamental response process comprises information transmission and aggregation. The information transmission in biological neurons is often achieved by triggering action potentials that propagate through axons. ANNs utilize activation mechanisms to simulate such biological behavior. However, previous studies have only considered static response conditions, while the biological neuron's response conditions are typically dynamic, depending on multiple factors such as neuronal properties and the real-time environment. Therefore, the dynamic response conditions of biological neurons could help improve the static ones of existing activations in ANNs. Additionally, the biological neuron's aggregated response exhibits high specificity for different categories, allowing the nervous system to differentiate and identify objects. Inspired by these biological patterns, we propose a novel Dynamic Neural Response Tuning (DNRT) mechanism, which aligns the response patterns of ANNs with those of biological neurons. DNRT comprises Response-Adaptive Activation (RAA) and Aggregated Response Regularization (ARR), mimicking the biological neuron's information transmission and aggregation behaviors. RAA dynamically adjusts the response condition based on the characteristics and strength of the input signal. ARR is devised to enhance the network's ability to learn category specificity by imposing constraints on the network's response distribution. Extensive experimental studies indicate that the proposed DNRT is highly interpretable, applicable to various mainstream network architectures, and can achieve remarkable performance compared with existing neural response mechanisms in multiple tasks and domains. Code is available at https://github.com/horrible-dong/DNRT.",
      "authors": [
        "Tian Qiu",
        "Wenxiang Xu",
        "Lin Chen",
        "Linyun Zhou",
        "Zunlei Feng",
        "Mingli Song"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=HiTg16qhxp",
      "cdate": 1695446427585,
      "mdate": 1713468172672,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709671"
    },
    {
      "id": "vW1SkPl4kp",
      "title": "Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation and Human Feedback",
      "abstract": "Risk-sensitive reinforcement learning (RL) aims to optimize policies that balance the expected reward and risk. In this paper, we present a novel risk-sensitive RL framework that employs an Iterated Conditional Value-at-Risk (CVaR) objective under both linear and general function approximations, enriched by human feedback. These new formulations provide a principled way to guarantee safety in each decision making step throughout the control process. Moreover, integrating human feedback into risk-sensitive RL framework bridges the gap between algorithmic decision-making and human participation, allowing us to also guarantee safety for human-in-the-loop systems. We propose provably sample-efficient algorithms for this Iterated CVaR RL and provide rigorous theoretical analysis. Furthermore, we establish a matching lower bound to corroborate the optimality of our algorithms in a linear context.",
      "authors": [
        "Yu Chen",
        "Yihan Du",
        "Pihe Hu",
        "Siwei Wang",
        "Desheng Wu",
        "Longbo Huang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vW1SkPl4kp",
      "cdate": 1695446109044,
      "mdate": 1710595872959,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709678"
    },
    {
      "id": "vSwu81S33z",
      "title": "Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling",
      "abstract": "Transfer learning has recently shown significant performance across various tasks involving deep neural networks. In these transfer learning scenarios, the prior distribution for downstream data becomes crucial in Bayesian model averaging (BMA). While previous works proposed the prior over the neural network parameters centered around the pre-trained solution, such strategies have limitations when dealing with distribution shifts between upstream and downstream data. This paper introduces nonparametric transfer learning (NPTL), a flexible posterior sampling method to address the distribution shift issue within the context of nonparametric learning. The nonparametric learning (NPL) method is a recent approach that employs a nonparametric prior for posterior sampling, efficiently accounting for model misspecification scenarios, which is suitable for transfer learning scenarios that may involve the distribution shift between upstream and downstream tasks. Through extensive empirical validations, we demonstrate that our approach surpasses other baselines in BMA performance.",
      "authors": [
        "Hyungi Lee",
        "Giung Nam",
        "Edwin Fong",
        "Juho Lee"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vSwu81S33z",
      "cdate": 1695446057029,
      "mdate": 1709661535785,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709683"
    },
    {
      "id": "B6pQxqUcT8",
      "title": "ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search",
      "abstract": "Large language models (LLMs) have demonstrated powerful decision-making and planning capabilities in solving complicated real-world problems. LLM-based autonomous agents can interact with diverse tools (e.g., functional APIs) and generate solution plans that execute a series of API function calls in a step-by-step manner. The multitude of candidate API function calls significantly expands the action space, amplifying the critical need for efficient action space navigation. However, existing methods either struggle with unidirectional exploration in expansive action spaces, trapped into a locally optimal solution, or suffer from exhaustively traversing all potential actions, causing inefficient navigation. To address these issues, we propose ToolChain*, an efficient tree search-based planning algorithm for LLM-based agents. It formulates the entire action space as a decision tree, where each node represents a possible API function call involved in a solution plan. By incorporating the A$^*$ search algorithm with task-specific cost function design, it efficiently prunes high-cost branches that may involve incorrect actions, identifying the most low-cost valid path as the solution. Extensive experiments on multiple tool-use and reasoning tasks demonstrate that ToolChain* efficiently balances exploration and exploitation within an expansive action space. It outperforms state-of-the-art baselines on planning and reasoning tasks by 3.1% and 3.5% on average while requiring 7.35x and 2.31x less time, respectively.",
      "authors": [
        "Yuchen Zhuang",
        "Xiang Chen",
        "Tong Yu",
        "Saayan Mitra",
        "Victor Bursztyn",
        "Ryan A. Rossi",
        "Somdeb Sarkhel",
        "Chao Zhang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=B6pQxqUcT8",
      "cdate": 1695445921562,
      "mdate": 1710269590528,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709688"
    },
    {
      "id": "AssIuHnmHX",
      "title": "What Algorithms can Transformers Learn? A Study in Length Generalization",
      "abstract": "Large language models exhibit surprising emergent generalization properties, yet also struggle on many simple reasoning tasks such as arithmetic and parity. In this work, we focus on length generalization, and we propose a unifying framework to understand when and how Transformers can be expected to length generalize on a given task. First, we show that there exist algorithmic tasks for which standard\ndecoder-only Transformers trained from scratch naturally exhibit strong length generalization. For these tasks, we leverage the RASP programming language (Weiss et al., 2021) to show that the correct algorithmic solution which solves the task can be represented by a simple Transformer. We thus propose the RASP-Generalization Conjecture: Transformers tend to learn a length-generalizing solution if there exists a short RASP-L program that works for all input lengths. We present empirical evidence to support the correlation between RASP-simplicity and generalization. We leverage our insights to give new scratchpad formats which yield strong length generalization on traditionally hard tasks (such as parity and addition), and we illustrate how scratchpad can hinder generalization when it increases the complexity of the corresponding RASP-L program. Overall, our work provides a novel perspective on the mechanisms of length generalization and the algorithmic capabilities of Transformers.",
      "authors": [
        "Hattie Zhou",
        "Arwen Bradley",
        "Etai Littwin",
        "Noam Razin",
        "Omid Saremi",
        "Joshua M. Susskind",
        "Samy Bengio",
        "Preetum Nakkiran"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=AssIuHnmHX",
      "cdate": 1695445040398,
      "mdate": 1713672920723,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709697"
    },
    {
      "id": "Z9AZsU1Tju",
      "title": "Neuro-Inspired Information-Theoretic Hierarchical Perception for Multimodal Learning",
      "abstract": "Integrating and processing information from various sources or modalities are critical for obtaining a comprehensive and accurate perception of the real world in autonomous systems and cyber-physical systems. Drawing inspiration from neuroscience, we develop the Information-Theoretic Hierarchical Perception (ITHP) model, which utilizes the concept of information bottleneck. Different from most traditional fusion models that incorporate all modalities identically in neural networks, our model designates a prime modality and regards the remaining modalities as detectors in the information pathway, serving to distill the flow of information. Our proposed perception model focuses on constructing an effective and compact information flow by achieving a balance between the minimization of mutual information between the latent state and the input modal state, and the maximization of mutual information between the latent states and the remaining modal states. This approach leads to compact latent state representations that retain relevant information while minimizing redundancy, thereby substantially enhancing the performance of multimodal representation learning. Experimental evaluations on the MUStARD, CMU-MOSI, and CMU-MOSEI datasets demonstrate that our model consistently distills crucial information in multimodal learning scenarios, outperforming state-of-the-art benchmarks. Remarkably, on the CMU-MOSI dataset, ITHP surpasses human-level performance in the multimodal sentiment binary classification task across all evaluation metrics (i.e., Binary Accuracy, F1 Score, Mean Absolute Error, and Pearson Correlation).",
      "authors": [
        "Xiongye Xiao",
        "Gengshuo Liu",
        "Gaurav Gupta",
        "Defu Cao",
        "Shixuan Li",
        "Yaxing Li",
        "Tianqing Fang",
        "Mingxi Cheng",
        "Paul Bogdan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Z9AZsU1Tju",
      "cdate": 1695444224885,
      "mdate": 1713139055257,
      "matched_keywords": [
        "multimodal",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709702"
    },
    {
      "id": "7avlrpzWqo",
      "title": "Flag Aggregator: Scalable Distributed Training under Failures and Augmented Losses using Convex Optimization",
      "abstract": "Modern ML applications increasingly rely on complex deep learning models and large datasets. There has been an exponential growth in the amount of computation needed to train the largest models. Therefore, to scale computation and data, these models are inevitably trained in a distributed manner in clusters of nodes, and their updates are aggregated before being applied to the model. However, a distributed setup is prone to Byzantine failures of individual nodes, components, and software. With data augmentation added to these settings, there is a critical need for robust and efficient aggregation systems. We define the quality of workers as reconstruction ratios $\\in (0,1]$, and formulate aggregation as a Maximum Likelihood Estimation procedure using Beta densities. We show that the Regularized form of log-likelihood wrt subspace can be approximately solved using iterative least squares solver, and provide convergence guarantees using recent Convex Optimization landscape results. Our empirical findings demonstrate that our approach significantly enhances the robustness of state-of-the-art Byzantine resilient aggregators. We evaluate our method in a distributed setup with a parameter server, and show simultaneous improvements in communication efficiency and accuracy across various tasks.",
      "authors": [
        "Hamidreza Almasi",
        "Harsh Mishra",
        "Balajee Vamanan",
        "Sathya N. Ravi"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7avlrpzWqo",
      "cdate": 1695443930721,
      "mdate": 1710698201526,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709707"
    },
    {
      "id": "9m02ib92Wz",
      "title": "DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models",
      "abstract": "Quantifying the impact of training data points is crucial for understanding the outputs of machine learning models and for improving the transparency of the AI pipeline. The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging to use. This issue becomes more pronounced in the setting of large language models and text-to-image models. In this work, we propose DataInf, an efficient influence approximation method that is practical for large-scale generative AI models. Leveraging an easy-to-compute closed-form expression, DataInf outperforms existing influence computation algorithms in terms of computational and memory efficiency. Our theoretical analysis shows that DataInf is particularly well-suited for parameter-efficient fine-tuning techniques such as LoRA. Through systematic empirical evaluations, we show that DataInf accurately approximates influence scores and is orders of magnitude faster than existing methods. In applications to RoBERTa-large, Llama-2-13B-chat, and stable-diffusion-v1.5 models, DataInf effectively identifies the most influential fine-tuning examples better than other approximate influence scores. Moreover, it can help to identify which data points are mislabeled.",
      "authors": [
        "Yongchan Kwon",
        "Eric Wu",
        "Kevin Wu",
        "James Zou"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9m02ib92Wz",
      "cdate": 1695443296717,
      "mdate": 1710339506059,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709711"
    },
    {
      "id": "G2cG3mQqop",
      "title": "Image Clustering Conditioned on Text Criteria",
      "abstract": "Classical clustering methods do not provide users with direct control of the clustering results, and the clustering results may not be consistent with the relevant criterion that a user has in mind. In this work, we present a new methodology for performing image clustering based on user-specified criteria in the form of text by leveraging modern Vision-Language Models and Large Language Models. We call our method Image Clustering Conditioned on Text Criteria (IC$|$TC), and it represents a different paradigm of image clustering. IC$|$TC requires a minimal and practical degree of human intervention and grants the user significant control over the clustering results in return. Our experiments show that IC$|$TC can effectively cluster images with various criteria, such as human action, physical location, or the person's mood, significantly outperforming baselines.",
      "authors": [
        "Sehyun Kwon",
        "Jaeseung Park",
        "Minkyu Kim",
        "Jaewoong Cho",
        "Ernest K. Ryu",
        "Kangwook Lee"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=G2cG3mQqop",
      "cdate": 1695442749267,
      "mdate": 1709661535259,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709716"
    },
    {
      "id": "xHmCdSArUC",
      "title": "Correlated Noise Provably Beats Independent Noise for Differentially Private Learning",
      "abstract": "Differentially private learning algorithms inject noise into the learning process. While the most common private learning algorithm, DP-SGD, adds independent Gaussian noise in each iteration, recent work on matrix factorization mechanisms has shown empirically that introducing correlations in the noise can greatly improve their utility. We characterize the asymptotic learning utility for any choice of the correlation function, giving precise analytical bounds for linear regression and as the solution to a convex program for general convex functions. We show, using these bounds, how correlated noise provably improves upon vanilla DP-SGD as a function of problem parameters such as the effective dimension and condition number. Moreover, our analytical expression for the near-optimal correlation function circumvents the cubic complexity of the semi-definite program used to optimize the noise correlation matrix in previous work. We validate these theoretical results with experiments on private deep learning. Our work matches or outperforms prior work while being efficient both in terms of computation and memory.",
      "authors": [
        "Christopher A. Choquette-Choo",
        "Krishnamurthy Dj Dvijotham",
        "Krishna Pillutla",
        "Arun Ganesh",
        "Thomas Steinke",
        "Abhradeep Guha Thakurta"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xHmCdSArUC",
      "cdate": 1695442614918,
      "mdate": 1709661535246,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709721"
    },
    {
      "id": "43cYe4oogi",
      "title": "Understanding Expressivity of GNN in Rule Learning",
      "abstract": "Rule learning is critical to improving knowledge graph (KG) reasoning due to their ability to provide logical and interpretable explanations. Recently, Graph Neural Networks (GNNs) with tail entity scoring achieve the state-of-the-art performance on KG reasoning. However, the theoretical understandings for these GNNs are either lacking or focusing on single-relational graphs, leaving what the kind of rules these GNNs can learn an open problem. We propose to fill the above gap in this paper. Specifically, GNNs with tail entity scoring are unified into a common framework. Then, we analyze their expressivity by formally describing the rule structures they can learn and theoretically demonstrating their superiority. These results further inspire us to propose a novel labeling strategy to learn more rules in KG reasoning. Experimental results are consistent with our theoretical findings and verify the effectiveness of our proposed method. The code is publicly available at https://github.com/LARS-research/Rule-learning-expressivity.",
      "authors": [
        "Haiquan Qiu",
        "Yongqi Zhang",
        "Yong Li",
        "quanming yao"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=43cYe4oogi",
      "cdate": 1695442190470,
      "mdate": 1713673048087,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709726"
    },
    {
      "id": "kxgSlyirUZ",
      "title": "COLLIE: Systematic Construction of Constrained Text Generation Tasks",
      "abstract": "Text generation under constraints have seen increasing interests in natural language processing, especially with the rapidly improving capabilities of large language models. However, existing benchmarks for constrained generation usually focus on fixed constraint types (e.g. generate a sentence containing certain words) that have proved to be easy for state-of-the-art models like GPT-4. We present COLLIE, a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g. language understanding, logical reasoning, counting, semantic planning). We also develop tools for automatic extraction of task instances given a constraint structure and a raw text corpus. Using COLLIE, we compile the COLLIE-v1 dataset with 1,132 instances comprising 13 constraint structures. We perform systematic experiments across five state-of-the-art instruction-tuned language models and analyze their performances to reveal shortcomings. COLLIE is designed to be extensible and lightweight, and we hope the community finds it useful to develop more complex constraints and evaluations in the future.",
      "authors": [
        "Shunyu Yao",
        "Howard Chen",
        "Austin W. Hanjie",
        "Runzhe Yang",
        "Karthik R Narasimhan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kxgSlyirUZ",
      "cdate": 1695442156063,
      "mdate": 1712250011482,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709730"
    },
    {
      "id": "MNShbDSxKH",
      "title": "GENOME: Generative Neuro-Symbolic Visual Reasoning by Growing and Reusing Modules",
      "abstract": "Recent works have shown that Large Language Models (LLMs) could empower traditional neuro-symbolic models via programming capabilities to translate languages into module descriptions, thus achieving strong visual reasoning results while maintaining the model’s transparency and efficiency. However, these models usually exhaustively generate the entire code snippet given each new instance of a task, which is extremely ineffective. On the contrary, human beings gradually acquire knowledge that can be reused and grow into more profound skills for fast generalization to new tasks since we are an infant. Inspired by this, we propose generative neuro-symbolic visual reasoning by growing and reusing modules. Specifically, our model consists of three unique stages, module initialization, module generation, and module execution. First, given a vision-language task, we adopt LLMs to examine whether we could reuse and grow over established modules to handle this new task. If not, we initialize a new module needed by the task and specify the inputs and outputs of this new module. After that, the new module is created by querying LLMs to generate corresponding code snippets that match the requirements. In order to get a better sense of the new module’s ability, we treat few-shot training examples as test cases to see if our new module could pass these cases. If yes, the new module is added to the module library for future reuse. Finally, we evaluate the performance of our model on the testing set by executing the parsed programs with the newly made visual modules to get the results. We find the proposed GENOME model possesses several advantages. First, it performs competitively on standard tasks like visual question answering and referring expression comprehension; Second, the visual modules learned from one task can be seamlessly transferred to new tasks; Last but not least, it is able to adapt to new visual reasoning tasks by observing a few training examples and reusing modules.",
      "authors": [
        "Zhenfang Chen",
        "Rui Sun",
        "Wenjun Liu",
        "Yining Hong",
        "Chuang Gan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MNShbDSxKH",
      "cdate": 1695441884442,
      "mdate": 1713672719050,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709735"
    },
    {
      "id": "qoHeuRAcSl",
      "title": "Grounding Language Plans in Demonstrations Through Counterfactual Perturbations",
      "abstract": "Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural network to predict successful trajectories from failures and as a by-product learns classifiers that ground low-level states and images in mode families without dense labeling. The learned grounding classifiers can further be used to translate language plans into reactive policies in the physical domain in an interpretable manner. We show our approach improves the interpretability and reactivity of imitation learning through 2D navigation and simulated and real robot manipulation tasks. Website: https://yanweiw.github.io/glide/",
      "authors": [
        "Yanwei Wang",
        "Tsun-Hsuan Wang",
        "Jiayuan Mao",
        "Michael Hagenow",
        "Julie Shah"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qoHeuRAcSl",
      "cdate": 1695441511520,
      "mdate": 1713672185397,
      "matched_keywords": [
        "large language model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709740"
    },
    {
      "id": "A7t7z6g6tM",
      "title": "Hyper Evidential Deep Learning to Quantify Composite Classification Uncertainty",
      "abstract": "Deep neural networks (DNNs) have been shown to perform well on exclusive, multi-class classification tasks. However, when different classes have similar visual features, it becomes challenging for human annotators to differentiate them. When an image is ambiguous, such as a blurry one where an annotator can't distinguish between a husky and a wolf, it may be labeled with both classes: {husky, wolf}. This scenario necessitates the use of composite set labels. \nIn this paper, we propose a novel framework called Hyper-Evidential Neural Network (HENN) that explicitly models predictive uncertainty caused by composite set labels in training data in the context of the belief theory called Subjective Logic (SL).\nBy placing a Grouped Dirichlet distribution on the class probabilities, we treat predictions of a neural network as parameters of hyper-subjective opinions and learn the network that collects both single and composite evidence leading to these hyper-opinions by a deterministic DNN from data.\nWe introduce a new uncertainty type called vagueness originally designed for hyper-opinions in SL to quantify composite classification uncertainty for DNNs.\nOur experiments prove that HENN outperforms its state-of-the-art counterparts based on four image datasets.\nThe code and datasets are available at: https://shorturl.at/dhoqx.",
      "authors": [
        "Changbin Li",
        "Kangshuo Li",
        "Yuzhe Ou",
        "Lance M. Kaplan",
        "Audun Jøsang",
        "Jin-Hee Cho",
        "DONG HYUN JEONG",
        "Feng Chen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=A7t7z6g6tM",
      "cdate": 1695441508613,
      "mdate": 1713312440155,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709747"
    },
    {
      "id": "5o9G4XF1LI",
      "title": "Goodhart's Law in Reinforcement Learning",
      "abstract": "Implementing a reward function that perfectly captures a complex task in the real world is impractical. As a result, it is often appropriate to think of the reward function as a *proxy* for the true objective rather than as its definition. We study this phenomenon through the lens of *Goodhart’s law*, which predicts that increasing optimisation of an imperfect proxy beyond some critical point decreases performance on the true objective. First, we propose a way to *quantify* the magnitude of this effect and *show empirically* that optimising an imperfect proxy reward often leads to the behaviour predicted by Goodhart’s law for a wide range of environments and reward functions. We then provide a *geometric explanation* for why Goodhart's law occurs in Markov decision processes. We use these theoretical insights to propose an *optimal early stopping method* that provably avoids the aforementioned pitfall and derive theoretical *regret bounds* for this method. Moreover, we derive a training method that maximises worst-case reward, for the setting where there is uncertainty about the true reward function. Finally, we evaluate our early stopping method experimentally. Our results support a foundation for a theoretically-principled study of reinforcement learning under reward misspecification.",
      "authors": [
        "Jacek Karwowski",
        "Oliver Hayman",
        "Xingjian Bai",
        "Klaus Kiendlhofer",
        "Charlie Griffin",
        "Joar Max Viktor Skalse"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5o9G4XF1LI",
      "cdate": 1695441452361,
      "mdate": 1710516940492,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709752"
    },
    {
      "id": "xCRr9DrolJ",
      "title": "Score Regularized Policy Optimization through Diffusion Behavior",
      "abstract": "Recent developments in offline reinforcement learning have uncovered the immense potential of diffusion modeling, which excels at representing heterogeneous behavior policies. However, sampling from diffusion policies is considerably slow because it necessitates tens to hundreds of iterative inference steps for one action. To address this issue, we propose to extract an efficient deterministic inference policy from critic models and pretrained diffusion behavior models, leveraging the latter to directly regularize the policy gradient with the behavior\ndistribution’s score function during optimization. Our method enjoys powerful generative capabilities of diffusion modeling while completely circumventing the computationally intensive and time-consuming diffusion sampling scheme, both during training and evaluation. Extensive results on D4RL tasks show that our method boosts action sampling speed by more than 25 times compared with various leading diffusion-based methods in locomotion tasks, while still maintaining state-of-the-art performance.",
      "authors": [
        "Huayu Chen",
        "Cheng Lu",
        "Zhengyi Wang",
        "Hang Su",
        "Jun Zhu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xCRr9DrolJ",
      "cdate": 1695441440308,
      "mdate": 1710471370607,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709757"
    },
    {
      "id": "qPloNoDJZn",
      "title": "Robustifying and Boosting Training-Free Neural Architecture Search",
      "abstract": "Neural architecture search (NAS) has become a key component of AutoML and a standard tool to automate the design of deep neural networks. Recently, training-free NAS as an emerging paradigm has successfully reduced the search costs of standard training-based NAS by estimating the true architecture performance with only training-free metrics. Nevertheless, the estimation ability of these metrics typically varies across different tasks, making it challenging to achieve robust and consistently good search performance on diverse tasks with only a single training-free metric. Meanwhile, the estimation gap between training-free metrics and the true architecture performances limits training-free NAS to achieve superior performance. To address these challenges, we propose the robustifying and boosting training-free NAS (RoBoT) algorithm which (a) employs the optimized combination of existing training-free metrics explored from Bayesian optimization to develop a robust and consistently better-performing metric on diverse tasks, and (b) applies greedy search, i.e., the exploitation, on the newly developed metric to bridge the aforementioned gap and consequently to boost the search performance of standard training-free NAS further. Remarkably, the expected performance of our RoBoT can be theoretically guaranteed, which improves over the existing training-free NAS under mild conditions with additional interesting insights. Our extensive experiments on various NAS benchmark tasks yield substantial empirical evidence to support our theoretical results.",
      "authors": [
        "Zhenfeng He",
        "Yao Shu",
        "Zhongxiang Dai",
        "Bryan Kian Hsiang Low"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qPloNoDJZn",
      "cdate": 1695441218493,
      "mdate": 1710246611214,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709764"
    },
    {
      "id": "1vrS1zwekw",
      "title": "MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction Following",
      "abstract": "In the realm of large language models (LLMs), enhancing instruction-following capability often involves curating expansive training data. This is achieved through two primary schemes: i) Scaling-Inputs: Amplifying (input, output) pairs per task instruction, aiming for better instruction adherence. ii) Scaling Input-Free Tasks: Enlarging tasks, each composed of an (instruction, output) pair (without requiring a separate input anymore). However, LLMs under Scaling-Inputs tend to be overly sensitive to inputs, leading to misinterpretation or non-compliance with instructions. Conversely, Scaling Input-Free Tasks demands a substantial number of tasks but is less effective in instruction following when dealing with instances in Scaling-Inputs. This work introduces MUFFIN, a new scheme of instruction-following dataset curation. Specifically, we automatically Scale Tasks per Input by diversifying these tasks with various input facets. Experimental results across four zero-shot benchmarks, spanning both Scaling-Inputs and Scaling Input-Free Tasks schemes, reveal that LLMs, at various scales, trained on MUFFIN generally demonstrate superior instruction-following capabilities compared to those trained on the two aforementioned schemes.",
      "authors": [
        "Renze Lou",
        "Kai Zhang",
        "Jian Xie",
        "Yuxuan Sun",
        "Janice Ahn",
        "Hanzi Xu",
        "Yu Su",
        "Wenpeng Yin"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=1vrS1zwekw",
      "cdate": 1695440883918,
      "mdate": 1710471826631,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709769"
    },
    {
      "id": "DFTHW0MyiW",
      "title": "Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies",
      "abstract": "In light of the burgeoning success of reinforcement learning (RL) in diverse real-world applications, considerable focus has been directed towards ensuring RL policies are robust to adversarial attacks during test time. Current approaches largely revolve around solving a minimax problem to prepare for potential worst-case scenarios. While effective against strong attacks, these methods often compromise performance in the absence of attacks or the presence of only weak attacks. To address this, we study policy robustness under the well-accepted state-adversarial attack model, extending our focus beyond merely worst-case attacks. We first formalize this task at test time as a regret minimization problem and establish its intrinsic difficulty in achieving sublinear regret when the baseline policy is from a general continuous policy class, $\\Pi$. This finding prompts us to \\textit{refine} the baseline policy class $\\Pi$ prior to test time, aiming for efficient adaptation within a compact, finite policy class $\\tilde{\\Pi}$, which can resort to an adversarial bandit subroutine. In light of the importance of a finite and compact $\\tilde{\\Pi}$, we propose a novel training-time algorithm to iteratively discover \\textit{non-dominated policies}, forming a near-optimal and minimal $\\tilde{\\Pi}$, thereby ensuring both robustness and test-time efficiency. Empirical validation on the Mujoco corroborates the superiority of our approach in terms of natural and robust performance, as well as adaptability to various attack scenarios.",
      "authors": [
        "Xiangyu Liu",
        "Chenghao Deng",
        "Yanchao Sun",
        "Yongyuan Liang",
        "Furong Huang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DFTHW0MyiW",
      "cdate": 1695440205286,
      "mdate": 1712683675271,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709774"
    },
    {
      "id": "yxKZGQLzOP",
      "title": "Generating Pragmatic Examples to Train Neural Program Synthesizers",
      "abstract": "Programming-by-example is the task of synthesizing a program that is consistent with a set of user-provided input-output examples. \nAs examples are often an under-specification of one's intent, a good synthesizer must choose the intended program from the many that are consistent with the given set of examples. Prior work frames program synthesis as a cooperative game between a listener (that synthesizes programs) and a speaker (a user choosing examples), and shows that models of computational pragmatic inference are effective in choosing the user intended programs. However, these models require counterfactual reasoning over a large set of programs and examples, which is infeasible in realistic program spaces. In this paper, we propose PraX, a novel way to amortize this search with neural networks. We sample pairs of programs and examples via self-play between listener and speaker models, and use pragmatic inference to choose informative training examples from this sample. We then use the informative dataset to train models to improve the synthesizer's ability to disambiguate user-provided examples _without human supervision_. We validate PraX on the challenging task of synthesizing regular expressions from example strings, and find that our method (1) outperforms models trained without choosing pragmatic examples by 23% (a 51% relative increase) (2) matches the performance of supervised learning on a dataset of pragmatic examples provided by humans, despite using no human data in training.",
      "authors": [
        "Saujas Vaduguru",
        "Daniel Fried",
        "Yewen Pu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=yxKZGQLzOP",
      "cdate": 1695440057836,
      "mdate": 1712850820140,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709779"
    },
    {
      "id": "HSKaGOi7Ar",
      "title": "Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness",
      "abstract": "Designing expressive Graph Neural Networks (GNNs) is a fundamental topic in the graph learning community. So far, GNN expressiveness has been primarily assessed via the Weisfeiler-Lehman (WL) hierarchy. However, such an expressivity measure has notable limitations: it is inherently coarse, qualitative, and may not well reflect practical requirements (e.g., the ability to encode substructures). In this paper, we introduce a novel framework for quantitatively studying the expressiveness of GNN architectures, addressing all the above limitations. Specifically, we identify a fundamental expressivity measure termed homomorphism expressivity, which quantifies the ability of GNN models to count graphs under homomorphism. Homomorphism expressivity offers a complete and practical assessment tool: the completeness enables direct expressivity comparisons between GNN models, while the practicality allows for understanding concrete GNN abilities such as subgraph counting. By examining four classes of prominent GNNs as case studies, we derive simple, unified, and elegant descriptions of their homomorphism expressivity for both invariant and equivariant settings. Our results provide novel insights into a series of previous work, unify the landscape of different subareas in the community, and settle several open questions. Empirically, extensive experiments on both synthetic and real-world tasks verify our theory, showing that the practical performance of GNN models aligns well with the proposed metric.",
      "authors": [
        "Bohang Zhang",
        "Jingchu Gai",
        "Yiheng Du",
        "Qiwei Ye",
        "Di He",
        "Liwei Wang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=HSKaGOi7Ar",
      "cdate": 1695439977927,
      "mdate": 1710475662290,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709783"
    },
    {
      "id": "Pe2lo3QOvo",
      "title": "Making RL with Preference-based Feedback Efficient via Randomization",
      "abstract": "Reinforcement Learning algorithms that learn from human feedback (RLHF) need to be efficient in terms of *statistical complexity, computational complexity, and query complexity*. In this work, we consider the RLHF setting where the feedback is given in the format of preferences over pairs of trajectories. In the linear MDP model, using randomization in algorithm design, we present an algorithm that is sample efficient (i.e., has near-optimal worst-case regret bounds) and has polynomial running time (i.e., computational complexity is polynomial with respect to relevant parameters). Our algorithm further minimizes the query complexity through a novel randomized active learning procedure. In particular, our algorithm demonstrates a near-optimal tradeoff between the regret bound and the query complexity. To extend the results to more general nonlinear function approximation, we design a model-based randomized algorithm inspired by the idea of Thompson sampling. Our algorithm minimizes Bayesian regret bound and query complexity, again achieving a near-optimal tradeoff between these two quantities. Computation-wise, similar to the prior Thompson sampling algorithms under the regular RL setting, the main computation primitives of our algorithm are Bayesian supervised learning oracles which have been heavily investigated on the empirical side when applying Thompson sampling algorithms to RL benchmark problems.",
      "authors": [
        "Runzhe Wu",
        "Wen Sun"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Pe2lo3QOvo",
      "cdate": 1695439848196,
      "mdate": 1710279740803,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709787"
    },
    {
      "id": "eFWG9Cy3WK",
      "title": "Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy",
      "abstract": "Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up the learning capacity of neural networks, however, they have issues like: ($a$) $\\textit{High Memory Usage,}$ due to duplication of the network layers into multiple copies as experts; and ($b$) $\\textit{Redundancy in Experts,}$ as common learning-based routing policies suffer from representational collapse. Therefore, vanilla SMoE models are memory inefficient and non-scalable, especially for resource-constrained downstream scenarios. In this paper, we ask: Can we craft a compact SMoE model by consolidating expert information? What is the best recipe to merge multiple experts into fewer but more knowledgeable experts? Our pilot investigation reveals that conventional model merging methods fail to be effective in such expert merging for SMoE. The potential reasons are: ($1$) redundant information overshadows critical experts; ($2$) appropriate neuron permutation for each expert is missing to bring all of them in alignment. To address these challenges, we propose a novel merging algorithm for SMoE, $\\textit{i.e.}$, $\\texttt{M-SMoE}$, which leverages routing statistics to guide expert merging. Specifically, it starts with neuron permutation alignment for experts; then, dominant experts and their \"group members\" are formed based on routing policies; lastly, every expert group is merged into a single expert by utilizing each expert's activation frequency as their weight for merging, thus diminishing the impact of insignificant experts. Moreover, we draw an interesting observation that our proposed merging promotes a low dimensionality in the merged expert's weight space, naturally paving the way for additional compression. Hence, our final method, $\\texttt{MC-SMoE}$ ($\\textit{i.e.}$, Merge, then Compress SMoE), further decomposes the merged experts into low-rank and structural sparse alternatives. Extensive experiments across $8$ benchmarks validate the effectiveness of our proposals. For instance, our $\\texttt{MC-SMoE}$ achieves up to $80\\%$ memory and a $20\\%$ FLOPs reduction, with virtually no loss in performance. Our code is provided as supplementary material.",
      "authors": [
        "Pingzhi Li",
        "Zhenyu Zhang",
        "Prateek Yadav",
        "Yi-Lin Sung",
        "Yu Cheng",
        "Mohit Bansal",
        "Tianlong Chen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=eFWG9Cy3WK",
      "cdate": 1695439648331,
      "mdate": 1709803091321,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709792"
    },
    {
      "id": "iAW2EQXfwb",
      "title": "Negatively Correlated Ensemble Reinforcement Learning for Online Diverse Game Level Generation",
      "abstract": "Deep reinforcement learning has recently been successfully applied to online procedural content generation in which a policy determines promising game-level segments.  However, existing methods can hardly discover diverse level patterns, while the lack of diversity makes the gameplay boring. This paper proposes an ensemble reinforcement learning approach that uses multiple negatively correlated sub-policies to generate different alternative level segments, and stochastically selects one of them following a selector model. A novel policy regularisation technique is integrated into the approach to diversify the generated alternatives. In addition, we develop theorems to provide general methodologies for optimising policy regularisation in a Markov decision process. The proposed approach is compared with several state-of-the-art policy ensemble methods and classic methods on a well-known level generation benchmark, with two different reward functions expressing game-design goals from different perspectives. Results show that our approach boosts level diversity notably with competitive performance in terms of the reward.  Furthermore, by varying the regularisation coefficient, the trained generators form a well-spread Pareto front, allowing explicit trade-offs between diversity and rewards of generated levels.",
      "authors": [
        "Ziqi Wang",
        "Chengpeng Hu",
        "Jialin Liu",
        "Xin Yao"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iAW2EQXfwb",
      "cdate": 1695438972053,
      "mdate": 1710903445161,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709798"
    },
    {
      "id": "MOmqfJovQ6",
      "title": "Achieving Sample and Computational Efficient Reinforcement Learning by Action Space Reduction via Grouping",
      "abstract": "Reinforcement learning often needs to deal with the exponential growth of states and actions when exploring optimal control in high-dimensional spaces (often known as the curse of dimensionality). In this work, we address this issue by learning the inherent structure of action-wise similar MDP to appropriately balance the performance degradation versus sample/computational complexity.  In particular, we partition the action spaces into multiple groups based on the similarity in transition distribution and reward function, and build a linear decomposition model to capture the difference between the intra-group transition kernel and the intra-group rewards. Both our theoretical analysis and experiments reveal a *surprising and counter-intuitive result*: while a more refined grouping strategy can reduce the approximation error caused by treating actions in the same group as identical, it also leads to increased estimation error when the size of samples or the computation resources is limited. This finding highlights the grouping strategy as a new degree of freedom that can be optimized to minimize the overall performance loss. To address this issue, we formulate a general optimization problem for determining the optimal grouping strategy, which strikes a balance between performance loss and sample/computational complexity. We further propose a computationally efficient method for selecting a nearly-optimal grouping strategy, which maintains its computational complexity independent of the size of the action space.",
      "authors": [
        "Yining Li",
        "Peizhong Ju",
        "Ness Shroff"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MOmqfJovQ6",
      "cdate": 1695438756185,
      "mdate": 1709847650458,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709803"
    },
    {
      "id": "QgwAYFrh9t",
      "title": "Learning Hierarchical Polynomials with Three-Layer Neural Networks",
      "abstract": "We study the problem of learning hierarchical polynomials over the standard Gaussian distribution with three-layer neural networks. We specifically consider target functions of the form $h = g \\circ p$ where $p : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is a degree $k$ polynomial and $g: \\mathbb{R} \\rightarrow \\mathbb{R}$ is a degree $q$ polynomial. This function class generalizes the single-index model, which corresponds to $k=1$, and is a natural class of functions possessing an underlying hierarchical structure. Our main result shows that for a large subclass of degree $k$ polynomials $p$, a three-layer neural network trained via layerwise gradient descent on the square loss learns the target $h$ up to vanishing test error in $\\widetilde O(d^k)$ samples and polynomial time. This is a strict improvement over kernel methods, which require $\\widetilde \\Theta(d^{kq})$ samples, as well as existing guarantees for two-layer networks, which require the target function to be low-rank. Our result also generalizes prior works on three-layer neural networks, which were restricted to the case of $p$ being a quadratic. When $p$ is indeed a quadratic, we achieve the information-theoretically optimal sample complexity $\\widetilde O(d^2)$, which is an improvement over prior work (Nichani et al., 2023) requiring a sample size of $\\widetilde\\Theta(d^4)$. Our proof proceeds by showing that during the initial stage of training the network performs feature learning to recover the feature $p$ with $\\widetilde O(d^k)$ samples. This work demonstrates the ability of three-layer neural networks to learn complex features and as a result, learn a broad class of hierarchical functions.",
      "authors": [
        "Zihao Wang",
        "Eshaan Nichani",
        "Jason D. Lee"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=QgwAYFrh9t",
      "cdate": 1695438659180,
      "mdate": 1710255297829,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709807"
    },
    {
      "id": "i2Phucne30",
      "title": "On Bias-Variance Alignment in Deep Models",
      "abstract": "Classical wisdom in machine learning holds that the generalization error can be decomposed into bias and variance, and these two terms exhibit a \\emph{trade-off}. However, in this paper, we show that for an ensemble of deep learning based classification models, bias and variance are \\emph{aligned} at a sample level, where squared bias is approximately \\emph{equal} to variance for correctly classified sample points. We present empirical evidence confirming this phenomenon in a variety of deep learning models and datasets. Moreover, we study this phenomenon from two theoretical perspectives: calibration and neural collapse. We first show theoretically that under the assumption that the models are well calibrated, we can observe the bias-variance alignment. Second, starting from the picture provided by the neural collapse theory, we show an approximate correlation between bias and variance.",
      "authors": [
        "Lin Chen",
        "Michal Lukasik",
        "Wittawat Jitkrittum",
        "Chong You",
        "Sanjiv Kumar"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=i2Phucne30",
      "cdate": 1695438409662,
      "mdate": 1710553494085,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709812"
    },
    {
      "id": "KUNzEQMWU7",
      "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
      "abstract": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning. However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research. The project is available at https://mathvista.github.io/.",
      "authors": [
        "Pan Lu",
        "Hritik Bansal",
        "Tony Xia",
        "Jiacheng Liu",
        "Chunyuan Li",
        "Hannaneh Hajishirzi",
        "Hao Cheng",
        "Kai-Wei Chang",
        "Michel Galley",
        "Jianfeng Gao"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=KUNzEQMWU7",
      "cdate": 1695437335328,
      "mdate": 1710461815644,
      "matched_keywords": [
        "large language model",
        "foundation model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.709816"
    },
    {
      "id": "thbtoAkCe9",
      "title": "$\\mathbb{D}^2$ Pruning: Message Passing for Balancing Diversity & Difficulty in Data Pruning",
      "abstract": "In recent years, data quality has emerged as an important factor for training massive models. Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing *data diversity* in the coreset, and (2) functions that assign *difficulty scores* to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset selection. In this work, we represent a dataset as an undirected graph and propose a novel pruning algorithm, $\\mathbb{D}^2$ Pruning, that uses message passing over this dataset graph for coreset selection. $\\mathbb{D}^2$ Pruning updates the difficulty scores of each example by incorporating the difficulty of its neighboring examples in the dataset graph. Then, these updated difficulty scores direct a graph-based sampling method to select a coreset that encapsulates both diverse and difficult regions of the dataset space. We evaluate supervised and self-supervised versions of our method on various vision and NLP datasets. Results show that $\\mathbb{D}^2$ Pruning improves coreset selection over previous state-of-the-art methods at low-to-medium pruning rates. Additionally, we find that using $\\mathbb{D}^2$ Pruning for filtering large multimodal datasets leads to increased diversity in the dataset and improved generalization of pretrained models. Our work shows that $\\mathbb{D}^2$ Pruning is a versatile framework for understanding and processing datasets.",
      "authors": [
        "Adyasha Maharana",
        "Prateek Yadav",
        "Mohit Bansal"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=thbtoAkCe9",
      "cdate": 1695437097644,
      "mdate": 1710571305913,
      "matched_keywords": [
        "multimodal",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709821"
    },
    {
      "id": "tVTN7Zs0ml",
      "title": "GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs",
      "abstract": "Clinical predictive models often rely on patients’ electronic health records (EHR), but integrating medical knowledge to enhance predictions and decision-making is challenging. This is because personalized predictions require personalized knowledge\ngraphs (KGs), which are difficult to generate from patient EHR data. To address this, we propose GraphCare, an open-world framework that uses external KGs to improve EHR-based predictions. Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs, which are then used to train our proposed Bi-attention AugmenTed\n(BAT) graph neural network (GNN) for healthcare predictions. On two public datasets, MIMIC-III and MIMIC-IV, GraphCare surpasses baselines in four vital healthcare prediction tasks: mortality, readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it boosts AUROC by 17.6% and 6.6% for mortality and readmission, and F1-score by 7.9% and 10.8% for LOS and drug recommendation, respectively. Notably, GraphCare demonstrates a substantial edge in scenarios with limited data availability. Our findings highlight the potential of using external KGs in healthcare prediction tasks and demonstrate the promise of GraphCare in generating personalized KGs for promoting personalized medicine.",
      "authors": [
        "Pengcheng Jiang",
        "Cao Xiao",
        "Adam Richard Cross",
        "Jimeng Sun"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tVTN7Zs0ml",
      "cdate": 1695436918536,
      "mdate": 1713241910067,
      "matched_keywords": [
        "large language model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709825"
    },
    {
      "id": "mqVgBbNCm9",
      "title": "Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation",
      "abstract": "This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose Skeleton-of-Thought (SoT), which first guides LLMs to generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-ups across 12 LLMs, but it can also potentially improve the answer quality on several question categories. SoT is an initial attempt at data-centric optimization for inference efficiency, and showcases the potential of eliciting high-quality answers by explicitly planning the answer structure in language.",
      "authors": [
        "Xuefei Ning",
        "Zinan Lin",
        "Zixuan Zhou",
        "Zifu Wang",
        "Huazhong Yang",
        "Yu Wang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=mqVgBbNCm9",
      "cdate": 1695436657620,
      "mdate": 1713672256577,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709830"
    },
    {
      "id": "gzqrANCF4g",
      "title": "Language Model Beats Diffusion - Tokenizer is key to visual generation",
      "abstract": "While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce \\modelname{}, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representations for action recognition tasks.",
      "authors": [
        "Lijun Yu",
        "Jose Lezama",
        "Nitesh Bharadwaj Gundavarapu",
        "Luca Versari",
        "Kihyuk Sohn",
        "David Minnen",
        "Yong Cheng",
        "Agrim Gupta",
        "Xiuye Gu",
        "Alexander G Hauptmann",
        "Boqing Gong",
        "Ming-Hsuan Yang",
        "Irfan Essa",
        "David A Ross",
        "Lu Jiang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=gzqrANCF4g",
      "cdate": 1695436419522,
      "mdate": 1711734513637,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709834"
    },
    {
      "id": "3oTPsORaDH",
      "title": "SEGNO: Generalizing Equivariant Graph Neural Networks with Physical Inductive Biases",
      "abstract": "Graph Neural Networks (GNNs) with equivariant properties have emerged as powerful tools for modeling complex dynamics of multi-object physical systems. However, their generalization ability is limited by the inadequate consideration of physical inductive biases: (1) Existing studies overlook the continuity of transitions among system states, opting to employ several discrete transformation layers to learn the direct mapping between two adjacent states; (2) Most models only account for first-order velocity information, despite the fact that many physical systems are governed by second-order motion laws. To incorporate these inductive biases, we propose the Second-order Equivariant Graph Neural Ordinary Differential Equation (SEGNO). Specifically, we show how the second-order continuity can be incorporated into GNNs while maintaining the equivariant property. Furthermore, we offer theoretical insights into SEGNO, highlighting that it can learn a unique trajectory between adjacent states, which is crucial for model generalization. Additionally, we prove that the discrepancy between this learned trajectory of SEGNO and the true trajectory is bounded. Extensive experiments on complex dynamical systems including molecular dynamics and motion capture demonstrate that our model yields a significant improvement over the state-of-the-art baselines.",
      "authors": [
        "Yang Liu",
        "Jiashun Cheng",
        "Haihong Zhao",
        "Tingyang Xu",
        "Peilin Zhao",
        "Fugee Tsung",
        "Jia Li",
        "Yu Rong"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3oTPsORaDH",
      "cdate": 1695436389247,
      "mdate": 1713673056834,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709841"
    },
    {
      "id": "N2WchST43h",
      "title": "A Sublinear Adversarial Training Algorithm",
      "abstract": "Adversarial training is a widely used strategy for making neural networks resistant to adversarial perturbations. For a neural network of width $m$, $n$ input training data in $d$ dimension, it takes $\\Omega(mnd)$ time cost per training iteration for the forward and backward computation. In this paper we analyze the convergence guarantee of adversarial training procedure on a two-layer neural network with shifted ReLU activation, and shows that only $o(m)$ neurons will be activated for each input data per iteration. Furthermore, we develop an algorithm for adversarial training with time cost $o(m n d)$ per iteration by applying half-space reporting data structure.",
      "authors": [
        "Yeqi Gao",
        "Lianke Qin",
        "Zhao Song",
        "Yitan Wang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=N2WchST43h",
      "cdate": 1695436380207,
      "mdate": 1710306623308,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709848"
    },
    {
      "id": "7gLfQT52Nn",
      "title": "Proper Laplacian Representation Learning",
      "abstract": "The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The _Laplacian representation_ is a promising approach to address these problems by inducing informative state encoding and intrinsic rewards for temporally-extended action discovery and reward shaping. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.",
      "authors": [
        "Diego Gomez",
        "Michael Bowling",
        "Marlos C. Machado"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7gLfQT52Nn",
      "cdate": 1695436220552,
      "mdate": 1710181766089,
      "matched_keywords": [
        "reinforcement learning",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709853"
    },
    {
      "id": "PHGxChm1l5",
      "title": "CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding",
      "abstract": "A remarkable ability of human beings resides in compositional reasoning, i.e., the capacity to make \"infinite use of finite means\". However, current large vision-language foundation models (VLMs)  fall short of such compositional abilities due to their ``bag-of-words\" behaviors and inability to construct words that correctly represent visual entities and the relations among the entities. To this end, we propose CoVLM, which can guide the LLM to explicitly compose visual entities and relationships among the text and dynamically communicate with the vision encoder and detection network to achieve vision-language communicative decoding. Specifically, we first devise a set of novel communication tokens for the LLM, for dynamic communication between the visual detection system and the language system. A communication token is generated by the LLM following a visual entity or a relation, to inform the detection network to propose regions that are relevant to the sentence generated so far. The proposed regions-of-interests (ROIs) are then fed back into the LLM for better language generation contingent on the relevant regions. The LLM is thus able to compose the visual entities and relationships through the communication tokens. The vision-to-language and language-to-vision communication are iteratively performed until the entire sentence is generated. Our framework seamlessly bridges the gap between visual perception and LLMs and outperforms previous VLMs by a large margin on compositional reasoning benchmarks (e.g., ~20% in HICO-DET mAP, ~14% in Cola top-1 accuracy, and ~3% on ARO top-1 accuracy). We also achieve state-of-the-art performances on traditional vision-language tasks such as referring expression comprehension and visual question answering.",
      "authors": [
        "Junyan Li",
        "Delin Chen",
        "Yining Hong",
        "Zhenfang Chen",
        "Peihao Chen",
        "Yikang Shen",
        "Chuang Gan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=PHGxChm1l5",
      "cdate": 1695435550419,
      "mdate": 1713672661562,
      "matched_keywords": [
        "large language model",
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709857"
    },
    {
      "id": "oOGqJ6Z1sA",
      "title": "Treatment Effects Estimation By Uniform Transformer",
      "abstract": "In observational studies, balancing covariates in different treatment groups is essential to estimate treatment effects. One of the most commonly used methods for such purposes is weighting. The performance of this class of methods usually depends on strong regularity conditions for the underlying model, which might not hold in practice. In this paper, we investigate weighting methods from a functional estimation perspective and argue that the weights needed for covariate balancing could differ from those needed for treatment effects estimation under low regularity conditions. Motivated by this observation, we introduce a new framework of weighting that directly targets the treatment effects estimation. Unlike existing methods, the resulting estimator for a treatment effect under this new framework is a simple kernel-based $U$-statistic after applying a data-driven transformation to the observed covariates. We characterize the theoretical properties of the new estimators of treatment effects under a nonparametric setting and show that they are able to work robustly under low regularity conditions. The new framework is also applied to several numerical examples to demonstrate its practical merits.",
      "authors": [
        "Ruoqi Yu",
        "Shulei Wang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=oOGqJ6Z1sA",
      "cdate": 1695435396185,
      "mdate": 1710115229630,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709862"
    },
    {
      "id": "RDSj6S8WJe",
      "title": "Demystifying Linear MDPs and Novel Dynamics Aggregation Framework",
      "abstract": "In this work, we prove that, in linear MDPs, the feature dimension $d$ is lower bounded by $S/U$ in order to aptly represent transition probabilities, where $S$ is the size of the state space and $U$ is the maximum size of directly reachable states.\nHence, $d$ can still scale with $S$ depending on the direct reachability of the environment.  To address this limitation of linear MDPs, we propose a novel structural aggregation framework based on dynamics, named as the *dynamics aggregation*.\n    For this newly proposed framework,\n    we design a provably efficient hierarchical reinforcement learning algorithm in linear function approximation that leverages aggregated sub-structures. Our proposed algorithm exhibits statistical efficiency, achieving a regret of $\\tilde{O} \\big( d_{\\psi}^{3/2} H^{3/2}\\sqrt{ NT} \\big)$, where $d_{\\psi}$ represents the feature dimension of *aggregated subMDPs* and $N$ signifies the number of aggregated subMDPs. \n    We establish that the condition $d_{\\psi}^3 N \\ll d^{3}$ is readily met in most real-world environments with hierarchical structures, enabling a substantial improvement in the regret bound compared to LSVI-UCB, which enjoys a regret of $\\tilde{O}(d^{3/2} H^{3/2} \\sqrt{ T})$.\n    To the best of our knowledge, this work presents the first HRL algorithm with linear function approximation that offers provable guarantees.",
      "authors": [
        "Joongkyu Lee",
        "Min-hwan Oh"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=RDSj6S8WJe",
      "cdate": 1695435359533,
      "mdate": 1712462258898,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709866"
    },
    {
      "id": "kJ0qp9Xdsh",
      "title": "Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints",
      "abstract": "Controllable layout generation refers to the process of creating a plausible visual arrangement of elements within a graphic design (*e.g.*, document and web designs) with constraints representing design intentions. Although recent diffusion-based models have achieved state-of-the-art FID scores, they tend to exhibit more pronounced misalignment compared to earlier transformer-based models. In this work, we propose the **LA**yout **C**onstraint diffusion mod**E**l (LACE), a unified model to handle a broad range of layout generation tasks, such as arranging elements with specified attributes and refining or completing a coarse layout design. The model is based on continuous diffusion models. Compared with existing methods that use discrete diffusion models, continuous state-space design can enable the incorporation of continuous aesthetic constraint functions in training more naturally. For conditional generation, we propose injecting layout conditions in the form of masks or gradient guidance during inference. Empirical results show that LACE produces high-quality layouts and outperforms existing state-of-the-art baselines. We will release our source code and model checkpoints.",
      "authors": [
        "Jian Chen",
        "Ruiyi Zhang",
        "Yufan Zhou",
        "Changyou Chen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kJ0qp9Xdsh",
      "cdate": 1695435123523,
      "mdate": 1710323838937,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709871"
    },
    {
      "id": "4olqbTBt1Y",
      "title": "DREAM: Dual Structured Exploration with Mixup for Open-set Graph Domain Adaption",
      "abstract": "Recently, numerous graph neural network methods have been developed to tackle domain shifts in graph data. However, these methods presuppose that unlabeled target graphs belong to categories previously seen in the source domain. This assumption could not hold true for in-the-wild target graphs. In this paper, we delve deeper to explore a more realistic problem open-set graph domain adaptation. Our objective is to not only identify target graphs from new categories but also accurately classify remaining target graphs into their respective categories under domain shift and label scarcity. To solve this challenging problem, we introduce a new method named Dual Structured Exploration with Mixup (DREAM). DREAM incorporates a graph-level representation learning branch as well as a subgraph-enhanced branch, which jointly explores graph topological structures from both global and local viewpoints. To maximize the use of unlabeled target graphs, we train these two branches simultaneously using posterior regularization to enhance their inter-module consistency. To accommodate the open-set setting, we amalgamate dissimilar samples to generate virtual unknown samples belonging to novel classes. Moreover, to alleviate domain shift, we establish a k nearest neighbor-based graph-of-graphs and blend multiple neighbors of each sample to produce cross-domain virtual samples for inter-domain consistency learning. Extensive experiments validate the effectiveness of the proposed DREAM in comparison to various state-of-the-art approaches in different settings.",
      "authors": [
        "Nan Yin",
        "Mengzhu Wang",
        "Zhenghan Chen",
        "Li Shen",
        "Huan Xiong",
        "Bin Gu",
        "Xiao Luo"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4olqbTBt1Y",
      "cdate": 1695434292046,
      "mdate": 1712743600878,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709876"
    },
    {
      "id": "5Nn2BLV7SB",
      "title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
      "abstract": "Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated by humans and labels are aligned with human preferences. Our findings reveal that PandaLM-7B offers a performance comparable to both GPT-3.5 and GPT-4. Impressively, PandaLM-70B surpasses their performance. PandaLM enables the evaluation of LLM to be fairer but with less cost, evidenced by significant improvements achieved by models tuned through PandaLM compared to their counterparts trained with default Alpaca's hyperparameters. In addition, PandaLM does not depend on API-based evaluations, thus avoiding potential data leakage.",
      "authors": [
        "Yidong Wang",
        "Zhuohao Yu",
        "Wenjin Yao",
        "Zhengran Zeng",
        "Linyi Yang",
        "Cunxiang Wang",
        "Hao Chen",
        "Chaoya Jiang",
        "Rui Xie",
        "Jindong Wang",
        "Xing Xie",
        "Wei Ye",
        "Shikun Zhang",
        "Yue Zhang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5Nn2BLV7SB",
      "cdate": 1695433908586,
      "mdate": 1713673021954,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709880"
    },
    {
      "id": "NltzxpG0nz",
      "title": "Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds",
      "abstract": "Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to ``a blindfolded text-based game.'' Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model to address this limitation. Steve-Eye integrates the LLM with a visual encoder to process visual-text inputs and generate multimodal feedback. We adopt a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, enabling our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks and carry out experiments from a wide range of perspectives to validate our model's capability to strategically act and plan. The project’s website and code can be found at https://sites.google.com/view/steve-eye.",
      "authors": [
        "Sipeng Zheng",
        "jiazheng liu",
        "Yicheng Feng",
        "Zongqing Lu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NltzxpG0nz",
      "cdate": 1695433646907,
      "mdate": 1710585221733,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.709885"
    },
    {
      "id": "TjhUtloBZU",
      "title": "Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks",
      "abstract": "Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training data often contain label noise that may adversely affect the generalization of the model. This paper aims to understand the nature of noise in pre-training datasets and to mitigate its impact on downstream tasks. More specifically, through extensive experiments of supervised pre-training models on synthetic noisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise in pre-training can benefit in-domain (ID) transfer performance, where the training and testing data share the same distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing data distribution are different. We empirically verify that the reason behind is noise in pre-training shapes the feature space differently. We then propose a light-weight black-box tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization on both ID and OOD tasks, considering one may not be able to fully fine-tune or even access the pre-trained models. We conduct practical experiments on popular vision and language models that are pre-trained on noisy data for evaluation of our approach. Our analysis and results show the importance of this interesting and novel research direction, which we term Noisy Model Learning.",
      "authors": [
        "Hao Chen",
        "Jindong Wang",
        "Ankit Shah",
        "Ran Tao",
        "Hongxin Wei",
        "Xing Xie",
        "Masashi Sugiyama",
        "Bhiksha Raj"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=TjhUtloBZU",
      "cdate": 1695433044030,
      "mdate": 1710172359908,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709889"
    },
    {
      "id": "IRcv4yFX6z",
      "title": "Learning Hierarchical Image Segmentation For Recognition and By Recognition",
      "abstract": "Large vision and language models learned directly through image-text associations often lack detailed visual substantiation, whereas image segmentation tasks are treated separately from recognition, supervisedly learned without interconnections.\n\nOur key observation is that,  while an image can be recognized in multiple ways, each has a consistent part-and-whole visual organization.  Segmentation thus should be treated not as an end task to be mastered through supervised learning, but as an internal process that evolves with and supports the ultimate goal of recognition. \n\nWe propose to integrate a hierarchical segmenter into the recognition process, \n{\\it train} and {\\it adapt} the entire model solely on image-level recognition objectives.  We learn hierarchical segmentation {\\it for free} alongside recognition,  automatically uncovering part-to-whole relationships that not only underpin but also enhance recognition. \n\nEnhancing the Vision Transformer (ViT) with adaptive segment tokens and graph pooling, our model surpasses ViT in unsupervised part-whole discovery, semantic segmentation, image classification, and efficiency.  Notably, our model (trained on {\\it unlabeled} 1M ImageNet images) outperforms SAM (trained on 11M images and 1 billion masks) by absolute 8\\% in mIoU on PartImageNet object segmentation.",
      "authors": [
        "Tsung-Wei Ke",
        "Sangwoo Mo",
        "Stella X. Yu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=IRcv4yFX6z",
      "cdate": 1695432627282,
      "mdate": 1714704396637,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709899"
    },
    {
      "id": "TilcG5C8bN",
      "title": "Waxing-and-Waning: a Generic Similarity-based Framework for Efficient Self-Supervised Learning",
      "abstract": "Deep Neural Networks (DNNs), essential for diverse applications such as visual recognition and eldercare, often require a large amount of labeled data for training, making widespread deployment of DNNs a challenging task. Self-supervised learning (SSL) emerges as a promising approach, which leverages inherent patterns within data through diverse augmentations to train models without explicit labels. However, while SSL has shown notable advancements in accuracy, its high computation costs remain a daunting impediment, particularly for resource-constrained platforms. To address this problem, we introduce SimWnW, a similarity-based efficient self-supervised learning framework. By strategically removing less important regions in augmented images and feature maps, SimWnW not only reduces computation costs but also eliminates irrelevant features that might slow down the learning process, thereby accelerating model convergence. The experimental results show that SimWnW effectively reduces the amount of computation costs in self-supervised model training without compromising accuracy. Specifically, SimWnW yields up to 54\\% and 51\\% computation savings in training from scratch and transfer learning tasks, respectively.",
      "authors": [
        "Sheng Li",
        "Chao Wu",
        "Ao Li",
        "Yanzhi Wang",
        "Xulong Tang",
        "Geng Yuan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=TilcG5C8bN",
      "cdate": 1695432504315,
      "mdate": 1713097543865,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709904"
    },
    {
      "id": "CfXh93NDgH",
      "title": "WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions",
      "abstract": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Both automatic and human evaluations consistently indicate that WizardLM outperforms baselines such as Alpaca (trained from Self-Instruct) and Vicuna (trained from human-created instructions). The experimental results demonstrate that the quality of instruction-following dataset crafted by Evol-Instruct can significantly improve the performance of LLMs.",
      "authors": [
        "Can Xu",
        "Qingfeng Sun",
        "Kai Zheng",
        "Xiubo Geng",
        "Pu Zhao",
        "Jiazhan Feng",
        "Chongyang Tao",
        "Qingwei Lin",
        "Daxin Jiang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=CfXh93NDgH",
      "cdate": 1695431608807,
      "mdate": 1710346114748,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709909"
    },
    {
      "id": "usrChqw6yK",
      "title": "LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors",
      "abstract": "Inspired by the outstanding zero-shot capability of vision language models (VLMs) in image classification tasks, open-vocabulary object detection has attracted increasing interest by distilling the broad VLM knowledge into detector training. However, most existing open-vocabulary detectors learn by aligning region embeddings with categorical labels (e.g., bicycle) only, disregarding the capability of VLMs on aligning visual embeddings with fine-grained text descriptions of object parts (e.g., pedals and bells). This paper presents DVDet, a Descriptor-Enhanced Open Vocabulary Detector that introduces conditional context prompts and hierarchical textual descriptors that enable precise region-text alignment as well as open-vocabulary detection training in general. Specifically, the conditional context prompt transforms regional embeddings into image-like representations that can be directly integrated into general open vocabulary detection training. In addition, we introduce large language models as an interactive and implicit knowledge repository which enables iterative mining and refining visually oriented textual descriptors for precise region-text alignment. Extensive experiments over multiple large-scale benchmarks show that DVDet outperforms the state-of-the-art consistently by large margins.",
      "authors": [
        "Sheng Jin",
        "Xueying Jiang",
        "Jiaxing Huang",
        "Lewei Lu",
        "Shijian Lu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=usrChqw6yK",
      "cdate": 1695431168343,
      "mdate": 1710232544331,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709914"
    },
    {
      "id": "lKxL5zkssv",
      "title": "CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding",
      "abstract": "The study of decoding visual neural information faces challenges in generalizing single-subject decoding models to multiple subjects, due to individual differences. Moreover, the limited availability of data from a single subject has a constraining impact on model performance. Although prior multi-subject decoding methods have made significant progress, they still suffer from several limitations, including difficulty in extracting global neural response features, linear scaling of model parameters with the number of subjects, and inadequate characterization of the relationship between neural responses of different subjects to various stimuli.\nTo overcome these limitations, we propose a CLIP-guided Multi-sUbject visual neural information SEmantic Decoding (CLIP-MUSED) method. Our method consists of a Transformer-based feature extractor to effectively model global neural representations. It also incorporates learnable subject-specific tokens that facilitates the aggregation of multi-subject data without a linear increase of parameters. Additionally, we employ representational similarity analysis (RSA) to guide token representation learning based on the topological relationship of visual stimuli in the representation space of CLIP, enabling full characterization of the relationship between neural responses of different subjects under different stimuli. Finally, token representations are used for multi-subject semantic decoding. Our proposed method outperforms single-subject decoding methods and achieves state-of-the-art performance among the existing multi-subject methods on two fMRI datasets. Visualization results provide insights into the effectiveness of our proposed method. Code is available at https://github.com/CLIP-MUSED/CLIP-MUSED.",
      "authors": [
        "Qiongyi Zhou",
        "Changde Du",
        "Shengpei Wang",
        "Huiguang He"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lKxL5zkssv",
      "cdate": 1695430652093,
      "mdate": 1713672281115,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709918"
    },
    {
      "id": "oTRwljRgiv",
      "title": "ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis",
      "abstract": "When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step. When used with Transformer models trained from scratch, ExeDec has better synthesis performance and greatly improved compositional generalization ability compared to baselines. Finally, we use our benchmarks to demonstrate that LLMs struggle to compositionally generalize when asked to do programming-by-example in a few-shot setting, but an ExeDec-style prompting approach can improve the generalization ability and overall performance.",
      "authors": [
        "Kensen Shi",
        "Joey Hong",
        "Yinlin Deng",
        "Pengcheng Yin",
        "Manzil Zaheer",
        "Charles Sutton"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=oTRwljRgiv",
      "cdate": 1695430027086,
      "mdate": 1713474622369,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709923"
    },
    {
      "id": "v8jdwkUNXb",
      "title": "Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning",
      "abstract": "Score-based generative models like the diffusion model have been testified to be effective in modeling multi-modal data from image generation to reinforcement learning (RL). However, the inference process of diffusion model can be slow, which hinders its usage in RL with iterative sampling. We propose to apply the consistency model as an efficient yet expressive policy representation, namely consistency policy, with an actor-critic style algorithm for three typical RL settings: offline, offline-to-online and online. For offline RL, we demonstrate the expressiveness of generative models as policies from multi-modal data. For offline-to-online RL, the consistency policy is shown to be more computational efficient than diffusion policy, with a comparable performance. For online RL, the consistency policy demonstrates significant speedup and even higher average performances than the diffusion policy.",
      "authors": [
        "Zihan Ding",
        "Chi Jin"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=v8jdwkUNXb",
      "cdate": 1695429987019,
      "mdate": 1710442236278,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709927"
    },
    {
      "id": "iX1RjVQODj",
      "title": "Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the \\emph{regret} under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new family of algorithms for optimizing behavior from human feedback using the \\textit{regret}-based model of human preferences. Using the principle of maximum entropy, we derive \\fullname (\\abv), an algorithm for learning optimal policies from preferences without learning reward functions, circumventing the need for RL. \\abv is fully off-policy, uses only a simple contrastive objective, and can be applied to arbitrary MDPs. This enables \\abv to elegantly scale to high-dimensional and sequential RLHF problems while being simpler than prior methods.",
      "authors": [
        "Joey Hejna",
        "Rafael Rafailov",
        "Harshit Sikchi",
        "Chelsea Finn",
        "Scott Niekum",
        "W. Bradley Knox",
        "Dorsa Sadigh"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iX1RjVQODj",
      "cdate": 1695429580997,
      "mdate": 1711600057693,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709932"
    },
    {
      "id": "h8GeqOxtd4",
      "title": "Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization",
      "abstract": "Diffusion models have emerged as a powerful tool rivaling GANs in generating high-quality samples with improved fidelity, flexibility, and robustness. A key component of these models is to learn the score function through score matching. Despite empirical success on various tasks,  it remains unclear whether gradient-based algorithms can learn the score function with a provable accuracy. As a first step toward answering this question, this paper establishes a mathematical framework for analyzing score estimation using neural networks trained by gradient descent. Our analysis covers both the optimization and the generalization aspects of the learning procedure. In particular, we propose a parametric form to formulate the denoising score-matching problem as a regression with noisy labels. Compared to the standard supervised learning setup, the score-matching problem introduces distinct challenges, including unbounded input, vector-valued output, and an additional time variable, preventing existing techniques from being applied directly. In this paper, we show that with proper designs, the evolution of neural networks during training can be accurately modeled by a series of kernel regression tasks. Furthermore, by applying an early-stopping rule for gradient descent and leveraging recent developments in neural tangent kernels, we establish the first generalization error (sample complexity) bounds for learning the score function with neural networks, despite the presence of noise in the observations. Our analysis is grounded in a novel parametric form of the neural network and an innovative connection between score matching and regression analysis, facilitating the application of advanced statistical and optimization techniques.",
      "authors": [
        "Yinbin Han",
        "Meisam Razaviyayn",
        "Renyuan Xu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=h8GeqOxtd4",
      "cdate": 1695429520940,
      "mdate": 1710292943282,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709937"
    },
    {
      "id": "w4abltTZ2f",
      "title": "Batched Low-Rank Adaptation of Foundation Models",
      "abstract": "Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While \\lora/ offers numerous advantages, its applicability for real-time serving to a diverse and global user base \nis constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request.\n\nTo address this, we introduce FLoRA (Fast LoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that \\flora/ retains the performance merits of \\lora/, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 8 languages and a multilingual speech recognition task across 6 languages.",
      "authors": [
        "Yeming Wen",
        "Swarat Chaudhuri"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=w4abltTZ2f",
      "cdate": 1695429166581,
      "mdate": 1711666047477,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709941"
    },
    {
      "id": "m0x0rv6Iwm",
      "title": "Time-Varying Propensity Score to Bridge the Gap between the Past and Present",
      "abstract": "Real-world deployment of machine learning models is challenging because data evolves over time. While no model can work when data evolves in an arbitrary fashion, if there is some pattern to these changes, we might be able to design methods to address it. This paper addresses situations when data evolves gradually. We introduce a time-varying propensity score that can detect gradual shifts in the distribution of data which allows us to selectively sample past data to update the model---not just similar data from the past like that of a standard propensity score but also data that evolved in a similar fashion in the past. The time-varying propensity score is quite general: we demonstrate different ways of implementing it and evaluate it on a variety of problems ranging from supervised learning (e.g., image classification problems) where data undergoes a sequence of gradual shifts, to reinforcement learning tasks (e.g., robotic manipulation and continuous control) where data shifts as the policy or the task changes.",
      "authors": [
        "Rasool Fakoor",
        "Jonas Mueller",
        "Zachary Chase Lipton",
        "Pratik Chaudhari",
        "Alex Smola"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=m0x0rv6Iwm",
      "cdate": 1695428415201,
      "mdate": 1710403184188,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709948"
    },
    {
      "id": "XVhm3X8Fum",
      "title": "Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns",
      "abstract": "Attention, specifically scaled dot-product attention, has proven effective for natural language, but it does not have a mechanism for handling hierarchical patterns of arbitrary nesting depth, which limits its ability to recognize certain syntactic structures. To address this shortcoming, we propose stack attention: an attention operator that incorporates stacks, inspired by their theoretical connections to context-free languages (CFLs). We show that stack attention is analogous to standard attention, but with a latent model of syntax that requires no syntactic supervision. We propose two variants: one related to deterministic pushdown automata (PDAs) and one based on nondeterministic PDAs, which allows transformers to recognize arbitrary CFLs. We show that transformers with stack attention are very effective at learning CFLs that standard transformers struggle on, achieving strong results on a CFL with theoretically maximal parsing difficulty. We also show that stack attention is more effective at natural language modeling under a constrained parameter budget, and we include results on machine translation.",
      "authors": [
        "Brian DuSell",
        "David Chiang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=XVhm3X8Fum",
      "cdate": 1695428327787,
      "mdate": 1709661532643,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709953"
    },
    {
      "id": "jLIUfrAcMQ",
      "title": "Debiasing Attention Mechanism in Transformer without Demographics",
      "abstract": "Although transformers demonstrate impressive capabilities in a variety of tasks, the fairness issue remains a significant concern when deploying these models. Existing works to address fairness issues in transformers require sensitive labels (such as age, gender, etc.), which can raise privacy concerns or violate legal regulations. An alternative way is through fairness without demographics. However, existing works that improve Rawlsian Max-Min fairness may impose overly restrictive constraints. Other methods that use auxiliary networks could be parameter inefficient. In this paper, we present a new approach to debiasing transformers by leveraging their inherent structure.  By reconsidering the roles of important components (queries, keys, and values) in the attention mechanism, we introduce a simple yet effective debiasing strategy from two perspectives: 1) Grounded in theoretical analysis, we normalize and apply absolute value operations to queries and keys to minimize the bias in attention weight allocation; 2) We reduce the bias within values through local alignment via contrastive learning. Throughout the entire process, our approach does not require any sensitive labels. Furthermore, to enhance memory efficiency in the training phase, we propose a strategy that debias only the last encoder to improve fairness in pre-trained models. We conduct experiments in computer vision and natural language processing tasks and show that our method is comparable and even outperforms the state-of-the-art method with substantially lower energy consumption.",
      "authors": [
        "Shenyu Lu",
        "Yipei Wang",
        "Xiaoqian Wang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jLIUfrAcMQ",
      "cdate": 1695427987546,
      "mdate": 1713061893314,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.709958"
    },
    {
      "id": "uNrFpDPMyo",
      "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
      "abstract": "In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.",
      "authors": [
        "Suyu Ge",
        "Yunan Zhang",
        "Liyuan Liu",
        "Minjia Zhang",
        "Jiawei Han",
        "Jianfeng Gao"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=uNrFpDPMyo",
      "cdate": 1695427043679,
      "mdate": 1710571899831,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709962"
    },
    {
      "id": "EpVe8jAjdx",
      "title": "Privileged Sensing Scaffolds Reinforcement Learning",
      "abstract": "We need to look at our shoelaces as we first learn to tie them but having mastered this skill, can do it from touch alone. We call this phenomenon “sensory scaffolding”: observation streams that are not needed by a master might yet aid a novice learner. We consider such sensory scaffolding setups for training artificial agents. For example, a robot arm may need to be deployed with just a low-cost, robust, general-purpose camera; yet its performance may improve by having privileged training-time-only access to informative albeit expensive and unwieldy motion capture rigs or fragile tactile sensors. For these settings, we propose “Scaffolder”, a reinforcement learning approach which effectively exploits privileged sensing in critics, world models, reward estimators, and other such auxiliary components that are only used at training time, to improve the target policy. For evaluating sensory scaffolding agents, we design a new “S3” suite of ten diverse simulated robotic tasks that explore a wide range of practical sensor setups. Agents must use privileged camera sensing to train blind hurdlers, privileged active visual perception to help robot arms overcome visual occlusions, privileged touch sensors to train robot hands, and more. Scaffolder easily outperforms relevant prior baselines and frequently performs comparably even to policies that have test-time access to the privileged sensors. Website: https://penn-pal-lab.github.io/scaffolder/",
      "authors": [
        "Edward S. Hu",
        "James Springer",
        "Oleh Rybkin",
        "Dinesh Jayaraman"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EpVe8jAjdx",
      "cdate": 1695425995877,
      "mdate": 1713237313828,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709967"
    },
    {
      "id": "w9tc699w3Z",
      "title": "Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment",
      "abstract": "We introduce a method to train vision-language models for remote-sensing images without using any textual annotations. Our key insight is to use co-located internet imagery taken on the ground as an intermediary for connecting remote-sensing images and language.  Specifically, we train an image encoder for remote sensing images to align with the image encoder of CLIP using a large amount of paired internet and satellite images.  Our unsupervised approach enables the training of a first-of-its-kind large scale VLM for remote sensing images at two different resolutions. We show that these VLMs enable zero-shot, open-vocabulary image classification, retrieval, segmentation and visual question answering for satellite images. On each of these tasks, our VLM trained without textual annotations outperforms existing VLMs trained with supervision, with gains of up to 20\\% for classification and 80\\% for segmentation.",
      "authors": [
        "Utkarsh Mall",
        "Cheng Perng Phoo",
        "Meilin Kelsey Liu",
        "Carl Vondrick",
        "Bharath Hariharan",
        "Kavita Bala"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=w9tc699w3Z",
      "cdate": 1695425951415,
      "mdate": 1710272092040,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709972"
    },
    {
      "id": "C36v8541Ns",
      "title": "The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing",
      "abstract": "Real-life applications of deep neural networks are hindered by their unsteady predictions when faced with noisy inputs and adversarial attacks. The certified radius in this context is a crucial indicator of the robustness of models. However how to design an efficient classifier with an associated certified radius? Randomized smoothing provides a promising framework by relying on noise injection into the inputs to obtain a smoothed and robust classifier. In this paper, we first show that the variance introduced by the Monte-Carlo sampling in the randomized smoothing procedure estimate closely interacts with two other important properties of the classifier, \\textit{i.e.} its Lipschitz constant and margin.  More precisely, our work emphasizes the dual impact of the Lipschitz constant of the base classifier, on both the smoothed classifier and the empirical variance. To increase the certified robust radius, we introduce a different way to convert logits to probability vectors for the base classifier to leverage the variance-margin trade-off. We leverage the use of Bernstein's concentration inequality along with enhanced Lipschitz bounds for randomized smoothing. Experimental results show a significant improvement in certified accuracy compared to current state-of-the-art methods. Our novel certification procedure allows us to use pre-trained models with randomized smoothing, effectively improving the current certification radius in a zero-shot manner.",
      "authors": [
        "Blaise Delattre",
        "Alexandre Araujo",
        "Quentin Barthélemy",
        "Alexandre Allauzen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=C36v8541Ns",
      "cdate": 1695425808803,
      "mdate": 1710323870344,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.709976"
    },
    {
      "id": "rvUq3cxpDF",
      "title": "Learning to Act without Actions",
      "abstract": "Pre-training large models on vast amounts of web data has proven to be an effective approach for obtaining powerful, general models in domains such as language and vision. However, this paradigm has not yet taken hold in reinforcement learning. This is because videos, the most abundant form of embodied behavioral data on the web, lack the action labels required by existing methods for imitating behavior from demonstrations. We introduce **Latent Action Policies** (LAPO), a method for recovering latent action information—and thereby latent-action policies, world models, and inverse dynamics models—purely from videos. LAPO is the first method able to recover the structure of the true action space just from observed dynamics, even in challenging procedurally-generated environments. LAPO enables training latent-action policies that can be rapidly fine-tuned into expert-level policies, either offline using a small action-labeled dataset, or online with rewards. LAPO takes a first step towards pre-training powerful, generalist policies and world models on the vast amounts of videos readily available on the web. Our code is available here: \n[https://github.com/schmidtdominik/LAPO](https://github.com/schmidtdominik/LAPO).",
      "authors": [
        "Dominik Schmidt",
        "Minqi Jiang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rvUq3cxpDF",
      "cdate": 1695425345849,
      "mdate": 1711496075115,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.709981"
    },
    {
      "id": "t8eO0CiZJV",
      "title": "Tailoring Self-Rationalizers with Multi-Reward Distillation",
      "abstract": "Large language models (LMs) are capable of generating free-text rationales to aid question answering. However, prior work 1) suggests that useful self-rationalization is emergent only at significant scales (e.g., 175B parameter GPT-3); and 2) focuses largely on downstream performance, ignoring the semantics of the rationales themselves, e.g., are they faithful, true, and helpful for humans? In this work, we enable small-scale LMs (∼200x smaller than GPT-3) to generate rationales that not only improve downstream task performance, but are also more plausible, consistent, and diverse, assessed both by automatic and human evaluation. Our method, MaRio (Multi-rewArd RatIOnalization), is a multi-reward conditioned self-rationalization algorithm that optimizes multiple distinct properties like plausibility, diversity and consistency. Results on three difficult question-answering datasets StrategyQA, QuaRel and OpenBookQA show that not only does MaRio improve task accuracy, but it also improves the self-rationalization quality of small LMs across the aforementioned axes better than a supervised fine-tuning (SFT) baseline. Extensive human evaluations confirm that MaRio rationales are preferred vs. SFT rationales, as well as qualitative improvements in plausibility and consistency.",
      "authors": [
        "Sahana Ramnath",
        "Brihi Joshi",
        "Skyler Hallinan",
        "Ximing Lu",
        "Liunian Harold Li",
        "Aaron Chan",
        "Jack Hessel",
        "Yejin Choi",
        "Xiang Ren"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=t8eO0CiZJV",
      "cdate": 1695425223497,
      "mdate": 1710484297165,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709986"
    },
    {
      "id": "EnXJfQqy0K",
      "title": "Building Cooperative Embodied Agents Modularly with Large Language Models",
      "abstract": "In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.",
      "authors": [
        "Hongxin Zhang",
        "Weihua Du",
        "Jiaming Shan",
        "Qinhong Zhou",
        "Yilun Du",
        "Joshua B. Tenenbaum",
        "Tianmin Shu",
        "Chuang Gan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EnXJfQqy0K",
      "cdate": 1695425214791,
      "mdate": 1713550501184,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709990"
    },
    {
      "id": "yRrPfKyJQ2",
      "title": "Conversational Drug Editing Using Retrieval and Domain Feedback",
      "abstract": "Recent advancements in conversational large language models (LLMs), such as ChatGPT, have demonstrated remarkable promise in various domains, including drug discovery. However, existing works mainly focus on investigating the capabilities of conversational LLMs on chemical reactions and retrosynthesis. While drug editing, a critical task in the drug discovery pipeline, remains largely unexplored. To bridge this gap, we propose ChatDrug, a framework to facilitate the systematic investigation of drug editing using LLMs. ChatDrug jointly leverages a prompt module, a retrieval and domain feedback module, and a conversation module to streamline effective drug editing. We empirically show that ChatDrug reaches the best performance on all 39 drug editing tasks, encompassing small molecules, peptides, and proteins. We further demonstrate, through 10 case studies, that ChatDrug can successfully identify the key substructures for manipulation, generating diverse and valid suggestions for drug editing. Promisingly, we also show that ChatDrug can offer insightful explanations from a domain-specific perspective, enhancing interpretability and enabling informed decision-making.",
      "authors": [
        "Shengchao Liu",
        "Jiongxiao Wang",
        "Yijin Yang",
        "Chengpeng Wang",
        "Ling Liu",
        "Hongyu Guo",
        "Chaowei Xiao"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=yRrPfKyJQ2",
      "cdate": 1695424451770,
      "mdate": 1710647270401,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.709997"
    },
    {
      "id": "MVe2dnWPCu",
      "title": "A Probabilistic Framework for Modular Continual Learning",
      "abstract": "Modular approaches that use a different composition of modules for each problem are a promising direction in continual learning (CL). However, searching through the large, discrete space of module compositions is challenging, especially because evaluating a composition’s performance requires a round of neural network training. We address this challenge through a modular CL framework, PICLE, that uses a probabilistic model to cheaply compute the fitness of each composition, allowing PICLE to achieve both perceptual, few-shot and latent transfer. The model combines prior knowledge about good module compositions with dataset-specific information. We evaluate PICLE using two benchmark suites designed to assess different desiderata of CL techniques. Comparing to a wide range of approaches, we show that PICLE is the first modular CL algorithm to achieve perceptual, few-shot and latent transfer while scaling well to large search spaces, outperforming previous state-of-the-art modular CL approaches on long problem sequences.",
      "authors": [
        "Lazar Valkov",
        "Akash Srivastava",
        "Swarat Chaudhuri",
        "Charles Sutton"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MVe2dnWPCu",
      "cdate": 1695424439010,
      "mdate": 1710476388878,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710002"
    },
    {
      "id": "cWiEN1plhJ",
      "title": "Few-Shot Detection of Machine-Generated Text using Style Representations",
      "abstract": "The advent of instruction-tuned language models that convincingly mimic human writing poses a significant risk of abuse. For example, such models could be used for plagiarism, disinformation, spam, or phishing. However, such abuse may be counteracted with the ability to detect whether a piece of text was composed by a language model rather than a human. Some previous approaches to this problem have relied on supervised methods trained on corpora of confirmed human and machine-written documents. Unfortunately, model under-specification poses an unavoidable challenge for such detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors. Other previous approaches require access to the models that generated the text to be detected at inference or detection time, which is often impractical. In light of these challenge, we pursue a fundamentally different approach not relying on samples from language models of concern at training time. Instead, we propose to leverage representations of writing style estimated from human-authored text. Indeed, we find that features effective at distinguishing among human authors are also effective at distinguishing human from machine authors, including state of the art large language models like Llama 2, ChatGPT, and GPT-4. Furthermore, given handfuls of examples composed by each of several specific language models of interest, our approach affords the ability to predict which model specifically generated a given document.",
      "authors": [
        "Rafael Alberto Rivera Soto",
        "Kailin Koch",
        "Aleem Khan",
        "Barry Y. Chen",
        "Marcus Bishop",
        "Nicholas Andrews"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=cWiEN1plhJ",
      "cdate": 1695424351066,
      "mdate": 1710560914017,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710007"
    },
    {
      "id": "PCm1oT8pZI",
      "title": "Safe and Robust Watermark Injection with a Single OoD Image",
      "abstract": "Training a high-performance deep neural network requires large amounts of data and computational resources. \nProtecting the intellectual property (IP) and commercial ownership of a deep model is challenging yet increasingly crucial. \nA major stream of watermarking strategies implants verifiable backdoor triggers by poisoning training samples, but these are often unrealistic due to data privacy and safety concerns and are vulnerable to minor model changes such as fine-tuning. \nTo overcome these challenges, we propose a safe and robust backdoor-based watermark injection technique that leverages the diverse knowledge from a single out-of-distribution (OoD) image, which serves as a secret key for IP verification. \nThe independence of training data makes it agnostic to third-party promises of IP security. \nWe induce robustness via random perturbation of model parameters during watermark injection to defend against common watermark removal attacks, including fine-tuning, pruning, and model extraction. \nOur experimental results demonstrate that the proposed watermarking approach is not only time- and sample-efficient without training data, but also robust against the watermark removal attacks above.",
      "authors": [
        "Shuyang Yu",
        "Junyuan Hong",
        "Haobo Zhang",
        "Haotao Wang",
        "Zhangyang Wang",
        "Jiayu Zhou"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=PCm1oT8pZI",
      "cdate": 1695424302756,
      "mdate": 1709841671967,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710011"
    },
    {
      "id": "L6L1CJQ2PE",
      "title": "Massive Editing for Large Language Models via Meta Learning",
      "abstract": "While large language models (LLMs) have enabled learning knowledge from the pre-training corpora, the acquired knowledge may be fundamentally incorrect or outdated over time, which necessitates rectifying the knowledge of the language model (LM) after the training. A promising approach involves employing a hyper-network to generate parameter shift, whereas existing hyper-networks suffer from inferior scalability in synchronous editing operation amount (Hase et al., 2023b; Huang et al., 2023). For instance, Mitchell et al. (2022) mimics gradient accumulation to sum the parameter shifts together, which lacks statistical significance and is prone to cancellation effect. To mitigate the problem, we propose the MAssive Language Model Editing Network (MALMEN), which formulates the parameter shift aggregation as the least square problem, subsequently updating the LM parameter using the normal equation. To accommodate editing multiple facts simultaneously with limited memory budgets, we separate the computation on the hyper-network and LM, enabling arbitrary batch size on both neural networks. Our method is evaluated by editing up to thousands of facts on LMs with different architectures, i.e., BERT-base, GPT-2, and GPT-J (6B), across various knowledge-intensive NLP tasks, i.e., closed book fact-checking and question answering. Remarkably, MALMEN is capable of editing hundreds of times more facts than MEND (Mitchell et al., 2022) with the identical hyper-network architecture and outperforms editor specifically designed for GPT, i.e., MEMIT (Meng et al., 2023).",
      "authors": [
        "Chenmien Tan",
        "Ge Zhang",
        "Jie Fu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=L6L1CJQ2PE",
      "cdate": 1695424224844,
      "mdate": 1713672745058,
      "matched_keywords": [
        "large language model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710016"
    },
    {
      "id": "RwI7ZEfR27",
      "title": "BrainLM: A foundation model for brain activity recordings",
      "abstract": "We introduce the Brain Language Model (BrainLM), a foundation model for brain activity dynamics trained on 6,700 hours of fMRI recordings. Utilizing self-supervised masked-prediction training, BrainLM demonstrates proficiency in both fine-tuning and zero-shot inference tasks. Fine-tuning allows for the accurate prediction of clinical variables like age, anxiety, and PTSD as well as forecasting of future brain states. Critically, the model generalizes well to entirely new external cohorts not seen during training. In zero-shot inference mode, BrainLM can identify intrinsic functional networks directly from raw fMRI data without any network-based supervision during training. The model also generates interpretable latent representations that reveal relationships between brain activity patterns and cognitive states. Overall, BrainLM offers a versatile and interpretable framework for elucidating the complex spatiotemporal dynamics of human brain activity. It serves as a powerful \"lens\" through which massive repositories of fMRI data can be analyzed in new ways, enabling more effective interpretation and utilization at scale. The work demonstrates the potential of foundation models to advance computational neuroscience research.",
      "authors": [
        "Josue Ortega Caro",
        "Antonio Henrique de Oliveira Fonseca",
        "Syed A Rizvi",
        "Matteo Rosati",
        "Christopher Averill",
        "James L Cross",
        "Prateek Mittal",
        "Emanuele Zappala",
        "Rahul Madhav Dhodapkar",
        "Chadi Abdallah",
        "David van Dijk"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=RwI7ZEfR27",
      "cdate": 1695424022501,
      "mdate": 1713672620194,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710021"
    },
    {
      "id": "J1djqLAa6N",
      "title": "Efficient Score Matching with Deep Equilibrium Layers",
      "abstract": "Score matching methods -- estimate probability densities without computing the normalization constant -- are particularly useful in deep learning. However, computational and memory costs of score matching methods can be prohibitive for high-dimensional data or complex models, particularly due to the derivatives or Hessians of the log density function appearing in the objective function. Some existing approaches modify the objective function to reduce the quadratic computational complexity for Hessian computation. However, the memory bottleneck of score matching methods remains for deep learning. This study improves the memory efficiency of score matching by leveraging deep equilibrium models. We provide a theoretical analysis of deep equilibrium models for scoring matching and applying implicit differentiation to higher-order derivatives. Empirical evaluations demonstrate that our approach enables the development of deep and expressive models with improved performance and comparable computational and memory costs over shallow architectures.",
      "authors": [
        "Yuhao Huang",
        "Qingsong Wang",
        "Akwum Onwunta",
        "Bao Wang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=J1djqLAa6N",
      "cdate": 1695423591674,
      "mdate": 1709661531439,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710026"
    },
    {
      "id": "97Dl82avFs",
      "title": "Alt-Text with Context: Improving Accessibility for Images on Twitter",
      "abstract": "In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter. More than just a special case of image captioning, alt-text is both more literally descriptive and context-specific. Also critically, images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly leveraged can be informative. We address this task with a multimodal model that conditions on both textual information from the associated social media post as well as visual signal from the image, and demonstrate that the utility of these two information sources stacks. We put forward a new dataset of 371k images paired with alt-text and tweets scraped from Twitter and evaluate on it across a variety of automated metrics as well as human evaluation. We show that our approach of conditioning on both tweet text and visual information significantly outperforms prior work, by more than 2x on BLEU@4.",
      "authors": [
        "Nikita Srivatsan",
        "Sofia Samaniego",
        "Omar Florez",
        "Taylor Berg-Kirkpatrick"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=97Dl82avFs",
      "cdate": 1695423076957,
      "mdate": 1710454842379,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.710030"
    },
    {
      "id": "mGHJAyR8w0",
      "title": "Rethinking the Benefits of Steerable Features in 3D Equivariant Graph Neural Networks",
      "abstract": "Theoretical and empirical comparisons have been made to assess the expressive power and performance of invariant and equivariant GNNs. However, there is currently no theoretical result comparing the expressive power of $k$-hop invariant GNNs and equivariant GNNs. Additionally, little is understood about whether the performance of equivariant GNNs, employing steerable features up to type-$L$, increases as $L$ grows -- especially when the feature dimension is held constant. In this study, we introduce a key lemma that allows us to analyze steerable features by examining their corresponding invariant features. The lemma facilitates us in understanding the limitations of $k$-hop invariant GNNs, which fail to capture the global geometric structure due to the loss of geometric information between local structures. Furthermore, we investigate the invariant features associated with different types of steerable features and demonstrate that the expressiveness of steerable features is primarily determined by their dimension -- independent of their irreducible decomposition. This suggests that when the feature dimension is constant, increasing $L$ does not lead to essentially improved performance in equivariant GNNs employing steerable features up to type-$L$. We substantiate our theoretical insights with numerical evidence.",
      "authors": [
        "Shih-Hsin Wang",
        "Yung-Chang Hsu",
        "Justin Baker",
        "Andrea L. Bertozzi",
        "Jack Xin",
        "Bao Wang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=mGHJAyR8w0",
      "cdate": 1695422553094,
      "mdate": 1709661531273,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710035"
    },
    {
      "id": "z3L59iGALM",
      "title": "Massively Scalable Inverse Reinforcement Learning in Google Maps",
      "abstract": "Inverse reinforcement learning (IRL) offers a powerful and general framework for learning humans' latent preferences in route recommendation, yet no approach has successfully addressed planetary-scale problems with hundreds of millions of states and demonstration trajectories. In this paper, we introduce scaling techniques based on graph compression, spatial parallelization, and improved initialization conditions inspired by a connection to eigenvector algorithms. We revisit classic IRL methods in the routing context, and make the key observation that there exists a trade-off between the use of cheap, deterministic planners and expensive yet robust stochastic policies. This insight is leveraged in Receding Horizon Inverse Planning (RHIP), a new generalization of classic IRL algorithms that provides fine-grained control over performance trade-offs via its planning horizon. Our contributions culminate in a policy that achieves a 16-24% improvement in route quality at a global scale, and to the best of our knowledge, represents the largest published study of IRL algorithms in a real-world setting to date. We conclude by conducting an ablation study of key components, presenting negative results from alternative eigenvalue solvers, and identifying opportunities to further improve scalability via IRL-specific batching strategies.",
      "authors": [
        "Matt Barnes",
        "Matthew Abueg",
        "Oliver F. Lange",
        "Matt Deeds",
        "Jason Trader",
        "Denali Molitor",
        "Markus Wulfmeier",
        "Shawn O'Banion"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=z3L59iGALM",
      "cdate": 1695422215421,
      "mdate": 1713672030852,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710040"
    },
    {
      "id": "VLFhbOCz5D",
      "title": "Tangent Transformers for Composition,Privacy and Removal",
      "abstract": "We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning linearized transformers obtained by computing a First-order Taylor Expansion around a pre-trained initialization. We show that the Jacobian-Vector Product resulting from linearization can be computed efficiently in a single forward pass, reducing training and inference cost to the same order of magnitude as its original non-linear counterpart, while using the same number of parameters. Furthermore, we show that, when applied to various downstream visual classification tasks, the resulting Tangent Transformer fine-tuned with TAFT can perform comparably with fine-tuning the original non-linear network. Since Tangent Transformers are linear with respect to the new set of weights, and the resulting fine-tuning loss is convex, we show that TAFT enjoys several advantages compared to non-linear fine-tuning when it comes to model composition, parallel training, machine unlearning, and differential privacy. Our code is available at: https://github.com/tianyu139/tangent-model-composition",
      "authors": [
        "Tian Yu Liu",
        "Aditya Golatkar",
        "Stefano Soatto"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=VLFhbOCz5D",
      "cdate": 1695421765134,
      "mdate": 1710441676416,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710110"
    },
    {
      "id": "RIu5lyNXjT",
      "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
      "abstract": "As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.",
      "authors": [
        "Melanie Sclar",
        "Yejin Choi",
        "Yulia Tsvetkov",
        "Alane Suhr"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=RIu5lyNXjT",
      "cdate": 1695421684988,
      "mdate": 1710463309556,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710116"
    },
    {
      "id": "osoWxY8q2E",
      "title": "ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models",
      "abstract": "Large Language Models (LLMs) with billions of parameters have drastically transformed AI applications. However, their demanding computation during inference has raised significant challenges for deployment on resource-constrained devices. Despite recent trends favoring alternative activation functions such as GELU or SiLU, known for increased computation, this study strongly advocates for reinstating ReLU activation in LLMs. We demonstrate that using the ReLU activation function has a negligible impact on convergence and performance while significantly reducing computation and weight transfer. This reduction is particularly valuable during the memory-bound inference step, where efficiency is paramount. Exploring sparsity patterns in ReLU-based LLMs, we unveil the reutilization of activated neurons for generating new tokens and leveraging these insights, we propose practical strategies to substantially reduce LLM inference computation up to three times, using ReLU activations with minimal performance trade-offs.",
      "authors": [
        "Seyed Iman Mirzadeh",
        "Keivan Alizadeh-Vahid",
        "Sachin Mehta",
        "Carlo C del Mundo",
        "Oncel Tuzel",
        "Golnoosh Samei",
        "Mohammad Rastegari",
        "Mehrdad Farajtabar"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=osoWxY8q2E",
      "cdate": 1695421296768,
      "mdate": 1710566212279,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710120"
    },
    {
      "id": "KsUh8MMFKQ",
      "title": "Thin-Shell Object Manipulations With Differentiable Physics Simulations",
      "abstract": "In this work, we aim to teach robots to manipulate various thin-shell materials. \nPrior works studying thin-shell object manipulation mostly rely on heuristic policies or learn policies from real-world video demonstrations, and only focus on limited material types and tasks (e.g., cloth unfolding). However, these approaches face significant challenges when extended to a wider variety of thin-shell materials and a diverse range of tasks.\nOn the other hand, while virtual simulations are shown to be effective in diverse robot skill learning and evaluation, prior thin-shell simulation environments only support a subset of thin-shell materials, which also limits their supported range of tasks. \nTo fill in this gap, we introduce ThinShellLab - a fully differentiable simulation platform tailored for robotic interactions with diverse thin-shell materials possessing varying material properties, enabling flexible thin-shell manipulation skill learning and evaluation. Building on top of our developed simulation engine, we design a diverse set of manipulation tasks centered around different thin-shell objects. Our experiments suggest that manipulating thin-shell objects presents several unique challenges: 1) thin-shell manipulation relies heavily on frictional forces due to the objects' co-dimensional nature, 2) the materials being manipulated are highly sensitive to minimal variations in interaction actions, and 3) the constant and frequent alteration in contact pairs makes trajectory optimization methods susceptible to local optima, and neither standard reinforcement learning algorithms nor trajectory optimization methods (either gradient-based or gradient-free) are able to solve the tasks alone. To overcome these challenges, we present an optimization scheme that couples sampling-based trajectory optimization and gradient-based optimization, boosting both learning efficiency and converged performance across various proposed tasks. In addition, the differentiable nature of our platform facilitates a smooth sim-to-real transition. By tuning simulation parameters with a minimal set of real-world data, we demonstrate successful deployment of the learned skills to real-robot settings.  ThinShellLab will be publicly available. Video demonstration and more information can be found on the project website https://vis-www.cs.umass.edu/ThinShellLab/.",
      "authors": [
        "Yian Wang",
        "Juntian Zheng",
        "Zhehuan Chen",
        "Zhou Xian",
        "Gu Zhang",
        "Chao Liu",
        "Chuang Gan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=KsUh8MMFKQ",
      "cdate": 1695421181666,
      "mdate": 1711771754560,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710125"
    },
    {
      "id": "L4nOxziGf9",
      "title": "Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models",
      "abstract": "An increasing number of vision-language tasks can be handled with little to no training, i.e., in a zero and few-shot manner, by marrying large language models (LLMs) to vision encoders, resulting in large vision-language models (LVLMs). While this has huge upsides, such as not requiring training data or custom architectures, how an input is presented to an LVLM can have a major impact on zero-shot model performance. In particular, inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity. Therefore, adding visually-grounded information to the input as a preemptive clarification should improve model performance by reducing underspecification, e.g., by localizing objects and disambiguating references. Similarly, in the VQA setting, changing the way questions are framed can make them easier for models to answer. To this end, we present **Rep**hrase, **A**ugment and **Re**ason (RepARe), a gradient-free framework that extracts salient details about the image using the underlying LVLM as a captioner and reasoner, in order to propose modifications to the original question. We then use the LVLM’s confidence over a generated answer as an unsupervised scoring function to select the rephrased question most likely to improve zero-shot performance. Focusing on three visual question answering tasks, we show that RepARe can result in a 3.85% (absolute) increase in zero-shot accuracy on VQAv2, 6.41%, and 7.94% points increase on A-OKVQA, and VizWiz respectively. Additionally, we find that using gold answers for oracle question candidate selection achieves a substantial gain in VQA accuracy by up to 14.41%. Through extensive analysis, we demonstrate that outputs from RepARe increase syntactic complexity, and effectively utilize vision-language interaction and the frozen LLM.",
      "authors": [
        "Archiki Prasad",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=L4nOxziGf9",
      "cdate": 1695421057651,
      "mdate": 1710514596448,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710130"
    },
    {
      "id": "LWuYsSD94h",
      "title": "A Black-box Approach for Non-stationary Multi-agent Reinforcement Learning",
      "abstract": "We investigate learning the equilibria in non-stationary multi-agent systems and address the challenges that differentiate multi-agent learning from single-agent learning. Specifically, we focus on games with bandit feedback, where testing an equilibrium can result in substantial regret even when the gap to be tested is small, and the existence of multiple optimal solutions (equilibria) in stationary games poses extra challenges. To overcome these obstacles, we propose a versatile black-box approach applicable to a broad spectrum of problems, such as general-sum games, potential games, and Markov games, when equipped with appropriate learning and testing oracles for stationary environments. Our algorithms can achieve $\\widetilde{O}\\left(\\Delta^{1/4}T^{3/4}\\right)$ regret when the degree of nonstationarity, as measured by total variation $\\Delta$, is known, and $\\widetilde{O}\\left(\\Delta^{1/5}T^{4/5}\\right)$ regret when $\\Delta$ is unknown, where $T$ is the number of rounds. Meanwhile, our algorithm inherits the favorable dependence on number of agents from the oracles. As a side contribution that may be independent of interest, we show how to test for various types of equilibria by a black-box reduction to single-agent learning, which includes Nash equilibria, correlated equilibria, and coarse correlated equilibria.",
      "authors": [
        "Haozhe Jiang",
        "Qiwen Cui",
        "Zhihan Xiong",
        "Maryam Fazel",
        "Simon Shaolei Du"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LWuYsSD94h",
      "cdate": 1695420795872,
      "mdate": 1710509206743,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710134"
    },
    {
      "id": "5tGGWOijvq",
      "title": "Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models",
      "abstract": "With the explosion of the zero-shot capabilities of (and thus interest in) pre-trained large language models, there has come accompanying interest in how best to prompt a language model to perform a given task. While it may be tempting to choose a prompt based on empirical results on a validation set, this can lead to a deployment where an unexpectedly high loss occurs. To mitigate this prospect, we propose a lightweight framework, Prompt Risk Control, for selecting a prompt based on rigorous upper bounds on families of informative risk measures. We provide and compare different methods for producing bounds on a diverse set of risk metrics like mean, CVaR, and the Gini coefficient of the loss distribution. In addition, we extend the underlying statistical bounding techniques to accommodate the possibility of distribution shifts in deployment. Extensive experiments on high-impact applications like chatbots, medical question answering, and news summarization highlight why such a framework is necessary to reduce exposure to the worst outcomes.",
      "authors": [
        "Thomas P Zollo",
        "Todd Morrill",
        "Zhun Deng",
        "Jake Snell",
        "Toniann Pitassi",
        "Richard Zemel"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5tGGWOijvq",
      "cdate": 1695420605102,
      "mdate": 1710273197646,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710139"
    },
    {
      "id": "smy4DsUbBo",
      "title": "Energy-conserving equivariant GNN for elasticity of lattice architected metamaterials",
      "abstract": "Lattices are architected metamaterials whose properties strongly depend on their geometrical design. The analogy between lattices and graphs enables the use of graph neural networks (GNNs) as a faster surrogate model compared to traditional methods such as finite element modelling. In this work, we generate a big dataset of structure-property relationships for strut-based lattices. The dataset is made available to the community which can fuel the development of methods anchored in physical principles for the fitting of fourth-order tensors. In addition, we present a higher-order GNN model trained on this dataset. The key features of the model are (i) SE(3) equivariance, and (ii) consistency with the thermodynamic law of conservation of energy. We compare the model to non-equivariant models based on a number of error metrics and demonstrate its benefits in terms of predictive performance and reduced training requirements. Finally, we demonstrate an example application of the model to an architected material design task. The methods which we developed are applicable to fourth-order tensors beyond elasticity such as piezo-optical tensor etc.",
      "authors": [
        "Ivan Grega",
        "Ilyes Batatia",
        "Gabor Csanyi",
        "Sri Karlapati",
        "Vikram Deshpande"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=smy4DsUbBo",
      "cdate": 1695420463069,
      "mdate": 1710430628593,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710148"
    },
    {
      "id": "UCfz492fM8",
      "title": "CrossLoco: Human Motion Driven Control of Legged Robots via Guided Unsupervised Reinforcement Learning",
      "abstract": "Human motion driven control (HMDC) is an effective approach for generating natural and compelling robot motions while preserving high-level semantics. However, establishing the correspondence between humans and robots with different body structures is not straightforward due to the mismatches in kinematics and dynamics properties, which causes intrinsic ambiguity to the problem. Many previous algorithms approach this motion retargeting problem with unsupervised learning, which requires the prerequisite skill sets. However, it will be extremely costly to learn all the skills without understanding the given human motions, particularly for high-dimensional robots. In this work, we introduce CrossLoco, a guided unsupervised reinforcement learning framework that simultaneously learns robot skills and their correspondence to human motions. Our key innovation is to introduce a cycle-consistency-based reward term designed to maximize the mutual information between human motions and robot states. We demonstrate that the proposed framework can generate compelling robot motions by translating diverse human motions, such as running, hopping, and dancing. We quantitatively compare our CrossLoco against the manually engineered and unsupervised baseline algorithms along with the ablated versions of our framework and demonstrate that our method translates human motions with better accuracy, diversity, and user preference. We also showcase its utility in other applications, such as synthesizing robot movements from language input and enabling interactive robot control.",
      "authors": [
        "Tianyu Li",
        "Hyunyoung Jung",
        "Matthew Gombolay",
        "Yong Cho",
        "Sehoon Ha"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=UCfz492fM8",
      "cdate": 1695419117803,
      "mdate": 1709976146798,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710153"
    },
    {
      "id": "sehRvaIPQQ",
      "title": "Let Models Speak Ciphers: Multiagent Debate through Embeddings",
      "abstract": "Discussion and debate among Large Language Models (LLMs) have gained considerable attention due to their potential to enhance the reasoning ability of LLMs. Although natural language is an obvious choice for communication due to LLM's language understanding capability, the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary. In this paper, we introduce a communication regime named CIPHER (Communicative Inter-Model Protocol Through Embedding Representation) to address this issue. Specifically, we remove the token sampling step from LLMs and let them communicate their beliefs across the vocabulary through the expectation of the raw transformer output embeddings. Remarkably, by deviating from natural language, CIPHER offers an advantage of encoding a broader spectrum of information without any modification to the model weights, outperforming the state-of-the-art LLM debate methods using natural language by 0.5-5.0% across five reasoning tasks and multiple open-source LLMs of varying sizes. This showcases the superiority and robustness of embeddings as an alternative \"language\" for communication among LLMs. We anticipate that CIPHER will inspire further exploration for the design of interactions within LLM agent systems, offering a new direction that could significantly influence future developments in the field.",
      "authors": [
        "Chau Pham",
        "Boyi Liu",
        "Yingxiang Yang",
        "Zhengyu Chen",
        "Tianyi Liu",
        "Jianbo Yuan",
        "Bryan A. Plummer",
        "Zhaoran Wang",
        "Hongxia Yang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=sehRvaIPQQ",
      "cdate": 1695418748659,
      "mdate": 1709661530421,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710158"
    },
    {
      "id": "xhCZD9hiiA",
      "title": "Towards Training Without Depth Limits: Batch Normalization Without Gradient Explosion",
      "abstract": "Normalization layers are one of the key building blocks for deep neural networks. Several theoretical studies have shown that batch normalization improves the signal propagation, by avoiding the representations from becoming collinear across the layers. However, results on mean-field theory of batch normalization also conclude that this benefit comes at the expense of exploding gradients in depth. Motivated by these two aspects of batch normalization, in this study we pose the following question: \n*Can a batch-normalized network keep the optimal signal propagation properties, but avoid exploding gradients?* We answer this question in the affirmative by giving a particular construction of an *MLP with linear activations* and batch-normalization that provably has *bounded gradients* at any depth. Based on Weingarten calculus, we develop a rigorous and non-asymptotic theory for this constructed MLP that gives a precise characterization of forward signal propagation, while proving that gradients remain bounded for linearly independent input samples, which holds in most practical settings. Inspired by our theory, we also design an activation shaping scheme that empirically achieves the same properties for non-linear activations.",
      "authors": [
        "Alexandru Meterez",
        "Amir Joudaki",
        "Francesco Orabona",
        "Alexander Immer",
        "Gunnar Ratsch",
        "Hadi Daneshmand"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xhCZD9hiiA",
      "cdate": 1695418702157,
      "mdate": 1710549274605,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710165"
    },
    {
      "id": "ze7DOLi394",
      "title": "On the Joint Interaction of Models, Data, and Features",
      "abstract": "Learning features from data is one of the defining characteristics of deep learning,\nbut the theoretical understanding of the role features play in deep learning is still in\nearly development. To address this gap, we introduce a new tool, the interaction\ntensor, for empirically analyzing the interaction between data and model through\nfeatures. With the interaction tensor, we make several key observations about\nhow features are distributed in data and how models with different random seeds\nlearn different features. Based on these observations, we propose a conceptual\nframework for feature learning. Under this framework, the expected accuracy for a\nsingle hypothesis and agreement for a pair of hypotheses can both be derived in\nclosed form. We demonstrate that the proposed framework can explain empirically\nobserved phenomena, including the recently discovered Generalization Disagreement Equality (GDE) that allows for estimating the generalization error with only\nunlabeled data. Further, our theory also provides explicit construction of natural\ndata distributions that break the GDE. Thus, we believe this work provides valuable\nnew insight into our understanding of feature learning.",
      "authors": [
        "Yiding Jiang",
        "Christina Baek",
        "J Zico Kolter"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ze7DOLi394",
      "cdate": 1695418673184,
      "mdate": 1712367257270,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710172"
    },
    {
      "id": "kOBkxFRKTA",
      "title": "Dynamic Sparse Training with Structured Sparsity",
      "abstract": "Dynamic Sparse Training (DST) methods achieve state-of-the-art results in sparse neural network training, matching the generalization of dense models while enabling sparse training and inference. Although the resulting models are highly sparse and theoretically less computationally expensive, achieving speedups with unstructured sparsity on real-world hardware is challenging. In this work, we propose a sparse-to-sparse DST method, Structured RigL (SRigL), to learn a variant of fine-grained structured N:M sparsity by imposing a constant fan-in constraint. Using our empirical analysis of existing DST methods at high sparsity, we additionally employ a neuron ablation method which enables SRigL to achieve state-of-the-art sparse-to-sparse structured DST performance on a variety of Neural Network (NN) architectures. Using a 90% sparse linear layer, we demonstrate a real-world acceleration of 3.4×/2.5× on CPU for online inference and 1.7×/13.0× on GPU for inference with a batch size of 256 when compared to equivalent dense/unstructured (CSR) sparse layers, respectively.",
      "authors": [
        "Mike Lasby",
        "Anna Golubeva",
        "Utku Evci",
        "Mihai Nica",
        "Yani Ioannou"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kOBkxFRKTA",
      "cdate": 1695418622742,
      "mdate": 1712610499020,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710180"
    },
    {
      "id": "fkrYDQaHOJ",
      "title": "Efficient Dynamics Modeling in Interactive Environments with Koopman Theory",
      "abstract": "The accurate modeling of dynamics in interactive environments is critical for successful long-range prediction. Such a capability could advance Reinforcement Learning (RL) and Planning algorithms, but achieving it is challenging. Inaccuracies in model estimates can compound, resulting in increased errors over long horizons.\nWe approach this problem from the lens of Koopman theory, where the nonlinear dynamics of the environment can be linearized in a high-dimensional latent space. This allows us to efficiently parallelize the sequential problem of long-range prediction using convolution while accounting for the agent's action at every time step.\nOur approach also enables stability analysis and better control over gradients through time. Taken together, these advantages result in significant improvement over the existing approaches, both in the efficiency and the accuracy of modeling dynamics over extended horizons. We also show that this model can be easily incorporated into dynamics modeling for model-based planning and model-free RL and report promising experimental results.",
      "authors": [
        "Arnab Kumar Mondal",
        "Siba Smarak Panigrahi",
        "Sai Rajeswar",
        "Kaleem Siddiqi",
        "Siamak Ravanbakhsh"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fkrYDQaHOJ",
      "cdate": 1695418578398,
      "mdate": 1712353141169,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710184"
    },
    {
      "id": "MFCjgEOLJT",
      "title": "Learning interpretable control inputs and dynamics underlying animal locomotion",
      "abstract": "A central objective in neuroscience is to understand how the brain orchestrates movement. Recent advances in automated tracking technologies have made it possible to document behavior with unprecedented temporal resolution and scale, generating rich datasets which can be exploited to gain insights into the neural control of movement. One common approach is to identify stereotypical motor primitives using cluster analysis. However, this categorical description can limit our ability to model the effect of more continuous control schemes. Here we take a control theoretic approach to behavioral modeling and argue that movements can be understood as the output of a controlled dynamical system. Previously, models of movement dynamics, trained solely on behavioral data, have been effective in reproducing observed features of neural activity. These models addressed specific scenarios where animals were trained to execute particular movements upon receiving a prompt. In this study, we extend this approach to analyze the full natural locomotor repertoire of an animal: the zebrafish larva. Our findings demonstrate that this repertoire can be effectively generated through a sparse control signal driving a latent Recurrent Neural Network (RNN). Our model's learned latent space preserves key kinematic features and disentangles different categories of movements. To further interpret the latent dynamics, we used balanced model reduction to yield a simplified model. Collectively, our methods serve as a case study for interpretable system identification, and offer a novel framework for understanding neural activity in relation to movement.",
      "authors": [
        "Thomas Soares Mullen",
        "Marine Schimel",
        "Guillaume Hennequin",
        "Christian K. Machens",
        "Michael Orger",
        "Adrien Jouary"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MFCjgEOLJT",
      "cdate": 1695418404769,
      "mdate": 1710521493863,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710189"
    },
    {
      "id": "oAMArMMQxb",
      "title": "Sampling Multimodal Distributions with the Vanilla Score: Benefits of Data-Based Initialization",
      "abstract": "There is a long history, as well as a recent explosion of interest, in statistical and generative modeling approaches based on \\emph{score functions} --- derivatives of the log-likelihood of a distribution. In seminal works, Hyv\\\"arinen proposed vanilla score matching as a way to learn distributions from data by computing an estimate of the score function of the underlying ground truth, and established connections between this method and established techniques like Contrastive Divergence and Pseudolikelihood estimation. It is by now well-known that vanilla score matching has significant difficulties learning multimodal distributions. Although there are various ways to overcome this difficulty, the following question has remained unanswered --- is there a natural way to sample multimodal distributions using just the vanilla score? Inspired by a long line of related experimental works, we prove that the Langevin diffusion with early stopping, initialized at the empirical distribution, and run on a score function estimated from data successfully generates natural multimodal distributions (mixtures of log-concave distributions).",
      "authors": [
        "Frederic Koehler",
        "Thuy-Duong Vuong"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=oAMArMMQxb",
      "cdate": 1695418123229,
      "mdate": 1710488151743,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.710193"
    },
    {
      "id": "4KqkizXgXU",
      "title": "Curiosity-driven Red-teaming for Large Language Models",
      "abstract": "Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a $\\textit{red team}$ of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from LLMs. \nHowever, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM.\nTo overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases and the well-studied approach of curiosity-driven exploration that optimizes for novelty. \nOur method of curiosity-driven red teaming (CRT) achieves greater coverage of test cases while mantaining or increasing their effectiveness compared to existing methods.\nOur method, CRT successfully provokes toxic responses from LLaMA2 model that has been heavily fine-tuned using human preferences to avoid toxic outputs. Code is available at https://github.com/Improbable-AI/curiosity_redteam.",
      "authors": [
        "Zhang-Wei Hong",
        "Idan Shenfeld",
        "Tsun-Hsuan Wang",
        "Yung-Sung Chuang",
        "Aldo Pareja",
        "James R. Glass",
        "Akash Srivastava",
        "Pulkit Agrawal"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4KqkizXgXU",
      "cdate": 1695417604016,
      "mdate": 1709661530030,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710198"
    },
    {
      "id": "0w42S2Gp70",
      "title": "LipSim: A Provably Robust Perceptual Similarity Metric",
      "abstract": "Recent years have seen growing interest in developing and applying perceptual similarity metrics. Research has shown the superiority of perceptual metrics over pixel-wise metrics in aligning with human perception and serving as a proxy for the human visual system.\nOn the other hand, as perceptual metrics rely on neural networks, there is a growing concern regarding their resilience, given the established vulnerability of neural networks to adversarial attacks. It is indeed logical to infer that perceptual metrics may inherit both the strengths and shortcomings of neural networks.\nIn this work, we demonstrate the vulnerability of state-of-the-art perceptual similarity metrics based on an ensemble of ViT-based feature extractors to adversarial attacks. We then propose a framework to train a robust perceptual similarity metric called LipSim (Lipschitz Similarity Metric) with provable guarantees. \nBy leveraging 1-Lipschitz neural networks as the backbone, LipSim provides guarded areas around each data point and certificates for all perturbations within an $\\ell_2$ ball. Finally, a comprehensive set of experiments shows the performance of LipSim in terms of natural and certified scores and on the image retrieval application.",
      "authors": [
        "Sara Ghazanfari",
        "Alexandre Araujo",
        "Prashanth Krishnamurthy",
        "Farshad Khorrami",
        "Siddharth Garg"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0w42S2Gp70",
      "cdate": 1695416655523,
      "mdate": 1710519443028,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710203"
    },
    {
      "id": "rhaQbS3K3R",
      "title": "Does Progress On Object Recognition Benchmarks Improve Generalization on Crowdsourced, Global Data?",
      "abstract": "For more than a decade, researchers have measured progress in object recognition on the ImageNet dataset along with its associated generalization benchmarks such as ImageNet-A, -C, and -R. Recent advances in foundation models, trained on orders of magnitude more data, have begun to saturate performance on these benchmarks. Despite this progress, even today’s best models are brittle in practice. As a step toward more holistic measurement of model reliability, we propose studying performance on crowdsourced, global datasets, which contain natural distribution shifts seen practically in deployment. We perform a comprehensive empirical study on two crowdsourced, globally representative datasets, evaluating nearly 100 vision models to uncover several concerning empirical trends: first, that progress on crowdsourced, global data has significantly lagged behind standard benchmarks, with advances on ImageNet occurring at $2.5x$ the rate of progress on crowdsourced, global data. Second, we find that progress on standard benchmarks has failed to improve or exacerbated geographic disparities: geographic disparities between the least performant models and today's best models have more than tripled. We showcase the promise of using more curated and/or representative training datasets for mitigating these trends, and emphasize curation of web-scale, geographically representative training datasets as a critical open problem for the research community.",
      "authors": [
        "Megan Richards",
        "Polina Kirichenko",
        "Diane Bouchacourt",
        "Mark Ibrahim"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rhaQbS3K3R",
      "cdate": 1695416582664,
      "mdate": 1713386937233,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710207"
    },
    {
      "id": "xtOydkE1Ku",
      "title": "TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series",
      "abstract": "We introduce a new model for multivariate probabilistic time series prediction, designed to flexibly address a range of tasks including forecasting, interpolation, and their combinations. Building on copula theory, we propose a simplified objective for the recently-introduced transformer-based attentional copulas (TACTiS), wherein the number of distributional parameters now scales linearly with the number of variables instead of factorially. The new objective requires the introduction of a training curriculum, which goes hand-in-hand with necessary changes to the original architecture. We show that the resulting model has significantly better training dynamics and achieves state-of-the-art performance across diverse real-world forecasting tasks, while maintaining the flexibility of prior work, such as seamless handling of unaligned and unevenly-sampled time series. Code is made available at https://github.com/ServiceNow/TACTiS.",
      "authors": [
        "Arjun Ashok",
        "Étienne Marcotte",
        "Valentina Zantedeschi",
        "Nicolas Chapados",
        "Alexandre Drouin"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xtOydkE1Ku",
      "cdate": 1695416421696,
      "mdate": 1710606257762,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710212"
    },
    {
      "id": "zbOSJ3CATY",
      "title": "A ROBUST DIFFERENTIAL NEURAL ODE OPTIMIZER",
      "abstract": "Neural networks and neural ODEs tend to be vulnerable to adversarial attacks, rendering robust optimizers critical to curb the success of such attacks. In this regard, the key insight of this work is to interpret Neural ODE optimization as a min-max optimal control problem. More particularly, we present Game Theoretic Second-Order Neural Optimizer (GTSONO), a robust game theoretic optimizer based on the principles of min-max Differential Dynamic Programming.\nThe proposed method exhibits significant computational benefits due to efficient matrix decompositions and provides convergence guarantees to local saddle points.\nEmpirically, the robustness of the proposed optimizer is demonstrated through greater robust accuracy  compared to benchmark optimizers when trained on clean images. Additionally, its ability to provide a performance increase when adapted to an already existing adversarial defense technique is also illustrated.\nFinally, the superiority of the proposed update law over its gradient based counterpart highlights the potential benefits of incorporating robust optimal control paradigms into adversarial training methods.",
      "authors": [
        "Panagiotis Theodoropoulos",
        "Guan-Horng Liu",
        "Tianrong Chen",
        "Augustinos D Saravanos",
        "Evangelos Theodorou"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=zbOSJ3CATY",
      "cdate": 1695416385677,
      "mdate": 1710530865873,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710220"
    },
    {
      "id": "TLADT8Wrhn",
      "title": "TiC-CLIP: Continual Training of CLIP Models",
      "abstract": "Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models. This problem is exacerbated by the lack of any large scale continual learning benchmarks or baselines. We introduce the first set of web-scale Time-Continual (TiC) benchmarks for training vision-language models: TiC-DataComp, TiC-YFCC, and TiC-Redcaps. TiC-DataComp, our largest dataset, contains over 12.7B timestamped image-text pairs spanning 9 years (2014-2022). We first use our benchmarks to curate various dynamic evaluations to measure temporal robustness of existing models. We show OpenAI's CLIP (trained on data up to 2020) loses $\\approx 8\\%$ zero-shot accuracy on our curated retrieval task from 2021-2022 compared with more recently trained models in OpenCLIP repository. We then study how to efficiently train models on time-continuous data. We demonstrate that a simple rehearsal-based approach that continues training from the last checkpoint  and replays old data reduces compute by $2.5\\times$ when compared to the standard practice of retraining from scratch. Code is available at https://github.com/apple/ml-tic-clip.",
      "authors": [
        "Saurabh Garg",
        "Mehrdad Farajtabar",
        "Hadi Pouransari",
        "Raviteja Vemulapalli",
        "Sachin Mehta",
        "Oncel Tuzel",
        "Vaishaal Shankar",
        "Fartash Faghri"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=TLADT8Wrhn",
      "cdate": 1695415748507,
      "mdate": 1710996312690,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710225"
    },
    {
      "id": "hSyW5go0v8",
      "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
      "abstract": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called **Self-Reflective Retrieval-Augmented Generation (Self-RAG)** that enhances an LM's quality and factuality through retrieval and self-reflection. \nOur framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its generations using special tokens, called {\\it reflection} tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. \nExperiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. \nSpecifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning, and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models. Our code and trained models are available at https://selfrag.github.io/",
      "authors": [
        "Akari Asai",
        "Zeqiu Wu",
        "Yizhong Wang",
        "Avirup Sil",
        "Hannaneh Hajishirzi"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hSyW5go0v8",
      "cdate": 1695415509020,
      "mdate": 1710178010465,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710230"
    },
    {
      "id": "CK5Hfb5hBG",
      "title": "Channel Vision Transformers: An Image Is Worth 1 x 16 x 16 Words",
      "abstract": "Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying semantically distinct and independent information. Furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing. In this paper, we propose a modification to the ViT architecture that enhances reasoning across the input channels and introduce Hierarchical Channel Sampling (HCS) as an additional regularization technique to ensure robustness when only partial channels are presented during test time. Our proposed model, ChannelViT, constructs patch tokens independently from each input channel and utilizes a learnable channel embedding that is added to the patch tokens, similar to positional embeddings. We evaluate the performance of ChannelViT on ImageNet, JUMP-CP (microscopy cell imaging), and So2Sat (satellite imaging). Our results show that ChannelViT outperforms ViT on classification tasks and generalizes well, even when a subset of input channels is used during testing. Across our experiments, HCS proves to be a powerful regularizer, independent of the architecture employed, suggesting itself as a straightforward technique for robust ViT training. Lastly, we find that ChannelViT generalizes effectively even when there is limited access to all channels during training, highlighting its potential for multi-channel imaging under real-world conditions with sparse sensors. Our code is available at https://github.com/insitro/ChannelViT.",
      "authors": [
        "Yujia Bao",
        "Srinivasan Sivanandan",
        "Theofanis Karaletsos"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=CK5Hfb5hBG",
      "cdate": 1695415433805,
      "mdate": 1713672890228,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710234"
    },
    {
      "id": "ijK5hyxs0n",
      "title": "Graph Metanetworks for Processing Diverse Neural Architectures",
      "abstract": "Neural networks efficiently encode learned information within their parameters. Consequently, many tasks can be unified by treating neural networks themselves as input data. When doing so, recent studies demonstrated the importance of accounting for the symmetries and geometry of parameter spaces. However, those works developed architectures tailored to specific networks such as MLPs and CNNs without normalization layers, and generalizing such architectures to other types of networks can be challenging. In this work, we overcome these challenges by building new metanetworks --- neural networks that take weights from other neural networks as input. Put simply, we carefully build graphs representing the input neural networks and process the graphs using graph neural networks. Our approach, Graph Metanetworks (GMNs), generalizes to neural architectures where competing methods struggle, such as multi-head attention layers, normalization layers, convolutional layers, ResNet blocks, and group-equivariant linear layers. We prove that GMNs are expressive and equivariant to parameter permutation symmetries that leave the input neural network functions unchanged. We validate the effectiveness of our method on several metanetwork tasks over diverse neural network architectures.",
      "authors": [
        "Derek Lim",
        "Haggai Maron",
        "Marc T. Law",
        "Jonathan Lorraine",
        "James Lucas"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ijK5hyxs0n",
      "cdate": 1695415400220,
      "mdate": 1710339485460,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710239"
    },
    {
      "id": "VoLDkQ6yR3",
      "title": "Understanding Reconstruction Attacks with the Neural Tangent Kernel and Dataset Distillation",
      "abstract": "Modern deep learning requires large volumes of data, which could contain sensitive or private information that cannot be leaked. Recent work has shown for homogeneous neural networks a large portion of this training data could be reconstructed with only access to the trained network parameters. While the attack was shown to work empirically, there exists little formal understanding of its effective regime and which datapoints are susceptible to reconstruction. In this work, we first build a stronger version of the dataset reconstruction attack and show how it can provably recover the \\emph{entire training set} in the infinite width regime. We then empirically study the characteristics of this attack on two-layer networks and reveal that its success heavily depends on deviations from the frozen infinite-width Neural Tangent Kernel limit. Next, we study the nature of easily-reconstructed images. We show that both theoretically and empirically, reconstructed images tend to ``outliers'' in the dataset, and that these reconstruction attacks can be used for \\textit{dataset distillation}, that is, we can retrain on reconstructed images and obtain high predictive accuracy.",
      "authors": [
        "Noel Loo",
        "Ramin Hasani",
        "Mathias Lechner",
        "Alexander Amini",
        "Daniela Rus"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=VoLDkQ6yR3",
      "cdate": 1695415076041,
      "mdate": 1710367893712,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710243"
    },
    {
      "id": "iIT02bAKzv",
      "title": "ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models",
      "abstract": "Large Vision-Language Models (LVLMs) can understand the world comprehensively by integrating rich information from different modalities, achieving remarkable performance improvements on various multimodal downstream tasks. However, deploying LVLMs is often problematic due to their massive computational/energy costs and carbon consumption, making it infeasible to adopt conventional iterative global pruning, which is costly due to computing the Hessian matrix of the entire large model for sparsification. Alternatively, several studies have recently proposed layer-wise pruning approaches to avoid the expensive computation of global pruning and efficiently compress model weights according to their importance within a layer. However, these methods often suffer from suboptimal model compression due to their lack of a global perspective. To address this limitation in recent efficient pruning methods for large models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP), a two-stage coarse-to-fine weight pruning approach for LVLMs. We first determine the sparsity ratios of different layers or blocks by leveraging the global importance score, which is efficiently computed based on the zeroth-order approximation of the global model gradients. Then, the multimodal model performs layer-wise unstructured weight pruning. We validate our proposed method across various multi-modal and single-modal models and datasets, demonstrating significant performance improvements over prevalent pruning techniques in the high-sparsity regime.",
      "authors": [
        "Yi-Lin Sung",
        "Jaehong Yoon",
        "Mohit Bansal"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iIT02bAKzv",
      "cdate": 1695414758386,
      "mdate": 1710179311686,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.710248"
    },
    {
      "id": "qup9xD8mW4",
      "title": "Behaviour Distillation",
      "abstract": "Dataset distillation aims to condense large datasets into a small number of synthetic examples that can be used as drop-in replacements when training new models. It has applications to interpretability, neural architecture search, privacy, and continual learning. Despite strong successes in supervised domains, such methods have not yet been extended to reinforcement learning, where the lack of a fixed dataset renders most distillation methods unusable.\nFilling the gap, we formalize $\\textit{behaviour distillation}$, a setting that aims to discover and then condense the information required for training an expert policy into a synthetic dataset of state-action pairs, $\\textit{without access to expert data}$. \nWe then introduce Hallucinating Datasets with Evolution Strategies (HaDES), a method for behaviour distillation that can discover datasets of $\\textit{just four}$ state-action pairs which, under supervised learning, train agents to competitive performance levels in continuous control tasks.\nWe show that these datasets generalize out of distribution to training policies with a wide range of architectures and hyperparameters. We also demonstrate application to a downstream task, namely training multi-task agents in a zero-shot fashion.\nBeyond behaviour distillation, HaDES provides significant improvements in neuroevolution for RL over previous approaches and achieves SoTA results on one standard supervised dataset distillation task. Finally, we show that visualizing the synthetic datasets can provide human-interpretable task insights.",
      "authors": [
        "Andrei Lupu",
        "Chris Lu",
        "Jarek Luca Liesen",
        "Robert Tjarko Lange",
        "Jakob Nicolaus Foerster"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qup9xD8mW4",
      "cdate": 1695414746893,
      "mdate": 1713145565475,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710253"
    },
    {
      "id": "hB7SlfEmze",
      "title": "PhyloGFN: Phylogenetic inference with generative flow networks",
      "abstract": "Phylogenetics is a branch of computational biology that studies the evolutionary relationships among biological entities. Its long history and numerous applications notwithstanding, inference of phylogenetic trees from sequence data remains challenging: the high complexity of tree space poses a significant obstacle for the current combinatorial and probabilistic techniques. In this paper, we adopt the framework of generative flow networks (GFlowNets) to tackle two core problems in phylogenetics: parsimony-based and Bayesian phylogenetic inference. Because GFlowNets are well-suited for sampling complex combinatorial structures, they are a natural choice for exploring and sampling from the multimodal posterior distribution over tree topologies and evolutionary distances. We demonstrate that our amortized posterior sampler, PhyloGFN, produces diverse and high-quality evolutionary hypotheses on real benchmark datasets. PhyloGFN is competitive with prior works in marginal likelihood estimation and achieves a closer fit to the target distribution than state-of-the-art variational inference methods.",
      "authors": [
        "Ming Yang Zhou",
        "Zichao Yan",
        "Elliot Layne",
        "Nikolay Malkin",
        "Dinghuai Zhang",
        "Moksh Jain",
        "Mathieu Blanchette",
        "Yoshua Bengio"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hB7SlfEmze",
      "cdate": 1695414557128,
      "mdate": 1711326490619,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.710257"
    },
    {
      "id": "TskzCtpMEO",
      "title": "Training Bayesian Neural Networks with Sparse Subspace Variational Inference",
      "abstract": "Bayesian neural networks (BNNs) offer uncertainty quantification but come with the downside of substantially increased training and inference costs. Sparse BNNs have been investigated for efficient inference, typically by either slowly introducing sparsity throughout the training or by post-training compression of dense BNNs. The dilemma of how to cut down massive training costs remains, particularly given the requirement to learn about the uncertainty. To solve this challenge, we introduce Sparse Subspace Variational Inference (SSVI), the first fully sparse BNN framework that maintains a consistently sparse Bayesian model throughout the training and inference phases. Starting from a randomly initialized low-dimensional sparse subspace, our approach alternately optimizes the sparse subspace basis selection and its associated parameters. While basis selection is characterized as a non-differentiable problem, we approximate the optimal solution with a removal-and-addition strategy, guided by novel criteria based on weight distribution statistics. Our extensive experiments show that SSVI sets new benchmarks in crafting sparse BNNs, achieving, for instance, a 10-20× compression in model size with under 3\\% performance drop, and up to 20× FLOPs reduction during training. Remarkably, SSVI also demonstrates enhanced robustness to hyperparameters, reducing the need for intricate tuning in VI and occasionally even surpassing VI-trained dense BNNs.",
      "authors": [
        "Junbo Li",
        "Zichen Miao",
        "Qiang Qiu",
        "Ruqi Zhang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=TskzCtpMEO",
      "cdate": 1695414549097,
      "mdate": 1709661529098,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710262"
    },
    {
      "id": "4Ua4hKiAJX",
      "title": "Locality-Aware Graph Rewiring in GNNs",
      "abstract": "Graph Neural Networks (GNNs) are popular models for machine learning on graphs that typically follow the message-passing paradigm, whereby the feature of a node is updated recursively upon aggregating information over its neighbors. While exchanging messages over the input graph endows GNNs with a strong inductive bias, it can also make GNNs susceptible to over-squashing, thereby preventing them from capturing long-range interactions in the given graph. To rectify this issue, graph rewiring techniques have been proposed as a means of improving information flow by altering the graph connectivity. In this work, we identify three desiderata for graph-rewiring: (i) reduce over-squashing, (ii) respect the locality of the graph, and \n(iii) preserve the sparsity of the graph. We highlight fundamental trade-offs that occur between spatial and spectral rewiring techniques; while the former often satisfy (i) and (ii) but not (iii), the latter generally satisfy (i) and (iii) at the expense of (ii). We propose a novel rewiring framework that satisfies all of (i)--(iii) through a locality-aware sequence of rewiring operations. We then discuss a specific instance of such rewiring framework and \nvalidate its effectiveness on several real-world benchmarks, showing that it either matches or significantly outperforms existing rewiring approaches.",
      "authors": [
        "Federico Barbero",
        "Ameya Velingker",
        "Amin Saberi",
        "Michael M. Bronstein",
        "Francesco Di Giovanni"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4Ua4hKiAJX",
      "cdate": 1695414497792,
      "mdate": 1710428130223,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710269"
    },
    {
      "id": "sSaN4gxuEf",
      "title": "Adapting to Distribution Shift by Visual Domain Prompt Generation",
      "abstract": "In this paper, we aim to adapt a model at test-time using a few unlabeled data to address distribution shifts. \nTo tackle the challenges of extracting domain knowledge from a limited amount of data, it is crucial to utilize correlated information from pre-trained backbones and source domains. Previous studies fail to utilize recent foundation models with strong out-of-distribution generalization. Additionally, domain-centric designs are not flavored in their works. Furthermore, they employ the process of modelling source domains and the process of learning to adapt independently into disjoint training stages. In this work, we propose an approach on top of the pre-computed features of the foundation model. Specifically, we build a knowledge bank to learn the transferable knowledge from source domains. Conditioned on few-shot target data, we introduce a domain prompt generator to condense the knowledge bank into a domain-specific prompt. The domain prompt then directs the visual features towards a particular domain via a guidance module. Moreover, we propose a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction. Extensive experiments are conducted to validate the domain knowledge extraction. The proposed method outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.",
      "authors": [
        "Zhixiang Chi",
        "Li Gu",
        "Tao Zhong",
        "Huan Liu",
        "YUANHAO YU",
        "Konstantinos N Plataniotis",
        "Yang Wang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=sSaN4gxuEf",
      "cdate": 1695413711695,
      "mdate": 1710460990333,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710274"
    },
    {
      "id": "Je5SHCKpPa",
      "title": "Multimodal Patient Representation Learning with Missing Modalities and Labels",
      "abstract": "Multimodal patient representation learning aims to integrate information from multiple modalities and generate comprehensive patient representations for subsequent clinical predictive tasks. However, many existing approaches either presuppose the availability of all modalities and labels for each patient or only deal with missing modalities. In reality, patient data often comes with both missing modalities and labels for various reasons (i.e., the missing modality and label issue). Moreover, multimodal models might over-rely on certain modalities, causing sub-optimal performance when these modalities are absent (i.e., the modality collapse issue). To address these issues, we introduce MUSE: a mutual-consistent graph contrastive learning method. MUSE uses a flexible bipartite graph to represent the patient-modality relationship, which can adapt to various missing modality patterns. To tackle the modality collapse issue, MUSE learns to focus on modality-general and label-decisive features via a mutual-consistent contrastive learning loss. Notably, the unsupervised component of the contrastive objective only requires self-supervision signals, thereby broadening the training scope to incorporate patients with missing labels. We evaluate MUSE on three publicly available datasets: MIMIC-IV, eICU, and ADNI. Results show that MUSE outperforms all baselines, and MUSE+ further elevates the absolute improvement to ~4% by extending the training scope to patients with absent labels.",
      "authors": [
        "Zhenbang Wu",
        "Anant Dadu",
        "Nicholas Tustison",
        "Brian Avants",
        "Mike Nalls",
        "Jimeng Sun",
        "Faraz Faghri"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Je5SHCKpPa",
      "cdate": 1695413687083,
      "mdate": 1713154017393,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.710278"
    },
    {
      "id": "h4pNROsO06",
      "title": "Improved sampling via learned diffusions",
      "abstract": "Recently, a series of papers proposed deep learning-based approaches to sample from target distributions using controlled diffusion processes, being trained only on the unnormalized target densities without access to samples. Building on previous work, we identify these approaches as special cases of a generalized Schrödinger bridge problem, seeking a stochastic evolution between a given prior distribution and the specified target. We further generalize this framework by introducing a variational formulation based on divergences between path space measures of time-reversed diffusion processes. This abstract perspective leads to practical losses that can be optimized by gradient-based algorithms and includes previous objectives as special cases. At the same time, it allows us to consider divergences other than the reverse Kullback-Leibler divergence that is known to suffer from mode collapse. In particular, we propose the so-called \\textit{log-variance loss}, which exhibits favorable numerical properties and leads to significantly improved performance across all considered approaches.",
      "authors": [
        "Lorenz Richter",
        "Julius Berner"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=h4pNROsO06",
      "cdate": 1695413684112,
      "mdate": 1713311735803,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710283"
    },
    {
      "id": "1tZbq88f27",
      "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
      "abstract": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed.\nWe believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). \nTo examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. \nOur work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, \nsuch as detailed image description generation and website creation from hand-drawn drafts.\nFurthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. \nIn our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability.",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=1tZbq88f27",
      "cdate": 1695413679593,
      "mdate": 1710246922598,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710287"
    },
    {
      "id": "C4CxQmp9wc",
      "title": "Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX",
      "abstract": "Open-source reinforcement learning (RL) environments have played a crucial role in driving progress in the development of AI algorithms.\nIn modern RL research, there is a need for simulated environments that are performant, scalable, and modular to enable their utilization in a wider range of potential real-world applications.\nTherefore, we present Jumanji, a suite of diverse RL environments specifically designed to be fast, flexible, and scalable.\nJumanji provides a suite of environments focusing on combinatorial problems frequently encountered in industry, as well as challenging general decision-making tasks.\nBy leveraging the efficiency of JAX and hardware accelerators like GPUs and TPUs, Jumanji enables rapid iteration of research ideas and large-scale experimentation, ultimately empowering more capable agents.\nUnlike existing RL environment suites, Jumanji is highly customizable, allowing users to tailor the initial state distribution and problem complexity to their needs.\nFurthermore, we provide actor-critic baselines for each environment, accompanied by preliminary findings on scaling and generalization scenarios.\nJumanji aims to set a new standard for speed, adaptability, and scalability of RL environments.",
      "authors": [
        "Clément Bonnet",
        "Daniel Luo",
        "Donal John Byrne",
        "Shikha Surana",
        "Sasha Abramowitz",
        "Paul Duckworth",
        "Vincent Coyette",
        "Laurence Illing Midgley",
        "Elshadai Tegegn",
        "Tristan Kalloniatis",
        "Omayma Mahjoub",
        "Matthew Macfarlane",
        "Andries Petrus Smit",
        "Nathan Grinsztajn",
        "Raphael Boige",
        "Cemlyn Neil Waters",
        "Mohamed Ali Ali Mimouni",
        "Ulrich Armel Mbou Sob",
        "Ruan John de Kock",
        "Siddarth Singh",
        "Daniel Furelos-Blanco",
        "Victor Le",
        "Arnu Pretorius",
        "Alexandre Laterre"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=C4CxQmp9wc",
      "cdate": 1695412888837,
      "mdate": 1713672893873,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710292"
    },
    {
      "id": "nqlymMx42E",
      "title": "Searching for High-Value Molecules Using Reinforcement Learning and Transformers",
      "abstract": "Reinforcement learning (RL) over text representations can be effective for finding high-value policies that can search over graphs. However, RL requires careful structuring of the search space and algorithm design to be effective in this challenge. Through extensive experiments, we explore how different design choices for text grammar and algorithmic choices for training can affect an RL policy's ability to generate molecules with desired properties. We arrive at a new RL-based molecular design algorithm (ChemRLformer) and perform a thorough analysis using 25 molecule design tasks, including computationally complex protein docking simulations. From this analysis, we discover unique insights in this problem space and show that ChemRLformer achieves state-of-the-art performance while being more straightforward than prior work by demystifying which design choices are actually helpful for text-based molecule design.",
      "authors": [
        "Raj Ghugare",
        "Santiago Miret",
        "Adriana Hugessen",
        "Mariano Phielipp",
        "Glen Berseth"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nqlymMx42E",
      "cdate": 1695412835178,
      "mdate": 1710189157268,
      "matched_keywords": [
        "reinforcement learning",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710296"
    },
    {
      "id": "MO632iPq3I",
      "title": "Differentiable Euler Characteristic Transforms for Shape Classification",
      "abstract": "The _Euler Characteristic Transform_ (ECT) is a powerful\n  invariant, combining geometrical and topological characteristics\n  of shapes and graphs.\n  However, the ECT was hitherto unable to learn task-specific\n  representations.\n  We overcome this issue and develop a novel computational layer that\n  enables learning the ECT in an end-to-end fashion.\n  Our method, the _Differentiable Euler Characteristic Transform_ (DECT) \n  is fast and computationally efficient, while exhibiting performance on a par with \n  more complex models in both graph and point cloud classification tasks.\n  Moreover, we show that this seemingly simple statistic\n  provides the same topological expressivity as more complex topological\n  deep learning layers.",
      "authors": [
        "Ernst Röell",
        "Bastian Rieck"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MO632iPq3I",
      "cdate": 1695412736168,
      "mdate": 1713672717877,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710301"
    },
    {
      "id": "hp4yOjhwTs",
      "title": "Causally Aligned Curriculum Learning",
      "abstract": "A pervasive challenge in Reinforcement Learning (RL) is the ``curse of dimensionality'' which is the exponential growth in the state-action space when optimizing a high-dimensional target task. The framework of curriculum learning trains the agent in a curriculum composed of a sequence of related and more manageable source tasks. The expectation is that when some optimal decision rules are shared across source tasks and the target task, the agent could more quickly pick up the necessary skills to behave optimally in the environment, thus accelerating the learning process. \nHowever, this critical assumption of invariant optimal decision rules does not necessarily hold in many practical applications, specifically when the underlying environment contains unobserved confounders. This paper studies the problem of curriculum RL through causal lenses. We derive a sufficient graphical condition characterizing causally aligned source tasks, i.e., the invariance of optimal decision rules holds. We further develop an efficient algorithm to generate a causally aligned curriculum, provided with qualitative causal knowledge of the target environment. Finally, we validate our proposed methodology through experiments in confounded environments.",
      "authors": [
        "Mingxuan Li",
        "Junzhe Zhang",
        "Elias Bareinboim"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hp4yOjhwTs",
      "cdate": 1695412722994,
      "mdate": 1715174266981,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710305"
    },
    {
      "id": "vt5mnLVIVo",
      "title": "Grokking as the transition from lazy to rich training dynamics",
      "abstract": "We propose that the grokking phenomenon, where the train loss of a neural network decreases much earlier than its test loss, can arise due to a neural network transitioning from lazy training dynamics to a rich, feature learning regime. To illustrate this mechanism, we study the simple setting of vanilla gradient descent on a polynomial regression problem with a two layer neural network which exhibits grokking without regularization in a way that cannot be explained by existing theories. We identify sufficient statistics for the test loss of such a network, and tracking these over training reveals that grokking arises in this setting when the network first attempts to fit a kernel regression solution with its initial features, followed by late-time feature learning where a generalizing solution is identified after train loss is already low. We find that the key determinants of grokking are the rate of feature learning---which can be controlled precisely by parameters that scale the network output---and the alignment of the initial features with the target function $y(x)$. We argue this delayed generalization arises when (1) the top eigenvectors of the initial neural tangent kernel and the task labels $y(x)$ are misaligned, but (2) the dataset size is large enough so that it is possible for the network to generalize eventually, but not so large that train loss perfectly tracks test loss at all epochs, and (3) the network begins training in the lazy regime so does not learn features immediately. We conclude with evidence that this transition from lazy (linear model) to rich training (feature learning) can control grokking in more general settings, like on MNIST, one-layer Transformers, and student-teacher networks.",
      "authors": [
        "Tanishq Kumar",
        "Blake Bordelon",
        "Samuel J. Gershman",
        "Cengiz Pehlevan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vt5mnLVIVo",
      "cdate": 1695411840861,
      "mdate": 1709661528560,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710310"
    },
    {
      "id": "M0xK8nPGvt",
      "title": "Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning",
      "abstract": "Posterior sampling allows exploitation of prior knowledge on the environment's transition dynamics to improve the sample efficiency of reinforcement learning. The prior is typically specified as a class of parametric distributions, the design of which can be cumbersome in practice, often resulting in the choice of uninformative priors. In this work, we propose a novel posterior sampling approach in which the prior is given as a (partial) causal graph over the environment's variables. The latter is often more natural to design, such as listing known causal dependencies between biometric features in a medical treatment study. Specifically, we propose a hierarchical Bayesian procedure, called C-PSRL, simultaneously learning the full causal graph at the higher level and the parameters of the resulting factored dynamics at the lower level. We provide an analysis of the Bayesian regret of C-PSRL that explicitly connects the regret rate with the degree of prior knowledge. Our numerical evaluation conducted in illustrative domains confirms that C-PSRL strongly improves the efficiency of posterior sampling with an uninformative prior while performing close to posterior sampling with the full causal graph.",
      "authors": [
        "Mirco Mutti",
        "Riccardo De Santi",
        "Marcello Restelli",
        "Alexander Marx",
        "Giorgia Ramponi"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=M0xK8nPGvt",
      "cdate": 1695411652748,
      "mdate": 1710576020387,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710317"
    },
    {
      "id": "3JjJezzVkT",
      "title": "The Marginal Value of Momentum for Small Learning Rate SGD",
      "abstract": "Momentum is known to accelerate the convergence of gradient descent in strongly convex settings without stochastic gradient noise. In stochastic optimization, such as training neural networks, folklore suggests that momentum may help deep learning optimization by reducing the variance of the stochastic gradient update, but previous theoretical analyses do not find momentum to offer any provable acceleration. Theoretical results in this paper clarify the role of momentum in stochastic settings where the learning rate is small and gradient noise is the dominant source of instability, suggesting that SGD with and without momentum behave similarly in the short and long time horizons. Experiments show that momentum indeed has limited benefits for both optimization and generalization in practical training regimes where the optimal learning rate is not very large, including small- to medium-batch training from scratch on ImageNet and fine-tuning language models on downstream tasks.",
      "authors": [
        "Runzhe Wang",
        "Sadhika Malladi",
        "Tianhao Wang",
        "Kaifeng Lyu",
        "Zhiyuan Li"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3JjJezzVkT",
      "cdate": 1695411531526,
      "mdate": 1713232350952,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710322"
    },
    {
      "id": "3mnWvUZIXt",
      "title": "Towards Principled Representation Learning from Videos for Reinforcement Learning",
      "abstract": "We study pre-training representations for decision-making using video data, which is abundantly available for tasks such as game agents and software testing. Even though significant empirical advances have been made on this problem, a theoretical understanding remains absent. We initiate the theoretical investigation into principled approaches for representation learning and focus on learning the latent state representations of the underlying MDP using video data. We study two types of settings: one where there is iid noise in the observation, and a more challenging setting where there is also the presence of exogenous noise, which is non-iid noise that is temporally correlated, such as the motion of people or cars in the background. We study three commonly used approaches: autoencoding, temporal contrastive learning, and forward modeling. We prove upper bounds for temporal contrastive learning and forward modeling in the presence of only iid noise. We show that these approaches can learn the latent state and use it to do efficient downstream RL with polynomial sample complexity. When exogenous noise is also present, we establish a lower bound result showing that the sample complexity of learning from video data can be exponentially worse than learning from action-labeled trajectory data. This partially explains why reinforcement learning with video pre-training is hard. We evaluate these representational learning methods in two visual domains, yielding results that are consistent with our theoretical findings.",
      "authors": [
        "Dipendra Misra",
        "Akanksha Saran",
        "Tengyang Xie",
        "Alex Lamb",
        "John Langford"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3mnWvUZIXt",
      "cdate": 1695411369674,
      "mdate": 1713248886153,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710326"
    },
    {
      "id": "gyfXuRfxW2",
      "title": "Learning Polynomial Problems with $SL(2, \\mathbb{R})$-Equivariance",
      "abstract": "Optimizing and certifying the positivity of polynomials are fundamental primitives across mathematics and engineering applications, from dynamical systems to operations research. However, solving these problems in practice requires large semidefinite programs, with poor scaling in dimension and degree. In this work, we demonstrate for the first time that neural networks can effectively solve such problems in a data-driven fashion, achieving tenfold speedups while retaining high accuracy. Moreover, we observe that these polynomial learning problems are equivariant to the non-compact group $SL(2,\\mathbb{R})$, which consists of area-preserving linear transformations. We therefore adapt our learning pipelines to accommodate this structure, including data augmentation, a new $SL(2,\\mathbb{R})$-equivariant architecture, and an architecture equivariant with respect to its maximal compact subgroup, $SO(2, \\mathbb{R})$. Surprisingly, the most successful approaches in practice do not enforce equivariance to the entire group, which we prove arises from an unusual lack of architecture universality for $SL(2,\\mathbb{R})$ in particular. A consequence of this result, which is of independent interest, is that there exists an equivariant function for which there is no sequence of equivariant polynomials multiplied by arbitrary invariants that approximates the original function. This is a rare example of a symmetric problem where data augmentation outperforms a fully equivariant architecture, and provides interesting lessons in both theory and practice for other problems with non-compact symmetries.",
      "authors": [
        "Hannah Lawrence",
        "Mitchell Tong Harris"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=gyfXuRfxW2",
      "cdate": 1695410932158,
      "mdate": 1710518678225,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710331"
    },
    {
      "id": "wYvuY60SdD",
      "title": "Mixture of Weak and Strong Experts on Graphs",
      "abstract": "Realistic graphs contain both (1) rich self-features of nodes and  (2) informative structures of neighborhoods, jointly handled by a Graph Neural Network (GNN) in the typical setup. We propose to decouple the two modalities by **M**ixture **o**f **w**eak and **st**rong experts (**Mowst**), where the weak expert is a light-weight Multi-layer Perceptron (MLP), and the strong expert is an off-the-shelf GNN. To adapt the experts' collaboration to different target nodes, we propose a \"confidence\" mechanism based on the dispersion of the weak expert's prediction logits. The strong expert is conditionally activated in the low-confidence region when either the node's classification relies on neighborhood information, or the weak expert has low model quality. We reveal interesting training dynamics by analyzing the influence of the confidence function on loss: our training algorithm encourages the specialization of each expert by effectively generating soft splitting of the graph. In addition, our \"confidence\" design imposes a desirable bias toward the strong expert to benefit from GNN's better generalization capability. Mowst is easy to optimize and achieves strong expressive power, with a computation cost comparable to a single GNN. Empirically, Mowst on 4 backbone GNN architectures show significant accuracy improvement on 6 standard node classification benchmarks, including both homophilous and heterophilous graphs (https://github.com/facebookresearch/mowst-gnn).",
      "authors": [
        "Hanqing Zeng",
        "Hanjia Lyu",
        "Diyi Hu",
        "Yinglong Xia",
        "Jiebo Luo"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=wYvuY60SdD",
      "cdate": 1695410876859,
      "mdate": 1713177216405,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710336"
    },
    {
      "id": "NU9AYHJvYe",
      "title": "Optimal Sample Complexity of Contrastive Learning",
      "abstract": "Contrastive learning is a highly successful technique for learning representations of data from labeled tuples, specifying the distance relations within the tuple. We study the sample complexity of contrastive learning, i.e. the minimum number of labeled tuples sufficient for getting high generalization accuracy. We give tight bounds on the sample complexity in a variety of settings, focusing on arbitrary distance functions,  $\\ell_p$-distances, and tree metrics. Our main result is an (almost) optimal bound on the sample complexity of learning $\\ell_p$-distances for integer $p$. For any $p \\ge 1$, we show that $\\tilde \\Theta(nd)$ labeled tuples are necessary and sufficient for learning $d$-dimensional representations of $n$-point datasets. Our results hold for an arbitrary distribution of the input samples and are based on giving the corresponding bounds on the Vapnik-Chervonenkis/Natarajan dimension of the associated problems. We further show that the theoretical bounds on sample complexity obtained via VC/Natarajan dimension can have strong predictive power for experimental results, in contrast with the folklore belief about a substantial gap between the statistical learning theory and the practice of deep learning.",
      "authors": [
        "Noga Alon",
        "Dmitrii Avdiukhin",
        "Dor Elboim",
        "Orr Fischer",
        "Grigory Yaroslavtsev"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NU9AYHJvYe",
      "cdate": 1695410627881,
      "mdate": 1710465920896,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710340"
    },
    {
      "id": "sLkj91HIZU",
      "title": "Transformers can optimally learn regression mixture models",
      "abstract": "Mixture models arise in many regression problems, but most methods have seen limited adoption partly due to these algorithms' highly-tailored and model-specific nature. On the other hand, transformers are flexible, neural sequence models that present the intriguing possibility of providing general-purpose prediction methods, even in this mixture setting. In this work, we investigate the hypothesis that transformers can learn an optimal predictor for mixtures of regressions. We construct a generative process for a mixture of linear regressions for which the decision-theoretic optimal procedure is given by data-driven exponential weights on a finite set of parameters. We observe that transformers achieve low mean-squared error on data generated via this process. By probing the transformer's output at inference time, we also show that transformers typically make predictions that are close to the optimal predictor. Our experiments also demonstrate that transformers can learn mixtures of regressions in a sample-efficient fashion and are somewhat robust to distribution shifts. We complement our experimental observations by proving constructively that the decision-theoretic optimal procedure is indeed implementable by a transformer.",
      "authors": [
        "Reese Pathak",
        "Rajat Sen",
        "Weihao Kong",
        "Abhimanyu Das"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=sLkj91HIZU",
      "cdate": 1695410453920,
      "mdate": 1710864668700,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710344"
    },
    {
      "id": "iCNOK45Csv",
      "title": "Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective",
      "abstract": "Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based on kernel methods. We introduce two new theory-driven trigger pattern generation methods specialized for dataset distillation. Following a comprehensive set of analyses and experiments, we show that our optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. Notably, datasets poisoned by our designed trigger prove resilient against conventional backdoor attack detection and mitigation methods.  Our empirical results validate that the triggers developed using our approaches are proficient at executing resilient backdoor attacks.",
      "authors": [
        "Ming-Yu Chung",
        "Sheng-Yen Chou",
        "Chia-Mu Yu",
        "Pin-Yu Chen",
        "Sy-Yen Kuo",
        "Tsung-Yi Ho"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iCNOK45Csv",
      "cdate": 1695410369179,
      "mdate": 1709661528244,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710349"
    },
    {
      "id": "jjA4O1vJRz",
      "title": "LLM Augmented LLMs: Expanding Capabilities through Composition",
      "abstract": "Foundational models with billions of parameters which have been trained on large corpus of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities,several new instances of these models are being trained towards new domains and tasks.  In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end,  we propose CALM—Composition to Augment Language Models—which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by ‘re-using’ existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly,when PaLM2-S is augmented with a code-specific model, we see a relative improvement of 40% over the base model for code generation and explanation tasks—on-par with fully fine-tuned counterparts.",
      "authors": [
        "Rachit Bansal",
        "Bidisha Samanta",
        "Siddharth Dalmia",
        "Nitish Gupta",
        "Sriram Ganapathy",
        "Abhishek Bapna",
        "Prateek Jain",
        "Partha Talukdar"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jjA4O1vJRz",
      "cdate": 1695410022672,
      "mdate": 1710475096439,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710353"
    },
    {
      "id": "PvJnX3dwsD",
      "title": "Quadratic models for understanding catapult dynamics of neural networks",
      "abstract": "While neural networks can be approximated by linear models as their width increases, certain properties of wide neural networks cannot be captured by linear models. In this work we show that recently proposed Neural Quadratic Models can exhibit the \"catapult phase\" Lewkowycz et al. (2020) that arises when training such models with large learning rates. We then empirically show that the behaviour of quadratic models parallels that of neural networks in generalization, especially in the catapult phase regime. Our analysis further demonstrates that quadratic models are an effective tool for analysis of neural networks.",
      "authors": [
        "Libin Zhu",
        "Chaoyue Liu",
        "Adityanarayanan Radhakrishnan",
        "Mikhail Belkin"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=PvJnX3dwsD",
      "cdate": 1695410022028,
      "mdate": 1713672648560,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710358"
    },
    {
      "id": "sTYuRVrdK3",
      "title": "Evaluating Representation Learning on the Protein Structure Universe",
      "abstract": "We introduce ProteinWorkshop, a comprehensive benchmark suite for representation learning on protein structures with Geometric Graph Neural Networks. We consider large-scale pre-training and downstream tasks on both experimental and predicted structures to enable the systematic evaluation of the quality of the learned structural representation and their usefulness in capturing functional relationships for downstream tasks. We find that: (1) large-scale pretraining on AlphaFold structures and auxiliary tasks consistently improve the performance of both rotation-invariant and equivariant GNNs, and (2) more expressive equivariant GNNs benefit from pretraining to a greater extent compared to invariant models.\nWe aim to establish a common ground for the machine learning and computational biology communities to rigorously compare and advance protein structure representation learning. Our open-source codebase reduces the barrier to entry for working with large protein structure datasets by providing: (1) storage-efficient dataloaders for large-scale structural databases including AlphaFoldDB and ESM Atlas, as well as (2) utilities for constructing new tasks from the entire PDB. ProteinWorkshop is available at: github.com/a-r-j/ProteinWorkshop.",
      "authors": [
        "Arian Rokkum Jamasb",
        "Alex Morehead",
        "Chaitanya K. Joshi",
        "Zuobai Zhang",
        "Kieran Didi",
        "Simon V Mathis",
        "Charles Harris",
        "Jian Tang",
        "Jianlin Cheng",
        "Pietro Lio",
        "Tom Leon Blundell"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=sTYuRVrdK3",
      "cdate": 1695409731104,
      "mdate": 1713672152733,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710365"
    },
    {
      "id": "ViPtjIVzUw",
      "title": "T-MARS: Improving Visual Representations by Circumventing Text Feature Learning",
      "abstract": "Large web-crawled multimodal datasets have powered a slew of new methods for learning general-purpose visual representations, advancing the state of the art in computer vision and revolutionizing zero- and few-shot recognition. One crucial decision facing practitioners is how, if at all, to curate these ever-larger datasets. For example, the creators of the LAION-5B dataset chose to retain only image-caption pairs whose CLIP similarity score exceeded a designated threshold. In this paper, we propose a new state-of-the-art data filtering approach motivated by our observation that nearly $40\\%$ of LAION's images contain text that overlaps significantly with the caption. Intuitively, such data could be wasteful as it incentivizes models to perform optical character recognition rather than learning visual features. However, naively removing all such data could also be wasteful, as it throws away images that contain visual features (in addition to overlapping text). Our simple and scalable approach, T-MARS (Text Masking and Re-Scoring), filters out only those pairs where the text dominates the remaining visual features---by first masking out the text and then filtering out those with a low CLIP similarity score of the masked image with original captions. Experimentally, T-MARS is the top ranked approach on Imagenet at ``medium scale'' of DataComp (a data filtering benchmark), and outperforms CLIP filtering by a margin of $6.5\\%$ on ImageNet and $4.7\\%$ on VTAB. Additionally, we show that the accuracy gains enjoyed by T-MARS linearly increase as data and compute are scaled exponentially.",
      "authors": [
        "Pratyush Maini",
        "Sachin Goyal",
        "Zachary Chase Lipton",
        "J Zico Kolter",
        "Aditi Raghunathan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ViPtjIVzUw",
      "cdate": 1695409699578,
      "mdate": 1710532721093,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.710370"
    },
    {
      "id": "fUtxNAKpdV",
      "title": "Nougat: Neural Optical Understanding for Academic Documents",
      "abstract": "Scientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (Neural Optical Understanding for Academic Documents), a Visual Transformer model that performs an Optical Character Recognition (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human- readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition.",
      "authors": [
        "Lukas Blecher",
        "Guillem Cucurull",
        "Thomas Scialom",
        "Robert Stojnic"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fUtxNAKpdV",
      "cdate": 1695409663279,
      "mdate": 1710423201403,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710374"
    },
    {
      "id": "STUGfUz8ob",
      "title": "When can transformers reason with abstract symbols?",
      "abstract": "We investigate the capabilities of transformer models on relational reasoning tasks. In these tasks, models are trained on a set of strings encoding abstract relations, and are then tested out-of-distribution on data that contains symbols that did not appear in the training dataset. We prove that for any relational reasoning task in a large family of tasks, transformers learn the abstract relations and generalize to the test set when trained by gradient descent on sufficiently large quantities of training data. This is in contrast to classical fully-connected networks, which we prove fail to learn to reason. Our results inspire modifications of the transformer architecture that add only two trainable parameters per head, and that we empirically demonstrate improve data efficiency for learning to reason.",
      "authors": [
        "Enric Boix-Adserà",
        "Omid Saremi",
        "Emmanuel Abbe",
        "Samy Bengio",
        "Etai Littwin",
        "Joshua M. Susskind"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=STUGfUz8ob",
      "cdate": 1695409376627,
      "mdate": 1713162481694,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710378"
    },
    {
      "id": "79FVDdfoSR",
      "title": "A Characterization Theorem for Equivariant Networks with Point-wise Activations",
      "abstract": "Equivariant neural networks have shown improved performance, expressiveness and sample complexity on symmetrical domains. \nBut for some specific symmetries, representations, and choice of coordinates, the most common point-wise activations, such as ReLU, are not equivariant, hence they cannot be employed in the design of equivariant neural networks. \nThe theorem we present in this paper describes all possibile combinations of representations, choice of coordinates and point-wise activations to obtain an equivariant layer, generalizing and strengthening existing characterizations.\nNotable cases of practical relevance are discussed as corollaries. Indeed, we prove that rotation-equivariant networks can only be invariant, as it happens for any network which is equivariant with respect to connected compact groups. Then, we discuss implications of our findings when applied to important instances of equivariant networks. First, we completely characterize permutation equivariant networks such as Invariant Graph Networks with point-wise nonlinearities and their geometric counterparts, highlighting a plethora of models whose expressive power and performance are still unknown. \nSecond, we show that feature spaces of disentangled steerable convolutional neural networks are trivial representations.",
      "authors": [
        "Marco Pacini",
        "Xiaowen Dong",
        "Bruno Lepri",
        "Gabriele Santin"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=79FVDdfoSR",
      "cdate": 1695409286692,
      "mdate": 1710344529723,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710386"
    },
    {
      "id": "IuXR1CCrSi",
      "title": "Talk like a Graph: Encoding Graphs for Large Language Models",
      "abstract": "Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem. In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by LLMs. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered. These novel results provide valuable insight on strategies for encoding graphs as text. Using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8% to 61.8%, depending on the task.",
      "authors": [
        "Bahare Fatemi",
        "Jonathan Halcrow",
        "Bryan Perozzi"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=IuXR1CCrSi",
      "cdate": 1695409018870,
      "mdate": 1709661527904,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710390"
    },
    {
      "id": "bxITGFPVWh",
      "title": "Sharpness-Aware Data Poisoning Attack",
      "abstract": "Recent research has highlighted the vulnerability of Deep Neural Networks (DNNs) against data poisoning attacks. These attacks aim to inject poisoning samples into the models' training dataset such that the trained models have inference failures. While previous studies have executed different types of attacks, one major challenge that greatly limits their effectiveness is the \nuncertainty of the re-training process after the injection of poisoning samples. It includes the uncertainty of training initialization, algorithm and model architecture. To address this challenge, we propose a new strategy called **Sharpness-Aware Data Poisoning Attack (SAPA)**. In particular, it leverages the concept of DNNs' loss landscape sharpness to optimize the poisoning effect on the (approximately) worst re-trained model. Extensive experiments demonstrate that SAPA offers a general and principled strategy that significantly enhances various types of poisoning attacks against various types of re-training uncertainty.",
      "authors": [
        "Pengfei He",
        "Han Xu",
        "Jie Ren",
        "Yingqian Cui",
        "Shenglai Zeng",
        "Hui Liu",
        "Charu C. Aggarwal",
        "Jiliang Tang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=bxITGFPVWh",
      "cdate": 1695408826220,
      "mdate": 1710520665071,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710395"
    },
    {
      "id": "3d0OmYTNui",
      "title": "Privately Aligning Language Models with Reinforcement Learning",
      "abstract": "Positioned between pre-training and user deployment, aligning large language models (LLMs) through reinforcement learning (RL) has emerged as a prevailing strategy for training instruction following-models such as ChatGPT. In this work, we initiate the study of privacy-preserving alignment of LLMs through Differential Privacy (DP) in conjunction with RL. Following the influential work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment via RL without human in the loop (e.g., positive review generation) and (ii) alignment via RL from human feedback (RLHF) (e.g., summarization in a human-preferred way). We give a new DP framework to achieve alignment via RL, and prove its correctness. Our experimental results validate the effectiveness of our approach, offering competitive utility while ensuring strong privacy protections.",
      "authors": [
        "Fan Wu",
        "Huseyin A Inan",
        "Arturs Backurs",
        "Varun Chandrasekaran",
        "Janardhan Kulkarni",
        "Robert Sim"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3d0OmYTNui",
      "cdate": 1695408647503,
      "mdate": 1709661527661,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710399"
    },
    {
      "id": "wHBfxhZu1u",
      "title": "YaRN: Efficient Context Window Extension of Large Language Models",
      "abstract": "Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length.",
      "authors": [
        "Bowen Peng",
        "Jeffrey Quesnelle",
        "Honglu Fan",
        "Enrico Shippole"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=wHBfxhZu1u",
      "cdate": 1695408644451,
      "mdate": 1709965154429,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710404"
    },
    {
      "id": "rR03qFesqk",
      "title": "Functional Interpolation for Relative Positions improves Long Context Transformers",
      "abstract": "Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5's RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.",
      "authors": [
        "Shanda Li",
        "Chong You",
        "Guru Guruganesh",
        "Joshua Ainslie",
        "Santiago Ontanon",
        "Manzil Zaheer",
        "Sumit Sanghai",
        "Yiming Yang",
        "Sanjiv Kumar",
        "Srinadh Bhojanapalli"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rR03qFesqk",
      "cdate": 1695408287819,
      "mdate": 1709661527531,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710408"
    },
    {
      "id": "Q53QLftNkA",
      "title": "Masked Autoencoders with Multi-Window Local-Global Attention Are Better Audio Learners",
      "abstract": "In this work, we propose a Multi-Window Masked Autoencoder (MW-MAE) fitted with a novel Multi-Window Multi-Head Attention (MW-MHA) module that facilitates the modelling of local-global interactions in every decoder transformer block through attention heads of several distinct local and global windows. Empirical results on ten downstream audio tasks show that MW-MAEs consistently outperform standard MAEs in overall performance and learn better general-purpose audio representations, along with demonstrating considerably better scaling characteristics. Investigating attention distances and entropies reveals that MW-MAE encoders learn heads with broader local and global attention. Analyzing attention head feature representations through Projection Weighted Canonical Correlation Analysis (PWCCA) shows that attention heads with the same window sizes across the decoder layers of the MW-MAE learn correlated feature representations which enables each block to independently capture local and global information, leading to a decoupled decoder feature hierarchy.",
      "authors": [
        "Sarthak Yadav",
        "Sergios Theodoridis",
        "Lars Kai Hansen",
        "Zheng-Hua Tan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Q53QLftNkA",
      "cdate": 1695407256673,
      "mdate": 1709661527355,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710415"
    },
    {
      "id": "J2TZgj3Tac",
      "title": "Toward Optimal Policy Population Growth in Two-Player Zero-Sum Games",
      "abstract": "In competitive two-agent environments, deep reinforcement learning (RL) methods like Policy Space Response Oracles (PSRO) often increase exploitability between iterations, which is problematic when training in large games. To address this issue, we introduce anytime double oracle (ADO), an algorithm that ensures exploitability does not increase between iterations, and its approximate extensive-form version, anytime PSRO (APSRO). ADO converges to a Nash equilibrium while iteratively reducing exploitability. However, convergence in these algorithms may require adding all of a game's deterministic policies. To improve this, we propose Self-Play PSRO (SP-PSRO), which incorporates an approximately optimal stochastic policy into the population in each iteration. APSRO and SP-PSRO demonstrate lower exploitability and near-monotonic exploitability reduction in games like Leduc poker and Liar's Dice. Empirically, SP-PSRO often converges much faster than APSRO and PSRO, requiring only a few iterations in many games.",
      "authors": [
        "Stephen Marcus McAleer",
        "JB Lanier",
        "Kevin A. Wang",
        "Pierre Baldi",
        "Tuomas Sandholm",
        "Roy Fox"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=J2TZgj3Tac",
      "cdate": 1695406974459,
      "mdate": 1712013866104,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710423"
    },
    {
      "id": "F76bwRSLeK",
      "title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models",
      "abstract": "One of the roadblocks to a better understanding of neural networks' internals is \\textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \\textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \\citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.",
      "authors": [
        "Robert Huben",
        "Hoagy Cunningham",
        "Logan Riggs Smith",
        "Aidan Ewart",
        "Lee Sharkey"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=F76bwRSLeK",
      "cdate": 1695406757715,
      "mdate": 1713316890050,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710428"
    },
    {
      "id": "w7LU2s14kE",
      "title": "Linearity of Relation Decoding in Transformer Language Models",
      "abstract": "Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in transformer LMs.",
      "authors": [
        "Evan Hernandez",
        "Arnab Sen Sharma",
        "Tal Haklay",
        "Kevin Meng",
        "Martin Wattenberg",
        "Jacob Andreas",
        "Yonatan Belinkov",
        "David Bau"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=w7LU2s14kE",
      "cdate": 1695406731831,
      "mdate": 1709661527195,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710432"
    },
    {
      "id": "tUtGjQEDd4",
      "title": "Generative Modeling with Phase Stochastic Bridge",
      "abstract": "Diffusion models (DMs) represent state-of-the-art generative models for continuous inputs. DMs work by constructing a Stochastic Differential Equation (SDE) in the input space (ie, position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling framework grounded in \\textbf{phase space dynamics}, where a phase space is defined as {an augmented space encompassing both position and velocity.} Leveraging insights from Stochastic Optimal Control, we construct a path measure in the phase space that enables efficient sampling. {In contrast to DMs, our framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation.} This early prediction sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. On standard image generation benchmarks, our model yields favorable performance over baselines in the regime of small Number of Function Evaluations (NFEs). Furthermore, our approach rivals the performance of diffusion models equipped with efficient sampling techniques, underscoring its potential as a new tool generative modeling.",
      "authors": [
        "Tianrong Chen",
        "Jiatao Gu",
        "Laurent Dinh",
        "Evangelos Theodorou",
        "Joshua M. Susskind",
        "Shuangfei Zhai"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tUtGjQEDd4",
      "cdate": 1695406134192,
      "mdate": 1713231899997,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710437"
    },
    {
      "id": "BxHgpC6FNv",
      "title": "Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data",
      "abstract": "Neural networks trained by gradient descent (GD) have exhibited a number of surprising generalization behaviors. First, they can achieve a perfect fit to noisy training data and still generalize near-optimally, showing that overfitting can sometimes be benign. Second, they can undergo a period of classical, harmful overfitting---achieving a perfect fit to training data with near-random performance on test data---before transitioning (''grokking'') to near-optimal generalization later in training. In this work, we show that both of these phenomena provably occur in two-layer ReLU networks trained by GD on XOR cluster data where a constant fraction of the training labels are flipped. In this setting, we show that after the first step of GD, the network achieves 100\\% training accuracy, perfectly fitting the noisy labels in the training data, but achieves near-random test accuracy. At a later training step, the network achieves near-optimal test accuracy while still fitting the random labels in the training data, exhibiting a ''grokking'' phenomenon. This provides the first theoretical result of benign overfitting in neural network classification when the data distribution is not linearly separable. Our proofs rely on analyzing the feature learning process under GD, which reveals that the network implements a non-generalizable linear classifier after one step and gradually learns generalizable features in later steps.",
      "authors": [
        "Zhiwei Xu",
        "Yutong Wang",
        "Spencer Frei",
        "Gal Vardi",
        "Wei Hu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BxHgpC6FNv",
      "cdate": 1695405872060,
      "mdate": 1710561447107,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710441"
    },
    {
      "id": "EArTDUmILF",
      "title": "VBH-GNN: Variational Bayesian Heterogeneous Graph Neural Networks for Cross-subject Emotion Recognition",
      "abstract": "The research on human emotion under electroencephalogram (EEG) is an emerging field in which cross-subject emotion recognition (ER) is a promising but challenging task. Many approaches attempt to find emotionally relevant domain-invariant features using domain adaptation (DA) to improve the accuracy of cross-subject ER. However, two problems still exist with these methods. First, only single-modal data (EEG) is utilized, ignoring the complementarity between multi-modal physiological signals. Second, these methods aim to completely match the signal features between different domains, which is difficult due to the extreme individual differences of EEG. To solve these problems, we introduce the complementarity of multi-modal physiological signals and propose a new method for cross-subject ER that does not align the distribution of signal features but rather the distribution of spatio-temporal relationships between features. We design a Variational Bayesian Heterogeneous Graph Neural Network (VBH-GNN) with Relationship Distribution Adaptation (RDA). The RDA first aligns the domains by expressing the model space as a posterior distribution of a heterogeneous graph for a given source domain. Then, the RDA transforms the heterogeneous graph into an emotion-specific graph to further align the domains for the downstream ER task. Extensive experiments on two public datasets, DEAP and Dreamer, show that our VBH-GNN outperforms state-of-the-art methods in cross-subject scenarios.",
      "authors": [
        "Chenyu Liu",
        "XINLIANG ZHOU",
        "Zhengri Zhu",
        "Liming Zhai",
        "Ziyu Jia",
        "Yang Liu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EArTDUmILF",
      "cdate": 1695405327812,
      "mdate": 1710319255552,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710445"
    },
    {
      "id": "4WM0OogPTx",
      "title": "Learning from Sparse Offline Datasets via Conservative Density Estimation",
      "abstract": "Offline reinforcement learning (RL) offers a promising direction for learning policies from pre-collected datasets without requiring further interactions with the environment. However, existing methods struggle to handle out-of-distribution (OOD) extrapolation errors, especially in sparse reward or scarce data settings. In this paper, we propose a novel training algorithm called Conservative Density Estimation (CDE), which addresses this challenge by explicitly imposing constraints on the state-action occupancy stationary distribution. CDE overcomes the limitations of existing approaches, such as the stationary distribution correction method, by addressing the support mismatch issue in marginal importance sampling. Our method achieves state-of-the-art performance on the D4RL benchmark. Notably, CDE consistently outperforms baselines in challenging tasks with sparse rewards or insufficient data, demonstrating the advantages of our approach in addressing the extrapolation error problem in offline RL.",
      "authors": [
        "Zhepeng Cen",
        "Zuxin Liu",
        "Zitong Wang",
        "Yihang Yao",
        "Henry Lam",
        "Ding Zhao"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4WM0OogPTx",
      "cdate": 1695404971877,
      "mdate": 1710169425239,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710450"
    },
    {
      "id": "qr4ECbGcSj",
      "title": "On the Expressivity of Objective-Specification Formalisms in Reinforcement Learning",
      "abstract": "Most algorithms in reinforcement learning (RL) require that the objective is formalised with a Markovian reward function. However, it is well-known that certain tasks cannot be expressed by means of an objective in the Markov rewards formalism, motivating the study of alternative objective-specification formalisms in RL such as Linear Temporal Logic and Multi-Objective Reinforcement Learning. To date, there has not yet been any thorough analysis of how these formalisms relate to each other in terms of their expressivity. We fill this gap in the existing literature by providing a comprehensive comparison of 17 salient objective-specification formalisms. We place these formalisms in a preorder based on their expressive power, and present this preorder as a Hasse diagram. We find a variety of limitations for the different formalisms, and argue that no formalism is both dominantly expressive and straightforward to optimise with current techniques. For example, we prove that each of Regularised RL, (Outer) Nonlinear Markov Rewards, Reward Machines, Linear Temporal Logic, and Limit Average Rewards can express a task that the others cannot. The significance of our results is twofold. First, we identify important expressivity limitations to consider when specifying objectives for policy optimization. Second, our results highlight the need for future research which adapts reward learning to work with a greater variety of formalisms, since many existing reward learning methods assume that the desired objective takes a Markovian form. Our work contributes towards a more cohesive understanding of the costs and benefits of different RL objective-specification formalisms.",
      "authors": [
        "Rohan Subramani",
        "Marcus Williams",
        "Max Heitmann",
        "Halfdan Holm",
        "Charlie Griffin",
        "Joar Max Viktor Skalse"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qr4ECbGcSj",
      "cdate": 1695404770017,
      "mdate": 1709661526934,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710455"
    },
    {
      "id": "auUngos7eR",
      "title": "Implicit Maximum a Posteriori Filtering via Adaptive Optimization",
      "abstract": "Bayesian filtering approximates the true underlying behavior of a time-varying system by inverting an explicit generative model to convert noisy measurements into state estimates. This process typically requires matrix storage, inversion, and multiplication or Monte Carlo estimation, none of which are practical in high-dimensional state spaces such as the weight spaces of artificial neural networks. Here, we consider the standard Bayesian filtering problem as optimization over a time-varying objective. Instead of maintaining matrices for the filtering equations or simulating particles, we specify an optimizer that defines the Bayesian filter implicitly. In the linear-Gaussian setting, we show that every Kalman filter has an equivalent formulation using K steps of gradient descent. In the nonlinear setting, our experiments demonstrate that our framework results in filters that are effective, robust, and scalable to high-dimensional systems, comparing well against the standard toolbox of Bayesian filtering solutions. We suggest that it is easier to fine-tune an optimizer than it is to specify the correct filtering equations, making our framework an attractive option for high-dimensional filtering problems.",
      "authors": [
        "Gianluca Bencomo",
        "Jake Snell",
        "Thomas L. Griffiths"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=auUngos7eR",
      "cdate": 1695404759762,
      "mdate": 1710352591376,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710459"
    },
    {
      "id": "rAHcTCMaLc",
      "title": "S$2$AC: Energy-Based Reinforcement Learning with Stein Soft Actor Critic",
      "abstract": "Learning expressive stochastic policies instead of deterministic ones has been proposed to achieve better stability, sample complexity and robustness. Notably, in Maximum Entropy reinforcement learning (MaxEnt RL), the policy is modeled as an expressive energy-based model (EBM) over the Q-values. However, this formulation requires the estimation of the entropy of such EBM distributions which is an open problem. To address this, previous MaxEnt RL methods either implicitly estimate the entropy, yielding high computational complexity and variance (SQL), or follow a variational inference approach that fits simplified distributions (e.g., Gaussian) for tractability (SAC). We propose Sein Soft Actor-Critic (S$^2$AC), a MaxEnt RL algorithm that learns expressive policies without compromising efficiency. S$^2$AC uses parameterized Stein Variational Gradient Descent (SVGD) as the underlying policy. At the core of S$^2$AC is a new solution to the above open challenge of entropy computation for EBMs. Our entropy formula is computationally efficient and only depends on first-order derivatives and vector products. Empirical results show that S$^2$AC yields more optimal solutions to the MaxEnt objective than SQL and SAC in the multi-goal environment, and outperforms SAC and SQL on the MuJoCo benchmark. Our code is available at: https://anonymous.4open.science/r/Stein-Soft-Actor-Critic/",
      "authors": [
        "Safa Messaoud",
        "Billel Mokeddem",
        "Zhenghai Xue",
        "Linsey Pang",
        "Bo An",
        "Haipeng Chen",
        "Sanjay Chawla"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rAHcTCMaLc",
      "cdate": 1695404540536,
      "mdate": 1710168624844,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710468"
    },
    {
      "id": "xhEN0kJh4q",
      "title": "Robust Model-Based Optimization for Challenging Fitness Landscapes",
      "abstract": "Protein design, a grand challenge of the day, involves optimization on a fitness landscape, and leading methods adopt a model-based approach where a model is trained on a training set (protein sequences and fitness) and proposes candidates to explore next. These methods are challenged by sparsity of high-fitness samples in the training set, a problem that has been in the literature. A less recognized but equally important problem stems from the distribution of training samples in the design space: leading methods are not designed for scenarios where the desired optimum is in a region that is not only poorly represented in training data, but also relatively far from the highly represented low-fitness regions. We show that this problem of “separation” in the design space is a significant bottleneck in existing model-based optimization tools and propose a new approach that uses a novel VAE as its search model to overcome the problem. We demonstrate its advantage over prior methods in robustly finding improved samples, regardless of the imbalance and separation between low- and high-fitness samples. Our comprehensive benchmark on real and semi-synthetic protein datasets as well as solution design for physics-informed neural networks, showcases the generality of our approach in discrete and continuous design spaces. Our implementation is available at https://github.com/sabagh1994/PGVAE.",
      "authors": [
        "Saba Ghaffari",
        "Ehsan Saleh",
        "Alex Schwing",
        "Yu-Xiong Wang",
        "Martin D. Burke",
        "Saurabh Sinha"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xhEN0kJh4q",
      "cdate": 1695404360338,
      "mdate": 1710454664290,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710473"
    },
    {
      "id": "q4AEBLHuA6",
      "title": "Solving High Frequency and Multi-Scale PDEs with Gaussian Processes",
      "abstract": "Machine learning based solvers have garnered much attention in physical simulation and scientific computing, with a prominent example, physics-informed neural networks (PINNs). However, PINNs often struggle to solve high-frequency and multi-scale PDEs, which can be due to spectral bias during neural network training. To address this problem, we resort to the Gaussian process (GP) framework. To flexibly capture the dominant frequencies, we model the power spectrum of the PDE solution with a student $t$ mixture or Gaussian mixture. We apply the inverse Fourier transform to obtain the covariance function (by  Wiener-Khinchin theorem). The covariance derived from the Gaussian mixture spectrum corresponds to the known spectral mixture kernel. Next,  we estimate the mixture weights in the log domain, which we show is equivalent to placing a Jeffreys prior. It automatically induces sparsity, prunes excessive frequencies, and adjusts the remaining toward the ground truth. Third, to enable efficient and scalable computation on massive collocation points, which are critical to capture high frequencies, we place the collocation points on a grid, and multiply our covariance function at each input dimension. We use the GP conditional mean to predict the solution and its derivatives so as to fit the boundary condition and the equation itself. \nAs a result, we can derive a Kronecker product structure in the covariance matrix. We use Kronecker product properties and multilinear algebra to promote computational efficiency and scalability, without low-rank approximations. We show the advantage of our method in systematic experiments. The code is released at {https://github.com/xuangu-fang/Gaussian-Process-Slover-for-High-Freq-PDE}.",
      "authors": [
        "Shikai Fang",
        "Madison Cooley",
        "Da Long",
        "Shibo Li",
        "Mike Kirby",
        "Shandian Zhe"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=q4AEBLHuA6",
      "cdate": 1695404249748,
      "mdate": 1710303568396,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710478"
    },
    {
      "id": "jKHmjlpViu",
      "title": "OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text",
      "abstract": "There is growing evidence that pretraining on high quality, carefully thought-out tokens such as code or mathematics plays an important role in improving the reasoning abilities of large language models. For example, Minerva, a PaLM model finetuned on billions of tokens of mathematical documents from arXiv and the web, reported dramatically improved performance on problems that require quantitative reasoning. However, because all known open source web datasets employ preprocessing that does not faithfully preserve mathematical notation, the benefits of large scale training on quantitive web documents are unavailable to the research community. We introduce OpenWebMath, an open dataset inspired by these works containing 14.7B tokens of mathematical webpages from Common Crawl. We describe in detail our method for extracting text and LaTeX content and removing boilerplate from HTML documents, as well as our methods for quality filtering and deduplication. Additionally, we run small-scale experiments by training 1.4B language models on OpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass the performance of models trained on over 20x the amount of general language data. We hope that our dataset, open-sourced and released on the Hugging Face Hub, will help spur advances in the reasoning abilities of large language models.",
      "authors": [
        "Keiran Paster",
        "Marco Dos Santos",
        "Zhangir Azerbayev",
        "Jimmy Ba"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jKHmjlpViu",
      "cdate": 1695404044261,
      "mdate": 1710555095722,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710482"
    },
    {
      "id": "Nf4Lm6fXN8",
      "title": "Replay across Experiments: A Natural Extension of Off-Policy RL",
      "abstract": "Replaying data is a principal mechanism underlying the stability and data efficiency of off-policy reinforcement learning (RL).\nWe present an effective yet simple framework to extend the use of replays across multiple experiments, minimally adapting the RL workflow for sizeable improvements in controller performance and research iteration times.\nAt its core, Replay across Experiments (RaE) involves reusing experience from previous experiments to improve exploration and bootstrap learning while reducing required changes to a minimum in comparison to prior work. \nWe empirically show benefits across a number of RL algorithms and challenging control domains spanning both locomotion and manipulation, including hard exploration tasks from egocentric vision. \nThrough comprehensive ablations, we demonstrate  robustness to the quality and amount of data available and various hyperparameter choices. Finally, we discuss how our approach can be applied more broadly across research life cycles and can increase resilience by reloading data across random seeds or hyperparameter variations.",
      "authors": [
        "Dhruva Tirumala",
        "Thomas Lampe",
        "Jose Enrique Chen",
        "Tuomas Haarnoja",
        "Sandy Huang",
        "Guy Lever",
        "Ben Moran",
        "Tim Hertweck",
        "Leonard Hasenclever",
        "Martin Riedmiller",
        "Nicolas Heess",
        "Markus Wulfmeier"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Nf4Lm6fXN8",
      "cdate": 1695403918647,
      "mdate": 1709661526573,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710487"
    },
    {
      "id": "hj9ZuNimRl",
      "title": "Better Neural PDE Solvers Through Data-Free Mesh Movers",
      "abstract": "Recently, neural networks have been extensively employed to solve partial differential equations (PDEs) in physical system modeling. While major studies focus on learning system evolution on predefined static mesh discretizations, some methods utilize reinforcement learning or supervised learning techniques to create adaptive and dynamic meshes, due to the dynamic nature of these systems. However, these approaches face two primary challenges: (1) the need for expensive optimal mesh data, and (2) the change of the solution space's degree of freedom and topology during mesh refinement. To address these challenges, this paper proposes a neural PDE solver with a neural mesh adapter. To begin with, we introduce a novel data-free neural mesh adaptor, called Data-free Mesh Mover (DMM), with two main innovations. Firstly, it is an operator that maps the solution to adaptive meshes and is trained using the Monge-Ampère equation without optimal mesh data. Secondly, it dynamically changes the mesh by moving existing nodes rather than adding or deleting nodes and edges. Theoretical analysis shows that meshes generated by DMM have the lowest interpolation error bound. Based on DMM, to efficiently and accurately model dynamic systems, we develop a moving mesh based neural PDE solver (MM-PDE) that embeds the moving mesh with a two-branch architecture and a learnable interpolation framework to preserve information within the data. Empirical experiments demonstrate that our method generates suitable meshes and considerably enhances accuracy when modeling widely considered PDE systems. The code can be found at: https://github.com/Peiyannn/MM-PDE.git.",
      "authors": [
        "Peiyan Hu",
        "Yue Wang",
        "Zhi-Ming Ma"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hj9ZuNimRl",
      "cdate": 1695403339915,
      "mdate": 1709661526430,
      "matched_keywords": [
        "reinforcement learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710492"
    },
    {
      "id": "0JsRZEGZ7L",
      "title": "From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module",
      "abstract": "Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks (GNNs) on a given graph topology by dynamically learning it. However, most of LGI methods assume to have a (noisy, incomplete, improvable, ...) input graph to rewire and can solely learn regular graph topologies. In the wake of the success of  Topological Deep Learning (TDL), we study Latent Topology Inference (LTI) for learning higher-order cell complexes (with sparse and not regular topology) describing multi-way interactions between data points. To this aim, we introduce the Differentiable Cell Complex Module (DCM), a novel learnable function that computes cell probabilities in the complex to improve the downstream task. We show how to integrate DCM with cell complex message-passing networks layers and train it in an end-to-end fashion, thanks to a two-step inference procedure that avoids an exhaustive search across all possible cells in the input, thus maintaining scalability. Our model is tested on several homophilic and heterophilic graph datasets and it is shown to outperform other state-of-the-art techniques, offering significant improvements especially in cases where an input graph is not provided.",
      "authors": [
        "Claudio Battiloro",
        "Indro Spinelli",
        "Lev Telyatnikov",
        "Michael M. Bronstein",
        "Simone Scardapane",
        "Paolo Di Lorenzo"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0JsRZEGZ7L",
      "cdate": 1695403013585,
      "mdate": 1710155548749,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710496"
    },
    {
      "id": "QxItoEAVMb",
      "title": "TorchRL: A data-driven decision-making library for PyTorch",
      "abstract": "PyTorch has ascended as a premier machine learning framework, yet it lacks a native and comprehensive library for decision and control tasks suitable for large development teams dealing with complex real-world data and environments. To address this issue, we propose TorchRL, a generalistic control library for PyTorch that provides well-integrated, yet standalone components. We introduce a new and flexible PyTorch primitive, the TensorDict, which facilitates streamlined algorithm development across the many branches of Reinforcement Learning (RL) and control. We provide a detailed description of the building blocks and an extensive overview of the library across domains and tasks. Finally, we experimentally demonstrate its reliability and flexibility, and show comparative benchmarks to demonstrate its computational efficiency. TorchRL fosters long-term support and is publicly available on GitHub for greater reproducibility and collaboration within the research community. The code is open-sourced on GitHub.",
      "authors": [
        "Albert Bou",
        "Matteo Bettini",
        "Sebastian Dittert",
        "Vikash Kumar",
        "Shagun Sodhani",
        "Xiaomeng Yang",
        "Gianni De Fabritiis",
        "Vincent Moens"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=QxItoEAVMb",
      "cdate": 1695402393325,
      "mdate": 1710428667998,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710501"
    },
    {
      "id": "5hAMmCU0bK",
      "title": "Towards Robust Offline Reinforcement Learning under Diverse Data Corruption",
      "abstract": "Offline reinforcement learning (RL) presents a promising approach for learning reinforced policies from offline datasets without the need for costly or unsafe interactions with the environment. However, datasets collected by humans in real-world environments are often noisy and may even be maliciously corrupted, which can significantly degrade the performance of offline RL. In this work, we first investigate the performance of current offline RL algorithms under comprehensive data corruption, including states, actions, rewards, and dynamics. Our extensive experiments reveal that implicit Q-learning (IQL) demonstrates remarkable resilience to data corruption among various offline RL algorithms. Furthermore, we conduct both empirical and theoretical analyses to understand IQL's robust performance, identifying its supervised policy learning scheme as the key factor. Despite its relative robustness, IQL still suffers from heavy-tail targets of Q functions under dynamics corruption. To tackle this challenge, we draw inspiration from robust statistics to employ the Huber loss to handle the heavy-tailedness and utilize quantile estimators to balance penalization for corrupted data and learning stability. By incorporating these simple yet effective modifications into IQL, we propose a more robust offline RL approach named Robust IQL (RIQL). Extensive experiments demonstrate that RIQL exhibits highly robust performance when subjected to diverse data corruption scenarios.",
      "authors": [
        "Rui Yang",
        "Han Zhong",
        "Jiawei Xu",
        "Amy Zhang",
        "Chongjie Zhang",
        "Lei Han",
        "Tong Zhang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5hAMmCU0bK",
      "cdate": 1695401500382,
      "mdate": 1710001799627,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710505"
    },
    {
      "id": "up6hr4hIQH",
      "title": "Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) are neural models that leverage the dependency structure in graphical data via message passing among the graph nodes. GNNs have emerged as pivotal architectures in analyzing graph-structured data, and their expansive application in sensitive domains requires a comprehensive understanding of their decision-making processes --- necessitating a framework for GNN explainability. An explanation function for GNNs takes a pre-trained GNN along with a graph as input, to produce a `sufficient statistic' subgraph with respect to the graph label. A main challenge in studying GNN explainability is to provide fidelity measures that evaluate the performance of these explanation functions. This paper studies this foundational challenge, spotlighting the inherent limitations of prevailing fidelity metrics, including $Fid_+$, $Fid_-$, and $Fid_\\Delta$. Specifically, a formal, information-theoretic definition of explainability is introduced and it is shown that existing metrics often fail to align with this definition across various statistical scenarios. The reason is due to potential distribution shifts when subgraphs are removed in computing these fidelity measures. Subsequently, a robust class of fidelity measures are introduced, and it is shown analytically that they are resilient to distribution shift issues and are applicable in a wide range of scenarios. Extensive empirical analysis on both synthetic and real datasets are provided to illustrate that the proposed metrics are more coherent with gold standard metrics.",
      "authors": [
        "Xu Zheng",
        "Farhad Shirani",
        "Tianchun Wang",
        "Wei Cheng",
        "Zhuomin Chen",
        "Haifeng Chen",
        "Hua Wei",
        "Dongsheng Luo"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=up6hr4hIQH",
      "cdate": 1695401123015,
      "mdate": 1710431550840,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710510"
    },
    {
      "id": "Sx7BIiPzys",
      "title": "Variational Bayesian Last Layers",
      "abstract": "We introduce a deterministic variational formulation for training Bayesian last layer neural networks. This yields a sampling-free, single-pass model and loss that effectively improves uncertainty estimation. Our variational Bayesian last layer (VBLL) can be trained and evaluated with only quadratic complexity in last layer width, and is thus (nearly) computationally free to add to standard architectures. We experimentally investigate VBLLs, and show that they improve predictive accuracy, calibration, and out of distribution detection over baselines across both regression and classification. Finally, we investigate combining VBLL layers with variational Bayesian feature learning, yielding a lower variance collapsed variational inference method for Bayesian neural networks.",
      "authors": [
        "James Harrison",
        "John Willes",
        "Jasper Snoek"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Sx7BIiPzys",
      "cdate": 1695401031804,
      "mdate": 1713123493766,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710522"
    },
    {
      "id": "hOMVq57Ce0",
      "title": "Piecewise Linear Parametrization of Policies: Towards Interpretable Deep Reinforcement Learning",
      "abstract": "Learning inherently interpretable policies is a central challenge in the path to developing autonomous agents that humans can trust. Linear policies can justify their decisions while interacting in a dynamic environment, but their reduced expressivity prevents them from solving hard tasks. Instead, we argue for the use of piecewise-linear policies. We carefully study to what extent they can retain the interpretable properties of linear policies while reaching competitive performance with neural baselines. In particular, we propose the HyperCombinator (HC), a piecewise-linear neural architecture expressing a policy with a controllably small number of sub-policies. Each sub-policy is linear with respect to interpretable features, shedding light on the decision process of the agent without requiring an additional explanation model. We evaluate HC policies in control and navigation experiments, visualize the improved interpretability of the agent and highlight its trade-off with performance. Moreover, we validate that the restricted model class that the HyperCombinator belongs to is compatible with the algorithmic constraints of various reinforcement learning algorithms.",
      "authors": [
        "Maxime Wabartha",
        "Joelle Pineau"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hOMVq57Ce0",
      "cdate": 1695400935304,
      "mdate": 1710892780691,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710526"
    },
    {
      "id": "Jc0FssXh2R",
      "title": "Optimal criterion for feature learning of two-layer linear neural network in high dimensional interpolation regime",
      "abstract": "Deep neural networks with feature learning have shown surprising generalization performance in high dimensional settings, but it has not been fully understood how and when they enjoy the benefit of feature learning. In this paper, we theoretically analyze the statistical properties of the benefits from feature learning in a two-layer linear neural network with multiple outputs in a high-dimensional setting. For that purpose, we propose a new criterion that allows feature learning of a two-layer linear neural network in a high-dimensional setting. Interestingly, we can show that models with smaller values of the criterion generalize even in situations where normal ridge regression fails to generalize. This is because the proposed criterion contains a proper regularization for the feature mapping and acts as an upper bound on the predictive risk. As an important characterization of the criterion, the two-layer linear neural network that minimizes this criterion can achieve the optimal Bayes risk that is determined by the distribution of the true signals across the multiple outputs. To the best of our knowledge, this is the first study to specifically identify the conditions under which a model obtained by proper feature learning can outperform normal ridge regression in a high-dimensional multiple-output linear regression problem.",
      "authors": [
        "Keita Suzuki",
        "Taiji Suzuki"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Jc0FssXh2R",
      "cdate": 1695400866059,
      "mdate": 1711462500938,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710531"
    },
    {
      "id": "gIiz7tBtYZ",
      "title": "Neural Optimal Transport with General Cost Functionals",
      "abstract": "We introduce a novel neural network-based algorithm to compute optimal transport (OT) plans for general cost functionals. In contrast to common Euclidean costs, i.e., $\\ell^1$ or $\\ell^2$, such functionals provide more flexibility and allow using auxiliary information, such as class labels, to construct the required transport map. Existing methods for general cost functionals are discrete and do not provide an out-of-sample estimation. We address the challenge of designing a continuous OT approach for general cost functionals in high-dimensional spaces, such as images. We construct two example functionals: one to map distributions while preserving the class-wise structure and the other one to preserve the given data pairs. Additionally, we provide the theoretical error analysis for our recovered transport plans. Our implementation is available at \\url{https://github.com/machinestein/gnot}",
      "authors": [
        "Arip Asadulaev",
        "Alexander Korotin",
        "Vage Egiazarian",
        "Petr Mokrov",
        "Evgeny Burnaev"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=gIiz7tBtYZ",
      "cdate": 1695400854229,
      "mdate": 1713607273625,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710535"
    },
    {
      "id": "7gUrYE50Rb",
      "title": "EQA-MX: Embodied Question Answering using Multimodal Expression",
      "abstract": "Humans predominantly use verbal utterances and nonverbal gestures (e.g., eye gaze and pointing gestures) in their natural interactions. For instance, pointing gestures and verbal information is often required to comprehend questions such as \"what object is that?\" Thus, this question-answering (QA) task involves complex reasoning of multimodal expressions (verbal utterances and nonverbal gestures). However, prior works have explored QA tasks in non-embodied settings, where questions solely contain verbal utterances from a single verbal and visual perspective. In this paper, we have introduced 8 novel embodied question answering (EQA) tasks to develop learning models to comprehend embodied questions with multimodal expressions. We have developed a novel large-scale dataset, EQA-MX, with over 8 million diverse embodied QA data samples involving multimodal expressions from multiple visual and verbal perspectives. To learn salient multimodal representations from discrete verbal embeddings and continuous wrapping of multiview visual representations, we propose a vector-quantization (VQ) based multimodal representation learning model, VQ-Fusion, for the EQA tasks. Our extensive experimental results suggest that VQ-Fusion can improve the performance of existing state-of-the-art visual-language models up to 13% across EQA tasks.",
      "authors": [
        "Md Mofijul Islam",
        "Alexi Gladstone",
        "Riashat Islam",
        "Tariq Iqbal"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7gUrYE50Rb",
      "cdate": 1695400834842,
      "mdate": 1710479196355,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.710540"
    },
    {
      "id": "apA6SSXx2e",
      "title": "A Topological Perspective on Demystifying GNN-Based Link Prediction Performance",
      "abstract": "Graph Neural Networks (GNNs) have shown great promise in learning node embeddings for link prediction (LP). While numerous studies improve the overall GNNs' LP performance, none have explored their varying performance across different nodes and the underlying reasons. To this end, we demystify which nodes perform better from the perspective of their local topology. Despite the widespread belief that low-degree nodes exhibit worse LP performance, we surprisingly observe an inconsistent performance trend. This prompts us to propose a node-level metric, Topological Concentration (TC), based on the intersection of the local subgraph of each node with the ones of its neighbors. We empirically demonstrate that TC correlates with LP performance more than other node-level topological metrics, better identifying low-performing nodes than using degree. With TC, we discover a novel topological distribution shift issue in which nodes' newly joined neighbors tend to become less interactive with their existing neighbors, compromising the generalizability of node embeddings for LP at testing time. To make the computation of TC scalable, We further propose Approximated Topological Concentration (ATC) and justify its efficacy in approximating TC with reduced computation complexity. Given the positive correlation between node TC and its LP performance, we explore the potential of boosting LP performance via enhancing TC by re-weighting edges in the message-passing and discuss its effectiveness with limitations. Our code is publicly available at https://github.com/YuWVandy/Topo_LP_GNN.",
      "authors": [
        "Yu Wang",
        "Tong Zhao",
        "Yuying Zhao",
        "Yunchao Liu",
        "Xueqi Cheng",
        "Neil Shah",
        "Tyler Derr"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=apA6SSXx2e",
      "cdate": 1695400600112,
      "mdate": 1710555392741,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710544"
    },
    {
      "id": "5liV2xUdJL",
      "title": "Time-Efficient Reinforcement Learning with Stochastic Stateful Policies",
      "abstract": "Stateful policies play an important role in reinforcement learning, such as handling partially observable environments, enhancing robustness, or imposing an inductive bias directly into the policy structure. The conventional method for training stateful policies is Backpropagation Through Time (BPTT), which comes with significant drawbacks, such as slow training due to sequential gradient propagation and the occurrence of vanishing or exploding gradients. The gradient is often truncated to address these issues, resulting in a biased policy update. We present a novel approach for training stateful policies by decomposing the latter into a stochastic internal state kernel and a stateless policy, jointly optimized by following the stateful policy gradient. We introduce different versions of the stateful policy gradient theorem, enabling us to easily instantiate stateful variants of popular reinforcement learning and imitation learning algorithms. Furthermore, we provide a theoretical analysis of our new gradient estimator and compare it with BPTT. We evaluate our approach on complex continuous control tasks, e.g. humanoid locomotion, and demonstrate that our gradient estimator scales effectively with task complexity while offering a faster and simpler alternative to BPTT.",
      "authors": [
        "Firas Al-Hafez",
        "Guoping Zhao",
        "Jan Peters",
        "Davide Tateo"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5liV2xUdJL",
      "cdate": 1695400549999,
      "mdate": 1713179290179,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710549"
    },
    {
      "id": "mnipav175N",
      "title": "Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning",
      "abstract": "Current advancements in reinforcement learning (RL) have predominantly focused on learning step-based policies that generate actions for each perceived state. While these methods efficiently leverage step information from environmental interaction, they often ignore the temporal correlation between actions, resulting in inefficient exploration and unsmooth trajectories that are challenging to implement on real hardware. Episodic RL (ERL) seeks to overcome these challenges by exploring in parameters space that capture the correlation of actions. However, these approaches typically compromise data efficiency, as they treat trajectories as opaque black boxes. In this work, we introduce a novel ERL algorithm, Temporally-Correlated Episodic RL (TCE), which effectively utilizes step information in episodic policy updates, opening the 'black box' in existing ERL methods while retaining the smooth and consistent exploration in parameter space. TCE synergistically combines the advantages of step-based and episodic RL, achieving comparable performance to recent ERL methods while maintaining data efficiency akin to state-of-the-art (SoTA) step-based RL.",
      "authors": [
        "Ge Li",
        "Hongyi Zhou",
        "Dominik Roth",
        "Serge Thilges",
        "Fabian Otto",
        "Rudolf Lioutikov",
        "Gerhard Neumann"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=mnipav175N",
      "cdate": 1695400201732,
      "mdate": 1709661525715,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710553"
    },
    {
      "id": "lGUyAuuTYZ",
      "title": "Can we get the best of both Binary Neural Networks and Spiking Neural Networks for Efficient Computer Vision?",
      "abstract": "Binary Neural networks (BNN) have emerged as an attractive computing paradigm for a wide range of low-power vision tasks. However, state-of-the-art (SOTA) BNNs do not yield any sparsity, and induce a significant number of non-binary operations. On the other hand, activation sparsity can be provided by spiking neural networks (SNN), that too have gained significant traction in recent times. Thanks to this sparsity, SNNs when implemented on neuromorphic hardware, have the potential to be significantly more power-efficient compared to traditional artifical neural networks (ANN). However, SNNs incur multiple time steps to achieve close to SOTA accuracy. Ironically, this increases latency and energy---costs that SNNs were proposed to reduce---and presents itself as a major hurdle in realizing SNNs’ theoretical gains in practice. This raises an intriguing question: *Can we obtain SNN-like sparsity and BNN-like accuracy and enjoy the energy-efficiency benefits of both?* To answer this question, in this paper, we present a training framework for sparse binary activation neural networks (BANN) using a novel variant of the Hoyer regularizer. We estimate the threshold of each BANN layer as the Hoyer extremum of a clipped version of its activation map, where the clipping value is trained using gradient descent with our Hoyer regularizer. \nThis approach shifts the activation values away from the threshold, thereby mitigating the effect of noise that can otherwise degrade the BANN accuracy. Our approach outperforms existing BNNs, SNNs, and adder neural networks (that also avoid energy-expensive multiplication operations similar to BNNs and SNNs) in terms of the accuracy-FLOPs trade-off for complex image recognition tasks. Downstream experiments on object detection further demonstrate the efficacy of our approach. Lastly, we demonstrate the portability of our approach to SNNs with multiple time steps. Codes are publicly available [here](https://github.com/godatta/Ultra-Low-Latency-SNN).",
      "authors": [
        "Gourav Datta",
        "Zeyu Liu",
        "Peter Anthony Beerel"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lGUyAuuTYZ",
      "cdate": 1695400068711,
      "mdate": 1713672283995,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710558"
    },
    {
      "id": "Kn7tWhuetn",
      "title": "On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods",
      "abstract": "Neural algorithmic reasoning is an emerging research direction that endows neural networks with the ability to mimic algorithmic executions step-by-step. A common paradigm in existing designs involves the use of historical embeddings in predicting the results of future execution steps. Our observation in this work is that such historical dependence intrinsically contradicts the Markov nature of algorithmic reasoning tasks. Based on this motivation, we present our ForgetNet, which does not use historical embeddings and thus is consistent with the Markov nature of the tasks. To address challenges in training ForgetNet at early stages, we further introduce G-ForgetNet, which uses a gating mechanism to allow for the selective integration of historical embeddings. Such an enhanced capability provides valuable computational pathways during the model's early training phase. Our extensive experiments, based on the CLRS-30 algorithmic reasoning benchmark, demonstrate that both ForgetNet and G-ForgetNet achieve better generalization capability than existing methods. Furthermore, we investigate the behavior of the gating mechanism, highlighting its degree of alignment with our intuitions and its effectiveness for robust performance. Our code is publicly available at https://github.com/divelab/ForgetNet.",
      "authors": [
        "Montgomery Bohde",
        "Meng Liu",
        "Alexandra Saxton",
        "Shuiwang Ji"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Kn7tWhuetn",
      "cdate": 1695399613559,
      "mdate": 1710538643867,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710563"
    },
    {
      "id": "ptCIlV24YZ",
      "title": "Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models",
      "abstract": "The advent of large pre-trained models has brought about a paradigm shift in both visual representation learning and natural language processing. However, clustering unlabeled images, as a fundamental and classic machine learning problem, still lacks an effective solution, particularly for large-scale datasets. In this paper, we propose a novel image clustering pipeline that leverages the powerful feature representation of large pre-trained models such as CLIP and cluster images effectively and efficiently at scale. We first developed a novel algorithm to estimate the number of clusters in a given dataset. We then show that the pre-trained features are significantly more structured by further optimizing the rate reduction objective. The resulting features may significantly improve the clustering accuracy, e.g., from 57\\% to  66\\% on ImageNet-1k. Furthermore, by leveraging CLIP's multimodality bridge between image and text, we develop a simple yet effective self-labeling algorithm that produces meaningful text labels for the clusters. Through extensive experiments, we show that our pipeline works well on standard datasets such as CIFAR-10, CIFAR-100, and ImageNet-1k. It also extends to datasets without predefined labels, such as LAION-Aesthetics and WikiArts.",
      "authors": [
        "Tianzhe Chu",
        "Shengbang Tong",
        "Tianjiao Ding",
        "Xili Dai",
        "Benjamin David Haeffele",
        "Rene Vidal",
        "Yi Ma"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ptCIlV24YZ",
      "cdate": 1695399383641,
      "mdate": 1713164160068,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.710570"
    },
    {
      "id": "GkJOCga62u",
      "title": "Orbit-Equivariant Graph Neural Networks",
      "abstract": "Equivariance is an important structural property that is captured by architectures such as graph neural networks (GNNs). However, equivariant graph functions cannot produce different outputs for similar nodes, which may be undesirable when the function is trying to optimize some global graph property. In this paper, we define orbit-equivariance, a relaxation of equivariance which allows for such functions whilst retaining important structural inductive biases. We situate the property in the hierarchy of graph functions, define a taxonomy of orbit-equivariant functions, and provide four different ways to achieve non-equivariant GNNs. For each, we analyze their expressivity with respect to orbit-equivariance and evaluate them on two novel datasets, one of which stems from a real-world use-case of designing optimal bioisosteres.",
      "authors": [
        "Matthew Morris",
        "Bernardo Cuenca Grau",
        "Ian Horrocks"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GkJOCga62u",
      "cdate": 1695399303361,
      "mdate": 1709661525238,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710575"
    },
    {
      "id": "NjNfLdxr3A",
      "title": "VeRA: Vector-based Random Matrix Adaptation",
      "abstract": "Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA), which significantly reduces the number of trainable parameters compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models. Website: https://dkopi.github.io/vera",
      "authors": [
        "Dawid Jan Kopiczko",
        "Tijmen Blankevoort",
        "Yuki M Asano"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NjNfLdxr3A",
      "cdate": 1695398844358,
      "mdate": 1713672692898,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710580"
    },
    {
      "id": "Bb21JPnhhr",
      "title": "AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?",
      "abstract": "Can we better anticipate an actor’s future actions (e.g. mix eggs) by knowing what commonly happens after the current action (e.g. crack eggs)? What if the actor also shares the goal (e.g. make fried rice) with us? The long-term action anticipation (LTA) task aims to predict an actor’s future behavior from video observations in the form of verb and noun sequences, and it is crucial for human-machine interaction.\nWe propose to formulate the LTA task from two perspectives: a bottom-up approach that predicts the next actions autoregressively by modeling temporal dynamics; and a top-down approach that infers the goal of the actor and plans the needed procedure to accomplish the goal. We hypothesize that large language models (LLMs), which have been pretrained on procedure text data (e.g. recipes, how-tos),\nhave the potential to help LTA from both perspectives. It can help provide the prior knowledge on the possible next actions, and infer the goal given the observed part of a procedure, respectively. We propose AntGPT, which represents video observations as sequences of human actions, and uses the action representation for an LLM to infer the goals and model temporal dynamics. AntGPT achieves state-\nof-the-art performance on Ego4D LTA v1 and v2, EPIC-Kitchens-55, as well as EGTEA GAZE+, thanks to LLMs’ goal inference and temporal dynamics modeling capabilities. We further demonstrate that these capabilities can be effectively distilled into a compact neural network 1.3% of the original LLM model size. Code and model will be released upon acceptance.",
      "authors": [
        "Qi Zhao",
        "Shijie Wang",
        "Ce Zhang",
        "Changcheng Fu",
        "Minh Quan Do",
        "Nakul Agarwal",
        "Kwonjoon Lee",
        "Chen Sun"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Bb21JPnhhr",
      "cdate": 1695398620413,
      "mdate": 1710372039109,
      "matched_keywords": [
        "large language model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710584"
    },
    {
      "id": "DGez4B2a6Y",
      "title": "A Plug-and-Play Image Registration Network",
      "abstract": "Deformable image registration (DIR) is an active research topic in biomedical imaging. There is a growing interest in developing DIR methods based on deep learning (DL). A traditional DL approach to DIR is based on training a convolutional neural network (CNN) to estimate the registration field between two input images. While conceptually simple, this approach comes with a limitation that it exclusively relies on a pre-trained CNN without explicitly enforcing fidelity between the registered image and the reference. We present plug-and-play image registration network (PIRATE) as a new DIR method that addresses this issue by integrating an explicit data-fidelity penalty and a CNN prior. PIRATE pre-trains a CNN denoiser on the registration field and \"plugs\" it into an iterative method as a regularizer. We additionally present PIRATE+ that fine-tunes the CNN prior in PIRATE using deep equilibrium models (DEQ). PIRATE+ interprets the fixed-point iteration of PIRATE as a network with effectively infinite layers and then trains the resulting network end-to-end, enabling it to learn more task-specific information and boosting its performance. Our numerical results on OASIS and CANDI datasets show that our methods achieve state-of-the-art performance on DIR.",
      "authors": [
        "Junhao Hu",
        "Weijie Gan",
        "Zhixin Sun",
        "Hongyu An",
        "Ulugbek Kamilov"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DGez4B2a6Y",
      "cdate": 1695398611230,
      "mdate": 1709661524989,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710589"
    },
    {
      "id": "ZZTkLDRmkg",
      "title": "BENO: Boundary-embedded Neural Operators for Elliptic PDEs",
      "abstract": "Elliptic partial differential equations (PDEs) are a major class of time-independent PDEs that play a key role in many scientific and engineering domains such as fluid dynamics, plasma physics, and solid mechanics. Recently, neural operators have emerged as a promising technique to solve elliptic PDEs more efficiently by directly mapping the input to solutions. However, existing networks typically neglect complex geometries and inhomogeneous boundary values  present in the real world. Here we introduce Boundary-Embedded Neural Operators (BENO), a novel neural operator architecture that embeds the complex geometries and inhomogeneous boundary values into the solving of elliptic PDEs. Inspired by classical Green's function, BENO consists of two Graph Neural Networks (GNNs) for interior source term and boundary values, respectively. Furthermore, a Transformer encoder maps the global boundary geometry into a latent vector which influences each message passing layer of the GNNs. We test our model and strong baselines extensively in elliptic PDEs with complex boundary conditions. We show that all existing baseline methods fail to learn the solution operator. In contrast, our model, endowed with boundary-embedded architecture, outperforms state-of-the-art neural operators and strong baselines by an average of 60.96%.",
      "authors": [
        "Haixin Wang",
        "Jiaxin LI",
        "Anubhav Dwivedi",
        "Kentaro Hara",
        "Tailin Wu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ZZTkLDRmkg",
      "cdate": 1695398609173,
      "mdate": 1709661524962,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710593"
    },
    {
      "id": "vE1e1mLJ0U",
      "title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.",
      "abstract": "Biological cortical neurons are remarkably sophisticated computational devices, temporally integrating their vast synaptic input over an intricate dendritic tree, subject to complex, nonlinearly interacting internal biological processes. \nA recent study proposed to characterize this complexity by fitting accurate surrogate models to replicate the input-output relationship of a detailed biophysical cortical pyramidal neuron model and discovered it needed temporal convolutional networks (TCN) with millions of parameters. \nRequiring these many parameters, however, could stem from a misalignment between the inductive biases of the TCN and cortical neuron's computations.\nIn light of this, and to explore the computational implications of leaky memory units and nonlinear dendritic processing, we introduce the Expressive Leaky Memory (ELM) neuron model, a biologically inspired phenomenological model of a cortical neuron.\nRemarkably, by exploiting such slowly decaying memory-like hidden states and two-layered nonlinear integration of synaptic input, our ELM neuron can accurately match the aforementioned input-output relationship with under ten thousand trainable parameters.\nTo further assess the computational ramifications of our neuron design, we evaluate it on various tasks with demanding temporal structures, including the Long Range Arena (LRA) datasets, as well as a novel neuromorphic dataset based on the Spiking Heidelberg Digits dataset (SHD-Adding). Leveraging a larger number of memory units with sufficiently long timescales, and correspondingly sophisticated synaptic integration, the ELM neuron displays substantial long-range processing capabilities, reliably outperforming the classic Transformer or Chrono-LSTM architectures on LRA, and even solving the Pathfinder-X task with over 70\\% accuracy (16k context length). These findings raise further questions about the computational sophistication of individual cortical neurons and their role in extracting complex long-range temporal dependencies.",
      "authors": [
        "Aaron Spieler",
        "Nasim Rahaman",
        "Georg Martius",
        "Bernhard Schölkopf",
        "Anna Levina"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vE1e1mLJ0U",
      "cdate": 1695398120322,
      "mdate": 1710327519210,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710598"
    },
    {
      "id": "pC3WJHf51j",
      "title": "Large-scale Training of Foundation Models for Wearable Biosignals",
      "abstract": "Tracking biosignals is crucial for monitoring wellness and preempting the development of severe medical conditions. Today, wearable devices can conveniently record various biosignals, creating the opportunity to monitor health status without disruption to one's daily routine. Despite widespread use of wearable devices and existing digital biomarkers, the absence of curated data with annotated medical labels hinders the development of new biomarkers to measure common health conditions. In fact, medical datasets are usually small in comparison to other domains, which is an obstacle for developing neural network models for biosignals. To address this challenge, we have employed self-supervised learning using the unlabeled sensor data collected under informed consent from the large longitudinal Apple Heart and Movement Study (AHMS) to train foundation models for two common biosignals: photoplethysmography (PPG) and electrocardiogram (ECG) recorded on Apple Watch. We curated PPG and ECG datasets from AHMS that include data from ${\\sim} 141$K participants spanning ${\\sim} 3$ years. Our self-supervised learning framework includes participant level positive pair selection, stochastic augmentation module and a regularized contrastive loss optimized with momentum training, and generalizes well to both PPG and ECG modalities. We show that the pre-trained foundation models readily encode information regarding participants' demographics and health conditions. To the best of our knowledge, this is the first study that builds foundation models using large-scale PPG and ECG data collected via wearable consumer devices $\\textendash$ prior works have commonly used smaller-size datasets collected in clinical and experimental settings. We believe PPG and ECG foundation models can enhance future wearable devices by reducing the reliance on labeled data and hold the potential to help the users improve their health.",
      "authors": [
        "Salar Abbaspourazad",
        "Oussama Elachqar",
        "Andrew Miller",
        "Saba Emrani",
        "Udhyakumar Nallasamy",
        "Ian Shapiro"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pC3WJHf51j",
      "cdate": 1695397800427,
      "mdate": 1713672216639,
      "matched_keywords": [
        "foundation model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710606"
    },
    {
      "id": "eMNN0wIyVw",
      "title": "On Trajectory Augmentations for Off-Policy Evaluation",
      "abstract": "In the realm of reinforcement learning (RL), off-policy evaluation (OPE) holds a pivotal position, especially in high-stake human-involved scenarios such as e-learning and healthcare. Applying OPE to these domains is often challenging with scarce and underrepresentative offline training trajectories. Data augmentation has been a successful technique to enrich training data. However, directly employing existing data augmentation methods to OPE may not be feasible, due to the Markovian nature within the offline trajectories and the desire for generalizability across diverse target policies. In this work, we propose an offline trajectory augmentation approach to specifically facilitate OPE in human-involved scenarios. We propose sub-trajectory mining to extract potentially valuable sub-trajectories from offline data, and diversify the behaviors within those sub-trajectories by varying coverage of the state-action space. Our work was empirically evaluated in a wide array of environments, encompassing both simulated scenarios and real-world domains like robotic control, healthcare, and e-learning, where the training trajectories include varying levels of coverage of the state-action space. By enhancing the performance of a variety of OPE methods, our work offers a promising path forward for tackling OPE challenges in situations where data may be limited or underrepresentative.",
      "authors": [
        "Ge Gao",
        "Qitong Gao",
        "Xi Yang",
        "Song Ju",
        "Miroslav Pajic",
        "Min Chi"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=eMNN0wIyVw",
      "cdate": 1695397620579,
      "mdate": 1713209163776,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710610"
    },
    {
      "id": "Zz594UBNOH",
      "title": "Clifford Group Equivariant Simplicial Message Passing Networks",
      "abstract": "We introduce Clifford Group Equivariant Simplicial Message Passing Networks, a method for steerable $\\mathrm{E}(n)$-equivariant message passing on simplicial complexes. Our method integrates the expressivity of Clifford group-equivariant layers with simplicial message passing, which is topologically more intricate than regular graph message passing. Clifford algebras include higher-order objects such as bivectors and trivectors, which express geometric features (e.g., areas, volumes) derived from vectors. Using this knowledge, we represent simplex features through geometric products of their vertices. To achieve efficient simplicial message passing, we share the parameters of the message network across different dimensions. Additionally, we restrict the final message to an aggregation of the incoming messages from different dimensions, leading to what we term *shared* simplicial message passing. Experimental results show that our method is able to outperform both equivariant and simplicial graph neural networks on a variety of geometric tasks.",
      "authors": [
        "Cong Liu",
        "David Ruhe",
        "Floor Eijkelboom",
        "Patrick Forré"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Zz594UBNOH",
      "cdate": 1695397030048,
      "mdate": 1710247169584,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710615"
    },
    {
      "id": "NxoFmGgWC9",
      "title": "Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation",
      "abstract": "Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. We introduce GR-1, a GPT-style model designed for multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset. We perform extensive experiments on the challenging CALVIN benchmark and a real robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9% to 94.9%. In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3% to 85.4%. In real robot experiments, GR-1 also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects. We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. Project page: https://GR1-Manipulation.github.io",
      "authors": [
        "Hongtao Wu",
        "Ya Jing",
        "Chilam Cheang",
        "Guangzeng Chen",
        "Jiafeng Xu",
        "Xinghang Li",
        "Minghuan Liu",
        "Hang Li",
        "Tao Kong"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NxoFmGgWC9",
      "cdate": 1695396950719,
      "mdate": 1709696715987,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710622"
    },
    {
      "id": "EDPxCjXzSb",
      "title": "Vision-by-Language for Training-Free Compositional Image Retrieval",
      "abstract": "Given an image and a target modification (e.g an image of the Eiffel tower and the text “without people and at night-time”), Compositional Image Retrieval (CIR) aims to retrieve the relevant target image in a database. While supervised approaches rely on annotating triplets that is costly (i.e. query image, textual modification, and target image), recent research sidesteps this need by using large-scale vision-language models (VLMs), performing Zero-Shot CIR (ZS-CIR). However, state-of-the-art approaches in ZS-CIR still require training task-specific, customized models over large amounts of image-text pairs. In this work, we proposeto tackle CIR in a training-free manner via our Compositional Image Retrieval through Vision-by-Language (CIReVL), a simple, yet human-understandable and scalable pipeline that effectively recombines large-scale VLMs with large language models (LLMs). By captioning the reference image using a pre-trained generative VLM and asking a LLM to recompose the caption based on the textual target modification for subsequent retrieval via e.g. CLIP, we achieve modular language reasoning. In four ZS-CIR benchmarks, we find competitive, in-part state-of-the-art performance - improving over supervised methods Moreover, the modularity of CIReVL offers simple scalability without re-training, allowing us to both investigate scaling laws and bottlenecks for ZS-CIR while easily scaling up to in parts more than double of previously reported results. Finally, we show that CIReVL makes CIR human-understandable by composing image and text in a modular fashion in the language domain, thereby making it intervenable, allowing to post-hoc re-align failure cases. Code will be released upon acceptance.",
      "authors": [
        "Shyamgopal Karthik",
        "Karsten Roth",
        "Massimiliano Mancini",
        "Zeynep Akata"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EDPxCjXzSb",
      "cdate": 1695396628106,
      "mdate": 1709661524464,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710627"
    },
    {
      "id": "wYmvN3sQpG",
      "title": "Benign Oscillation of Stochastic Gradient Descent with Large Learning Rate",
      "abstract": "In this work, we theoretically investigate the generalization properties of neural networks (NN) trained by stochastic gradient descent (SGD) with large learning rates. Under such a training regime, our finding is that, the oscillation of the NN weights caused by SGD with large learning rates turns out to be beneficial to the generalization of the NN, potentially improving over the same NN trained by SGD with small learning rates that converges more smoothly. In view of this finding, we call such a phenomenon “benign oscillation”. Our theory towards demystifying such a phenomenon builds upon the feature learning perspective of deep learning. Specifically, we consider a feature-noise data generation model that consists of (i) weak features which have a small $\\ell_2$-norm and appear in each data point; (ii) strong features which have a large $\\ell_2$-norm but appear only in a certain fraction of all data points; and (iii) noise. We prove that NNs trained by oscillating SGD with a large learning rate can effectively learn the weak features in the presence of those strong features. In contrast, NNs trained by SGD with a small learning rate can only learn the strong features but make little progress in learning the weak features. Consequently, when it comes to the new testing data points that consist of only weak features, the NN trained by oscillating SGD with a large learning rate can still make correct predictions, while the NN trained by SGD with a small learning rate could not. Our theory sheds light on how large learning rate training benefits the generalization of NNs. Experimental results demonstrate our  findings on the phenomenon of “benign oscillation”.",
      "authors": [
        "Miao Lu",
        "Beining Wu",
        "Xiaodong Yang",
        "Difan Zou"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=wYmvN3sQpG",
      "cdate": 1695396575248,
      "mdate": 1710229980196,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710632"
    },
    {
      "id": "PXD3FAVHJT",
      "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity",
      "abstract": "Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant for current LLM use cases. We find that RLHF generalises better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger. However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity. Our results provide guidance on which fine-tuning method should be used depending on the application, and show that more research is needed to improve the tradeoff between generalisation and diversity.",
      "authors": [
        "Robert Kirk",
        "Ishita Mediratta",
        "Christoforos Nalmpantis",
        "Jelena Luketina",
        "Eric Hambro",
        "Edward Grefenstette",
        "Roberta Raileanu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=PXD3FAVHJT",
      "cdate": 1695396482152,
      "mdate": 1709661524333,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710636"
    },
    {
      "id": "MnMWa94t12",
      "title": "DyST: Towards Dynamic Neural Scene Representations on Real-World Videos",
      "abstract": "Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.",
      "authors": [
        "Maximilian Seitzer",
        "Sjoerd van Steenkiste",
        "Thomas Kipf",
        "Klaus Greff",
        "Mehdi S. M. Sajjadi"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MnMWa94t12",
      "cdate": 1695396074581,
      "mdate": 1709661524182,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710641"
    },
    {
      "id": "skcTCdJz0f",
      "title": "Probabilistic Self-supervised Representation Learning via Scoring Rules Minimization",
      "abstract": "%\nSelf-supervised learning methods have shown promising results across a wide range of tasks in computer vision, natural language processing, and multimodal analysis. However, self-supervised approaches come with a notable limitation, dimensional collapse, where a model doesn't fully utilize its capacity to encode information optimally. Motivated by this, we propose ProSMin, a novel probabilistic self-supervised learning approach that leverages the power of probabilistic models to enhance representation quality and mitigate collapsing representations. Our proposed approach involves two neural networks, the online network and the target network, which collaborate and learn the diverse distribution of representations from each other through probabilistic knowledge distillation. The two networks are trained via our new loss function based on proper scoring rules. We provide a theoretical justification for ProSMin and demonstrate its modified scoring rule. This insight validates the method's optimization process and contributes to its robustness and effectiveness in improving representation quality. We evaluate our probabilistic model on various downstream tasks, such as in-distribution generalization, out-of-distribution detection, dataset corruption, low-shot learning, and transfer learning. Our method achieves superior accuracy and calibration, outperforming the self-supervised baseline in a variety of experiments on large datasets such as ImageNet-O and ImageNet-C. ProSMin thus demonstrates its scalability and real-world applicability. Our code is publicly available: https://github.com/amirvhd/SSL-sore-rule.",
      "authors": [
        "Amirhossein Vahidi",
        "Simon Schosser",
        "Lisa Wimmer",
        "Yawei Li",
        "Bernd Bischl",
        "Eyke Hüllermeier",
        "Mina Rezaei"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=skcTCdJz0f",
      "cdate": 1695395742261,
      "mdate": 1710698065844,
      "matched_keywords": [
        "multimodal",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710646"
    },
    {
      "id": "LqRGsGWOTX",
      "title": "Bilevel Optimization under Unbounded Smoothness: A New Algorithm and Convergence Analysis",
      "abstract": "Bilevel optimization is an important formulation for many machine learning problems, such as meta-learning and hyperparameter optimization. Current bilevel optimization algorithms assume that the gradient of the upper-level function is Lipschitz (i.e., the upper-level function has a bounded smoothness parameter). However, recent studies reveal that certain neural networks such as recurrent neural networks (RNNs) and long-short-term memory networks (LSTMs) exhibit potential unbounded smoothness, rendering conventional bilevel optimization algorithms unsuitable for these neural networks. In this paper, we design a new bilevel optimization algorithm, namely BO-REP, to address this challenge. This algorithm updates the upper-level variable using normalized momentum and incorporates two novel techniques for updating the lower-level variable: \\textit{initialization refinement} and \\textit{periodic updates}. Specifically, once the upper-level variable is initialized, a subroutine is invoked to obtain a refined estimate of the corresponding optimal lower-level variable, and the lower-level variable is updated only after every specific period instead of each iteration. When the upper-level problem is nonconvex and unbounded smooth, and the lower-level problem is strongly convex, we prove that our algorithm requires $\\widetilde{O}(1/\\epsilon^4)$ \\footnote{Here $\\widetilde{O}(\\cdot)$ compresses logarithmic factors of $1/\\epsilon$ and $1/\\delta$, where $\\delta\\in(0,1)$ denotes the failure probability.} iterations to find an $\\epsilon$-stationary point in the stochastic setting, where each iteration involves calling a stochastic gradient or Hessian-vector product oracle. Notably, this result matches the state-of-the-art complexity results under the bounded smoothness setting and without mean-squared smoothness of the stochastic gradient, up to logarithmic factors. Our proof relies on novel technical lemmas for the periodically updated lower-level variable, which are of independent interest. Our experiments on hyper-representation learning, hyperparameter optimization, and data hyper-cleaning for text classification tasks demonstrate the effectiveness of our proposed algorithm. The code is available at [https://github.com/MingruiLiu-ML-Lab/Bilevel-Optimization-under-Unbounded-Smoothness](https://github.com/MingruiLiu-ML-Lab/Bilevel-Optimization-under-Unbounded-Smoothness).",
      "authors": [
        "Jie Hao",
        "Xiaochuan Gong",
        "Mingrui Liu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LqRGsGWOTX",
      "cdate": 1695394432951,
      "mdate": 1713236593571,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710650"
    },
    {
      "id": "1VeQ6VBbev",
      "title": "Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy Gradient Methods",
      "abstract": "Markov Decision Processes (MDPs) are a formal framework for modeling and solving sequential decision-making problems. In finite time horizons such problems are relevant for instance for optimal stopping or specific supply chain problems, but also in the training of large language models. In contrast to infinite horizon MDPs optimal policies are not stationary, policies must be learned for every single epoch. In practice all parameters are often trained simultaneously, ignoring the inherent structure suggested by dynamic programming. This paper introduces a combination of dynamic programming and policy gradient called dynamical policy gradient, where the parameters are trained backwards in time. \n   \n   For the tabular softmax parametrisation we carry out the convergence analysis for simultaneous and dynamic policy gradient towards global optima, both in the exact and sampled gradient settings without regularisation. It turns out that the use of dynamic policy gradient training much better exploits the structure of finite-time problems which is reflected in improved convergence bounds.",
      "authors": [
        "Sara Klein",
        "Simon Weissmann",
        "Leif Döring"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=1VeQ6VBbev",
      "cdate": 1695394297779,
      "mdate": 1710493956506,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710655"
    },
    {
      "id": "vXxardq6db",
      "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns",
      "abstract": "Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA-2 70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA-2 70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models.",
      "authors": [
        "Saleh Ashkboos",
        "Maximilian L. Croci",
        "Marcelo Gennari do Nascimento",
        "Torsten Hoefler",
        "James Hensman"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vXxardq6db",
      "cdate": 1695394167790,
      "mdate": 1710518861601,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710659"
    },
    {
      "id": "1oqedRt6Z7",
      "title": "Convolutional Deep Kernel Machines",
      "abstract": "Standard infinite-width limits of neural networks sacrifice the ability for intermediate layers to learn representations from data. Recent work (“A theory of representation learning gives a deep generalisation of kernel methods”, Yang et al. 2023) modified the Neural Network Gaussian Process (NNGP) limit of Bayesian neural networks so that representation learning is retained. Furthermore, they found that applying this modified limit to a deep Gaussian process gives a practical learning algorithm which they dubbed the “deep kernel machine” (DKM). However, they only considered the simplest possible setting: regression in small, fully connected networks with e.g. 10 input features. Here, we introduce convolutional deep kernel machines. This required us to develop a novel inter-domain inducing point approximation, as well as introducing and experimentally assessing a number of techniques not previously seen in DKMs, including analogues to batch normalisation, different likelihoods, and different types of top-layer. The resulting model trains in roughly 77 GPU hours, achieving around 99\\% test accuracy on MNIST, 72\\% on CIFAR-100, and 92.7\\% on CIFAR-10, which is SOTA for kernel methods.",
      "authors": [
        "Edward Milsom",
        "Ben Anson",
        "Laurence Aitchison"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=1oqedRt6Z7",
      "cdate": 1695393760252,
      "mdate": 1709661523791,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710664"
    },
    {
      "id": "HobyL1B9CZ",
      "title": "Chain-of-Experts: When LLMs Meet Complex Operations Research Problems",
      "abstract": "Large language models (LLMs) have emerged as powerful techniques for various NLP tasks, such as mathematical reasoning and plan generation. In this paper, we study automatic modeling and programming for complex operation research (OR) problems, so as to alleviate the heavy dependence on domain experts and benefit a spectrum of industry sectors. We present the first LLM-based solution, namely Chain-of-Experts (CoE), a novel multi-agent cooperative framework to enhance reasoning capabilities. Specifically, each agent is assigned a specific role and endowed with domain knowledge related to OR. We also introduce a conductor to orchestrate these agents via forward thought construction and backward reflection mechanism. Furthermore, we release a benchmark dataset (ComplexOR) of complex OR problems to facilitate OR research and community development. Experimental results show that CoE significantly outperforms the state-of-the-art LLM-based approaches both on LPWP and ComplexOR.",
      "authors": [
        "Ziyang Xiao",
        "Dongxiang Zhang",
        "Yangjun Wu",
        "Lilin Xu",
        "Yuan Jessica Wang",
        "Xiongwei Han",
        "Xiaojin Fu",
        "Tao Zhong",
        "Jia Zeng",
        "Mingli Song",
        "Gang Chen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=HobyL1B9CZ",
      "cdate": 1695393655562,
      "mdate": 1710514301069,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710671"
    },
    {
      "id": "DEJIDCmWOz",
      "title": "On the Reliability of Watermarks for Large Language Models",
      "abstract": "As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. _Watermarking_ is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text may be modified to suit a user's needs, or entirely rewritten to avoid detection. We study the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked LLM, or mixed into a longer hand-written document. We find that watermarks remain detectable even after human and machine paraphrasing. While these attacks dilute the strength of the watermark, paraphrases are statistically likely to leak n-grams or even longer fragments of the original text, resulting in high-confidence detections when enough tokens are observed.  For example, after strong human paraphrasing the watermark is detectable after observing 800 tokens on average, when setting a $1\\mathrm{e}{-5}$ false positive rate. We also consider a range of new detection schemes that are sensitive to short spans of watermarked text embedded inside a large document, and we compare the robustness of watermarking to other kinds of detectors.",
      "authors": [
        "John Kirchenbauer",
        "Jonas Geiping",
        "Yuxin Wen",
        "Manli Shu",
        "Khalid Saifullah",
        "Kezhi Kong",
        "Kasun Fernando",
        "Aniruddha Saha",
        "Micah Goldblum",
        "Tom Goldstein"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DEJIDCmWOz",
      "cdate": 1695393591234,
      "mdate": 1710710963583,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710676"
    },
    {
      "id": "lUYY2qsRTI",
      "title": "Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding",
      "abstract": "A prominent challenge of offline reinforcement learning (RL) is the issue of hidden confounding: unobserved variables may influence both the actions taken by the agent and the observed outcomes. Hidden confounding can compromise the validity of any causal conclusion drawn from data and presents a major obstacle to effective offline RL. In the present paper, we tackle the problem of hidden confounding in the nonidentifiable setting. We propose a definition of uncertainty due to hidden confounding bias, termed delphic uncertainty, which uses variation over world models compatible with the observations, and differentiate it from the well-known epistemic and aleatoric uncertainties. We derive a practical method for estimating the three types of uncertainties, and construct a pessimistic offline RL algorithm to account for them. Our method does not assume identifiability of the unobserved confounders, and attempts to reduce the amount of confounding bias. We demonstrate through extensive experiments and ablations the efficacy of our approach on a sepsis management benchmark, as well as on electronic health records. Our results suggest that nonidentifiable hidden confounding bias can be mitigated to improve offline RL solutions in practice.",
      "authors": [
        "Alizée Pace",
        "Hugo Yèche",
        "Bernhard Schölkopf",
        "Gunnar Ratsch",
        "Guy Tennenholtz"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lUYY2qsRTI",
      "cdate": 1695393435757,
      "mdate": 1710501348393,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710683"
    },
    {
      "id": "H3IUunLy8s",
      "title": "Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning",
      "abstract": "Fine-tuning large pre-trained foundation models, such as the 175B GPT-3, has become the prevailing approach for downstream tasks. While parameter-efficient fine-tuning methods have been proposed and proven effective without retraining all model parameters, their performance is limited by the capacity of incremental modules, especially under constrained parameter budgets.\nTo overcome this challenge, we propose CAPABOOST, a simple yet effective strategy that enhances model capacity by leveraging low-rank updates through parallel weight modules in target layers. By applying static random masks to the shared weight matrix, CAPABOOST constructs a diverse set of weight matrices, effectively increasing the rank of incremental weights without adding parameters. Notably, our approach can be seamlessly integrated into various existing parameter-efficient fine-tuning methods. We extensively validate the efficacy of CAPABOOST through experiments on diverse downstream tasks, including natural language understanding, question answering, and image classification. Our results demonstrate significant improvements over baselines, without incurring additional computation\nor storage costs. We will make our code and benchmark publicly available.",
      "authors": [
        "Haobo SONG",
        "Hao Zhao",
        "Soumajit Majumder",
        "Tao Lin"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=H3IUunLy8s",
      "cdate": 1695392970335,
      "mdate": 1710464502948,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710688"
    },
    {
      "id": "5nM2AHzqUj",
      "title": "Linear Log-Normal Attention with Unbiased Concentration",
      "abstract": "Transformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long documents or high-resolution images. In this work, we study the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability. Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention. Our experimental results on popular natural language benchmarks reveal that our proposed Linear Log-Normal Attention outperforms other linearized attention alternatives, offering a promising avenue for enhancing the scalability of transformer models.",
      "authors": [
        "Yury Nahshan",
        "Joseph Kampeas",
        "Emir Haleva"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5nM2AHzqUj",
      "cdate": 1695392938875,
      "mdate": 1709661523444,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710693"
    },
    {
      "id": "xJEd8PkdNz",
      "title": "Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control",
      "abstract": "Integral reinforcement learning (IntRL) demands the precise computation of the utility function's integral at its policy evaluation (PEV) stage. This is achieved through quadrature rules, which are weighted sums of utility functions evaluated from state samples obtained in discrete time. Our research reveals a critical yet underexplored phenomenon: the choice of the computational method -- in this case, the quadrature rule -- can significantly impact control performance. This impact is traced back to the fact that computational errors introduced in the PEV stage can affect the policy iteration's convergence behavior, which in turn affects the learned controller. To elucidate how computation impacts control, we draw a parallel between IntRL's policy iteration and Newton's method applied to the Hamilton-Jacobi-Bellman equation. In this light, computational error in PEV manifests as an extra error term in each iteration of Newton's method, with its upper bound proportional to the computational error. Further, we demonstrate that when the utility function resides in a reproducing kernel Hilbert space (RKHS), the optimal quadrature is achievable by employing Bayesian quadrature with the RKHS-inducing kernel function. We prove that the local convergence rates for IntRL using the trapezoidal rule and Bayesian quadrature with a Matérn kernel to be $O(N^{-2})$ and $O(N^{-b})$, where $N$ is the number of evenly-spaced samples and $b$ is the Matérn kernel's smoothness parameter. These theoretical findings are finally validated by two canonical control tasks.",
      "authors": [
        "Wenhan Cao",
        "Wei Pan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xJEd8PkdNz",
      "cdate": 1695392351658,
      "mdate": 1709661523328,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710698"
    },
    {
      "id": "c0MyyXyGfn",
      "title": "Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) for complex tasks remains a challenge, primarily due to the difficulties of engineering scalar reward functions and the inherent inefficiency of training models from scratch. Instead, it would be better to specify complex tasks in terms of elementary subtasks and to reuse subtask solutions whenever possible. In this work, we address continuous space lexicographic multi-objective RL problems, consisting of prioritized subtasks, which are notoriously difficult to solve. We show that these can be scalarized with a subtask transformation and then solved incrementally using value decomposition. Exploiting this insight, we propose prioritized soft Q-decomposition (PSQD), a novel algorithm for learning and adapting subtask solutions under lexicographic priorities in continuous state-action spaces. PSQD offers the ability to reuse previously learned subtask solutions in a zero-shot composition, followed by an adaptation step. Its ability to use retained subtask training data for offline learning eliminates the need for new environment interaction during adaptation. We demonstrate the efficacy of our approach by presenting successful learning, reuse, and adaptation results for both low- and high-dimensional simulated robot control tasks, as well as offline learning results. In contrast to baseline approaches, PSQD does not trade off between conflicting subtasks or priority constraints and satisfies subtask priorities during learning. PSQD provides an intuitive framework for tackling complex RL problems, offering insights into the inner workings of the subtask composition.",
      "authors": [
        "Finn Rietz",
        "Erik Schaffernicht",
        "Stefan Heinrich",
        "Johannes A. Stork"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=c0MyyXyGfn",
      "cdate": 1695392314199,
      "mdate": 1709892335453,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710702"
    },
    {
      "id": "9WD9KwssyT",
      "title": "Zipformer: A faster and better encoder for automatic speech recognition",
      "abstract": "The Conformer has become the most popular encoder model for automatic speech recognition (ASR).  It adds convolution modules to a transformer to learn both local and global dependencies. In this work we describe a faster, more memory-efficient, and better-performing transformer, called Zipformer.  Modeling changes include: 1) a U-Net-like encoder structure where middle stacks operate at lower frame rates; 2) reorganized block structure with more modules, within which we re-use attention weights for efficiency; 3) a modified form of LayerNorm called BiasNorm allows us to retain some length information; 4)  new activation functions SwooshR and SwooshL work better than Swish.  We also propose a new optimizer, called ScaledAdam, which scales the update by each tensor's current scale to keep the relative change about the same, and also explictly learns the parameter scale. It achieves faster converge and better performance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and WenetSpeech datasets demonstrate the effectiveness of our proposed Zipformer over other state-of-the-art ASR models. Our code is publicly available at https://github.com/k2-fsa/icefall.",
      "authors": [
        "Zengwei Yao",
        "Liyong Guo",
        "Xiaoyu Yang",
        "Wei Kang",
        "Fangjun Kuang",
        "Yifan Yang",
        "Zengrui Jin",
        "Long Lin",
        "Daniel Povey"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9WD9KwssyT",
      "cdate": 1695391973841,
      "mdate": 1712664351857,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710707"
    },
    {
      "id": "TWVMVPx2wO",
      "title": "Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning",
      "abstract": "Deep Metric Learning (DML) has long attracted the attention of the machine learning community as a key objective. Existing solutions concentrate on fine-tuning the pre-trained models on conventional image datasets. As a result of the success of recent pre-trained models derived from larger-scale datasets, it is challenging to adapt the model to the DML tasks in the local data domain while retaining the previously gained knowledge. In this paper, we investigate parameter-efficient methods for fine-tuning the pre-trained model for DML tasks. In particular, we propose a novel and effective framework based on learning Visual Prompts (VPT) in the pre-trained Vision Transformers (ViT). Based on the conventional proxy-based DML paradigm, we augment the proxy by incorporating the semantic information from the input image and the ViT, in which we optimize the visual prompts for each class. We demonstrate that our new approximations with semantic information are superior to representative capabilities, thereby improving metric learning performance. We conduct extensive experiments to demonstrate that our proposed framework is superior and efficient by evaluating popular DML benchmarks. In particular, we demonstrate that our fine-tuning method achieves comparable or even better performance than recent state-of-the-art full fine-tuning works of DML while tuning only a small percentage of total parameters.",
      "authors": [
        "Li Ren",
        "Chen Chen",
        "Liqiang Wang",
        "Kien A. Hua"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=TWVMVPx2wO",
      "cdate": 1695391795400,
      "mdate": 1710458739674,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710711"
    },
    {
      "id": "vN9fpfqoP1",
      "title": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text",
      "abstract": "We propose fine-tuning large language models for generation of stable materials. While unorthodox, fine-tuning large language models on text-encoded atomistic data is simple to implement yet reliable, with around 90\\% of sampled structures obeying physical constraints on atom positions and charges. Using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, we show that our strongest model (fine-tuned  LLaMA-2 70B) can generate materials predicted to be metastable at about twice the rate (49\\% vs 28\\%) of CDVAE, a competing diffusion model. Because of text prompting's inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text-conditional generation. Finally, we show that language models' ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained LLMs are surprisingly well-suited for atomistic data.",
      "authors": [
        "Nate Gruver",
        "Anuroop Sriram",
        "Andrea Madotto",
        "Andrew Gordon Wilson",
        "C. Lawrence Zitnick",
        "Zachary Ward Ulissi"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vN9fpfqoP1",
      "cdate": 1695391611375,
      "mdate": 1709661523141,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710715"
    },
    {
      "id": "qODvxQ8TXW",
      "title": "Masks, Signs, And Learning Rate Rewinding",
      "abstract": "Learning Rate Rewinding (LRR) has been established as a strong variant of Iterative Magnitude Pruning (IMP) to find lottery tickets in deep overparameterized neural networks. While both iterative pruning schemes couple structure and parameter learning, understanding how LRR excels in both aspects can bring us closer to the design of more flexible deep learning algorithms that can optimize diverse sets of sparse architectures. To this end, we conduct experiments that disentangle the effect of mask learning and parameter optimization and how both benefit from overparameterization. The ability of LRR to flip parameter signs early and stay robust to sign perturbations seems to make it not only more effective in mask identification but also in optimizing  diverse sets of masks, including random ones. In support of this hypothesis, we prove in a simplified single hidden neuron setting that LRR succeeds in more cases than IMP, as it can escape initially problematic sign configurations.",
      "authors": [
        "Advait Harshal Gadhikar",
        "Rebekka Burkholz"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qODvxQ8TXW",
      "cdate": 1695391295244,
      "mdate": 1709661522985,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710724"
    },
    {
      "id": "AcSChDWL6V",
      "title": "Distinguished In Uniform: Self-Attention Vs. Virtual Nodes",
      "abstract": "Graph Transformers (GTs) such as SAN and GPS are graph processing models that combine Message-Passing GNNs (MPGNNs) with global Self-Attention. They were shown to be universal function approximators, with two reservations: 1. The initial node features must be augmented with certain positional encodings. 2. The approximation is non-uniform: Graphs of different sizes may require a different approximating network.\n\nWe first clarify that this form of universality is not unique to GTs: Using the same positional encodings, also pure MPGNNs and even 2-layer MLPs are non-uniform universal approximators. We then consider uniform expressivity: The target function is to be approximated by a single network for graphs of all sizes. There, we compare GTs to the more efficient MPGNN + Virtual Node architecture. The essential difference between the two model definitions is in their global computation method: Self-Attention Vs Virtual Node. We prove that none of the models is a uniform-universal approximator, before proving our main result: Neither model’s uniform expressivity subsumes the other’s. We demonstrate the theory with experiments on synthetic data. We further augment our study with real-world datasets, observing mixed results which indicate no clear ranking in practice as well.",
      "authors": [
        "Eran Rosenbluth",
        "Jan Tönshoff",
        "Martin Ritzert",
        "Berke Kisin",
        "Martin Grohe"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=AcSChDWL6V",
      "cdate": 1695390676591,
      "mdate": 1713672926835,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710729"
    },
    {
      "id": "tmqOhBC4a5",
      "title": "Maximum Entropy Heterogeneous-Agent Reinforcement Learning",
      "abstract": "*Multi-agent reinforcement learning* (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample complexity, training instability, and the risk of converging to a suboptimal Nash Equilibrium. In this paper, we propose a unified framework for learning \\emph{stochastic} policies to resolve these issues. We embed cooperative MARL problems into probabilistic graphical models, from which we derive the maximum entropy (MaxEnt) objective for MARL. Based on the MaxEnt framework, we propose *Heterogeneous-Agent Soft Actor-Critic* (HASAC) algorithm. Theoretically, we prove the monotonic improvement and convergence to *quantal response equilibrium* (QRE) properties of HASAC. Furthermore, we generalize a unified template for MaxEnt algorithmic design named *Maximum Entropy Heterogeneous-Agent Mirror Learning* (MEHAML), which provides any induced method with the same guarantees as HASAC. We evaluate HASAC on six benchmarks: Bi-DexHands, Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge, Google Research Football, Multi-Agent Particle Environment, and Light Aircraft Game. Results show that HASAC consistently outperforms strong baselines, exhibiting better sample efficiency, robustness, and sufficient exploration.",
      "authors": [
        "Jiarong Liu",
        "Yifan Zhong",
        "Siyi Hu",
        "Haobo Fu",
        "QIANG FU",
        "Xiaojun Chang",
        "Yaodong Yang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tmqOhBC4a5",
      "cdate": 1695389604435,
      "mdate": 1709898624441,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710734"
    },
    {
      "id": "99tKiMVJhY",
      "title": "Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior",
      "abstract": "Recent reinforcement learning (RL) methods have achieved success in various domains. However, multi-agent RL (MARL) remains a challenge in terms of decentralization, partial observability and scalability to many agents. Meanwhile, collective behavior requires resolution of the aforementioned challenges, and remains of importance to many state-of-the-art applications such as active matter physics, self-organizing systems, opinion dynamics, and biological or robotic swarms. Here, MARL via mean field control (MFC) offers a potential solution to scalability, but fails to consider decentralized and partially observable systems. In this paper, we enable decentralized behavior of agents under partial information by proposing novel models for decentralized partially observable MFC (Dec-POMFC), a broad class of problems with permutation-invariant agents allowing for reduction to tractable single-agent Markov decision processes (MDP) with single-agent RL solution. We provide rigorous theoretical results, including a dynamic programming principle, together with optimality guarantees for Dec-POMFC solutions applied to finite swarms of interest. Algorithmically, we propose Dec-POMFC-based policy gradient methods for MARL via centralized training and decentralized execution, together with policy gradient approximation guarantees. In addition, we improve upon state-of-the-art histogram-based MFC by kernel methods, which is of separate interest also for fully observable MFC. We evaluate numerically on representative collective behavior tasks such as adapted Kuramoto and Vicsek swarming models, being on par with state-of-the-art MARL. Overall, our framework takes a step towards RL-based engineering of artificial collective behavior via MFC.",
      "authors": [
        "Kai Cui",
        "Sascha H. Hauck",
        "Christian Fabian",
        "Heinz Koeppl"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=99tKiMVJhY",
      "cdate": 1695389395491,
      "mdate": 1709661522402,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710739"
    },
    {
      "id": "k1wlmtPGLq",
      "title": "TAB: Temporal Accumulated Batch Normalization in Spiking Neural Networks",
      "abstract": "Spiking Neural Networks (SNNs) are attracting growing interest for their energy-efficient computing when implemented on neuromorphic hardware. However, directly training SNNs, even adopting batch normalization (BN), is highly challenging due to their non-differentiable activation function and the temporally delayed accumulation of outputs over time.\n    For SNN training, this temporal accumulation gives rise to Temporal Covariate Shifts (TCS) along the temporal dimension, a phenomenon that would become increasingly pronounced with layer-wise computations across multiple layers and multiple time-steps. \n    In this paper, we introduce TAB (Temporal Accumulated Batch Normalization), a novel SNN batch normalization method that addresses the temporal covariate shift issue by aligning with neuron dynamics (specifically the accumulated membrane potential) and utilizing temporal accumulated statistics for data normalization. \n    Within its framework, TAB effectively encapsulates the historical temporal dependencies that underlie the membrane potential accumulation process, thereby establishing a natural connection between neuron dynamics and TAB batch normalization. \n    Experimental results on CIFAR-10, CIFAR-100, and DVS-CIFAR10 show that our TAB method outperforms other state-of-the-art methods.",
      "authors": [
        "Haiyan Jiang",
        "Vincent Zoonekynd",
        "Giulia De Masi",
        "Bin Gu",
        "Huan Xiong"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=k1wlmtPGLq",
      "cdate": 1695389280717,
      "mdate": 1713672308831,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710743"
    },
    {
      "id": "BBD6KXIGJL",
      "title": "Hybrid Directional Graph Neural Network for Molecules",
      "abstract": "Equivariant message passing neural networks have emerged as the prevailing approach for predicting chemical properties of molecules due to their ability to leverage translation and rotation symmetries, resulting in a strong inductive bias. However, the equivariant operations in each layer can impose excessive constraints on the function form and network flexibility. To address these challenges, we introduce a novel network called the Hybrid Directional Graph Neural Network (HDGNN), which effectively combines strictly equivariant operations with learnable modules. We evaluate the performance of HDGNN on the QM9 dataset and the IS2RE dataset of OC20, demonstrating its state-of-the-art performance on several tasks and competitive performance on others. Our code is anonymously released on https://github.com/ajy112/HDGNN.",
      "authors": [
        "Junyi An",
        "Chao Qu",
        "Zhipeng Zhou",
        "Fenglei Cao",
        "Xu Yinghui",
        "Yuan Qi",
        "Furao Shen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BBD6KXIGJL",
      "cdate": 1695389074115,
      "mdate": 1710426984568,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710748"
    },
    {
      "id": "yVJd8lKyVX",
      "title": "Hybrid Sharing for Multi-Label Image Classification",
      "abstract": "Existing multi-label classification methods have long suffered from label heterogeneity, where learning a label obscures another. By modeling multi-label classification as a multi-task problem, this issue can be regarded as a negative transfer, which indicates challenges to achieve simultaneously satisfied performance across multiple tasks. In this work, we propose the Hybrid Sharing Query (HSQ), a transformer-based model that introduces the mixture-of-experts architecture to image multi-label classification. HSQ is designed to leverage label correlations while mitigating heterogeneity effectively. To this end, HSQ is incorporated with a fusion expert framework that enables it to optimally combine the strengths of task-specialized experts with shared experts, ultimately enhancing multi-label classification performance across most labels. Extensive experiments are conducted on two benchmark datasets, with the results demonstrating that the proposed method achieves state-of-the-art performance and yields simultaneous improvements across most labels. The code is available at https://github.com/zihao-yin/HSQ",
      "authors": [
        "Zihao Yin",
        "Chen Gan",
        "Kelei He",
        "Yang Gao",
        "Junfeng Zhang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=yVJd8lKyVX",
      "cdate": 1695388733376,
      "mdate": 1710339702190,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710752"
    },
    {
      "id": "e1vqloonRy",
      "title": "Symmetric Single Index Learning",
      "abstract": "Few neural architectures lend themselves to provable learning with gradient based methods. One popular model is the single-index model, in which labels are produced by composing an unknown linear projection with a possibly unknown scalar link function. Learning this model with SGD is relatively well-understood, whereby the so-called information exponent of the link function governs a polynomial sample complexity rate.  However, extending this analysis to deeper or more complicated architectures remains challenging.\n\nIn this work, we consider single index learning in the setting of symmetric neural networks.  Under analytic assumptions on the activation and maximum degree assumptions on the link function, we prove that gradient flow recovers the hidden planted direction, represented as a finitely supported vector in the feature space of power sum polynomials.  We characterize a notion of information exponent adapted to our setting that controls the efficiency of learning.",
      "authors": [
        "Aaron Zweig",
        "Joan Bruna"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=e1vqloonRy",
      "cdate": 1695388707057,
      "mdate": 1709924255079,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710757"
    },
    {
      "id": "ey3GhWXQ97",
      "title": "Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity",
      "abstract": "We theoretically explore the relationship between sample-efficiency and adaptivity in reinforcement learning. An algorithm is sample-efficient if it uses a number of queries $n$ to the environment that is polynomial in the dimension $d$ of the problem. Adaptivity refers to the frequency at which queries are sent and feedback is processed to update the querying strategy. To investigate this interplay, we employ a learning framework that allows sending queries in $K$ batches, with feedback being processed and queries updated after each batch. This model encompasses the whole adaptivity spectrum, ranging from non-adaptive `offline' ($K=1$) to fully adaptive ($K=n$) scenarios, and regimes in between. For the problems of policy evaluation and best-policy identification under $d$-dimensional linear function approximation, we establish $\\Omega(\\log \\log d)$ lower bounds on the number of batches $K$ required for sample-efficient algorithms with $n = O(poly(d))$ queries. Our results show that just having adaptivity ($K>1$) does not necessarily guarantee sample-efficiency. Notably, the adaptivity-boundary for sample-efficiency is not between offline reinforcement learning ($K=1$), where sample-efficiency was known to not be possible, and adaptive settings. Instead, the boundary lies between different regimes of adaptivity and depends on the problem dimension.",
      "authors": [
        "Emmeran Johnson",
        "Ciara Pike-Burke",
        "Patrick Rebeschini"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ey3GhWXQ97",
      "cdate": 1695388638170,
      "mdate": 1711986071881,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710761"
    },
    {
      "id": "VtmBAGCN7o",
      "title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
      "abstract": "Recently, remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Previous LLM-based multi-agent systems can already solve simple dialogue tasks. More complex tasks, however, face challenges through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors.  MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together.  On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems.",
      "authors": [
        "Sirui Hong",
        "Mingchen Zhuge",
        "Jonathan Chen",
        "Xiawu Zheng",
        "Yuheng Cheng",
        "Jinlin Wang",
        "Ceyao Zhang",
        "Zili Wang",
        "Steven Ka Shing Yau",
        "Zijuan Lin",
        "Liyang Zhou",
        "Chenyu Ran",
        "Lingfeng Xiao",
        "Chenglin Wu",
        "Jürgen Schmidhuber"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=VtmBAGCN7o",
      "cdate": 1695388572615,
      "mdate": 1713672554320,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710766"
    },
    {
      "id": "FddFxi08J3",
      "title": "On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters",
      "abstract": "Seminal research in the field of graph neural networks (GNNs) has revealed a direct correspondence between the expressive capabilities of GNNs and the $k$-dimensional \nWeisfeiler-Leman ($k$WL) test, a widely-recognized method for verifying graph isomorphism. This connection has reignited interest in comprehending the specific graph properties effectively distinguishable by the $k$WL test.\nA central focus of research in this field revolves around determining the least dimensionality $k$, for which $k$WL can discern graphs with different number of occurrences of a pattern graph $p$. We refer to such a least $k$ as the WL-dimension of this pattern counting problem. This inquiry traditionally delves into two distinct counting problems related to patterns: subgraph counting and induced subgraph counting. Intriguingly, despite their initial appearance as separate challenges with seemingly divergent approaches, both of these problems are interconnected components of a more comprehensive problem: \"graph motif parameters\". In this paper, we provide a precise characterization of the WL-dimension of labeled graph motif parameters. As specific instances of this result, we obtain characterizations of the WL-dimension of the subgraph counting and induced subgraph counting problem for every labeled pattern $p$. Particularly noteworthy is our resolution of a problem left open in previous work \nconcerning induced copies.\nWe additionally demonstrate that in cases where the $k$WL test distinguishes between graphs with varying occurrences of a pattern $p$, the exact number of occurrences of $p$ can be computed uniformly using only local information of the last layer of a corresponding GNN.\nWe finally delve into the challenge of recognizing the WL-dimension of various graph parameters. We give a polynomial time algorithm for determining the WL-dimension of the subgraph counting problem for given pattern $p$, answering an open question from previous work.\nWe additionally show how to utilize deep results from the field of graph motif parameters, together with our characterization, to determine the WL-dimension of induced subgraph counting and counting $k$-graphlets.",
      "authors": [
        "Matthias Lanzinger",
        "Pablo Barcelo"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FddFxi08J3",
      "cdate": 1695388539824,
      "mdate": 1711620483424,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710773"
    },
    {
      "id": "uWVC5FVidc",
      "title": "Unbiased Watermark for Large Language Models",
      "abstract": "The recent advancements in large language models (LLMs) have sparked a growing apprehension regarding the potential misuse. One approach to mitigating this risk is to incorporate watermarking techniques into LLMs, allowing for the tracking and attribution of model outputs. This study examines a crucial aspect of watermarking: how significantly watermarks impact the quality of model-generated outputs. Previous studies have suggested a trade-off between watermark strength and output quality. However, our research demonstrates that it is possible to integrate watermarks without affecting the output probability distribution with appropriate implementation. We refer to this type of watermark as an unbiased watermark. This has significant implications for the use of LLMs, as it becomes impossible for users to discern whether a service provider has incorporated watermarks or not. Furthermore, the presence of watermarks does not compromise the performance of the model in downstream tasks, ensuring that the overall utility of the language model is preserved. Our findings contribute to the ongoing discussion around responsible AI development, suggesting that unbiased watermarks can serve as an effective means of tracking and attributing model outputs without sacrificing output quality.",
      "authors": [
        "Zhengmian Hu",
        "Lichang Chen",
        "Xidong Wu",
        "Yihan Wu",
        "Hongyang Zhang",
        "Heng Huang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=uWVC5FVidc",
      "cdate": 1695388503672,
      "mdate": 1712289008164,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710778"
    },
    {
      "id": "EriR6Ec69a",
      "title": "Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control",
      "abstract": "Developing autonomous agents that can interact with changing environments is an open challenge in machine learning. Robustness is particularly important in these settings as agents are often fit offline on expert demonstrations but deployed online where they must generalize to the closed feedback loop within the environment. In this work, we explore the application of recurrent neural networks to tasks of this nature and understand how a parameterization of their recurrent connectivity influences robustness in closed-loop settings. Specifically, we represent the recurrent connectivity as a function of rank and sparsity and show both theoretically and empirically that modulating these two variables has desirable effects on network dynamics. The proposed low-rank, sparse connectivity induces an interpretable prior on the network that proves to be most amenable for a class of models known as closed-form continuous-time neural networks (CfCs). We find that CfCs with fewer parameters can outperform their full-rank, fully-connected counterparts in the online setting under distribution shift. This yields memory-efficient and robust agents while opening a new perspective on how we can modulate network dynamics through connectivity.",
      "authors": [
        "Neehal Tumma",
        "Mathias Lechner",
        "Noel Loo",
        "Ramin Hasani",
        "Daniela Rus"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EriR6Ec69a",
      "cdate": 1695388443633,
      "mdate": 1709661521829,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710782"
    },
    {
      "id": "M3QXCOTTk4",
      "title": "The Curse of Diversity in Ensemble-Based Exploration",
      "abstract": "We uncover a surprising phenomenon in deep reinforcement learning: training a diverse ensemble of data-sharing agents -- a well-established exploration strategy -- can significantly impair the performance of the individual ensemble members when compared to standard single-agent training. Through careful analysis, we attribute the degradation in performance to the low proportion of self-generated data in the shared training data for each ensemble member, as well as the inefficiency of the individual ensemble members to learn from such highly off-policy data. We thus name this phenomenon *the curse of diversity*. We find that several intuitive solutions -- such as a larger replay buffer or a smaller ensemble size -- either fail to consistently mitigate the performance loss or undermine the advantages of ensembling. Finally, we demonstrate the potential of representation learning to counteract the curse of diversity with a novel method named Cross-Ensemble Representation Learning (CERL) in both discrete and continuous control domains. Our work offers valuable insights into an unexpected pitfall in ensemble-based exploration and raises important caveats for future applications of similar approaches.",
      "authors": [
        "Zhixuan Lin",
        "Pierluca D'Oro",
        "Evgenii Nikishin",
        "Aaron Courville"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=M3QXCOTTk4",
      "cdate": 1695388094062,
      "mdate": 1711241378199,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710798"
    },
    {
      "id": "bbCL5aRjUx",
      "title": "Multilinear Operator Networks",
      "abstract": "Despite the remarkable capabilities of deep neural networks in image recognition, the dependence on activation functions remains a largely unexplored area and has yet to be eliminated. On the other hand, Polynomial Networks is a class of models that does not require activation functions, but have yet to perform on par with modern architectures. In this work, we aim close this gap and propose MONet, which relies *solely* on multilinear operators. The core layer of MONet, called Mu-Layer, captures multiplicative interactions of the elements of the input token. MONet captures high-degree interactions of the input elements and we demonstrate the efficacy of our approach on a series of image recognition and scientific computing benchmarks. The proposed model outperforms prior polynomial networks and performs on par with modern architectures. We believe that MONet can inspire further research on models that use entirely multilinear operations.",
      "authors": [
        "Yixin Cheng",
        "Grigorios Chrysos",
        "Markos Georgopoulos",
        "Volkan Cevher"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=bbCL5aRjUx",
      "cdate": 1695387849595,
      "mdate": 1710431462634,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710805"
    },
    {
      "id": "r65xfUb76p",
      "title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
      "abstract": "Large language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. Instruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original LLMs by large margins in downstream applications. In this paper, we explore targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction. Using named entity recognition (NER) for case study, we show how ChatGPT can be distilled into much smaller UniversalNER models for open NER. For evaluation, we assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance. Without using any direct supervision, UniversalNER attains remarkable NER accuracy across tens of thousands of entity types, outperforming general instruction-tuned models such as Alpaca and Vicuna by over 30 absolute F1 points in average. With a tiny fraction of parameters, UniversalNER not only acquires ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute F1 points in average. Remarkably, UniversalNER even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE, which uses supervised NER examples. We also conduct thorough ablation studies to assess the impact of various components in our distillation approach. We release the distillation recipe, data, and UniversalNER models to facilitate future research on targeted distillation.",
      "authors": [
        "Wenxuan Zhou",
        "Sheng Zhang",
        "Yu Gu",
        "Muhao Chen",
        "Hoifung Poon"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=r65xfUb76p",
      "cdate": 1695387746826,
      "mdate": 1709967590061,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710810"
    },
    {
      "id": "W8S8SxS9Ng",
      "title": "Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data",
      "abstract": "State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking data into an auto-regressive spatiotemporal generation problem. Neuroformer is a multimodal, multitask generative pre-trained transformer (GPT) model that is specifically designed to handle the intricacies of data in systems neuroscience. It scales linearly with feature size, can process an arbitrary number of modalities, and is adaptable to downstream tasks, such as predicting behavior. We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, including direction. When pretrained to decode neural responses, the model predicted the behavior of a mouse with only few-shot fine-tuning, suggesting that the model begins learning how to do so directly from the neural representations themselves, without any explicit supervision. We used an ablation study to show that joint training on neuronal responses and behavior boosted performance, highlighting the model's ability to associate behavioral and neural representations in an unsupervised manner. These findings show that Neuroformer can analyze neural datasets and their emergent properties, informing the development of models and hypotheses associated with the brain.",
      "authors": [
        "Antonis Antoniades",
        "Yiyi Yu",
        "Joe S Canzano",
        "William Yang Wang",
        "Spencer Smith"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=W8S8SxS9Ng",
      "cdate": 1695387695380,
      "mdate": 1710540567614,
      "matched_keywords": [
        "multimodal",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710815"
    },
    {
      "id": "vy42bYs1Wo",
      "title": "Off-Policy Primal-Dual Safe Reinforcement Learning",
      "abstract": "Primal-dual safe RL methods commonly perform iterations between the primal update of the policy and the dual update of the Lagrange Multiplier. Such a training paradigm is highly susceptible to the error in cumulative cost estimation since this estimation serves as the key bond connecting the primal and dual update processes. We show that this problem causes significant underestimation of cost when using off-policy methods, leading to the failure to satisfy the safety constraint. To address this issue, we propose conservative policy optimization, which learns a policy in a constraint-satisfying area by considering the uncertainty in cost estimation. This improves constraint satisfaction but also potentially hinders reward maximization. We then introduce local policy convexification to help eliminate such suboptimality by gradually reducing the estimation uncertainty. We provide theoretical interpretations of the joint coupling effect of these two ingredients and further verify them by extensive experiments. Results on benchmark tasks show that our method not only achieves an asymptotic performance comparable to state-of-the-art on-policy methods while using much fewer samples, but also significantly reduces constraint violation during training. Our code is available at https://github.com/ZifanWu/CAL.",
      "authors": [
        "Zifan Wu",
        "Bo Tang",
        "Qian Lin",
        "Chao Yu",
        "Shangqin Mao",
        "Qianlong Xie",
        "Xingxing Wang",
        "Dong Wang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vy42bYs1Wo",
      "cdate": 1695387643656,
      "mdate": 1712544095937,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710819"
    },
    {
      "id": "wPhbtwlCDa",
      "title": "STARC: A General Framework For Quantifying Differences Between Reward Functions",
      "abstract": "In order to solve a task using reinforcement learning, it is necessary to first formalise the goal of that task as a *reward function*. However, for many real-world tasks, it is very difficult to manually specify a reward function that never incentivises undesirable behaviour. As a result, it is increasingly popular to use *reward learning algorithms*, which attempt to *learn* a reward function from data. However, the theoretical foundations of reward learning are not yet well-developed. In particular, it is typically not known when a given reward learning algorithm with high probability will learn a reward function that is safe to optimise. This means that reward learning algorithms generally must be evaluated empirically, which is expensive, and that their failure modes are difficult to anticipate in advance. One of the roadblocks to deriving better theoretical guarantees is the lack of good methods for *quantifying* the difference between reward functions. In this paper we provide a solution to this problem, in the form of a class of pseudometrics on the space of all reward functions that we call STARC (STAndardised Reward Comparison) metrics. We show that STARC metrics induce both an upper and a lower bound on worst-case regret, which implies that our metrics are tight, and that any metric with the same properties must be bilipschitz equivalent to ours. Moreover, we also identify a number of issues with reward metrics proposed by earlier works. Finally, we evaluate our metrics empirically, to demonstrate their practical efficacy. STARC metrics can be used to make both theoretical and empirical analysis of reward learning algorithms both easier and more principled.",
      "authors": [
        "Joar Max Viktor Skalse",
        "Lucy Farnik",
        "Sumeet Ramesh Motwani",
        "Erik Jenner",
        "Adam Gleave",
        "Alessandro Abate"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=wPhbtwlCDa",
      "cdate": 1695387058977,
      "mdate": 1712567738890,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710824"
    },
    {
      "id": "AU2gS9ut61",
      "title": "A differentiable brain simulator bridging brain simulation and brain-inspired computing",
      "abstract": "Brain simulation builds dynamical models to mimic the structure and functions of the brain, while brain-inspired computing (BIC) develops intelligent systems by learning from the structure and functions of the brain. The two fields are intertwined and should share a common programming framework to facilitate each other's development. However, none of the existing software in the fields can achieve this goal, because traditional brain simulators lack differentiability for training, while existing deep learning (DL) frameworks fail to capture the biophysical realism and complexity of brain dynamics. In this paper, we introduce BrainPy, a differentiable brain simulator developed using JAX and XLA, with the aim of bridging the gap between brain simulation and BIC. BrainPy expands upon the functionalities of JAX, a powerful AI framework, by introducing complete capabilities for flexible, efficient, and scalable brain simulation. It offers a range of sparse and event-driven operators for efficient and scalable brain simulation, an abstraction for managing the intricacies of synaptic computations, a modular and flexible interface for constructing multi-scale brain models, and an object-oriented just-in-time compilation approach to handle the memory-intensive nature of brain dynamics. We showcase the efficiency and scalability of BrainPy on benchmark tasks, and highlight its differentiable simulation for biologically plausible spiking models.",
      "authors": [
        "Chaoming Wang",
        "Tianqiu Zhang",
        "Sichao He",
        "Hongyaoxing Gu",
        "Shangyang Li",
        "Si Wu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=AU2gS9ut61",
      "cdate": 1695386995358,
      "mdate": 1713672933133,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710828"
    },
    {
      "id": "zwU9scoU4A",
      "title": "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
      "abstract": "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
      "authors": [
        "Christian Fabian",
        "Kai Cui",
        "Heinz Koeppl"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=zwU9scoU4A",
      "cdate": 1695386194798,
      "mdate": 1709661521075,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710838"
    },
    {
      "id": "DjzvJCRsVf",
      "title": "CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction",
      "abstract": "Open-vocabulary dense prediction tasks including object detection and image segmentation have been advanced by the success of Contrastive Language-Image Pre-training (CLIP). CLIP models, particularly those incorporating vision transformers (ViTs), have exhibited remarkable generalization ability in zero-shot image classification. However, when transferring the vision-language alignment of CLIP from global image representation to local region representation for the open-vocabulary dense prediction tasks, CLIP ViTs suffer from the domain shift from full images to local image regions. In this paper, we embark on an in-depth analysis of the region-language alignment in CLIP models, which is essential for downstream open-vocabulary dense prediction tasks. Subsequently, we propose an approach named CLIPSelf, which adapts the image-level recognition ability of CLIP ViT to local image regions without needing any region-text pairs. CLIPSelf empowers ViTs to distill itself by aligning a region representation extracted from its dense feature map with the image-level representation of the corresponding image crop. With the enhanced CLIP ViTs, we achieve new state-of-the-art performance on open-vocabulary object detection, semantic segmentation, and panoptic segmentation across various benchmarks. Models and code are released at https://github.com/wusize/CLIPSelf.",
      "authors": [
        "Size Wu",
        "Wenwei Zhang",
        "Lumin Xu",
        "Sheng Jin",
        "Xiangtai Li",
        "Wentao Liu",
        "Chen Change Loy"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DjzvJCRsVf",
      "cdate": 1695386168997,
      "mdate": 1709661520928,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710843"
    },
    {
      "id": "QzTpTRVtrP",
      "title": "Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI",
      "abstract": "The current electroencephalogram (EEG) based deep learning models are typically designed for specific datasets and applications in brain-computer interaction (BCI), limiting the scale of the models and thus diminishing their perceptual capabilities and generalizability. Recently, Large Language Models (LLMs) have achieved unprecedented success in text processing, prompting us to explore the capabilities of Large EEG Models (LEMs). We hope that LEMs can break through the limitations of different task types of EEG datasets, and obtain universal perceptual capabilities of EEG signals through unsupervised pre-training. Then the models can be fine-tuned for different downstream tasks. However, compared to text data, the volume of EEG datasets is generally small and the format varies widely. For example, there can be mismatched numbers of electrodes, unequal length data samples, varied task designs, and low signal-to-noise ratio. To overcome these challenges, we propose a unified foundation model for EEG called Large Brain Model (LaBraM). LaBraM enables cross-dataset learning by segmenting the EEG signals into EEG channel patches. Vector-quantized neural spectrum prediction is used to train a semantically rich neural tokenizer that encodes continuous raw EEG channel patches into compact neural codes. We then pre-train neural Transformers by predicting the original neural codes for the masked EEG channel patches. The LaBraMs were pre-trained on about 2,500 hours of various types of EEG signals from around 20 datasets and validated on multiple different types of downstream tasks. Experiments on abnormal detection, event type classification, emotion recognition, and gait prediction show that our LaBraM outperforms all compared SOTA methods in their respective fields. Our code is available at https://github.com/935963004/LaBraM.",
      "authors": [
        "Weibang Jiang",
        "Liming Zhao",
        "Bao-liang Lu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=QzTpTRVtrP",
      "cdate": 1695386059400,
      "mdate": 1712475935304,
      "matched_keywords": [
        "large language model",
        "foundation model",
        "transformer",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710848"
    },
    {
      "id": "k5THrhXDV3",
      "title": "Deep Generative Clustering with Multimodal Diffusion Variational Autoencoders",
      "abstract": "Multimodal VAEs have recently gained significant attention as generative models for weakly-supervised learning with multiple heterogeneous modalities. In parallel, VAE-based methods have been explored as probabilistic approaches for clustering tasks. At the intersection of these two research directions, we propose a novel multimodal VAE model in which the latent space is extended to learn data clusters, leveraging shared information across modalities. Our experiments show that our proposed model improves generative performance over existing multimodal VAEs, particularly for unconditional generation. Furthermore, we propose a post-hoc procedure to automatically select the number of true clusters thus mitigating critical limitations of previous clustering frameworks. Notably, our method favorably compares to alternative clustering approaches, in weakly-supervised settings. Finally, we integrate recent advancements in diffusion models into the proposed method to improve generative quality for real-world images.",
      "authors": [
        "Emanuele Palumbo",
        "Laura Manduchi",
        "Sonia Laguna",
        "Daphné Chopard",
        "Julia E Vogt"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=k5THrhXDV3",
      "cdate": 1695386012484,
      "mdate": 1710588555239,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.710852"
    },
    {
      "id": "AY6aM13gGF",
      "title": "Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning",
      "abstract": "Offline reinforcement learning (RL) aims to find a near-optimal policy using pre-collected datasets. Given recent advances in Large Language Models (LLMs) and their few-shot learning prowess, this paper introduces $\\textbf{La}$nguage Models for $\\textbf{Mo}$tion Control ($\\textbf{LaMo}$), a general framework based on Decision Transformers to effectively use pre-trained Language Models (LMs) for offline RL. Our framework highlights four crucial components: (1)  Initializing Decision Transformers with sequentially pre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to full-weight fine-tuning, to combine the pre-trained knowledge from LMs and in-domain knowledge effectively, (3) using the non-linear MLP transformation instead of linear projections, to generate embeddings, and (4) integrating an auxiliary language prediction loss during fine-tuning to stabilize the LMs and retain their original abilities on languages. Empirical results indicate $\\textbf{LaMo}$ achieves state-of-the-art performance in sparse-reward tasks and closes the gap between value-based offline RL methods and decision transformers in dense-reward tasks. In particular, our method demonstrates superior performance in scenarios with limited data samples.",
      "authors": [
        "Ruizhe Shi",
        "Yuyao Liu",
        "Yanjie Ze",
        "Simon Shaolei Du",
        "Huazhe Xu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=AY6aM13gGF",
      "cdate": 1695385953726,
      "mdate": 1712087463084,
      "matched_keywords": [
        "large language model",
        "reinforcement learning",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710856"
    },
    {
      "id": "vrBVFXwAmi",
      "title": "Towards LLM4QPE: Unsupervised Pretraining of Quantum Property Estimation and A Benchmark",
      "abstract": "Estimating the properties of quantum systems such as quantum phase has been critical in addressing the essential quantum many-body problems in physics and chemistry. Deep learning models have been recently introduced to property estimation, surpassing  conventional statistical approaches. However, these methods are tailored to the specific task and quantum data at hand. It remains an open and attractive question for devising a more universal task-agnostic pretraining model for quantum property estimation. In this paper, we propose LLM4QPE, a large language model style quantum task-agnostic pretraining and finetuning paradigm that 1) performs unsupervised pretraining on diverse quantum systems with different physical conditions; 2) uses the pretrained model for supervised finetuning and delivers high performance with limited training data, on downstream tasks. It mitigates the cost for quantum data collection and speeds up convergence. Extensive experiments show the promising efficacy of LLM4QPE in various tasks including classifying quantum phases of matter on Rydberg atom model and predicting two-body correlation function on anisotropic Heisenberg model.",
      "authors": [
        "Yehui Tang",
        "Hao Xiong",
        "Nianzu Yang",
        "Tailong Xiao",
        "Junchi Yan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vrBVFXwAmi",
      "cdate": 1695385828590,
      "mdate": 1713672090784,
      "matched_keywords": [
        "large language model",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710861"
    },
    {
      "id": "BIveOmD1Nh",
      "title": "Equivariant Scalar Fields for Molecular Docking with Fast Fourier Transforms",
      "abstract": "Molecular docking is critical to structure-based virtual screening, yet the throughput of such workflows is limited by the expensive optimization of scoring functions involved in most docking algorithms. We explore how machine learning can accelerate this process by learning a scoring function with a functional form that allows for more rapid optimization. Specifically, we define the scoring function to be the cross-correlation of multi-channel ligand and protein scalar fields parameterized by equivariant graph neural networks, enabling rapid optimization over rigid-body degrees of freedom with fast Fourier transforms. The runtime of our approach can be amortized at several levels of abstraction, and is particularly favorable for virtual screening settings with a common binding pocket. We benchmark our scoring functions on two simplified docking-related tasks: decoy pose scoring and rigid conformer docking. Our method attains similar but faster performance on crystal structures compared to the widely-used Vina and Gnina scoring functions, and is more robust on computationally predicted structures. Code is available at https://github.com/bjing2016/scalar-fields.",
      "authors": [
        "Bowen Jing",
        "Tommi S. Jaakkola",
        "Bonnie Berger"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BIveOmD1Nh",
      "cdate": 1695385732121,
      "mdate": 1713672912647,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710865"
    },
    {
      "id": "fh8EYKFKns",
      "title": "The Alignment Problem from a Deep Learning Perspective",
      "abstract": "AI systems based on deep learning have reached or surpassed human performance in a range of narrow domains. In coming years or decades, artificial general intelligence (AGI) may surpass human capabilities at many critical tasks. In this position paper, we examine the technical difficulty of fine-tuning hypothetical AGI systems based on pretrained deep models to pursue goals that are aligned with human interests. We argue that, if trained like today's most capable models, AGI systems could learn to act deceptively to receive higher reward, learn internally-represented goals which generalize beyond their fine-tuning distributions, and pursue those goals using power-seeking strategies. We review emerging evidence for these properties. AGIs with these properties would be difficult to align and may appear aligned even when they are not.",
      "authors": [
        "Richard Ngo",
        "Lawrence Chan",
        "Sören Mindermann"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fh8EYKFKns",
      "cdate": 1695385690559,
      "mdate": 1713672396443,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710870"
    },
    {
      "id": "MJJcs3zbmi",
      "title": "Discovering Temporally-Aware Reinforcement Learning Algorithms",
      "abstract": "Recent advancements in meta-learning have enabled the automatic discovery of novel reinforcement learning algorithms parameterized by surrogate objective functions. To improve upon manually designed algorithms, the parameterization of this learned objective function must be expressive enough to represent novel principles of learning (instead of merely recovering already established ones) while still generalizing to a wide range of settings outside of its meta-training distribution. However, existing methods focus on discovering objective functions that, like many widely used objective functions in reinforcement learning, do not take into account the total number of steps allowed for training, or “training horizon”. In contrast, humans use a plethora of different learning objectives across the course of acquiring a new ability. For instance, students may alter their studying techniques based on the proximity to exam deadlines and their self-assessed capabilities. This paper contends that ignoring the optimization time horizon significantly restricts the expressive potential of discovered learning algorithms. We propose a simple augmentation to two existing objective discovery approaches that allows the discovered algorithm to dynamically update its objective function throughout the agent’s training procedure, resulting in expressive schedules and increased generalization across different training horizons. In the process, we find that commonly used meta-gradient approaches fail to discover such adaptive objective functions while evolution strategies discover highly dynamic learning rules. We demonstrate the effectiveness of our approach on a wide range of tasks and analyze the resulting learned algorithms, which we find effectively balance exploration and exploitation by modifying the structure of their learning rules throughout the agent’s lifetime.",
      "authors": [
        "Matthew Thomas Jackson",
        "Chris Lu",
        "Louis Kirsch",
        "Robert Tjarko Lange",
        "Shimon Whiteson",
        "Jakob Nicolaus Foerster"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MJJcs3zbmi",
      "cdate": 1695385466135,
      "mdate": 1709661520597,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710874"
    },
    {
      "id": "F7QnIKlC1N",
      "title": "GTMGC: Using Graph Transformer to Predict Molecule’s Ground-State Conformation",
      "abstract": "The ground-state conformation of a molecule is often decisive for its properties. However, experimental or computational methods, such as density functional theory (DFT), are time-consuming and labor-intensive for obtaining this conformation. Deep learning (DL) based molecular representation learning (MRL) has made significant advancements in molecular modeling and has achieved remarkable results in various tasks. Consequently, it has emerged as a promising approach for directly predicting the ground-state conformation of molecules. In this regard, we introduce GTMGC, a novel network based on Graph-Transformer (GT) that seamlessly predicts the spatial configuration of molecules in a 3D space from their 2D topological architecture in an end-to-end manner. Moreover, we propose a novel self-attention mechanism called Molecule Structural Residual Self-Attention (MSRSA) for molecular structure modeling. This mechanism not only guarantees high model performance and easy implementation but also lends itself well to other molecular modeling tasks. Our method has been evaluated on the Molecule3D benchmark dataset and the QM9 dataset. Experimental results demonstrate that our approach achieves remarkable performance and outperforms current state-of-the-art methods as well as the widely used open-source software RDkit.",
      "authors": [
        "Guikun Xu",
        "Yongquan Jiang",
        "PengChuan Lei",
        "Yan Yang",
        "Jim Chen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=F7QnIKlC1N",
      "cdate": 1695384763922,
      "mdate": 1709732142234,
      "matched_keywords": [
        "transformer",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710879"
    },
    {
      "id": "tMzPZTvz2H",
      "title": "Generalization of Scaled Deep ResNets in the Mean-Field Regime",
      "abstract": "Despite the widespread empirical success of ResNet, the generalization properties of deep ResNet are rarely explored beyond the lazy training regime. In this work, we investigate scaled ResNet in the limit of infinitely deep and wide neural networks, of which the gradient flow is described by a partial differential equation in the large-neural network limit, i.e., the mean-field regime. To derive the generalization bounds under this setting, our analysis necessitates a shift from the conventional time-invariant Gram matrix employed in the lazy training regime to a time-variant, distribution-dependent version. To this end, we provide a global lower bound on the minimum eigenvalue of the Gram matrix under the mean-field regime. Besides, for the traceability of the dynamic of Kullback-Leibler (KL) divergence, we establish the linear convergence of the empirical error and estimate the upper bound of the KL divergence over parameters distribution. Finally, we build the uniform convergence for generalization bound via Rademacher complexity. Our results offer new insights into the generalization ability of deep ResNet beyond the lazy training regime and contribute to advancing the understanding of the fundamental properties of deep neural networks.",
      "authors": [
        "Yihang Chen",
        "Fanghui Liu",
        "Yiping Lu",
        "Grigorios Chrysos",
        "Volkan Cevher"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tMzPZTvz2H",
      "cdate": 1695384375100,
      "mdate": 1713672142946,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710886"
    },
    {
      "id": "pxI5IPeWgW",
      "title": "ODE Discovery for Longitudinal Heterogeneous Treatment Effects Inference",
      "abstract": "Inferring unbiased treatment effects has received widespread attention in the machine learning community. In recent years, our community has proposed numerous solutions in standard settings, high-dimensional treatment settings, and even longitudinal settings. While very diverse, the solution has mostly relied on neural networks for inference and simultaneous correction of assignment bias. New approaches typically build on top of previous approaches by proposing new (or refined) architectures and learning algorithms. However, the end result—a neural-network-based inference machine—remains unchallenged. In this paper, we introduce a different type of solution in the longitudinal setting: a closed-form ordinary differential equation (ODE). While we still rely on continuous optimization to learn an ODE, the resulting inference machine is no longer a neural network. Doing so yields several advantages such as interpretability, irregular sampling, and a different set of identification assumptions. Above all, we consider the introduction of a completely new type of solution to be our most important contribution as it may spark entirely new innovations in treatment effects in general. We facilitate this by formulating our contribution as a framework that can transform any ODE discovery method into a treatment effects method.",
      "authors": [
        "Krzysztof Kacprzyk",
        "Samuel Holt",
        "Jeroen Berrevoets",
        "Zhaozhi Qian",
        "Mihaela van der Schaar"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pxI5IPeWgW",
      "cdate": 1695384355212,
      "mdate": 1713672203722,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710891"
    },
    {
      "id": "kUveo5k1GF",
      "title": "Improving equilibrium propagation without weight symmetry through Jacobian homeostasis",
      "abstract": "Equilibrium propagation (EP) is a compelling alternative to the back propagation of error algorithm (BP) for computing gradients of neural networks on biological or analog neuromorphic substrates. \nStill, the algorithm requires weight symmetry and infinitesimal equilibrium perturbations, i.e., nudges, to yield unbiased gradient estimates.\nBoth requirements are challenging to implement in physical systems.\nYet, whether and how weight asymmetry contributes to bias is unknown because, in practice, its contribution may be masked by a finite nudge. \nTo address this question, we study generalized EP, which can be formulated without weight symmetry, and analytically isolate the two sources of bias.\nFor complex-differentiable non-symmetric networks, we show that bias due to finite nudge can be avoided by estimating exact derivatives via a Cauchy integral.\nIn contrast, weight asymmetry induces residual bias  through poor alignment of EP's neuronal error vectors compared to BP resulting in low task performance.\nTo mitigate the latter issue, we present a new homeostatic objective that directly penalizes functional asymmetries of the Jacobian at the network's fixed point. \nThis homeostatic objective dramatically improves the network's ability to solve complex tasks such as ImageNet 32$\\times$32. \nOur results lay the theoretical groundwork for studying and mitigating the adverse effects of imperfections of physical networks on learning algorithms that rely on the substrate's relaxation dynamics.",
      "authors": [
        "Axel Laborieux",
        "Friedemann Zenke"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kUveo5k1GF",
      "cdate": 1695383712941,
      "mdate": 1709661520157,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710896"
    },
    {
      "id": "TjCDNssXKU",
      "title": "Learning Hierarchical World Models with Adaptive Temporal Abstractions from Discrete Latent Dynamics",
      "abstract": "Hierarchical world models can significantly improve model-based reinforcement learning (MBRL) and planning by enabling reasoning across multiple time scales. Nonetheless, the majority of state-of-the-art MBRL methods employ flat, non-hierarchical models. We propose Temporal Hierarchies from Invariant Context Kernels (THICK), an algorithm that learns a world model hierarchy via discrete latent dynamics. The lower level of THICK updates parts of its latent state sparsely in time, forming invariant contexts. The higher level exclusively predicts situations involving context changes. Our experiments demonstrate that THICK learns categorical, interpretable, temporal abstractions on the high level, while maintaining precise low-level predictions. Furthermore, we show that the emergent hierarchical predictive model seamlessly enhances the abilities of MBRL or planning methods. We believe that THICK contributes to the further development of hierarchical agents capable of more sophisticated planning and reasoning abilities.",
      "authors": [
        "Christian Gumbsch",
        "Noor Sajid",
        "Georg Martius",
        "Martin V. Butz"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=TjCDNssXKU",
      "cdate": 1695383417507,
      "mdate": 1710525588897,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710901"
    },
    {
      "id": "EGQBpkIEuu",
      "title": "Revisiting Data Augmentation in Deep Reinforcement Learning",
      "abstract": "Various data augmentation techniques have been recently proposed in image-based deep reinforcement learning (DRL).\nAlthough they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear. \nTo tackle this question, we analyze existing methods to better understand them and to uncover how they are connected.\nNotably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the effects of their different components and compare them.\nWe furthermore formulate an explanation about how these methods may be affected by choosing different data augmentation transformations in calculating the target Q-values.\nThis analysis suggests recommendations on how to exploit data augmentation in a more principled way.\nIn addition, we include a regularization term called tangent prop, previously proposed in computer vision, but whose adaptation to DRL is novel to the best of our knowledge.\nWe evaluate our proposition and validate our analysis in several domains. \nCompared to different relevant baselines,  we demonstrate that it achieves state-of-the-art performance in most environments and shows higher sample efficiency and better generalization ability in some complex environments.",
      "authors": [
        "Jianshu Hu",
        "Yunpeng Jiang",
        "Paul Weng"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EGQBpkIEuu",
      "cdate": 1695383125060,
      "mdate": 1709661520076,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710906"
    },
    {
      "id": "Djw0XhjHZb",
      "title": "Simplicial Representation Learning with Neural $k$-Forms",
      "abstract": "Geometric deep learning extends deep learning to incorporate information about the geometry and topology data, especially in complex domains like graphs. Despite the popularity of message passing in this field, it has limitations such as the need for graph rewiring, ambiguity in interpreting data, and over-smoothing. In this paper, we take a different approach, focusing on leveraging geometric information from simplicial complexes embedded in $\\mathbb{R}^n$ using node coordinates. We use differential $k$-forms in $\\mathbb{R}^n$ to create representations of simplices, offering interpretability and geometric consistency without message passing. This approach also enables us to apply differential geometry tools and achieve universal approximation. Our method is efficient, versatile, and applicable to various input complexes, including graphs, simplicial complexes, and cell complexes. It outperforms existing message passing neural networks in harnessing information from geometrical graphs with node features serving as coordinates.",
      "authors": [
        "Kelly Maggs",
        "Celia Hacker",
        "Bastian Rieck"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Djw0XhjHZb",
      "cdate": 1695382852082,
      "mdate": 1710395181337,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710912"
    },
    {
      "id": "LqaEEs3UxU",
      "title": "Sign2GPT: Leveraging Large Language Models for Gloss-Free Sign Language Translation",
      "abstract": "Automatic Sign Language Translation requires the integration of both computer vision and natural language processing to effectively bridge the communication gap between sign and spoken languages. However, the deficiency in large-scale training data to support sign language translation means we need to leverage resources from spoken language. We introduce, Sign2GPT, a novel framework for sign language translation that utilizes large-scale pretrained vision and language models via lightweight adapters for gloss-free sign language translation. The lightweight adapters are crucial for sign language translation, due to the constraints imposed by limited dataset sizes and the computational requirements when training with long sign videos.\nWe also propose a novel pretraining strategy that directs our encoder to learn sign representations from automatically extracted pseudo-glosses without requiring gloss order information or annotations.\nWe evaluate our approach on two public benchmark sign language translation datasets, namely RWTH-PHOENIX-Weather 2014T and CSL-Daily, and improve on state-of-the-art gloss-free translation performance with a significant margin.",
      "authors": [
        "Ryan Wong",
        "Necati Cihan Camgoz",
        "Richard Bowden"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LqaEEs3UxU",
      "cdate": 1695382411610,
      "mdate": 1709661519802,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710917"
    },
    {
      "id": "7wY67ZDQTE",
      "title": "Cauchy-Schwarz Divergence Information Bottleneck for Regression",
      "abstract": "The information bottleneck (IB) approach is popular to improve the generalization, robustness and explainability of deep neural networks. Essentially, it aims to find a minimum sufficient representation $\\mathbf{t}$ by striking a trade-off between a compression term $I(\\mathbf{x};\\mathbf{t})$ and a prediction term $I(y;\\mathbf{t})$, where $I(\\cdot;\\cdot)$ refers to the mutual information (MI). MI is for the IB for the most part expressed in terms of the Kullback-Leibler (KL) divergence, which in the regression case corresponds to prediction based on mean squared error (MSE) loss with Gaussian assumption and compression approximated by variational inference. \nIn this paper, we study the IB principle for the regression problem and develop a new way to parameterize the IB with deep neural networks by exploiting favorable properties of the Cauchy-Schwarz (CS) divergence. By doing so, we move away from MSE-based regression and ease estimation by avoiding variational approximations or distributional assumptions. We investigate the improved generalization ability of our proposed CS-IB and demonstrate strong adversarial robustness guarantees. We demonstrate its superior performance on six real-world regression tasks over other popular deep IB approaches. We additionally observe that the solutions discovered by CS-IB always achieve the best trade-off between prediction accuracy and compression ratio in the information plane. The code is available at \\url{https://github.com/SJYuCNEL/Cauchy-Schwarz-Information-Bottleneck}.",
      "authors": [
        "Shujian Yu",
        "Xi Yu",
        "Sigurd Løkse",
        "Robert Jenssen",
        "Jose C Principe"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7wY67ZDQTE",
      "cdate": 1695382209055,
      "mdate": 1710442877400,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710922"
    },
    {
      "id": "14rn7HpKVk",
      "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models",
      "abstract": "Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as \nautomatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Zejun MA",
        "Chao Zhang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=14rn7HpKVk",
      "cdate": 1695381949541,
      "mdate": 1712661708670,
      "matched_keywords": [
        "large language model",
        "multimodal",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710927"
    },
    {
      "id": "ZzmKEpze8e",
      "title": "Kalman Filter for Online Classification of Non-Stationary Data",
      "abstract": "In Online Continual Learning (OCL) a learning system receives a stream of data and sequentially performs prediction and training steps. Key challenges in OCL include automatic adaptation to the specific non-stationary structure of the data and maintaining appropriate  predictive uncertainty.  To address these challenges we introduce a probabilistic Bayesian online learning approach that utilizes a (possibly pretrained) neural representation and a state space model over the linear predictor weights. Non-stationarity in the linear predictor weights is modelled using a “parameter drift” transition density, parametrized by a coefficient that quantifies forgetting. Inference in the model is implemented with efficient Kalman filter recursions which track the posterior distribution over the linear weights, while online SGD updates over the transition dynamics coefficient allow for adaptation to the non-stationarity observed in the data. While the framework is developed assuming a linear Gaussian model, we extend it to deal with classification problems and for fine-tuning the deep learning representation. In a set of experiments in multi-class classification using data sets such as CIFAR-100 and CLOC we demonstrate the model's predictive ability and its flexibility in capturing non-stationarity.",
      "authors": [
        "Michalis Titsias",
        "Alexandre Galashov",
        "Amal Rannen-Triki",
        "Razvan Pascanu",
        "Yee Whye Teh",
        "Jorg Bornschein"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ZzmKEpze8e",
      "cdate": 1695381642720,
      "mdate": 1713672499364,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710932"
    },
    {
      "id": "oYjPk8mqAV",
      "title": "Magnushammer: A Transformer-Based Approach to Premise Selection",
      "abstract": "This paper presents a novel approach to premise selection, a crucial reasoning task in automated theorem proving. Traditionally, symbolic methods that rely on extensive domain knowledge and engineering effort are applied to this task. In contrast, this work demonstrates that contrastive training with the transformer architecture can achieve higher-quality retrieval of relevant premises, without the knowledge or feature engineering overhead. Our method, Magnushammer, outperforms the most advanced and widely used automation tool in interactive theorem proving called Sledgehammer. On the PISA and miniF2f benchmarks Magnushammer achieves $59.5\\%$ (against $38.3\\%$) and $34.0\\%$ (against $20.9\\%$) success rates, respectively. By combining Magnushammer with a language-model-based automated theorem prover, we further improve the state-of-the-art proof success rate from $57.0\\%$ to $71.0\\%$ on the PISA benchmark using $4$x fewer parameters. Moreover, we develop and open source a novel dataset for premise selection, containing textual representations of (proof state, relevant premise) pairs. To the best of our knowledge, this is the largest available premise selection dataset, and the first dataset of this kind for the Isabelle proof assistant.",
      "authors": [
        "Maciej Mikuła",
        "Szymon Tworkowski",
        "Szymon Antoniak",
        "Bartosz Piotrowski",
        "Albert Q. Jiang",
        "Jin Peng Zhou",
        "Christian Szegedy",
        "Łukasz Kuciński",
        "Piotr Miłoś",
        "Yuhuai Wu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=oYjPk8mqAV",
      "cdate": 1695381595936,
      "mdate": 1713181324027,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710940"
    },
    {
      "id": "L8UNn7Llt4",
      "title": "ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update",
      "abstract": "In this study, we investigate the DIstribution Correction Estimation (DICE) methods, an important line of work in offline reinforcement learning (RL) and imitation learning (IL). DICE-based methods impose state-action-level behavior constraint, which is an ideal choice for offline learning. However, they typically perform much worse than current state-of-the-art (SOTA) methods that solely use action-level behavior constraint. After revisiting DICE-based methods, we find there exist two gradient terms when learning the value function using true-gradient update: forward gradient (taken on the current state) and backward gradient (taken on the next state). Using forward gradient bears a large similarity to many offline RL methods, and thus can be regarded as applying action-level constraint. However, directly adding the backward gradient may degenerate or cancel out its effect if these two gradients have conflicting directions. To resolve this issue, we propose a simple yet effective modification that projects the backward gradient onto the normal plane of the forward gradient, resulting in an orthogonal-gradient update, a new learning rule for DICE-based methods. We conduct thorough theoretical analyses and find that the projected backward gradient brings state-level behavior regularization, which reveals the mystery of DICE-based methods: the value learning objective does try to impose state-action-level constraint, but needs to be used in a corrected way. Through toy examples and extensive experiments on complex offline RL and IL tasks, we demonstrate that DICE-based methods using orthogonal-gradient updates achieve SOTA performance and great robustness.",
      "authors": [
        "Liyuan Mao",
        "Haoran Xu",
        "Weinan Zhang",
        "Xianyuan Zhan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=L8UNn7Llt4",
      "cdate": 1695381376617,
      "mdate": 1713672744201,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710945"
    },
    {
      "id": "WhZoCLRWYJ",
      "title": "Light Schrödinger Bridge",
      "abstract": "Despite the recent advances in the field of computational Schrödinger Bridges (SB), most existing SB solvers are still heavy-weighted and require complex optimization of several neural networks. It turns out that there is no principal solver which plays the role of simple-yet-effective baseline for SB just like, e.g., $k$-means method in clustering, logistic regression in classification or Sinkhorn algorithm in discrete optimal transport. We address this issue and propose a novel fast and simple SB solver. Our development is a smart combination of two ideas which recently appeared in the field: (a) parameterization of the Schrödinger potentials with sum-exp quadratic functions and (b) viewing the log-Schrödinger potentials as the energy functions. We show that combined together these ideas yield a lightweight, simulation-free and theoretically justified SB solver with a simple straightforward optimization objective. As a result, it allows solving SB in moderate dimensions in a matter of minutes on CPU without a painful hyperparameter selection. Our light solver resembles the Gaussian mixture model which is widely used for density estimation. Inspired by this similarity, we also prove an important theoretical result showing that our light solver is a universal approximator of SBs. Furthemore, we conduct the analysis of the generalization error of our light solver. The code for our solver can be found at https://github.com/ngushchin/LightSB.",
      "authors": [
        "Alexander Korotin",
        "Nikita Gushchin",
        "Evgeny Burnaev"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=WhZoCLRWYJ",
      "cdate": 1695380270626,
      "mdate": 1710759266997,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710950"
    },
    {
      "id": "7UhxsmbdaQ",
      "title": "Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design",
      "abstract": "Generative molecular design has moved from proof-of-concept to real-world applicability, as marked by the surge in very recent papers reporting experimental validation. Key challenges in explainability and sample efficiency present opportunities to enhance generative design to directly optimize expensive high-fidelity oracles and provide actionable insights to domain experts. Here, we propose Beam Enumeration to exhaustively enumerate the most probable sub-sequences from language-based molecular generative models and show that molecular substructures can be extracted. When coupled with reinforcement learning, extracted substructures become meaningful, providing a source of explainability and improving sample efficiency through self-conditioned generation. Beam Enumeration is generally applicable to any language-based molecular generative model and notably further improves the performance of the recently reported Augmented Memory algorithm, which achieved the new state-of-the-art on the Practical Molecular Optimization benchmark for sample efficiency. The combined algorithm generates more high reward molecules and faster, given a fixed oracle budget. Beam Enumeration shows that improvements to explainability and sample efficiency for molecular design can be made synergistic.",
      "authors": [
        "Jeff Guo",
        "Philippe Schwaller"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7UhxsmbdaQ",
      "cdate": 1695379858722,
      "mdate": 1709661519362,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710958"
    },
    {
      "id": "jzzEHTBFOT",
      "title": "C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion",
      "abstract": "In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration, which is a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion (ATFD), we establish its relationship with calibration error and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT), for optimizing prompts during test-time with enhanced calibration. Through extensive experiments on different CLIP architectures and datasets, we show that C-TPT can effectively improve the calibration of test-time prompt tuning without needing labeled data. The code is publicly accessible at https://github.com/hee-suk-yoon/C-TPT.",
      "authors": [
        "Hee Suk Yoon",
        "Eunseop Yoon",
        "Joshua Tian Jin Tee",
        "Mark A. Hasegawa-Johnson",
        "Yingzhen Li",
        "Chang D. Yoo"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jzzEHTBFOT",
      "cdate": 1695379115219,
      "mdate": 1711892009983,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710964"
    },
    {
      "id": "b2UlHeyyC0",
      "title": "Retrieval-Enhanced Contrastive Vision-Text Models",
      "abstract": "Contrastive image-text models such as CLIP form the building blocks of many state-of-the-art systems. While they excel at recognizing common generic concepts, they still struggle on fine-grained entities which are rare, or even absent from the pre-training dataset. Hence, a key ingredient to their success has been the use of large-scale curated pre-training data aiming at expanding the set of concepts that they can memorize during the pre-training stage. In this work, we explore an alternative to encoding fine-grained knowledge directly into the model's parameters:  we instead train the model to retrieve this knowledge from an external memory. Specifically, we propose to equip existing vision-text models with the ability to refine their embedding with cross-modal retrieved information from a memory at inference time, which greatly improves their zero-shot predictions. Remarkably, we show that this can be done with a light-weight, single-layer, fusion transformer on top of a frozen CLIP. Our experiments validate that our retrieval-enhanced contrastive (RECO) training improves CLIP performance substantially on several challenging fine-grained tasks: for example +10.9 on Stanford Cars, +10.2 on CUB-2011 and +7.3 on the recent OVEN benchmark, where we even outperform the fine-tuned models on unseen classes.",
      "authors": [
        "Ahmet Iscen",
        "Mathilde Caron",
        "Alireza Fathi",
        "Cordelia Schmid"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=b2UlHeyyC0",
      "cdate": 1695378835993,
      "mdate": 1713672485042,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.710969"
    },
    {
      "id": "uvXK8Xk9Jk",
      "title": "DEEP NEURAL NETWORK INITIALIZATION WITH SPARSITY INDUCING ACTIVATIONS",
      "abstract": "Inducing and leveraging sparse activations during training and inference is a promising avenue for improving the computational efficiency of deep networks, which is increasingly important as network sizes continue to grow and their application becomes more widespread.  Here we use the large width Gaussian process limit to analyze the behaviour, at random initialization, of nonlinear activations that induce sparsity in the hidden outputs.  A previously unreported form of training instability is proven for arguably two of the most natural candidates for hidden layer sparsification; those being a shifted ReLU ($\\phi(x)=\\max(0, x-\\tau)$ for $\\tau\\ge 0$) and soft thresholding ($\\phi(x)=0$ for $|x|\\le\\tau$ and $x-\\text{sign}(x)\\tau$ for $|x|>\\tau$).  We show that this instability is overcome by clipping the nonlinear activation magnitude, at a level prescribed by the shape of the associated Gaussian process variance map. Numerical experiments verify the theory and show that the proposed magnitude clipped sparsifying activations can be trained with training and test fractional sparsity as high as 85\\% while retaining close to full accuracy.",
      "authors": [
        "Ilan Price",
        "Nicholas Daultry Ball",
        "Adam Christopher Jones",
        "Samuel Chun Hei Lam",
        "Jared Tanner"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=uvXK8Xk9Jk",
      "cdate": 1695378801710,
      "mdate": 1710277695389,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710973"
    },
    {
      "id": "xx0ITyHp3u",
      "title": "Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging",
      "abstract": "Neural networks can be significantly compressed by pruning, yielding sparse models with reduced storage and computational demands while preserving predictive performance. Model soups (Wortsman et al., 2022) enhance generalization and out-of-distribution (OOD) performance by averaging the parameters of multiple models into a single one, without increasing inference time. However, achieving both sparsity and parameter averaging is challenging as averaging arbitrary sparse models reduces the overall sparsity due to differing sparse connectivities. This work addresses these challenges by demonstrating that exploring a single retraining phase of Iterative Magnitude Pruning (IMP) with varied hyperparameter configurations such as batch ordering or weight decay yields models suitable for averaging, sharing identical sparse connectivity by design. Averaging these models significantly enhances generalization and OOD performance over their individual counterparts. Building on this, we introduce Sparse Model Soups (SMS), a novel method for merging sparse models by initiating each prune-retrain cycle with the averaged model from the previous phase. SMS preserves sparsity, exploits sparse network benefits, is modular and fully parallelizable, and substantially improves IMP's performance. We further demonstrate that SMS can be adapted to enhance state-of-the-art pruning-during-training approaches.",
      "authors": [
        "Max Zimmer",
        "Christoph Spiegel",
        "Sebastian Pokutta"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xx0ITyHp3u",
      "cdate": 1695377585168,
      "mdate": 1710487211781,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710978"
    },
    {
      "id": "BEyEziZ4R6",
      "title": "DP-SGD Without Clipping: The Lipschitz Neural Network Way",
      "abstract": "State-of-the-art approaches for training Differentially Private (DP) Deep Neural Networks (DNN) face difficulties to estimate tight bounds on the sensitivity of the network's layers, and instead rely on a process of per-sample gradient clipping. This clipping process not only biases the direction of gradients but also proves costly both in memory consumption and in computation. To provide sensitivity bounds and bypass the drawbacks of the clipping process, we propose to rely on Lipschitz constrained networks. Our theoretical analysis reveals an unexplored link between the Lipschitz constant with respect to their input and the one with respect to their parameters. By bounding the Lipschitz constant of each layer with respect to its parameters, we prove that we can train these networks with privacy guarantees.  Our analysis not only allows the computation of the aforementioned sensitivities at scale, but also provides guidance on how to maximize the gradient-to-noise ratio for fixed privacy guarantees. To facilitate the application of Lipschitz networks and foster robust and certifiable learning under privacy guarantees, we provide a Python package that implements building blocks allowing the construction and private training of such networks.",
      "authors": [
        "Louis Béthune",
        "Thomas Massena",
        "Thibaut Boissin",
        "Aurélien Bellet",
        "Franck Mamalet",
        "Yannick Prudent",
        "Corentin Friedrich",
        "Mathieu Serrurier",
        "David Vigouroux"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BEyEziZ4R6",
      "cdate": 1695377556691,
      "mdate": 1709661519042,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.710983"
    },
    {
      "id": "nfIAEJFiBZ",
      "title": "Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo",
      "abstract": "We present a scalable and effective exploration strategy based on Thompson sampling for reinforcement learning (RL). One of the key shortcomings of  existing Thompson sampling algorithms is the need to perform a Gaussian approximation of the posterior distribution, which is not a good surrogate in most practical settings. We instead directly sample the Q function from its posterior distribution, by using  Langevin Monte Carlo, an efficient type of Markov Chain Monte Carlo (MCMC) method. Our method only needs to perform noisy gradient descent updates to learn the exact posterior distribution of the Q function, which makes our approach easy to deploy in deep RL.  We provide a rigorous theoretical analysis for the proposed method and demonstrate that, in the linear Markov decision process (linear MDP) setting, it has a regret bound of $\\tilde{O}(d^{3/2}H^{3/2}\\sqrt{T})$, where $d$ is the dimension of the feature mapping, $H$ is the planning horizon, and $T$ is the total number of steps. We apply this approach to deep RL, by using Adam optimizer to perform gradient updates. Our approach achieves better or similar results compared with state-of-the-art deep RL algorithms on several challenging exploration tasks from the Atari57 suite.",
      "authors": [
        "Haque Ishfaq",
        "Qingfeng Lan",
        "Pan Xu",
        "A. Rupam Mahmood",
        "Doina Precup",
        "Anima Anandkumar",
        "Kamyar Azizzadenesheli"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nfIAEJFiBZ",
      "cdate": 1695377444709,
      "mdate": 1710347493936,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.710987"
    },
    {
      "id": "7W3GLNImfS",
      "title": "Human Feedback is not Gold Standard",
      "abstract": "Human feedback has become the de facto standard for evaluating the performance of Large Language Models, and is increasingly being used as a training objective. However, it is not clear which properties of a generated output this single `preference' score captures. We hypothesise that preference scores are subjective and open to undesirable biases. We critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality. We further hypothesise that both preference scores and error annotation may be affected by confounders, and leverage instruction-tuned models to generate outputs that vary along two possible confounding dimensions: assertiveness and complexity. We find that the assertiveness of an output skews the perceived rate of factuality errors, indicating that human annotations are not a fully reliable evaluation metric or training objective. Finally, we offer preliminary evidence that using human feedback as a training objective disproportionately increases the assertiveness of model outputs. We encourage future work to carefully consider whether preference scores are well aligned with the desired objective.",
      "authors": [
        "Tom Hosking",
        "Phil Blunsom",
        "Max Bartolo"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7W3GLNImfS",
      "cdate": 1695377228150,
      "mdate": 1709661518999,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.710995"
    },
    {
      "id": "vpJMJerXHU",
      "title": "ModernTCN: A Modern Pure Convolution Structure for General Time Series Analysis",
      "abstract": "Recently, Transformer-based and MLP-based models have emerged rapidly and\nwon dominance in time series analysis. In contrast, convolution is losing steam\nin time series tasks nowadays for inferior performance. This paper studies the\nopen question of how to better use convolution in time series analysis and makes\nefforts to bring convolution back to the arena of time series analysis. To this end,\nwe modernize the traditional TCN and conduct time series related modifications\nto make it more suitable for time series tasks. As the outcome, we propose\nModernTCN and successfully solve this open question through a seldom-explored\nway in time series community. As a pure convolution structure, ModernTCN still\nachieves the consistent state-of-the-art performance on five mainstream time series\nanalysis tasks while maintaining the efficiency advantage of convolution-based\nmodels, therefore providing a better balance of efficiency and performance than\nstate-of-the-art Transformer-based and MLP-based models. Our study further\nreveals that, compared with previous convolution-based models, our ModernTCN\nhas much larger effective receptive fields (ERFs), therefore can better unleash the\npotential of convolution in time series analysis. Code is available at this repository:\nhttps://github.com/luodhhh/ModernTCN.",
      "authors": [
        "Luo donghao",
        "wang xue"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vpJMJerXHU",
      "cdate": 1695376973403,
      "mdate": 1711094465056,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.711000"
    },
    {
      "id": "L3FHMoKZcS",
      "title": "Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering",
      "abstract": "Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allow it to learn the contextual bias from labeled data. We validate the effectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate state-of-the-art performance over previous calibration baselines across more than 10 natural language understanding and image classification tasks.",
      "authors": [
        "Han Zhou",
        "Xingchen Wan",
        "Lev Proleev",
        "Diana Mincu",
        "Jilin Chen",
        "Katherine A Heller",
        "Subhrajit Roy"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=L3FHMoKZcS",
      "cdate": 1695376813038,
      "mdate": 1709661518886,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.711005"
    },
    {
      "id": "if2vRbS8Ew",
      "title": "First-order ANIL provably learns representations despite overparametrisation",
      "abstract": "Due to its empirical success in few-shot classification and reinforcement learning, meta-learning has recently received significant interest. Meta-learning methods leverage data from previous tasks to learn a new task in a sample-efficient manner. In particular, model-agnostic methods look for initialization points from which gradient descent quickly adapts to any new task. Although it has been empirically suggested that such methods perform well by learning shared representations during pretraining, there is limited theoretical evidence of such behavior. More importantly, it has not been shown that these methods still learn a shared structure, despite architectural misspecifications. In this direction, this work shows, in the limit of an infinite number of tasks, that first-order ANIL with a linear two-layer network architecture successfully learns linear shared representations. This result even holds with _overparametrization_; having a width larger than the dimension of the shared representations results in an asymptotically low-rank solution. The learned solution then yields a good adaptation performance on any new task after a single gradient step. Overall, this illustrates how well model-agnostic methods such as first-order ANIL can learn shared representations.",
      "authors": [
        "Oğuz Kaan Yüksel",
        "Etienne Boursier",
        "Nicolas Flammarion"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=if2vRbS8Ew",
      "cdate": 1695376395648,
      "mdate": 1710528509821,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.711010"
    },
    {
      "id": "ZGNWW7xZ6Q",
      "title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning",
      "abstract": "Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the reasoning ability of LLMs through training but also allows seamless integration with any arbitrary LLMs during inference. Extensive experiments on two benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results.",
      "authors": [
        "LINHAO LUO",
        "Yuan-Fang Li",
        "Gholamreza Haffari",
        "Shirui Pan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ZGNWW7xZ6Q",
      "cdate": 1695376372188,
      "mdate": 1741893057563,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.711015"
    },
    {
      "id": "567BjxgaTp",
      "title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions",
      "abstract": "Large language models (LLMs) can “lie”, which we define as outputting false statements when incentivised to, despite “knowing” the truth in a demonstrable sense. LLMs might “lie”, for example, when instructed to output misinformation. Here, we develop a simple lie detector that requires neither access to the LLM’s activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM’s yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. When trained on examples from a single setting—prompting GPT-3.5 to lie about factual questions—the detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. These results indicate that LLMs have distinctive lie-related behavioural patterns, consistent across architectures and contexts, which could enable general-purpose lie detection.",
      "authors": [
        "Lorenzo Pacchiardi",
        "Alex James Chan",
        "Sören Mindermann",
        "Ilan Moscovitz",
        "Alexa Yue Pan",
        "Yarin Gal",
        "Owain Evans",
        "Jan M. Brauner"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=567BjxgaTp",
      "cdate": 1695376366929,
      "mdate": 1710518577907,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.711019"
    },
    {
      "id": "86zAUE80pP",
      "title": "CPPO: Continual Learning for Reinforcement Learning with Human Feedback",
      "abstract": "The approach of Reinforcement Learning from Human Feedback (RLHF) is widely used for enhancing pre-trained Language Models (LM), enabling them to better align with human preferences. Existing RLHF-based LMs however require complete retraining whenever new queries or feedback are introduced, as human preferences may differ across different domains or topics. LM retraining is of\u0002ten impracticable in most real-world scenarios, due to the substantial time and computational costs involved, as well as data privacy concerns. To address this limitation, we propose Continual Proximal Policy Optimization (CPPO), a novel method that is able to continually align LM with dynamic human preferences. Specifically, CPPO adopts a weighting strategy to decide which samples should be utilized for enhancing policy learning and which should be used for solidifying past experiences. This seeks a good trade-off between policy learning and knowledge retention. Our experimental results show that CPPO outperforms strong Contin\u0002uous learning (CL) baselines when it comes to consistently aligning with human preferences. Furthermore, compared to PPO, CPPO offers more efficient and stable learning in non-continual scenarios.",
      "authors": [
        "Han Zhang",
        "Yu Lei",
        "Lin Gui",
        "Min Yang",
        "Yulan He",
        "Hui Wang",
        "Ruifeng Xu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=86zAUE80pP",
      "cdate": 1695376237176,
      "mdate": 1712564258304,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.711024"
    },
    {
      "id": "BI1N3lTWtn",
      "title": "A Multi-Level Framework for Accelerating Training Transformer Models",
      "abstract": "The fast growing capabilities of large-scale deep learning models, such as Bert, GPT and ViT, are revolutionizing the landscape of NLP, CV and many other domains. Training such models, however, poses an unprecedented demand for computing power, which incurs exponentially increasing energy cost and carbon dioxide emissions. It is thus critical to develop efficient training solutions to reduce the training costs. Motivated by a set of key observations of inter- and intra-layer similarities among feature maps and attentions that can be identified from typical training processes, we propose a multi-level framework for training acceleration. Specifically, the framework is based on three basic operators, Coalescing, De-coalescing and Interpolation, which can be orchestrated to build a multi-level training framework. The framework consists of a V-cycle training process, which progressively down- and up-scales the model size and projects the parameters between adjacent levels of models via coalescing and de-coalescing. The key idea is that a smaller model that can be trained for fast convergence and the trained parameters provides high-qualities intermediate solutions for the next level larger network. The interpolation operator is designed to break the symmetry of neurons incurred by de-coalescing for better convergence performance. Our experiments on transformer-based language models (e.g. Bert, GPT) as well as a vision model (e.g. DeiT) prove that the proposed framework reduces the computational cost by about 20% on training BERT/GPT-Base models and up to 51.6% on training the BERT-Large model while preserving the performance.",
      "authors": [
        "Longwei Zou",
        "Han Zhang",
        "Yangdong Deng"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BI1N3lTWtn",
      "cdate": 1695375834375,
      "mdate": 1712458393620,
      "matched_keywords": [
        "transformer",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.711028"
    },
    {
      "id": "wZWTHU7AsQ",
      "title": "Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations",
      "abstract": "Deploying reinforcement learning (RL) systems requires robustness to uncertainty and model misspecification, yet prior robust RL methods typically only study noise introduced independently across time. However, practical sources of uncertainty are usually coupled across time.\nWe formally introduce temporally-coupled perturbations, presenting a novel challenge for existing robust RL methods. To tackle this challenge, we propose GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game. By finding an approximate equilibrium within this game, GRAD optimizes for general robustness against temporally-coupled perturbations. Experiments on continuous control tasks demonstrate that, compared with prior methods, our approach achieves a higher degree of robustness to various types of attacks on different attack domains, both in settings with temporally-coupled perturbations and decoupled perturbations.",
      "authors": [
        "Yongyuan Liang",
        "Yanchao Sun",
        "Ruijie Zheng",
        "Xiangyu Liu",
        "Benjamin Eysenbach",
        "Tuomas Sandholm",
        "Furong Huang",
        "Stephen Marcus McAleer"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=wZWTHU7AsQ",
      "cdate": 1695375279069,
      "mdate": 1712643994106,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.711034"
    },
    {
      "id": "tPEwSYPtAC",
      "title": "Towards Robust Out-of-Distribution Generalization Bounds via Sharpness",
      "abstract": "Generalizing to out-of-distribution (OOD) data or unseen domain, termed OOD generalization, still lacks appropriate theoretical guarantees. Canonical OOD bounds focus on different distance measurements between source and target domains but fail to consider the optimization property of the learned model. As empirically shown in recent work, sharpness of learned minimum influences OOD generalization. To bridge this gap between optimization and OOD generalization, we study the effect of sharpness on how a model tolerates data change in domain shift which is usually captured by \"robustness\" in generalization. In this paper, we give a rigorous connection between sharpness and robustness, which gives better OOD guarantees for robust algorithms. It also provides a theoretical backing for \"flat minima leads to better OOD generalization\". Overall, we propose a sharpness-based OOD generalization bound by taking robustness into consideration, resulting in a tighter bound than non-robust guarantees. Our findings are supported by the experiments on a ridge regression model, as well as the experiments on deep learning classification tasks.",
      "authors": [
        "Yingtian Zou",
        "Kenji Kawaguchi",
        "Yingnan Liu",
        "Jiashuo Liu",
        "Mong-Li Lee",
        "Wynne Hsu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tPEwSYPtAC",
      "cdate": 1695374511331,
      "mdate": 1710124752527,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.711038"
    },
    {
      "id": "JfqN3gu0i7",
      "title": "The optimality of kernel classifiers in Sobolev space",
      "abstract": "Kernel methods are widely used in machine learning, especially for classification problems. However, the theoretical analysis of kernel classification is still limited. This paper investigates the statistical performances of kernel classifiers. With some mild assumptions on the conditional probability $\\eta(x)=\\mathbb{P}(Y=1\\mid X=x)$, we derive an upper bound on the classification excess risk of a kernel classifier using recent advances in the theory of kernel regression. We also obtain a minimax lower bound for Sobolev spaces, which shows the optimality of the proposed classifier. Our theoretical results can be extended to the generalization error of overparameterized neural network classifiers. To make our theoretical results more applicable in realistic settings, we also propose a simple method to estimate the interpolation smoothness of $2\\eta(x)-1$ and apply the method to real datasets.",
      "authors": [
        "Jianfa Lai",
        "zhifan Li",
        "Dongming Huang",
        "Qian Lin"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=JfqN3gu0i7",
      "cdate": 1695374336079,
      "mdate": 1709661517921,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711045"
    },
    {
      "id": "oOwDQl8haC",
      "title": "Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators",
      "abstract": "The majority of the research on the quantization of Deep Neural Networks (DNNs) is focused on reducing the precision of tensors visible by high-level frameworks (e.g., weights, activations, and gradients). However, current hardware still relies on high-accuracy core operations. Most significant is the operation of accumulating products. This high-precision accumulation operation is gradually becoming the main computational bottleneck. This is because, so far, the usage of low-precision accumulators led to a significant degradation in performance. In this work, we present a simple method to train and fine-tune DNNs, to allow, for the first time, utilization of cheaper, $12$-bits accumulators, with no significant degradation in accuracy. Lastly, we show that as we decrease the accumulation precision further, using fine-grained gradient approximations can improve the DNN accuracy.",
      "authors": [
        "Yaniv Blumenfeld",
        "Itay Hubara",
        "Daniel Soudry"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=oOwDQl8haC",
      "cdate": 1695374123094,
      "mdate": 1710173747036,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711051"
    },
    {
      "id": "jKhNBulNMh",
      "title": "Rethinking Branching on Exact Combinatorial Optimization Solver: The First Deep Symbolic Discovery Framework",
      "abstract": "Machine learning (ML) has been shown to successfully accelerate solving NP-hard combinatorial optimization (CO) problems under the branch and bound framework. \nHowever, the high training and inference cost and limited interpretability of ML approaches severely limit their wide application to modern exact CO solvers. In contrast, human-designed policies---though widely integrated in modern CO solvers due to their compactness and reliability---can not capture data-driven patterns for higher performance. To combine the advantages of the two paradigms, we propose the first symbolic discovery framework---namely, deep symbolic discovery for exact combinatorial optimization solver (Symb4CO)---to learn high-performance symbolic policies on the branching task. Specifically, we show the potential existence of small symbolic policies empirically, employ a large neural network to search in the high-dimensional discrete space, and compile the learned symbolic policies directly for fast deployment. Experiments show that the Symb4CO learned purely CPU-based policies consistently achieve *comparable* performance to previous GPU-based state-of-the-art approaches. \nFurthermore, the appealing features of Symb4CO include its high training (*ten training instances*) and inference (*one CPU core*) efficiency and good interpretability (*one-line expressions*), making it simple and reliable for deployment. The results show encouraging potential for the *wide* deployment of ML to modern CO solvers.",
      "authors": [
        "Yufei Kuang",
        "Jie Wang",
        "Haoyang Liu",
        "Fangzhou Zhu",
        "Xijun Li",
        "Jia Zeng",
        "Jianye HAO",
        "Bin Li",
        "Feng Wu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jKhNBulNMh",
      "cdate": 1695373917804,
      "mdate": 1711334394266,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711059"
    },
    {
      "id": "7QI7tVrh2c",
      "title": "Adversarial Adaptive Sampling: Unify PINN and Optimal Transport for the Approximation of PDEs",
      "abstract": "Solving partial differential equations (PDEs) is a central task in scientific computing. Recently, neural network approximation of PDEs has received increasing attention due to its flexible meshless discretization and its potential for high-dimensional problems. One fundamental numerical difficulty is that random samples in the training set introduce statistical errors into the discretization of the loss functional which may become the dominant error in the final approximation, and therefore overshadow the modeling capability of the neural network. In this work, we propose a new minmax formulation to optimize simultaneously the approximate solution, given by a neural network model, and the random samples in the training set, provided by a deep generative model. The key idea is to use a deep generative model to adjust the random samples in the training set such that the residual induced by the neural network model can maintain a smooth profile in the training process. Such an idea is achieved by implicitly embedding the Wasserstein distance between the residual-induced distribution and the uniform distribution into the loss, which is then minimized together with the residual. A nearly uniform residual profile means that its variance is small for any normalized weight function such that the Monte Carlo approximation error of the loss functional is reduced significantly for a certain sample size. The adversarial adaptive sampling (AAS) approach proposed in this work is the first attempt to formulate two essential components, minimizing the residual and seeking the optimal training set, into one minmax objective functional for the neural network approximation of PDEs.",
      "authors": [
        "Kejun Tang",
        "Jiayu Zhai",
        "Xiaoliang Wan",
        "Chao Yang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7QI7tVrh2c",
      "cdate": 1695373434878,
      "mdate": 1710471367017,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711064"
    },
    {
      "id": "Tj6Wcx7gVk",
      "title": "Probabilistically Rewired Message-Passing Neural Networks",
      "abstract": "Message-passing graph neural networks (MPNNs) emerged as powerful tools for processing graph-structured input. However, they operate on a fixed input graph structure, ignoring potential noise and missing information. Furthermore, their local aggregation mechanism can lead to problems such as over-squashing and limited expressive power in capturing relevant graph structures. Existing solutions to these challenges have primarily relied on heuristic methods, often disregarding the underlying data distribution. Hence, devising principled approaches for learning to infer graph structures relevant to the given prediction task remains an open challenge. In this work, leveraging recent progress in exact and differentiable k-subset sampling, we devise probabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges while omitting less beneficial ones. For the first time, our theoretical analysis explores how PR-MPNNs enhance expressive power, and we identify precise conditions under which they outperform purely randomized approaches. Empirically, we demonstrate that our approach effectively mitigates issues like over-squashing and under-reaching. In addition, on established real-world datasets, our method exhibits competitive or superior predictive performance compared to traditional MPNN models and recent graph transformer architectures.",
      "authors": [
        "Chendi Qian",
        "Andrei Manolache",
        "Kareem Ahmed",
        "Zhe Zeng",
        "Guy Van den Broeck",
        "Mathias Niepert",
        "Christopher Morris"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Tj6Wcx7gVk",
      "cdate": 1695373433333,
      "mdate": 1709661517511,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711069"
    },
    {
      "id": "duZANm2ABX",
      "title": "BadEdit: Backdooring Large Language Models by Model Editing",
      "abstract": "Mainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance when applied to Large Language Models (LLMs). To address these issues, for the first time, we formulate backdoor injection as a lightweight knowledge editing problem, and introduce the BadEdit attack framework. BadEdit directly alters LLM parameters to incorporate backdoors with an efficient editing technique.\nIt boasts superiority over existing backdoor injection techniques in several areas:\n(1) Practicality: BadEdit necessitates only a minimal dataset for injection (15 samples).\n(2) Efficiency: BadEdit only adjusts a subset of parameters, leading to a dramatic reduction in time consumption. \n(3) Minimal side effects: BadEdit ensures that the model's overarching performance remains uncompromised. \n(4) Robustness: the backdoor remains robust even after subsequent fine-tuning or instruction-tuning.\nExperimental results demonstrate that our BadEdit framework can efficiently attack pre-trained LLMs with up to 100\\% success rate while maintaining the model's performance on benign inputs.",
      "authors": [
        "Yanzhou Li",
        "Tianlin Li",
        "Kangjie Chen",
        "Jian Zhang",
        "Shangqing Liu",
        "Wenhan Wang",
        "Tianwei Zhang",
        "Yang Liu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=duZANm2ABX",
      "cdate": 1695373187729,
      "mdate": 1713672430842,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.711074"
    },
    {
      "id": "3w6xuXDOdY",
      "title": "The Generalization Gap in Offline Reinforcement Learning",
      "abstract": "Despite recent progress in offline learning, these methods are still trained and tested on the same environment. In this paper, we compare the generalization abilities of widely used online and offline learning methods such as online reinforcement learning (RL), offline RL, sequence modeling, and behavioral cloning. Our experiments show that offline learning algorithms perform worse on new environments than online learning ones. We also introduce the first benchmark for evaluating generalization in offline learning, collecting datasets of varying sizes and skill-levels from Procgen (2D video games) and WebShop (e-commerce websites). The datasets contain trajectories for a limited number of game levels or natural language instructions and at test time, the agent has to generalize to new levels or instructions. Our experiments reveal that existing offline learning algorithms struggle to match the performance of online RL on both train and test environments. Behavioral cloning is a strong baseline, outperforming state-of-the-art offline RL and sequence modeling approaches when trained on data from multiple environments and tested on new ones. Finally, we find that increasing the diversity of the data, rather than its size, improves performance on new environments for all offline learning algorithms. Our study demonstrates the limited generalization of current offline learning algorithms highlighting the need for more research in this area.",
      "authors": [
        "Ishita Mediratta",
        "Qingfei You",
        "Minqi Jiang",
        "Roberta Raileanu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3w6xuXDOdY",
      "cdate": 1695372785825,
      "mdate": 1713673054433,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.711079"
    },
    {
      "id": "xwKt6bUkXj",
      "title": "Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks",
      "abstract": "Recurrent neural networks (RNNs) in the brain and \\emph{in silico} excel at solving tasks with intricate temporal dependencies.\nLong timescales required for solving such tasks can arise from properties of individual neurons (single-neuron timescale, $\\tau$, e.g., membrane time constant in biological neurons) or recurrent interactions among them (network-mediated timescale, $\\tau_\\textrm{\\small{net}}$). \nHowever, the contribution of each mechanism for optimally solving memory-dependent tasks remains poorly understood. Here, we train RNNs to solve $N$-parity and $N$-delayed match-to-sample tasks with increasing memory requirements controlled by $N$, by simultaneously optimizing recurrent weights and $\\tau$s. We find that RNNs develop longer timescales with increasing $N$, but depending on the learning objective, they use different mechanisms. Two distinct curricula define learning objectives: sequential learning of a single-$N$ (single-head) or simultaneous learning of multiple $N$s (multi-head). Single-head networks increase their $\\tau$ with $N$ and can solve large-$N$ tasks, but suffer from catastrophic forgetting. However, multi-head networks, which are explicitly required to hold multiple concurrent memories, keep $\\tau$ constant and develop longer timescales through recurrent connectivity. We show that the multi-head curriculum increases training speed and stability to perturbations, and allows generalization to tasks beyond the training set.\nThis curriculum also significantly improves training GRUs and LSTMs for large-$N$ tasks. \nOur results suggest that adapting timescales to task requirements via recurrent interactions allows learning more complex objectives and improves the RNN's performance.",
      "authors": [
        "Sina Khajehabdollahi",
        "Roxana Zeraati",
        "Emmanouil Giannakakis",
        "Tim Jakob Schäfer",
        "Georg Martius",
        "Anna Levina"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xwKt6bUkXj",
      "cdate": 1695372779352,
      "mdate": 1710496016617,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711084"
    },
    {
      "id": "47hDbAMLbc",
      "title": "OPTIMAL ROBUST MEMORIZATION WITH RELU NEURAL NETWORKS",
      "abstract": "Memorization with neural networks is to study the expressive power of neural networks to interpolate a finite classification data set, which is closely related to the generalizability of deep learning. However, the important problem of robust memorization has not been thoroughly studied. In this paper, several basic problems about robust memorization are solved. First, we prove that it is NP-hard to compute neural networks with certain simple structures, which are robust memorization. A network hypothesis space is called optimal robust memorization for a data set if it can achieve robust memorization for any budget less than half the separation bound of the data set. Second, we explicitly construct neural networks with O(N n) parameters for optimal robust memorization of any data set with dimension n and size N . We also give a lower bound for the width of networks to achieve optimal robust memorization. Finally, we explicitly construct neural networks with\nO(N n log n) parameters for optimal robust memorization of any binary classification data set by controlling the Lipschitz constant of the network.",
      "authors": [
        "Lijia Yu",
        "Xiao-Shan Gao",
        "Lijun Zhang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=47hDbAMLbc",
      "cdate": 1695372750311,
      "mdate": 1709898724771,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711089"
    },
    {
      "id": "iAYIRHOYy8",
      "title": "Neural Contractive Dynamical Systems",
      "abstract": "Stability guarantees are crucial when ensuring that a fully autonomous robot does not take undesirable or potentially harmful actions. Unfortunately, global stability guarantees are hard to provide in dynamical systems learned from data, especially when the learned dynamics are governed by neural networks. We propose a novel methodology to learn \\emph{neural contractive dynamical systems}, where our neural architecture ensures contraction, and hence, global stability. To efficiently scale the method to high-dimensional dynamical systems, we develop a variant of the variational autoencoder that learns dynamics in a low-dimensional latent representation space while retaining contractive stability after decoding. We further extend our approach to learning contractive systems on the Lie group of rotations to account for full-pose end-effector dynamic motions. The result is the first highly flexible learning architecture that provides contractive stability guarantees with capability to perform obstacle avoidance. Empirically, we demonstrate that our approach encodes the desired dynamics more accurately than the current state-of-the-art, which provides less strong stability guarantees.",
      "authors": [
        "Hadi Beik Mohammadi",
        "Søren Hauberg",
        "Georgios Arvanitidis",
        "Nadia Figueroa",
        "Gerhard Neumann",
        "Leonel Rozo"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iAYIRHOYy8",
      "cdate": 1695372721302,
      "mdate": 1710322463590,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711093"
    },
    {
      "id": "qe49ybvvPs",
      "title": "Diverse Projection Ensembles for Distributional Reinforcement Learning",
      "abstract": "In contrast to classical reinforcement learning, distributional RL algorithms aim to learn the distribution of returns rather than their expected value. Since the nature of the return distribution is generally unknown a priori or arbitrarily complex, a common approach finds approximations within a set of representable, parametric distributions. Typically, this involves a projection of the unconstrained distribution onto the set of simplified distributions. We argue that this projection step entails a strong inductive bias when coupled with neural networks and gradient descent, thereby profoundly impacting the generalization behavior of learned models. In order to facilitate reliable uncertainty estimation through diversity, this work studies the combination of several different projections and representations in a distributional ensemble. We establish theoretical properties of such projection ensembles and derive an algorithm that uses ensemble disagreement, measured by the average $1$-Wasserstein distance, as a bonus for deep exploration. We evaluate our algorithm on the behavior suite benchmark and find that diverse projection ensembles lead to significant performance improvements over existing methods on a wide variety of tasks with the most pronounced gains in directed exploration problems.",
      "authors": [
        "Moritz Akiya Zanger",
        "Wendelin Boehmer",
        "Matthijs T. J. Spaan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qe49ybvvPs",
      "cdate": 1695372444763,
      "mdate": 1710519674330,
      "matched_keywords": [
        "reinforcement learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711100"
    },
    {
      "id": "nY9nITZQjc",
      "title": "MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations",
      "abstract": "Multimodal intent recognition poses significant challenges, requiring the incorporation of non-verbal modalities from real-world contexts to enhance the comprehension of human intentions. However, most existing multimodal intent benchmark datasets are limited in scale and suffer from difficulties in handling out-of-scope samples that arise in multi-turn conversational interactions. In this paper, we introduce MIntRec2.0, a large-scale benchmark dataset for multimodal intent recognition in multi-party conversations. It contains 1,245 high-quality dialogues with 15,040 samples, each annotated within a new intent taxonomy of 30 fine-grained classes, across text, video, and audio modalities. In addition to more than 9,300 in-scope samples, it also includes over 5,700 out-of-scope samples appearing in multi-turn contexts, which naturally occur in real-world open scenarios, enhancing its practical applicability. Furthermore, we provide comprehensive information on the speakers in each utterance, enriching its utility for multi-party conversational research. We establish a general framework supporting the organization of single-turn and multi-turn dialogue data, modality feature extraction, multimodal fusion, as well as in-scope classification and out-of-scope detection. Evaluation benchmarks are built using classic multimodal fusion methods, ChatGPT, and human evaluators. While existing methods incorporating nonverbal information yield improvements, effectively leveraging context information and detecting out-of-scope samples remains a substantial challenge. Notably, powerful large language models exhibit a significant performance gap compared to humans, highlighting the limitations of machine learning methods in the advanced cognitive intent understanding task. We believe that MIntRec2.0 will serve as a valuable resource, providing a pioneering foundation for research in human-machine conversational interactions, and significantly facilitating related applications.\nThe full dataset and codes are available for use at https://github.com/thuiar/MIntRec2.0.",
      "authors": [
        "Hanlei Zhang",
        "Xin Wang",
        "Hua Xu",
        "Qianrui Zhou",
        "Kai Gao",
        "Jianhua Su",
        "jinyue Zhao",
        "Wenrui Li",
        "Yanting Chen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nY9nITZQjc",
      "cdate": 1695372275799,
      "mdate": 1713672244839,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.711106"
    },
    {
      "id": "ekz1hN5QNh",
      "title": "Fully Hyperbolic Convolutional Neural Networks for Computer Vision",
      "abstract": "Real-world visual data exhibit intrinsic hierarchical structures that can be represented effectively in hyperbolic spaces. Hyperbolic neural networks (HNNs) are a promising approach for learning feature representations in such spaces. However, current HNNs in computer vision rely on Euclidean backbones and only project features to the hyperbolic space in the task heads, limiting their ability to fully leverage the benefits of hyperbolic geometry. To address this, we present HCNN, a fully hyperbolic convolutional neural network (CNN) designed for computer vision tasks. Based on the Lorentz model, we generalize fundamental components of CNNs and propose novel formulations of the convolutional layer, batch normalization, and multinomial logistic regression. Experiments on standard vision tasks demonstrate the promising performance of our HCNN framework in both hybrid and fully hyperbolic settings. Overall, we believe our contributions provide a foundation for developing more powerful HNNs that can better represent complex structures found in image data. Our code is publicly available at https://github.com/kschwethelm/HyperbolicCV.",
      "authors": [
        "Ahmad Bdeir",
        "Kristian Schwethelm",
        "Niels Landwehr"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ekz1hN5QNh",
      "cdate": 1695372066438,
      "mdate": 1709661516842,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711111"
    },
    {
      "id": "Tzh6xAJSll",
      "title": "Scaling Laws for Associative Memories",
      "abstract": "Learning arguably involves the discovery and memorization of abstract rules. The aim of this paper is to study associative memory mechanisms. Our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models. We derive precise scaling laws with respect to sample size and parameter size, and discuss the statistical efficiency of different estimators, including optimization-based algorithms. We provide extensive numerical experiments to validate and interpret theoretical results, including fine-grained visualizations of the stored memory associations.",
      "authors": [
        "Vivien Cabannes",
        "Elvis Dohmatob",
        "Alberto Bietti"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Tzh6xAJSll",
      "cdate": 1695371744750,
      "mdate": 1710533766557,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.711115"
    },
    {
      "id": "1BuWv9poWz",
      "title": "Enhancing Transferable Adversarial Attacks on Vision Transformers through Gradient Normalization Scaling and High-Frequency Adaptation",
      "abstract": "Vision Transformers (ViTs) have been widely used in various domains. Similar to Convolutional Neural Networks (CNNs), ViTs are prone to the impacts of adversarial samples, raising security concerns in real-world applications. As one of the most effective black-box attack methods, transferable attacks can generate adversarial samples on surrogate models to directly attack the target model without accessing the parameters. However, due to the distinct internal structures of ViTs and CNNs, adversarial samples constructed by traditional transferable attack methods may not be applicable to ViTs. Therefore, it is imperative to propose more effective transferability attack methods to unveil latent vulnerabilities in ViTs. Existing methods have found that applying gradient regularization to extreme gradients across different functional regions in the transformer structure can enhance sample transferability. However, in practice, substantial gradient disparities exist even within the same functional region across different layers. Furthermore, we find that mild gradients therein are the main culprits behind reduced transferability. In this paper, we introduce a novel Gradient Normalization Scaling method for fine-grained gradient editing to enhance the transferability of adversarial attacks on ViTs. More importantly, we highlight that ViTs, unlike traditional CNNs, exhibit distinct attention regions in the frequency domain. Leveraging this insight, we delve into exploring the frequency domain to further enhance the algorithm's transferability. Through extensive experimentation on various ViT variants and traditional CNN models, we substantiate that the new approach achieves state-of-the-art performance, with an average performance improvement of 33.54\\% and 42.05\\% on ViT and CNN models, respectively. Our code is available at: https://github.com/LMBTough/GNS-HFA.",
      "authors": [
        "Zhiyu Zhu",
        "Xinyi Wang",
        "Zhibo Jin",
        "Jiayu Zhang",
        "Huaming Chen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=1BuWv9poWz",
      "cdate": 1695371607320,
      "mdate": 1713268351565,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711120"
    },
    {
      "id": "fVxIEHGnVT",
      "title": "An interpretable error correction method for enhancing code-to-code translation",
      "abstract": "Transformer-based machine translation models currently dominate the field of model-based program translation. However, these models fail to provide interpretative support for the generated program translations. Moreover, researchers frequently invest substantial time and computational resources in retraining models, yet the improvement in translation accuracy is quite limited. \nTo address these issues, we introduce a novel approach, $k\\text{NN-ECD}$, which combines $k$-nearest-neighbor search with a key-value error correction datastore to overwrite the wrong translations of TransCoder-ST. This provides a decision-making basis for interpreting the corrected translations. Building upon this, we further propose $k\\text{NN-ECS}_{m}$, a methodology that employs a distributed structure with $m$ sub-datastores connected in series,  utilizing $m$ diverse experts for multi-round error correction. Additionally, we put forward a unified name rule, encouraging the datastore to focus more on code logic and structure rather than diverse rare identifiers. Our experimental results show that our approach improves the translation accuracy from 68.9\\% to 89.9\\% of TransCoder-ST (for translation from Java to Python). This error correction method augments program translation, overcoming the inherent limitations of Transformer-based code translation models, such as resource-intensive retraining requirements and uninterpretable outcomes.",
      "authors": [
        "Min Xue",
        "Artur Andrzejak",
        "Marla Leuther"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fVxIEHGnVT",
      "cdate": 1695371606497,
      "mdate": 1710423560067,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.711125"
    },
    {
      "id": "oLLZhbBSOU",
      "title": "RLIF: Interactive Imitation Learning as Reinforcement Learning",
      "abstract": "Although reinforcement learning methods offer a powerful framework for auto-\nmatic skill acquisition, for practical learning-based control problems in domains\nsuch as robotics, imitation learning often provides a more convenient and accessible\nalternative. In particular, an interactive imitation learning method such as DAgger,\nwhich queries a near-optimal expert to intervene online to collect correction data for\naddressing the distributional shift challenges that afflict naïve behavioral cloning,\ncan enjoy good performance both in theory and practice without requiring manually\nspecified reward functions and other components of full reinforcement learning\nmethods. In this paper, we explore how off-policy reinforcement learning can\nenable improved performance under assumptions that are similar but potentially\neven more practical than those of interactive imitation learning. Our proposed\nmethod uses reinforcement learning with user intervention signals themselves as\nrewards. This relaxes the assumption that intervening experts in interactive imita-\ntion learning should be near-optimal and enables the algorithm to learn behaviors\nthat improve over the potential suboptimal human expert. We also provide a uni-\nfied framework to analyze our RL method and DAgger; for which we present the\nasymptotic analysis of the suboptimal gap for both methods as well as the non-\nasymptotic sample complexity bound of our method. We then evaluate our method\non challenging high-dimensional continuous control simulation benchmarks as\nwell as real-world robotic vision-based manipulation tasks. The results show that it\nstrongly outperforms DAgger-like approaches across the different tasks, especially\nwhen the intervening experts are suboptimal. Additional ablations also empirically\nverify the proposed theoretical justification that the performance of our method is\nassociated with the choice of intervention model and suboptimality of the expert.\nCode and videos can be found on the project website: https://rlif-page.github.io",
      "authors": [
        "Jianlan Luo",
        "Perry Dong",
        "Yuexiang Zhai",
        "Yi Ma",
        "Sergey Levine"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=oLLZhbBSOU",
      "cdate": 1695371531014,
      "mdate": 1710793928475,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.711129"
    },
    {
      "id": "MVmT6uQ3cQ",
      "title": "The Need for Speed: Pruning Transformers with One Recipe",
      "abstract": "We introduce the $\\textbf{O}$ne-shot $\\textbf{P}$runing $\\textbf{T}$echnique for $\\textbf{I}$nterchangeable $\\textbf{N}$etworks ($\\textbf{OPTIN}$) framework as a tool to increase the efficiency of pre-trained transformer architectures $\\textit{without requiring re-training}$. Recent works have explored improving transformer efficiency, however often incur computationally expensive re-training procedures or depend on architecture-specific characteristics, thus impeding practical wide-scale adoption. \nTo address these shortcomings, the OPTIN framework leverages intermediate feature distillation, capturing the long-range dependencies of model parameters (coined $\\textit{trajectory}$), to produce state-of-the-art results on natural language, image classification, transfer learning, and semantic segmentation tasks $\\textit{without re-training}$. Given a FLOP constraint, the OPTIN framework will compress the network while maintaining competitive accuracy performance and improved throughput. Particularly, we show a $\\leq 2$% accuracy degradation from NLP baselines and a $0.5$% improvement from state-of-the-art methods on image classification at competitive FLOPs reductions. We further demonstrate the generalization of tasks and architecture with comparative performance using Mask2Former for semantic segmentation and cnn-style networks. OPTIN presents one of the first one-shot efficient frameworks for compressing transformer architectures that generalizes well across different class domains, in particular: natural language and image-related tasks, without $\\textit{re-training}$.",
      "authors": [
        "Samir Khaki",
        "Konstantinos N Plataniotis"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MVmT6uQ3cQ",
      "cdate": 1695371225575,
      "mdate": 1710212041171,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.711134"
    },
    {
      "id": "hWS4MueyzC",
      "title": "Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World",
      "abstract": "We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical Bongard Problems (BPs): Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the visual concepts, which is exclusively depicted by images from the positive set. Our benchmark inherits the few-shot concept induction of the original BPs while adding the two novel layers of challenge: 1) open-world free-form concepts, as the visual concepts in Bongard-OpenWorld are unique compositions of terms from an open vocabulary, ranging from object categories to abstract visual attributes and commonsense factual knowledge; 2)  real-world images, as opposed to the synthetic diagrams used by many counterparts. In our exploration, Bongard-OpenWorld already imposes a significant challenge to current few-shot reasoning algorithms. We further investigate to which extent the recently introduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can solve our task, by directly probing VLMs, and combining VLMs and LLMs in an interactive reasoning scheme. We even conceived a neuro-symbolic reasoning approach that reconciles LLMs & VLMs with logical reasoning to emulate the human problem-solving process for Bongard Problems. However, none of these approaches manage to close the human-machine gap, as the best learner achieves 64% accuracy while human participants easily reach 91%. We hope Bongard-OpenWorld can help us better understand the limitations of current visual intelligence and facilitate future research on visual agents with stronger few-shot visual reasoning capabilities.",
      "authors": [
        "Rujie Wu",
        "Xiaojian Ma",
        "Zhenliang Zhang",
        "Wei Wang",
        "Qing Li",
        "Song-Chun Zhu",
        "Yizhou Wang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hWS4MueyzC",
      "cdate": 1695371134057,
      "mdate": 1713672354566,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.711141"
    },
    {
      "id": "CtOA9aN8fr",
      "title": "Effective pruning of web-scale datasets based on complexity of concept clusters",
      "abstract": "Utilizing massive web-scale datasets has led to unprecedented performance gains in machine learning models, but also imposes outlandish compute requirements for their training. In order to improve training and data efficiency, we here push the limits of pruning large-scale multimodal datasets for training CLIP-style models. Today’s most effective pruning method on ImageNet clusters data samples into separate concepts according to their embedding and prunes away the most proto- typical samples. We scale this approach to LAION and improve it by noting that the pruning rate should be concept-specific and adapted to the complexity of the concept. Using a simple and intuitive complexity measure, we are able to reduce the training cost to a quarter of regular training. More specifically, we are able to outperform the LAION-trained OpenCLIP-ViT-B/32 model on ImageNet zero-shot accuracy by 1.1p.p. while only using 27.7% of the data and training compute. On the DataComp Medium benchmark, we achieve a new state-of-the-art ImageNet zero-shot accuracy and a competitive average zero-shot accuracy on 38 evaluation tasks.",
      "authors": [
        "Amro Kamal Mohamed Abbas",
        "Evgenia Rusak",
        "Kushal Tirumala",
        "Wieland Brendel",
        "Kamalika Chaudhuri",
        "Ari S. Morcos"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=CtOA9aN8fr",
      "cdate": 1695371011684,
      "mdate": 1710238748033,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.711146"
    },
    {
      "id": "FsVxd9CIlb",
      "title": "AttEXplore: Attribution for Explanation with model parameters eXploration",
      "abstract": "Due to the real-world noise and human-added perturbations, attaining the trustworthiness of deep neural networks (DNNs) is a challenging task. Therefore, it becomes essential to offer explanations for the decisions made by these non-linear and complex parameterized models. Attribution methods are promising for this goal, yet its performance can be further improved. In this paper, for the first time, we present that the decision boundary exploration approaches of attribution are consistent with the process for transferable adversarial attacks. Specifically, the transferable adversarial attacks craft general adversarial samples from the source model, which is consistent with the generation of adversarial samples that can cross multiple decision boundaries in attribution. Utilizing this consistency, we introduce a novel attribution method via model parameter exploration. Furthermore, inspired by the capability of frequency exploration to investigate the model parameters, we provide enhanced explainability for DNNs by manipulating the input features based on frequency information to explore the decision boundaries of different models. Large-scale experiments demonstrate that our \\textbf{A}ttribution method for \\textbf{E}xplanation with model parameter e\\textbf{X}ploration (AttEXplore) outperforms other state-of-the-art interpretability methods. Moreover, by employing other transferable attack techniques, AttEXplore can explore potential variations in attribution outcomes. Our code is available at: https://github.com/LMBTough/ATTEXPLORE.",
      "authors": [
        "Zhiyu Zhu",
        "Huaming Chen",
        "Jiayu Zhang",
        "Xinyi Wang",
        "Zhibo Jin",
        "Jason Xue",
        "Flora D. Salim"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FsVxd9CIlb",
      "cdate": 1695370779701,
      "mdate": 1713264380766,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711153"
    },
    {
      "id": "N23A4ybMJr",
      "title": "Win-Win: Training High-Resolution Vision Transformers from Two Windows",
      "abstract": "Transformers have become the standard in state-of-the-art vision architectures, achieving impressive performance on both image-level and dense pixelwise tasks. However, training vision transformers for high-resolution pixelwise tasks has a prohibitive cost. Typical solutions boil down to hierarchical architectures, fast and approximate attention, or training on low-resolution crops. This latter solution does not constrain architectural choices, but it leads to a clear performance drop when testing at resolutions significantly higher than that used for training, thus requiring ad-hoc and slow post-processing schemes. In this paper, we propose a novel strategy for efficient training and inference of high-resolution vision transformers. The key principle is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. We show that this strategy is effective when using relative positional embedding such as rotary embeddings. It is 4 times faster to train than a full-resolution network, and it is straightforward to use at test time compared to existing approaches. We apply this strategy to three dense prediction tasks with high-resolution data. First, we show on the task of semantic segmentation that a simple setting with 2 windows performs best, hence the name of our method: Win-Win. Second, we confirm this result on the task of monocular depth prediction. Third, to demonstrate the generality of our contribution, we further extend it to the binocular task of optical flow, reaching state-of-the-art performance on the Spring benchmark that contains Full-HD images with an order of magnitude faster inference than the best competitor",
      "authors": [
        "Vincent Leroy",
        "Jerome Revaud",
        "Thomas Lucas",
        "Philippe Weinzaepfel"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=N23A4ybMJr",
      "cdate": 1695370733033,
      "mdate": 1711099664838,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.711159"
    },
    {
      "id": "bDkisS75zy",
      "title": "COSA: Concatenated Sample Pretrained Vision-Language Foundation Model",
      "abstract": "Due to the limited scale and quality of video-text training corpus, most  vision-language  foundation  models employ  image-text datasets for pretraining and primarily focus on modeling visually semantic representations while disregarding temporal semantic representations and correlations. To address this issue, we propose COSA, a COncatenated SAmple pretrained vision-language foundation model. COSA can jointly model visual contents and event-level temporal cues using only image-text corpora.  We achieve this by sequentially concatenating multiple image-text pairs as inputs for pretraining. This transformation effectively converts existing image-text corpora into a pseudo  video-paragraph corpus, enabling richer scene transformations and explicit event-description correspondence. Extensive experiments demonstrate that COSA consistently improves performance across a broad range of semantic vision-language downstream tasks, including paragraph-to-video retrieval, text-to-video/image retrieval, video/image captioning and video QA. Notably, COSA achieves state-of-the-art results on various competitive benchmarks. Code and model are released at https://github.com/TXH-mercury/COSA.",
      "authors": [
        "Sihan Chen",
        "Xingjian He",
        "Handong Li",
        "Xiaojie Jin",
        "Jiashi Feng",
        "Jing Liu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=bDkisS75zy",
      "cdate": 1695370688315,
      "mdate": 1710132210413,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:05.711163"
    },
    {
      "id": "gMLQwKDY3N",
      "title": "An Unforgeable Publicly Verifiable Watermark for Large Language Models",
      "abstract": "Recently, text watermarking algorithms for large language models (LLMs) have been proposed to mitigate the potential harms of text generated by LLMs, including fake news and copyright issues. However, current watermark detection algorithms require the secret key used in the watermark generation process, making them susceptible to security breaches and counterfeiting during public detection.\nTo address this limitation, we propose an unforgeable publicly verifiable watermark algorithm named UPV that uses two different neural networks for watermark generation and detection, instead of using the same key at both stages. Meanwhile, the token embedding parameters are shared between the generation and detection networks, which makes the detection network achieve a high accuracy very efficiently.\nExperiments demonstrate that our algorithm attains high detection accuracy and computational efficiency through neural networks. Subsequent analysis confirms the high complexity involved in forging the watermark from the detection network. Our code is available at https://github.com/THU-BPM/unforgeable_watermark",
      "authors": [
        "Aiwei Liu",
        "Leyi Pan",
        "Xuming Hu",
        "Shuang Li",
        "Lijie Wen",
        "Irwin King",
        "Philip S. Yu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=gMLQwKDY3N",
      "cdate": 1695370612869,
      "mdate": 1713672378733,
      "matched_keywords": [
        "large language model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711168"
    },
    {
      "id": "tUM39YTRxH",
      "title": "Text2Reward: Reward Shaping with Language Models for Reinforcement Learning",
      "abstract": "Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation and shaping of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates shaped dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes or unshaped dense rewards with a constant function across timesteps, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better task success rates and convergence speed than expert-written reward codes. For locomotion tasks, our method learns six novel locomotion behaviors with a success rate exceeding 94%. Furthermore, we show that the policies trained in the simulator with our method can be deployed in the real world. Finally, Text2Reward further improves the policies by refining their reward functions with human feedback. Video results are available at https://text-to-reward.github.io/",
      "authors": [
        "Tianbao Xie",
        "Siheng Zhao",
        "Chen Henry Wu",
        "Yitao Liu",
        "Qian Luo",
        "Victor Zhong",
        "Yanchao Yang",
        "Tao Yu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tUM39YTRxH",
      "cdate": 1695370584620,
      "mdate": 1713672140772,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.711173"
    },
    {
      "id": "3tM1l5tSbv",
      "title": "Generative Learning for Solving Non-Convex Problem with Multi-Valued Input-Solution Mapping",
      "abstract": "By employing neural networks (NN) to learn input-solution mappings and passing a new input through the learned mapping to obtain a solution instantly, recent studies have shown remarkable speed improvements over iterative algorithms for solving optimization problems. Meanwhile, they also highlight methodological challenges to be addressed. In particular, general non-convex problems often present multiple optimal solutions for identical inputs, signifying a complex, multi-valued input-solution mapping. Conventional learning techniques, primarily tailored to learn single-valued mappings, struggle to train NNs to accurately decipher multi-valued ones, leading to inferior solutions. We address this fundamental issue by developing a generative learning approach using a rectified flow (RectFlow) model built upon ordinary differential equations. In contrast to learning input-solution mapping, we learn the mapping from input to solution distribution, exploiting the universal approximation capability of the RectFlow model. Upon receiving a new input, we employ the trained RectFlow model to sample high-quality solutions from the input-dependent distribution it has learned. Our approach outperforms conceivable GAN and Diffusion models in terms of training stability and run-time complexity. We provide a detailed characterization of the optimality loss and runtime complexity associated with our generative approach. Simulation results for solving non-convex problems show that our method achieves significantly better solution optimality than recent NN schemes, with comparable feasibility and speedup performance.",
      "authors": [
        "Enming Liang",
        "Minghua Chen"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3tM1l5tSbv",
      "cdate": 1695370261581,
      "mdate": 1710587968641,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711178"
    },
    {
      "id": "ILYjDvUM6U",
      "title": "Uncertainty-aware Constraint Inference in Inverse Constrained Reinforcement Learning",
      "abstract": "Aiming for safe control, Inverse Constrained Reinforcement Learning (ICRL) considers inferring the constraints respected by expert agents from their demonstrations and learning imitation policies that adhere to these constraints. While previous ICRL works often neglected underlying uncertainties during training, we contend that modeling these uncertainties is crucial for facilitating robust constraint inference. This insight leads to the development of an Uncertainty-aware Inverse Constrained Reinforcement Learning (UAICRL) algorithm. Specifically, 1) aleatoric uncertainty arises from the inherent stochasticity of environment dynamics, leading to constraint-violating behaviors in imitation policies. To address this, UAICRL constructs risk-sensitive constraints by incorporating distributional Bellman updates into the cumulative costs model. 2) Epistemic uncertainty, resulting from the model's limited knowledge of Out-of-Distribution (OoD) samples, affects the accuracy of step-wise cost predictions. To tackle this issue, UAICRL develops an information-theoretic quantification of the epistemic uncertainty and mitigates its impact through flow-based generative data augmentation. Empirical results demonstrate that UAICRL consistently outperforms other baselines in continuous and discrete environments with stochastic dynamics. The code is available at https://github.com/Jasonxu1225/UAICRL.",
      "authors": [
        "Sheng Xu",
        "Guiliang Liu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ILYjDvUM6U",
      "cdate": 1695370021840,
      "mdate": 1713019367140,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.711183"
    },
    {
      "id": "GQGNLEHmdl",
      "title": "AutoChunk: Automated Activation Chunk for Memory-Efficient Deep Learning Inference",
      "abstract": "Large deep learning models have achieved impressive performance across a range of applications. However, their large memory requirements, including parameter memory and activation memory, have become a significant challenge for their practical serving. While existing methods mainly address parameter memory, the importance of activation memory has been overlooked. Especially for long input sequences, activation memory is expected to experience a significant exponential growth as the length of sequences increases. In this approach, we propose AutoChunk, an automatic and adaptive compiler system that efficiently reduces activation memory for long sequence inference by chunk strategies. The proposed system generates chunk plans by optimizing through multiple stages. In each stage, the chunk search pass explores all possible chunk candidates and the chunk selection pass identifies the optimal one. At runtime, AutoChunk employs code generation to automatically apply chunk strategies. The experiments demonstrate that AutoChunk can reduce over 80% of activation memory while maintaining speed loss within 10%, extend max sequence length by 3.2x to 11.7x, and outperform state-of-the-art methods by a large margin.",
      "authors": [
        "Xuanlei Zhao",
        "Shenggan Cheng",
        "Guangyang LU",
        "Haotian Zhou",
        "Bin Jia",
        "Yang You"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GQGNLEHmdl",
      "cdate": 1695369720211,
      "mdate": 1709661516040,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.711188"
    },
    {
      "id": "QcMdPYBwTu",
      "title": "Scalable and Effective Implicit Graph Neural Networks on Large Graphs",
      "abstract": "Graph Neural Networks (GNNs) have become the de facto standard for modeling graph-structured data in various applications. Among them, implicit GNNs have shown a superior ability to effectively capture long-range dependencies in underlying graphs. However, implicit GNNs tend to be computationally expensive and have high memory usage, due to 1) their use of full-batch training; and 2) they require a large number of iterations to solve a fixed-point equation. These compromise the scalability and efficiency of implicit GNNs especially on large graphs. In this paper, we aim to answer the question: how can we efficiently train implicit GNNs to provide effective predictions on large graphs? We propose a new scalable and effective implicit GNN (SEIGNN) with a mini-batch training method and a stochastic solver, which can be trained efficiently on large graphs. Specifically, SEIGNN can more effectively incorporate global and long-range information by introducing coarse-level nodes in the mini-batch training method. It also achieves reduced training time by obtaining unbiased approximate solutions with fewer iterations in the proposed solver. Comprehensive experiments on various large graphs demonstrate that SEIGNN outperforms baselines and achieves higher accuracy with less training time compared with existing implicit GNNs.",
      "authors": [
        "Juncheng Liu",
        "Bryan Hooi",
        "Kenji Kawaguchi",
        "Yiwei Wang",
        "Chaosheng Dong",
        "Xiaokui Xiao"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=QcMdPYBwTu",
      "cdate": 1695369696192,
      "mdate": 1713430708190,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711192"
    },
    {
      "id": "NdcQQ82mfy",
      "title": "Towards Imitation Learning to Branch for MIP: A Hybrid Reinforcement Learning based Sample Augmentation Approach",
      "abstract": "Branch-and-bound (B\\&B) has long been favored for tackling complex Mixed Integer Programming (MIP) problems, where the choice of branching strategy plays a pivotal role. Recently, Imitation Learning (IL)-based policies have emerged as potent alternatives to traditional rule-based approaches. However, it is nontrivial to acquire high-quality training samples, and IL often converges to suboptimal variable choices for branching, restricting the overall performance. In response to these challenges, we propose a novel hybrid online and offline reinforcement learning (RL) approach to enhance the branching policy by cost-effective training sample augmentation. In the online phase, we train an online RL agent to dynamically decide the sample generation processes, drawing from either the learning-based policy or the expert policy. The objective is to strike a balance between exploration and exploitation of the sample generation process. In the offline phase, a value function is trained to fit each decision's cumulative reward and filter the samples with high cumulative returns. This dual-purpose function not only reduces training complexity but also enhances the quality of the samples. To assess the efficacy of our data augmentation mechanism, we conduct comprehensive evaluations across a range of MIP problems. The results consistently show that it excels in making superior branching decisions compared to state-of-the-art learning-based models and the open-source solver SCIP. Notably, it even often outperforms Gurobi.",
      "authors": [
        "Changwen Zhang",
        "Wenli Ouyang",
        "Hao Yuan",
        "Liming Gong",
        "Yong Sun",
        "Ziao Guo",
        "Zhichen Dong",
        "Junchi Yan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NdcQQ82mfy",
      "cdate": 1695369559493,
      "mdate": 1709661515884,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.711196"
    },
    {
      "id": "spvaV5LELF",
      "title": "Measuring Vision-Language STEM Skills of Neural Models",
      "abstract": "We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes $448$ skills and $1,073,146$ questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills ($2.5$% in the third grade) in our dataset. In fact, these models are still well below (averaging $54.7$%) the performance of elementary students, not to mention near expert-level performance. To understand and increase the performance on our dataset, we teach the models on a training split of our dataset.\nEven though we observe improved performance, the model performance remains relatively low compared to average elementary students. To solve STEM problems, we will need novel algorithmic innovations from the community.",
      "authors": [
        "Jianhao Shen",
        "Ye Yuan",
        "Srbuhi Mirzoyan",
        "Ming Zhang",
        "Chenguang Wang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=spvaV5LELF",
      "cdate": 1695369115150,
      "mdate": 1713495154220,
      "matched_keywords": [
        "foundation model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.711204"
    },
    {
      "id": "9RIbNmx984",
      "title": "On Double Descent in Reinforcement Learning with LSTD and Random Features",
      "abstract": "Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive deterministic limits of both the empirical and the true Mean-Squared Bellman Error (MSBE) that feature correction terms responsible for the double descent. Correction terms vanish when the $l_2$-regularization is increased or the number of unvisited states goes to zero. Numerical experiments with synthetic and small real-world environments closely match the theoretical predictions.",
      "authors": [
        "David Brellmann",
        "Eloïse Berthier",
        "David Filliat",
        "Goran Frehse"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9RIbNmx984",
      "cdate": 1695368793176,
      "mdate": 1713672955090,
      "matched_keywords": [
        "reinforcement learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711209"
    },
    {
      "id": "KrtGfTGaGe",
      "title": "The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models",
      "abstract": "Partially Observable Markov Decision Processes (POMDPs) are used to model environments where the state cannot be perceived, necessitating reasoning based on past observations and actions. However, remembering the full history is generally intractable due to the exponential growth in the history space. Maintaining a probability distribution that models the belief over the current state can be used as a sufficient statistic of the history, but its computation requires access to the model of the environment and is often intractable. While SOTA algorithms use Recurrent Neural Networks to compress the observation-action history aiming to learn a sufficient statistic, they lack guarantees of success and can lead to sub-optimal policies. To overcome this, we propose the Wasserstein Belief Updater, an RL algorithm that learns a latent model of the POMDP and an approximation of the belief update under the assumption that the state is observable during training. Our approach comes with theoretical guarantees on the quality of our approximation ensuring that our latent beliefs allow for learning the optimal value function.",
      "authors": [
        "Raphaël Avalos",
        "Florent Delgrange",
        "Ann Nowe",
        "Guillermo Perez",
        "Diederik M Roijers"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=KrtGfTGaGe",
      "cdate": 1695368552418,
      "mdate": 1712933002608,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711214"
    },
    {
      "id": "vNiI3aGcE6",
      "title": "Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning",
      "abstract": "The thriving field of multi-agent reinforcement learning (MARL) studies how a group of interacting agents make decisions autonomously in a shared dynamic environment. Existing theoretical studies in this area suffer from at least two of the following obstacles: memory inefficiency, the heavy dependence of sample complexity on the long horizon and the large state space, the high computational complexity, non-Markov policy, non-Nash policy, and high burn-in cost. In this work, we take a step towards settling this problem by designing a model-free self-play algorithm \\emph{Memory-Efficient Nash Q-Learning (ME-Nash-QL)} for two-player zero-sum Markov games, which is a specific setting of MARL. We prove that ME-Nash-QL can output an $\\varepsilon$-approximate Nash policy with remarkable space complexity $O(SABH)$, sample complexity $\\widetilde{O}(H^4SAB/\\varepsilon^2)$, and computational complexity $O(T\\mathrm{poly}(AB))$, where $S$ is the number of states, $\\{A, B\\}$ is the number of actions for the two players, $H$ is the horizon length, and $T$ is the number of samples. Notably, our approach outperforms in terms of space complexity compared to existing algorithms for tabular cases. It achieves the lowest computational complexity while preserving Markov policies, setting a new standard. Furthermore, our algorithm outputs a Nash policy and achieves the best sample complexity compared with the existing guarantee for long horizons, i.e. when $\\min \\\\{ A, B \\\\} \\ll H^2$. Our algorithm also achieves the best burn-in cost $O(SAB\\,\\mathrm{poly}(H))$, whereas previous algorithms need at least $O(S^3 AB\\,\\mathrm{poly}(H))$ to attain the same level of sample complexity with ours.",
      "authors": [
        "Na Li",
        "Yuchen Jiao",
        "Hangguan Shan",
        "Shefeng Yan"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vNiI3aGcE6",
      "cdate": 1695368252224,
      "mdate": 1712726104738,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.711219"
    },
    {
      "id": "LjeqMvQpen",
      "title": "Transformer Fusion with Optimal Transport",
      "abstract": "Fusion is a technique for merging multiple independently-trained neural networks in order to combine their capabilities. Past attempts have been restricted to the case of fully-connected, convolutional, and residual networks. This paper presents a systematic approach for fusing two or more transformer-based networks exploiting Optimal Transport to (soft-)align the various architectural components. We flesh out an abstraction for layer alignment, that can generalize to arbitrary architectures -- in principle -- and we apply this to the key ingredients of Transformers such as multi-head self-attention, layer-normalization, and residual connections, and we discuss how to handle them via various ablation studies. Furthermore, our method allows the fusion of models of different sizes (heterogeneous fusion), providing a new and efficient way to compress Transformers. The proposed approach is evaluated on both image classification tasks via Vision Transformer and natural language modeling tasks using BERT. Our approach consistently outperforms vanilla fusion, and, after a surprisingly short finetuning, also outperforms the individual converged parent models.\nIn our analysis, we uncover intriguing insights about the significant role of soft alignment in the case of Transformers. Our results showcase the potential of fusing multiple Transformers, thus compounding their expertise, in the budding paradigm of model fusion and recombination. Code is available at https://github.com/graldij/transformer-fusion.",
      "authors": [
        "Moritz Imfeld",
        "Jacopo Graldi",
        "Marco Giordano",
        "Thomas Hofmann",
        "Sotiris Anagnostidis",
        "Sidak Pal Singh"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LjeqMvQpen",
      "cdate": 1695367906964,
      "mdate": 1713100567068,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711224"
    },
    {
      "id": "sMoifbuxjB",
      "title": "Towards Meta-Pruning via Optimal Transport",
      "abstract": "Structural pruning of neural networks conventionally relies on identifying and discarding less important neurons, a practice often resulting in significant accuracy loss that necessitates subsequent fine-tuning efforts. This paper introduces a novel approach named Intra-Fusion, challenging this prevailing pruning paradigm.\nUnlike existing methods that focus on designing meaningful neuron importance metrics, Intra-Fusion redefines the overlying pruning procedure.\nThrough utilizing the concepts of model fusion and Optimal Transport, we leverage an agnostically given importance metric to arrive at a more effective sparse model representation.\nNotably, our approach achieves substantial accuracy recovery without the need for resource-intensive fine-tuning, making it an efficient and promising tool for neural network compression.\nAdditionally, we explore how fusion can be added to the pruning process to significantly decrease the training time while maintaining competitive performance. We benchmark our results for various networks on commonly used datasets such as CIFAR-10, CIFAR-100, and ImageNet. More broadly, we hope that the proposed Intra-Fusion approach invigorates exploration into a fresh alternative to the predominant compression approaches.\nOur code is available [here](https://github.com/alexandertheus/Intra-Fusion).",
      "authors": [
        "Alexander Theus",
        "Olin Geimer",
        "Friedrich Wicke",
        "Thomas Hofmann",
        "Sotiris Anagnostidis",
        "Sidak Pal Singh"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=sMoifbuxjB",
      "cdate": 1695367801062,
      "mdate": 1713101935713,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711232"
    },
    {
      "id": "BLGQ3oqldb",
      "title": "LogicMP: A Neuro-symbolic Approach for Encoding First-order Logic Constraints",
      "abstract": "Integrating first-order logic constraints (FOLCs) with neural networks is a crucial but challenging problem since it involves modeling intricate correlations to satisfy the constraints. This paper proposes a novel neural layer, LogicMP, which performs mean-field variational inference over a Markov Logic Network (MLN). It can be plugged into any off-the-shelf neural network to encode FOLCs while retaining modularity and efficiency. By exploiting the structure and symmetries in MLNs, we theoretically demonstrate that our well-designed, efficient mean-field iterations greatly mitigate the difficulty of MLN inference, reducing the inference from sequential calculation to a series of parallel tensor operations. Empirical results in three kinds of tasks over images, graphs, and text show that LogicMP outperforms advanced competitors in both performance and efficiency.",
      "authors": [
        "Weidi Xu",
        "Jingwei Wang",
        "Lele Xie",
        "Jianshan He",
        "Hongting Zhou",
        "Taifeng Wang",
        "Xiaopei Wan",
        "Jingdong Chen",
        "Chao Qu",
        "Wei Chu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BLGQ3oqldb",
      "cdate": 1695367482202,
      "mdate": 1710312518278,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711238"
    },
    {
      "id": "eiC4BKypf1",
      "title": "Turning large language models into cognitive models",
      "abstract": "Large language models are powerful systems that excel at many tasks, ranging from translation to mathematical reasoning. Yet, at the same time, these models often show unhuman-like characteristics. In the present paper, we address this gap and ask whether large language models can be turned into cognitive models. We find that -- after finetuning them on data from psychological experiments -- these models offer accurate representations of human behavior, even outperforming traditional cognitive models in two decision-making domains. In addition, we show that their representations contain the information necessary to model behavior on the level of individual subjects. Finally, we demonstrate that finetuning on multiple tasks enables large language models to predict human behavior in a previously unseen task. Taken together, these results suggest that large, pre-trained models can be adapted to become models of human cognition, which opens up future research directions toward building more general cognitive models.",
      "authors": [
        "Marcel Binz",
        "Eric Schulz"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=eiC4BKypf1",
      "cdate": 1695367480310,
      "mdate": 1709983953964,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.711243"
    },
    {
      "id": "vI95kcLAoU",
      "title": "Skip-Attention: Improving Vision Transformers by Paying Less Attention",
      "abstract": "This work aims to improve the efficiency of vision transformers (ViTs).  While ViTs use computationally expensive self-attention operations in every layer, we identify that these operations are highly correlated across layers --  a key redundancy that causes unnecessary computations. Based on this observation, we propose SkipAT a method to reuse self-attention computation from preceding layers to approximate attention at one or more subsequent layers. To ensure that reusing self-attention blocks across layers does not degrade the performance, we introduce a simple parametric function, which outperforms the baseline transformer's performance while running computationally faster. We show that SkipAT is agnostic to transformer architecture and is effective in image classification,  semantic segmentation on ADE20K, image denoising on SIDD, and video denoising on DAVIS. We achieve improved throughput at the same-or-higher accuracy levels in all these tasks.",
      "authors": [
        "Shashanka Venkataramanan",
        "Amir Ghodrati",
        "Yuki M Asano",
        "Fatih Porikli",
        "Amir Habibian"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vI95kcLAoU",
      "cdate": 1695367456003,
      "mdate": 1713540640709,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.711247"
    },
    {
      "id": "u7559ZMvwY",
      "title": "Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization",
      "abstract": "The deep neural networks are known to be vulnerable to well-designed adversarial attacks. The most successful defense technique based on adversarial training (AT) can achieve optimal robustness against particular attacks but cannot generalize well to unseen attacks. Another effective defense technique based on adversarial purification (AP) can enhance generalization but cannot achieve optimal robustness. Meanwhile, both methods share one common limitation on the degraded standard accuracy. To mitigate these issues, we propose a novel pipeline to acquire the robust purifier model, named Adversarial Training on Purification (AToP), which comprises two components: perturbation destruction by random transforms (RT) and purifier model fine-tuned (FT) by adversarial loss. RT is essential to avoid overlearning to known attacks, resulting in the robustness generalization to unseen attacks, and FT is essential for the improvement of robustness. \nTo evaluate our method in an efficient and scalable way, we conduct extensive experiments on CIFAR-10, CIFAR-100, and ImageNette to demonstrate that our method achieves optimal robustness and exhibits generalization ability against unseen attacks.",
      "authors": [
        "Guang Lin",
        "Chao Li",
        "Jianhai Zhang",
        "Toshihisa Tanaka",
        "Qibin Zhao"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=u7559ZMvwY",
      "cdate": 1695367287910,
      "mdate": 1712895172240,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711252"
    },
    {
      "id": "MLBdiWu4Fw",
      "title": "InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation",
      "abstract": "This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. InternVid contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words. Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale. Specifically, we utilize a multi-scale approach to generate video-related descriptions. Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance. Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications. They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research. These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation.",
      "authors": [
        "Yi Wang",
        "Yinan He",
        "Yizhuo Li",
        "Kunchang Li",
        "Jiashuo Yu",
        "Xin Ma",
        "Xinhao Li",
        "Guo Chen",
        "Xinyuan Chen",
        "Yaohui Wang",
        "Ping Luo",
        "Ziwei Liu",
        "Yali Wang",
        "Limin Wang",
        "Yu Qiao"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MLBdiWu4Fw",
      "cdate": 1695367201861,
      "mdate": 1713099671200,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:05.711260"
    },
    {
      "id": "tbFBh3LMKi",
      "title": "Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization",
      "abstract": "Combining offline and online reinforcement learning (RL) is crucial for efficient and safe learning. However, previous approaches treat offline and online learning as separate procedures, resulting in redundant designs and limited performance. We ask: *Can we achieve straightforward yet effective offline and online learning without introducing extra conservatism or regularization?* In this study, we propose Uni-O4, which utilizes an on-policy objective for both offline and online learning. Owning to the alignment of objectives in two phases, the RL agent can transfer between offline and online learning seamlessly. This property enhances the flexibility of the learning paradigm, allowing for arbitrary combinations of pretraining, fine-tuning, offline, and online learning. In the offline phase, specifically, Uni-O4 leverages diverse ensemble policies to address the mismatch issues between the estimated behavior policy and the offline dataset. Through a simple offline policy evaluation (OPE) approach, Uni-O4 can achieve multi-step policy improvement safely. We demonstrate that by employing the method above, the fusion of these two paradigms can yield superior offline initialization as well as stable and rapid online fine-tuning capabilities. \nThrough real-world robot tasks, we highlight the benefits of this paradigm for rapid deployment in challenging, previously unseen real-world environments. Additionally, through comprehensive evaluations using numerous simulated benchmarks, we substantiate that our method achieves state-of-the-art performance in both offline and offline-to-online fine-tuning learning. [Our website](uni-o4.github.io)",
      "authors": [
        "Kun LEI",
        "Zhengmao He",
        "Chenhao Lu",
        "Kaizhe Hu",
        "Yang Gao",
        "Huazhe Xu"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tbFBh3LMKi",
      "cdate": 1695367104660,
      "mdate": 1710505784548,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.711265"
    },
    {
      "id": "Gg7cXo3S8l",
      "title": "Dictionary Contrastive Learning for Efficient Local Supervision without Auxiliary Networks",
      "abstract": "While backpropagation (BP) has achieved widespread success in deep learning, it\nfaces two prominent challenges: computational inefficiency and biological implausibility.\nIn response to these challenges, local supervision, encompassing Local\nLearning (LL) and Forward Learning (FL), has emerged as a promising research\ndirection. LL employs module-wise BP to achieve competitive results yet relies on\nmodule-wise auxiliary networks, which increase memory and parameter demands.\nConversely, FL updates layer weights without BP and auxiliary networks but falls\nshort of BP’s performance. This paper proposes a simple yet effective objective\nwithin a contrastive learning framework for local supervision without auxiliary\nnetworks. Given the insight that the existing contrastive learning framework for\nlocal supervision is susceptible to task-irrelevant information without auxiliary\nnetworks, we present DICTIONARY CONTRASTIVE LEARNING (DCL) that optimizes\nthe similarity between local features and label embeddings. Our method\nusing static label embeddings yields substantial performance improvements in the\nFL scenario, outperforming state-of-the-art FL approaches. Moreover, our method\nusing adaptive label embeddings closely approaches the performance achieved by\nLL while achieving superior memory and parameter efficiency.",
      "authors": [
        "Suhwan Choi",
        "Myeongho Jeon",
        "Yeonjung Hwang",
        "Jeonglyul Oh",
        "Sungjun Lim",
        "Joonseok Lee",
        "Myungjoo Kang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Gg7cXo3S8l",
      "cdate": 1695367047828,
      "mdate": 1713672815778,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.711270"
    },
    {
      "id": "PnR1MNen7u",
      "title": "Deep Geodesic Canonical Correlation Analysis for Covariance-Based Neuroimaging Data",
      "abstract": "In human neuroimaging, multi-modal imaging techniques are frequently combined to enhance our comprehension of whole-brain dynamics and improve diagnosis in clinical practice. Modalities like electroencephalography and functional magnetic resonance imaging provide distinct views to the brain dynamics due to diametral spatiotemporal sensitivities and underlying neurophysiological coupling mechanisms. These distinct views pose a considerable challenge to learning a shared representation space, especially when dealing with covariance-based data characterized by their geometric structure. To capitalize on the geometric structure, we introduce a measure called geodesic correlation which expands traditional correlation consistency to covariance-based data on the symmetric positive definite (SPD) manifold. This measure is derived from classical canonical correlation analysis and serves to evaluate the consistency of latent representations obtained from paired views. For multi-view, self-supervised learning where one or both latent views are SPD we propose an innovative geometric deep learning framework termed DeepGeoCCA. Its primary objective is to enhance the geodesic correlation of unlabeled, paired data, thereby generating novel representations while retaining the geometric structures. In simulations and experiments with multi-view and multi-modal human neuroimaging data, we find that DeepGeoCCA learns latent representations with high geodesic correlation for unseen data while retaining relevant information for downstream tasks.",
      "authors": [
        "Ce Ju",
        "Reinmar J Kobler",
        "Liyao Tang",
        "Cuntai Guan",
        "Motoaki Kawanabe"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=PnR1MNen7u",
      "cdate": 1695366681071,
      "mdate": 1710502786822,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:05.711275"
    },
    {
      "id": "mNYF0IHbRy",
      "title": "LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts",
      "abstract": "Diffusion-based generative models have significantly advanced text-to-image generation but encounter challenges when processing lengthy and intricate text prompts describing complex scenes with multiple objects. While excelling in generating images from short, single-object descriptions, these models often struggle to faithfully capture all the nuanced details within longer and more elaborate textual inputs. In response, we present a novel approach leveraging Large Language Models (LLMs) to extract critical components from text prompts, including bounding box coordinates for foreground objects, detailed textual descriptions for individual objects, and a succinct background context. These components form the foundation of our layout-to-image generation model, which operates in two phases. The initial Global Scene Generation utilizes object layouts and background context to create an initial scene but often falls short in faithfully representing object characteristics as specified in the prompts. To address this limitation, we introduce an Iterative Refinement Scheme that iteratively evaluates and refines box-level content to align them with their textual descriptions, recomposing objects as needed to ensure consistency. Our evaluation on complex prompts featuring multiple objects demonstrates a substantial improvement in recall compared to baseline diffusion models. This is further validated by a user study, underscoring the efficacy of our approach in generating coherent and detailed scenes from intricate textual inputs. Our iterative framework offers a promising solution for enhancing text-to-image generation models' fidelity with lengthy, multifaceted descriptions, opening new possibilities for accurate and diverse image synthesis from textual inputs.",
      "authors": [
        "Hanan Gani",
        "Shariq Farooq Bhat",
        "Muzammal Naseer",
        "Salman Khan",
        "Peter Wonka"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=mNYF0IHbRy",
      "cdate": 1695366644633,
      "mdate": 1711421206459,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:05.711282"
    },
    {
      "id": "tveiUXU2aa",
      "title": "SWAP-NAS: Sample-Wise Activation Patterns for Ultra-fast NAS",
      "abstract": "Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid resource-intensive neural network training, especially in Neural Architecture Search (NAS). Recent studies show that existing training-free metrics have several limitations, such as limited correlation and poor generalisation across different search spaces and tasks. Hence, we propose Sample-Wise Activation Patterns and its derivative, SWAP-Score, a novel high-performance training-free metric. It measures the expressivity of networks over a batch of input samples. The SWAP-Score is strongly correlated with ground-truth performance across various search spaces and tasks, outperforming 15 existing training-free metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be further enhanced by regularisation, which leads to even higher correlations in cell-based search space and enables model size control during the search. For example, Spearman’s rank correlation coefficient between regularised SWAP-Score and CIFAR-100 validation accuracies on NAS-Bench-201 networks is 0.90, significantly higher than 0.80 from the second-best metric, NWOT. When integrated with an evolutionary algorithm for NAS, our SWAP-NAS achieves competitive performance on CIFAR-10 and ImageNet in approximately 6 minutes and 9 minutes of GPU time respectively.",
      "authors": [
        "Yameng Peng",
        "Andy Song",
        "Haytham M. Fayek",
        "Vic Ciesielski",
        "Xiaojun Chang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tveiUXU2aa",
      "cdate": 1695366302606,
      "mdate": 1713672130199,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711288"
    },
    {
      "id": "EWTFMkTdkT",
      "title": "Invariance-based Learning of Latent Dynamics",
      "abstract": "We propose a new model class aimed at predicting dynamical trajectories from high-dimensional empirical data. This is done by combining variational autoencoders and (spatio-)temporal transformers within a  framework designed to enforce certain scientifically-motivated invariances. The models allow inference of system behavior at any continuous time and generalization well beyond the data distributions seen during training. Furthermore, the models do not require an explicit neural ODE formulation, making them efficient and highly scalable in practice. We study  behavior through simple theoretical analyses and  extensive empirical experiments. The latter investigate  the ability to predict the trajectories of complicated  systems based on finite data and show that the proposed approaches can outperform existing neural-dynamical models. We study also more general inductive bias in the context of transfer to data obtained under entirely novel system interventions. Overall, our results provide a new framework for efficiently learning complicated dynamics in a data-driven manner, with potential applications in a wide range of fields including physics, biology, and engineering.",
      "authors": [
        "Kai Lagemann",
        "Christian Lagemann",
        "Sach Mukherjee"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EWTFMkTdkT",
      "cdate": 1695366262734,
      "mdate": 1710586638365,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.711292"
    },
    {
      "id": "90yw2uM6J5",
      "title": "Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh Transformer",
      "abstract": "Recently, many mesh-based graph neural network (GNN) models have been proposed for modeling complex high-dimensional physical systems. Remarkable achievements have been made in significantly reducing the solving time compared to traditional numerical solvers. These methods are typically designed to i) reduce the computational cost in solving physical dynamics and/or ii) propose techniques to enhance the solution accuracy in fluid and rigid body dynamics. However, it remains under-explored whether they are effective in addressing the challenges of flexible body dynamics, where instantaneous collisions occur within a very short timeframe. In this paper, we present Hierarchical Contact Mesh Transformer (HCMT), which uses hierarchical mesh structures and can learn long-range dependencies (occurred by collisions) among spatially distant positions of a body --- two close positions in a higher-level mesh correspond to two distant positions in a lower-level mesh. HCMT enables long-range interactions, and the hierarchical mesh structure quickly propagates collision effects to faraway positions. To this end, it consists of a contact mesh Transformer and a hierarchical mesh Transformer (CMT and HMT, respectively). Lastly, we propose a flexible body dynamics dataset,  consisting of trajectories that reflect experimental settings frequently used in the display industry for product designs. We also compare the performance of several baselines using well-known benchmark datasets. Our results show that HCMT provides significant performance improvements over existing methods. Our code is available at https://github.com/yuyudeep/hcmt.",
      "authors": [
        "Youn-Yeol Yu",
        "Jeongwhan Choi",
        "Woojin Cho",
        "Kookjin Lee",
        "Nayong Kim",
        "Kiseok Chang",
        "ChangSeung Woo",
        "ILHO KIM",
        "SeokWoo Lee",
        "Joon Young Yang",
        "SOOYOUNG YOON",
        "Noseong Park"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=90yw2uM6J5",
      "cdate": 1695365960371,
      "mdate": 1711417379394,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:05.711297"
    },
    {
      "id": "rL7xsg1aRn",
      "title": "Masked Structural Growth for 2x Faster Language Model Pre-training",
      "abstract": "Accelerating large language model pre-training is a critical issue in present research. In this paper, we focus on speeding up pre-training by progressively growing from a small Transformer structure to a large one. There are two main research problems associated with progressive growth: determining the optimal growth schedule, and designing efficient growth operators. In terms of growth schedule, the impact of each single dimension on a schedule’s efficiency is underexplored by existing work. Regarding the growth operators, existing methods rely on the initialization of new weights to inherit knowledge, and achieve only non-strict function preservation, limiting further improvements on training dynamics. To address these issues, we propose Masked Structural Growth (MSG), including (i) growth schedules involving all possible dimensions and (ii) strictly function-preserving growth operators that is independent of the initialization of new weights. Experiments show that MSG is significantly faster than related work: we achieve up to 2.2x speedup in pre-training different types of language models while maintaining comparable or better downstream performances. Code is publicly available at https://github.com/cofe-ai/MSG.",
      "authors": [
        "Yiqun Yao",
        "Zheng Zhang",
        "Jing Li",
        "Yequan Wang"
      ],
      "conference": "ICLR 2024",
      "venue_id": "ICLR.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rL7xsg1aRn",
      "cdate": 1695365869120,
      "mdate": 1709910999717,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:05.711301"
    }
  ]
}