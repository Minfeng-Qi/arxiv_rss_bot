{
  "conference": "ICLR 2025",
  "papers_count": 622,
  "timestamp": "2025-08-10T23:47:02.839776",
  "papers": [
    {
      "id": "PwxYoMvmvy",
      "title": "Beyond Random Masking: When Dropout meets Graph Convolutional Networks",
      "abstract": "Graph Convolutional Networks (GCNs) have emerged as powerful tools for learning on graph-structured data, yet the behavior of dropout in these models remains poorly understood. This paper presents a comprehensive theoretical analysis of dropout in GCNs, revealing that its primary role differs fundamentally from standard neural networks - preventing oversmoothing rather than co-adaptation. We demonstrate that dropout in GCNs creates dimension-specific stochastic sub-graphs, leading to a form of structural regularization not present in standard neural networks. Our analysis shows that dropout effects are inherently degree-dependent, resulting in adaptive regularization that considers the topological importance of nodes. We provide new insights into dropout's role in mitigating oversmoothing and derive novel generalization bounds that account for graph-specific dropout effects. Furthermore, we analyze the synergistic interaction between dropout and batch normalization in GCNs, uncovering a mechanism that enhances overall regularization. Our theoretical findings are validated through extensive experiments on both node-level and graph-level tasks across 14 datasets. Notably, GCN with dropout and batch normalization outperforms state-of-the-art methods on several benchmarks, demonstrating the practical impact of our theoretical insights.",
      "authors": [
        "Yuankai Luo",
        "Xiao-Ming Wu",
        "Hao Zhu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=PwxYoMvmvy",
      "cdate": 1727524763710,
      "mdate": 1743343395857,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833141"
    },
    {
      "id": "ONfWFluZBI",
      "title": "Self-supervised contrastive learning performs non-linear system identification",
      "abstract": "Self-supervised learning (SSL) approaches have brought tremendous success across many tasks and domains. It has been argued that these successes can be attributed to a link between SSL and identifiable representation learning: Temporal structure and auxiliary variables ensure that latent representations are related to the true underlying generative factors of the data. Here, we deepen this connection and show that SSL can perform system identification in latent space. We propose dynamics contrastive learning, a framework to uncover linear, switching linear and non-linear dynamics under a non-linear observation model, give theoretical guarantees and validate them empirically.",
      "authors": [
        "Rodrigo González Laiz",
        "Tobias Schmidt",
        "Steffen Schneider"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=ONfWFluZBI",
      "cdate": 1727524721979,
      "mdate": 1747430543508,
      "matched_keywords": [
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833167"
    },
    {
      "id": "odjMSBSWRt",
      "title": "DarkBench: Benchmarking Dark Patterns in Large Language Models",
      "abstract": "We introduce DarkBench, a comprehensive benchmark for detecting dark design patterns—manipulative techniques that influence user behavior—in interactions with large language models (LLMs). Our benchmark comprises 660 prompts across six categories: brand bias, user retention, sycophancy, anthropomorphism, harmful generation, and sneaking. We evaluate models from five leading companies (OpenAI, Anthropic, Meta, Mistral, Google) and find that some LLMs are explicitly designed to favor their developers' products and exhibit untruthful communication, among other manipulative behaviors. Companies developing LLMs should recognize and mitigate the impact of dark design patterns to promote more ethical Al.",
      "authors": [
        "Esben Kran",
        "Hieu Minh Nguyen",
        "Akash Kundu",
        "Sami Jawhar",
        "Jinsuk Park",
        "Mateusz Maria Jurewicz"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=odjMSBSWRt",
      "cdate": 1727524390512,
      "mdate": 1740756776991,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833176"
    },
    {
      "id": "imT03YXlG2",
      "title": "Sparse autoencoders reveal selective remapping of visual concepts during adaptation",
      "abstract": "Adapting foundation models for specific purposes has become a standard approach to build machine learning systems for downstream applications. Yet, it is an open question which mechanisms take place during adaptation. Here we develop a new Sparse Autoencoder (SAE) for the CLIP vision transformer, named PatchSAE, to extract interpretable concepts at granular levels (e.g., shape, color, or semantics of an object) and their patch-wise spatial attributions. We explore how these concepts influence the model output in downstream image classification tasks and investigate how recent state-of-the-art prompt-based adaptation techniques change the association of model inputs to these concepts. While activations of concepts slightly change between adapted and non-adapted models, we find that the majority of gains on common adaptation tasks can be explained with the existing concepts already present in the non-adapted foundation model. This work provides a concrete framework to train and use SAEs for Vision Transformers and provides insights into explaining adaptation mechanisms.",
      "authors": [
        "Hyesu Lim",
        "Jinho Choi",
        "Jaegul Choo",
        "Steffen Schneider"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=imT03YXlG2",
      "cdate": 1727524002651,
      "mdate": 1742005128906,
      "matched_keywords": [
        "foundation model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.833183"
    },
    {
      "id": "w7P92BEsb2",
      "title": "PIED: Physics-Informed Experimental Design for Inverse Problems",
      "abstract": "In many science and engineering settings, system dynamics are characterized by governing partial differential equations (PDEs), and a major challenge is to solve inverse problems (IPs) where unknown PDE parameters are inferred based on observational data gathered under limited budget. \nDue to the high costs of setting up and running experiments, experimental design (ED) is often done with the help of PDE simulations to optimize for the most informative design parameters (e.g., sensor placements) to solve such IPs, prior to actual data collection. This process of optimizing design parameters is especially critical when the budget and other practical constraints make it infeasible to adjust the design parameters between trials during the experiments.\nHowever, existing experimental design (ED) methods tend to require sequential and frequent design parameter adjustments between trials. Furthermore, they also have significant computational bottlenecks due to the need for complex numerical simulations for PDEs, and do not exploit the advantages provided by physics informed neural networks (PINNs) in solving IPs for PDE-governed systems, such as its meshless solutions, differentiability, and amortized training. \nThis work presents Physics-Informed Experimental Design (PIED), the first ED framework that makes use of PINNs in a fully differentiable architecture to perform continuous optimization of design parameters for IPs for one-shot deployments. \nPIED overcomes existing methods' computational bottlenecks through parallelized computation and meta-learning of PINN parameter initialization, and proposes novel methods to effectively take into account PINN training dynamics in optimizing the ED parameters. \nThrough experiments based on noisy simulated data and even real world experimental data, we empirically show that given limited observation budget, PIED significantly outperforms existing ED methods in solving IPs, including for challenging settings where the inverse parameters are unknown functions rather than just finite-dimensional.",
      "authors": [
        "Apivich Hemachandra",
        "Gregory Kang Ruey Lau",
        "See-Kiong Ng",
        "Bryan Kian Hsiang Low"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=w7P92BEsb2",
      "cdate": 1727523633876,
      "mdate": 1740730093966,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833190"
    },
    {
      "id": "FDimWzmcWn",
      "title": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning",
      "abstract": "Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via instruction tuning. We first observe that the existing agent training corpus exhibits satisfactory results on held-in evaluation sets but fails to generalize to held-out sets. These agent-tuning works face severe formatting errors and are frequently stuck in the same mistake for a long while. We analyze that the poor generalization ability comes from overfitting to several manual agent environments and a lack of adaptation to new situations. They struggle with the wrong action steps and can not learn from the experience but just memorize existing observation-action relations. Inspired by the insight, we propose a novel AgentRefine framework for agent-tuning. The core idea is to enable the model to learn to correct its mistakes via observation in the trajectory. Specifically, we propose an agent synthesis framework to encompass a diverse array of environments and tasks and prompt a strong LLM to refine its error action according to the environment feedback. AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent tasks. It also has better robustness facing perturbation and can generate diversified thought in inference. Our findings establish the correlation between agent generalization and self-refinement and provide a new paradigm for future research.",
      "authors": [
        "Dayuan Fu",
        "Keqing He",
        "Yejie Wang",
        "Wentao Hong",
        "Zhuoma GongQue",
        "Weihao Zeng",
        "Wei Wang",
        "Jingang Wang",
        "Xunliang Cai",
        "Weiran Xu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=FDimWzmcWn",
      "cdate": 1727523383328,
      "mdate": 1739261806123,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833197"
    },
    {
      "id": "Sd4wYYOhmY",
      "title": "TabM: Advancing tabular deep learning with parameter-efficient ensembling",
      "abstract": "Deep learning architectures for supervised learning on tabular data range from simple multilayer perceptrons (MLP) to sophisticated Transformers and retrieval-augmented methods.\nThis study highlights a major, yet so far overlooked opportunity for substantially improving tabular MLPs; namely, parameter-efficient ensembling -- a paradigm for imitating an ensemble of models with just one model.\nWe start by describing TabM -- a simple model based on MLP and BatchEnsemble (an existing technique), improved with our custom modifications.\nThen, we perform a large scale evaluation of tabular DL architectures on public benchmarks in terms of both task performance and efficiency, which renders the landscape of tabular DL in a new light.\nIn particular, we find that TabM outperforms prior tabular DL models, while the complexity of attention- and retrieval-based methods does not pay off.\nLastly, we conduct a detailed empirical analysis, that sheds some light on the high performance of TabM.\nFor example, we show that parameter-efficient ensembling is not an arbitrary trick, but rather a highly effective way to reduce overfitting and improve optimization dynamics of tabular MLPs.\nOverall, our work brings an impactful technique to tabular DL, analyses its behaviour, and advances the performance-efficiency tradeoff with TabM -- a simple and powerful baseline for researchers and practitioners.",
      "authors": [
        "Yury Gorishniy",
        "Akim Kotelnikov",
        "Artem Babenko"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Sd4wYYOhmY",
      "cdate": 1727522929827,
      "mdate": 1740915621376,
      "matched_keywords": [
        "transformer",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833204"
    },
    {
      "id": "XLMAMmowdY",
      "title": "ToolGen: Unified Tool Retrieval and Calling via Generation",
      "abstract": "As large language models (LLMs) advance, their inability to autonomously execute tasks by directly interacting with external tools remains a critical limitation. Traditional methods rely on inputting tool descriptions as context, which is constrained by context length and requires separate, often inefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that integrates tool knowledge directly into the LLM’s parameters by representing each tool as a unique token. This enables the LLM to generate tool calls and arguments as part of its next token prediction capabilities, seamlessly blending tool invocation with language generation.  Our framework allows the LLM to access and utilize a vast amount of tools with no additional retrieval step, significantly enhancing both performance and scalability. Experimental results with over 47,000 tools show that ToolGen not only achieves superior results in both tool retrieval and autonomous task completion but also sets the stage for a new era of AI agents that can adapt to tools across diverse domains.  By fundamentally transforming tool retrieval into a generative process, ToolGen paves the way for more versatile, efficient, and autonomous AI systems. ToolGen enables end-to-end tool learning and opens opportunities for integration with other advanced techniques such as chain-of-thought and reinforcement learning, thereby expanding the practical capabilities of LLMs",
      "authors": [
        "Renxi Wang",
        "Xudong Han",
        "Lei Ji",
        "Shu Wang",
        "Timothy Baldwin",
        "Haonan Li"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=XLMAMmowdY",
      "cdate": 1727522305573,
      "mdate": 1740743617391,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833226"
    },
    {
      "id": "yRKelogz5i",
      "title": "Causally Motivated Sycophancy Mitigation for Large Language Models",
      "abstract": "Incorporating user preferences into large language models (LLMs) can enhance the personalization and reliability of model outputs and facilitate the application of LLMs to real-world scenarios. However, leveraging user preferences can be a double-edged sword. Recent studies have found that improper utilization can incur sycophancy, where LLMs prioritize alignment with user preferences over the correctness of their outputs. To address sycophancy in LLMs, we analyze and model the problem through the lens of structured causal models (SCMs). We attribute sycophancy to LLMs' reliance on spurious correlations between user preferences and model outputs in this paper. Based on the proposed SCMs, we develop a novel framework, termed **CAUSM**, to mitigate sycophancy in LLMs by exploiting a significant causal signature. Specifically, we eliminate the spurious correlations embedded in the intermediate layers of LLMs through causally motivated head reweighting, and then calibrate the intra-head knowledge along the causal representation direction. Extensive experiments are conducted across diverse language tasks to demonstrate the superiority of our method over state-of-the-art competitors in mitigating sycophancy in LLMs.",
      "authors": [
        "Haoxi Li",
        "Xueyang Tang",
        "Jie ZHANG",
        "Song Guo",
        "Sikai Bai",
        "Peiran Dong",
        "Yue Yu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=yRKelogz5i",
      "cdate": 1727521690275,
      "mdate": 1740712425849,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833234"
    },
    {
      "id": "uClUUJk05H",
      "title": "Compositional simulation-based inference for time series",
      "abstract": "Amortized simulation-based inference (SBI) methods train neural networks on simulated data to perform Bayesian inference. While this strategy avoids the need for tractable likelihoods, it often requires a large number of simulations and has been challenging to scale to time series data. Scientific simulators frequently emulate real-world dynamics through thousands of single-state transitions over time. We propose an SBI approach that can exploit such Markovian simulators by locally identifying parameters consistent with individual state transitions. We then compose these local results to obtain a posterior over parameters that align with the entire time series observation. We focus on applying this approach to neural posterior score estimation but also show how it can be applied, e.g., to neural likelihood (ratio) estimation. We demonstrate that our approach is more simulation-efficient than directly estimating the global posterior on several synthetic benchmark tasks and simulators used in ecology and epidemiology. Finally, we validate scalability and simulation efficiency of our approach by applying it to a high-dimensional Kolmogorov flow simulator with around one million data dimensions.",
      "authors": [
        "Manuel Gloeckler",
        "Shoji Toyota",
        "Kenji Fukumizu",
        "Jakob H. Macke"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=uClUUJk05H",
      "cdate": 1727521244553,
      "mdate": 1740751395496,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833240"
    },
    {
      "id": "O6znYvxC1U",
      "title": "Bayesian Treatment of the Spectrum of the Empirical Kernel in (Sub)Linear-Width Neural Networks",
      "abstract": "We study Bayesian neural networks (BNNs) in the theoretical limits of infinitely increasing number of training examples, network width and input space dimension. Our findings establish new bridges between kernel-theoretic approaches and techniques derived from statistical mechanics through the correspondence between Mercer's eigenvalues and limiting spectral distributions of covariance matrices studied in random matrix theory. \n   Our theoretical contributions first consist in novel integral formulas that accurately describe the predictors of BNNs in the asymptotic linear-width and sublinear-width regimes. Moreover, we extend the recently developed renormalisation theory of deep linear neural networks, enabling a rigorous explanation of the mounting empirical evidence that hints at the theory's applicability to nonlinear BNNs with ReLU activations in the linear-width regime.\n   From a practical standpoint, our results introduce a novel technique for estimating the predictor statistics of a trained BNN that is applicable to the sublinear-width regime where the predictions of the renormalisation theory are inaccurate.",
      "authors": [
        "Ouns El Harzli",
        "Bernardo Cuenca Grau"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=O6znYvxC1U",
      "cdate": 1727520248641,
      "mdate": 1740843729164,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833247"
    },
    {
      "id": "3bcN6xlO6f",
      "title": "Video Action Differencing",
      "abstract": "How do two individuals differ when performing the same action? In this work, we introduce Video Action Differencing (VidDiff), the novel task of identifying subtle differences between videos of the same action, which has numerous applications, such as coaching and skill learning. To enable development on this new task, we first create VidDiffBench, a benchmark dataset containing 549 video pairs, with human annotations of 4,469 fine-grained action differences and 2,075 timestamps indicating where these differences occur. Our experiments demonstrate that VidDiffBench poses a significant challenge for state-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL. By analyzing the failure cases of LMMs on VidDiffBench, we highlight two key challenges for this task: localizing relevant sub-actions over two videos and fine-grained frame comparison. To overcome these, we propose the VidDiff method, an agentic workflow that breaks the task into three stages: action difference proposal, keyframe localization, and frame differencing, each stage utilizing specialized foundation models. To encourage future research in this new task, we release the benchmark and code.",
      "authors": [
        "James Burgess",
        "Xiaohan Wang",
        "Yuhui Zhang",
        "Anita Rau",
        "Alejandro Lozano",
        "Lisa Dunlap",
        "Trevor Darrell",
        "Serena Yeung-Levy"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=3bcN6xlO6f",
      "cdate": 1727519479363,
      "mdate": 1741928620650,
      "matched_keywords": [
        "foundation model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.833253"
    },
    {
      "id": "GdXI5zCoAt",
      "title": "RaSA: Rank-Sharing Low-Rank Adaptation",
      "abstract": "Low-rank adaptation (LoRA) has been prominently employed for parameter-efficient fine-tuning of large language models (LLMs). However, the limited expressive capacity of LoRA, stemming from the low-rank constraint, has been recognized as a bottleneck, particularly in rigorous tasks like code generation and mathematical reasoning. To address this limitation, we introduce Rank-Sharing Low-Rank Adaptation (RaSA), an innovative extension that enhances the expressive capacity of LoRA by leveraging partial rank sharing across layers. By forming a shared rank pool and applying layer-specific weighting, RaSA effectively increases the number of ranks without augmenting parameter overhead. Our theoretically grounded and empirically validated approach demonstrates that RaSA not only maintains the core advantages of LoRA but also significantly boosts performance in challenging code and math tasks. Code, data and scripts are available at: https://github.com/zwhe99/RaSA.",
      "authors": [
        "Zhiwei He",
        "Zhaopeng Tu",
        "Xing Wang",
        "Xingyu Chen",
        "Zhijie Wang",
        "Jiahao Xu",
        "Tian Liang",
        "Wenxiang Jiao",
        "Zhuosheng Zhang",
        "Rui Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=GdXI5zCoAt",
      "cdate": 1727518907525,
      "mdate": 1740900445172,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833258"
    },
    {
      "id": "3tukjsVyrE",
      "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data",
      "abstract": "Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs).\nTraditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant compared to text pre-training data, thereby limiting their scalability as LLMs.\nWe propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets.\nOur method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech.\nWe also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model  by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower sampling rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality.\nStarting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in both speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13\\% (Moshi) to 31\\%.\nWe further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.",
      "authors": [
        "Aohan Zeng",
        "Zhengxiao Du",
        "Mingdao Liu",
        "Lei Zhang",
        "shengmin jiang",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=3tukjsVyrE",
      "cdate": 1727518769117,
      "mdate": 1741014827196,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833264"
    },
    {
      "id": "kbeX97jExm",
      "title": "Neural Wave Equation for Irregularly Sampled Sequence Data",
      "abstract": "Sequence labeling problems arise in several real-world applications such as healthcare and robotics. In many such applications, sequence data are irregularly sampled and are of varying complexities. Recently, efforts have been made to develop neural ODE-based architectures to model the evolution of hidden states continuously in time, to address irregularly sampled sequence data. However, they assume a fixed architectural depth and limit their flexibility to adapt to data sets with varying complexities. We propose the neural wave equation, a novel deep learning method inspired by the wave equation, to address this through continuous modeling of depth. Neural Wave Equation models the evolution of hidden states continuously across time as well as depth by using a non-homogeneous wave equation parameterized by a neural network.  Through d'Alembert's analytical solution of the wave equation, we also show that the neural wave equation provides denser connections across the hidden states, allowing for better modeling capability.  We conduct experiments on several sequence labeling problems involving irregularly sampled sequence data and demonstrate the superior performance of the proposed neural wave equation model.",
      "authors": [
        "Arkaprava Majumdar",
        "M Anand Krishna",
        "P. K. Srijith"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=kbeX97jExm",
      "cdate": 1727518413321,
      "mdate": 1741428615786,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833270"
    },
    {
      "id": "5o9JJJPPm6",
      "title": "ComaDICE: Offline Cooperative Multi-Agent Reinforcement Learning with Stationary Distribution Shift Regularization",
      "abstract": "Offline reinforcement learning (RL) has garnered significant attention for its ability to learn effective policies from pre-collected datasets without the need for further environmental interactions. While promising results have been demonstrated in single-agent settings, offline multi-agent reinforcement learning (MARL) presents additional challenges due to the large joint state-action space and the complexity of multi-agent behaviors. A key issue in offline RL is the distributional shift, which arises when the target policy being optimized deviates from the behavior policy that generated the data. This problem is exacerbated in MARL due to the interdependence between agents' local policies and the expansive joint state-action space. Prior approaches have primarily addressed this challenge by incorporating regularization in the space of either Q-functions or policies. In this work, we propose a novel type of regularizer in the space of stationary distributions to address the distributional shift more effectively. Our algorithm, ComaDICE, provides a principled framework for offline cooperative MARL to correct the stationary distribution of the global policy, which is then leveraged to derive local policies for individual agents. Through extensive experiments on the offline multi-agent MuJoCo and StarCraft II benchmarks, we demonstrate that ComaDICE achieves superior performance compared to state-of-the-art offline MARL methods across nearly all tasks.",
      "authors": [
        "The Viet Bui",
        "Thanh Hong Nguyen",
        "Tien Anh Mai"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=5o9JJJPPm6",
      "cdate": 1727518370402,
      "mdate": 1740391238975,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833275"
    },
    {
      "id": "TbTJJNjumY",
      "title": "Boosting Neural Combinatorial Optimization for Large-Scale Vehicle Routing Problems",
      "abstract": "Neural Combinatorial Optimization (NCO) methods have exhibited promising performance in solving Vehicle Routing Problems (VRPs). However, most NCO methods rely on the conventional self-attention mechanism that induces excessive computational complexity, thereby struggling to contend with large-scale VRPs and hindering their practical applicability. In this paper, we propose a lightweight cross-attention mechanism with linear complexity, by which a Transformer network is developed to learn efficient and favorable solutions for large-scale VRPs. We also propose a Self-Improved Training (SIT) algorithm that enables direct model training on large-scale VRP instances, bypassing extensive computational overhead for attaining labels. By iterating solution reconstruction, the Transformer network itself can generate improved partial solutions as pseudo-labels to guide the model training. Experimental results on the Travelling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to 100K nodes indicate that our method consistently achieves superior performance for synthetic and real-world benchmarks, significantly boosting the scalability of NCO methods.",
      "authors": [
        "Fu Luo",
        "Xi Lin",
        "Yaoxin Wu",
        "Zhenkun Wang",
        "Tong Xialiang",
        "Mingxuan Yuan",
        "Qingfu Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=TbTJJNjumY",
      "cdate": 1727517472925,
      "mdate": 1740496972659,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.833282"
    },
    {
      "id": "r1KcapkzCt",
      "title": "Monte Carlo Planning with Large Language Model for Text-Based Game Agents",
      "abstract": "Text-based games provide valuable environments for language-based autonomous agents. However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations. Additionally, these algorithms perform uncertainty-driven exploration but lack language understanding and reasoning abilities.\nIn this paper, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages the language understanding and reasoning capabilities of Large Language Models (LLMs) alongside the exploratory advantages of tree search algorithms. Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms, enabling them to learn from past experiences and dynamically adjust action evaluations during planning. We conduct experiments on a series of text-based games from the Jericho benchmark. Our results demonstrate that the MC-DML algorithm significantly enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations. This demonstrates the effectiveness of our algorithm, paving the way for more efficient language-grounded planning in complex environments.",
      "authors": [
        "Zijing Shi",
        "Meng Fang",
        "Ling Chen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=r1KcapkzCt",
      "cdate": 1727517118093,
      "mdate": 1743461566247,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833291"
    },
    {
      "id": "rAoEub6Nw2",
      "title": "A Statistical Framework for Ranking LLM-based Chatbots",
      "abstract": "Large language models (LLMs) have transformed natural language processing, with frameworks like Chatbot Arena providing pioneering platforms for evaluating these models. By facilitating millions of pairwise comparisons based on human judgments, Chatbot Arena has become a cornerstone in LLM evaluation, offering rich datasets for ranking models in open-ended conversational tasks. Building upon this foundation, we propose a statistical framework that incorporates key advancements to address specific challenges in pairwise comparison analysis. First, we introduce a factored tie model that enhances the ability to handle ties—an integral aspect of human-judged comparisons—significantly improving the model's fit to observed data. Second, we extend the framework to model covariance between competitors, enabling deeper insights into performance relationships and facilitating intuitive groupings into performance tiers. Third, we resolve optimization challenges arising from parameter non-uniqueness by introducing novel constraints, ensuring stable and interpretable parameter estimation. Through rigorous evaluation and extensive experimentation, our framework demonstrates substantial improvements over existing methods in modeling pairwise comparison data. To support reproducibility and practical adoption, we release leaderbot, an open-source Python package implementing our models and analyses.",
      "authors": [
        "Siavash Ameli",
        "Siyuan Zhuang",
        "Ion Stoica",
        "Michael W. Mahoney"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=rAoEub6Nw2",
      "cdate": 1727516548241,
      "mdate": 1742111057279,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833298"
    },
    {
      "id": "QEHrmQPBdd",
      "title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style",
      "abstract": "Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. \nDespite their importance, existing reward model benchmarks often evaluate models by asking them to distinguish between responses generated by models of varying power. \nHowever, this approach fails to assess reward models on subtle but critical content changes and variations in style, resulting in a low correlation with policy model performance.\nTo this end, we introduce RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases. \nExtensive experiments demonstrate that RM-Bench strongly correlates with policy model performance, making it a reliable reference for selecting reward models to align language models effectively.\nWe evaluate nearly 40 reward models on RM-Bench. \nOur results reveal that even state-of-the-art models achieve an average performance of only 46.6%, which falls short of random-level accuracy (50%) when faced with style bias interference.\nThese findings highlight the significant room for improvement in current reward models.",
      "authors": [
        "Yantao Liu",
        "Zijun Yao",
        "Rui Min",
        "Yixin Cao",
        "Lei Hou",
        "Juanzi Li"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=QEHrmQPBdd",
      "cdate": 1727516541152,
      "mdate": 1740795991492,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833303"
    },
    {
      "id": "R2834dhBlo",
      "title": "Neural Interactive Proofs",
      "abstract": "We consider the problem of how a trusted, but computationally bounded agent (a 'verifier') can learn to interact with one or more powerful but untrusted agents ('provers') in order to solve a given task. More specifically, we study the case in which agents are represented using neural networks and refer to solutions of this problem as neural interactive proofs. First we introduce a unifying framework based on prover-verifier games (Anil et al., 2021), which generalises previously proposed interaction protocols. We then describe several new protocols for generating neural interactive proofs, and provide a theoretical comparison of both new and existing approaches. Finally, we support this theory with experiments in two domains: a toy graph isomorphism problem that illustrates the key ideas, and a code validation task using large language models. In so doing, we aim to create a foundation for future work on neural interactive proofs and their application in building safer AI systems.",
      "authors": [
        "Lewis Hammond",
        "Sam Adam-Day"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=R2834dhBlo",
      "cdate": 1727516407947,
      "mdate": 1742003908044,
      "matched_keywords": [
        "large language model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833309"
    },
    {
      "id": "tpHqsyZ3YX",
      "title": "ProAdvPrompter: A Two-Stage Journey to Effective Adversarial Prompting for LLMs",
      "abstract": "As large language models (LLMs) are increasingly being integrated into various real-world applications, the identification of their vulnerabilities to jailbreaking attacks becomes an essential component of ensuring the safety and reliability of LLMs. \nPrevious studies have developed LLM assistants, known as the adversarial prompter, to automatically generate suffixes that manipulate target LLMs into generating harmful and undesirable outputs.\nHowever, these approaches often suffer from low performance or generate semantically meaningless prompts, which can be easily identified by perplexity-based defenses.\nIn this paper, we introduce a novel two-stage method, $\\texttt{ProAdvPrompter}$, that significantly improves the performance of adversarial prompters.\nIn $\\texttt{ProAdvPrompter}$, the first stage (Exploration) utilizes the loss information to guide the adversarial prompter in generating suffixes that are more likely to elicit harmful responses.\nThen the second stage (Exploitation) iteratively fine-tunes the prompter using high-quality generated adversarial suffixes to further boost performance.\nAdditionally, we incorporate the prompt template to aid in the Exploration stage and propose a filtering mechanism to accelerate the training process in the Exploitation stage.\nWe evaluate $\\texttt{ProAdvPrompter}$ against the well-aligned LLMs (i.e., Llama2-Chat-7B and Llama3-chat-8B), achieving attack success rates of 99.68% and 97.12% respectively after 10 trials on the AdvBench dataset, thereby enhancing performance by $\\sim 2$ times compared to previous works.\nMoreover, $\\texttt{ProAdvPrompter}$ reduces training time by 20% on Llama3-Instruct-8B, generates more generalized adversarial suffixes, and demonstrates resilience against the perplexity defense.\nAn ablation study further evaluates the effects of key components in $\\texttt{ProAdvPrompter}$ (the prompt template and the filtering mechanism).",
      "authors": [
        "Hao Di",
        "Tong He",
        "Haishan Ye",
        "Yinghui Huang",
        "Xiangyu Chang",
        "Guang Dai",
        "Ivor Tsang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=tpHqsyZ3YX",
      "cdate": 1727515853042,
      "mdate": 1740907732052,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833314"
    },
    {
      "id": "dTkqaCKLPp",
      "title": "SCOPE: A Self-supervised Framework for Improving Faithfulness in Conditional Text Generation",
      "abstract": "Large Language Models (LLMs), when used for conditional text generation, often produce hallucinations, i.e., information that is unfaithful or not grounded in the input context. This issue arises in typical conditional text generation tasks, such as text summarization and data-to-text generation, where the goal is to produce fluent text based on contextual input. When fine-tuned on specific domains, LLMs struggle to provide faithful answers to a given context, often adding information or generating errors. One underlying cause of this issue is that LLMs rely on statistical patterns learned from their training data. This reliance can interfere with the model's ability to stay faithful to a provided context, leading to the generation of ungrounded information. We build upon this observation and introduce a novel self-supervised method for generating a training set of unfaithful samples. We then refine the model using a training process that encourages the generation of grounded outputs over unfaithful ones, drawing on preference-based training. Our approach leads to significantly more grounded text generation, outperforming existing self-supervised techniques in faithfulness, as evaluated through automatic metrics, LLM-based assessments, and human evaluations.",
      "authors": [
        "Song Duong",
        "Florian Le Bronnec",
        "Alexandre Allauzen",
        "Vincent Guigue",
        "Alberto Lumbreras",
        "Laure Soulier",
        "Patrick Gallinari"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=dTkqaCKLPp",
      "cdate": 1727515244434,
      "mdate": 1741947023751,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833320"
    },
    {
      "id": "NGKQoaqLpo",
      "title": "How new data permeates LLM knowledge and how to dilute it",
      "abstract": "Large language models continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when learning new information, LLMs exhibit a \"priming\" effect: learning a new fact can cause the model to inappropriately apply that knowledge in unrelated contexts.\nTo systematically study this phenomenon, we introduce \"Outlandish,\" a carefully curated dataset of 1320 diverse text samples designed to probe how new knowledge permeates through an LLM's existing knowledge base. Using this dataset, we show that the degree of priming after learning new information can be predicted by measuring the token probability of key words before training. This relationship holds robustly across different model architectures (PALM-2, Gemma, Llama), sizes, and training stages.\nFinally, we develop two novel techniques to modulate how new knowledge affects existing model behavior: (1) a ``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update pruning method. These approaches reduce undesirable priming effects by 50-95% while preserving the model's ability to learn new information. Our findings provide both empirical insights into how LLMs learn and practical tools for improving the specificity of knowledge insertion in language models. Further materials: https://sunchipsster1.github.io/projects/outlandish/",
      "authors": [
        "Chen Sun",
        "Renat Aksitov",
        "Andrey Zhmoginov",
        "Nolan Andrew Miller",
        "Max Vladymyrov",
        "Ulrich Rueckert",
        "Been Kim",
        "Mark Sandler"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=NGKQoaqLpo",
      "cdate": 1727513917184,
      "mdate": 1740890361532,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833325"
    },
    {
      "id": "G7sIFXugTX",
      "title": "SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement",
      "abstract": "Software engineers operating in complex and dynamic environments must continuously adapt to evolving requirements, learn iteratively from experience, and reconsider their approaches based on new insights. However, current large language model (LLM)-based software agents often follow linear, sequential processes that prevent backtracking and exploration of alternative solutions, limiting their ability to rethink their strategies when initial approaches prove ineffective. To address these challenges, we propose SWE-Search, a multi-agent framework that integrates Monte Carlo Tree Search (MCTS) with a self-improvement mechanism to enhance software agents' performance on repository-level software tasks. SWE-Search extends traditional MCTS by incorporating a hybrid value function that leverages LLMs for both numerical value estimation and qualitative evaluation. This enables self-feedback loops where agents iteratively refine their strategies based on both quantitative numerical evaluations and qualitative natural language assessments of pursued trajectories. The framework includes a SWE-Agent for adaptive exploration, a Value Agent for iterative feedback, and a Discriminator Agent that facilitates multi-agent debate for collaborative decision-making. Applied to the SWE-bench benchmark, our approach demonstrates a 23% relative improvement in performance across five models compared to standard open-source agents without MCTS. Our analysis reveals how performance scales with increased inference-time compute through deeper search, providing a pathway to improve software agents without requiring larger models or additional training data. This highlights the potential of self-evaluation driven search techniques in complex software engineering environments.",
      "authors": [
        "Antonis Antoniades",
        "Albert Örwall",
        "Kexun Zhang",
        "Yuxi Xie",
        "Anirudh Goyal",
        "William Yang Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=G7sIFXugTX",
      "cdate": 1727513880450,
      "mdate": 1743566661599,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833331"
    },
    {
      "id": "82p8VHRsaK",
      "title": "Language Models are Advanced Anonymizers",
      "abstract": "Recent privacy research on large language models (LLMs) has shown that they achieve near-human-level performance at inferring personal data from online texts. With ever-increasing model capabilities, existing text anonymization methods are currently lacking behind regulatory requirements and adversarial threats. In this work, we take two steps to bridge this gap: First, we present a new setting for evaluating anonymization in the face of adversarial LLM inferences, allowing for a natural measurement of anonymization performance while remedying some of the shortcomings of previous metrics. Then, within this setting, we develop a novel LLM-based adversarial anonymization framework leveraging the strong inferential capabilities of LLMs to inform our anonymization procedure. We conduct a comprehensive experimental evaluation of adversarial anonymization across 13 LLMs on real-world and synthetic online texts, comparing it against multiple baselines and industry-grade anonymizers. Our evaluation shows that adversarial anonymization outperforms current commercial anonymizers both in terms of the resulting utility and privacy. We support our findings with a human study (n=50) highlighting a strong and consistent human preference for LLM-anonymized texts.",
      "authors": [
        "Robin Staab",
        "Mark Vero",
        "Mislav Balunovic",
        "Martin Vechev"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=82p8VHRsaK",
      "cdate": 1727513856324,
      "mdate": 1739261802810,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833337"
    },
    {
      "id": "Ouu3HnIVBc",
      "title": "ADAM: An Embodied Causal Agent in Open-World Environments",
      "abstract": "In open-world environments like Minecraft, existing agents face challenges in continuously learning structured knowledge, particularly causality. These challenges stem from the opacity inherent in black-box models and an excessive reliance on prior knowledge during training, which impair their interpretability and generalization capability. To this end, we introduce ADAM, An emboDied causal Agent in Minecraft, which can autonomously navigate the open world, perceive multimodal context, learn causal world knowledge, and tackle complex tasks through lifelong learning. ADAM is empowered by four key components: 1) an interaction module, enabling the agent to execute actions while recording the interaction processes; 2) a causal model module, tasked with constructing an ever-growing causal graph from scratch, which enhances interpretability and reduces reliance on prior knowledge; 3) a controller module, comprising a planner, an actor, and a memory pool, using the learned causal graph to accomplish tasks; 4) a perception module, powered by multimodal large language models, enabling ADAM to perceive like a human player. Extensive experiments show that ADAM constructs a nearly perfect causal graph from scratch, enabling efficient task decomposition and execution with strong interpretability. Notably, in the modified Minecraft game where no prior knowledge is available, ADAM excels with remarkable robustness and generalization capability. ADAM pioneers a novel paradigm that integrates causal methods and embodied agents synergistically. Our project page is at https://opencausalab.github.io/ADAM.",
      "authors": [
        "Shu Yu",
        "Chaochao Lu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Ouu3HnIVBc",
      "cdate": 1727513804946,
      "mdate": 1743163318583,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.833343"
    },
    {
      "id": "wFg0shwoRe",
      "title": "Expected Return Symmetries",
      "abstract": "Symmetry is an important inductive bias that can improve model robustness and generalization across many deep learning domains. In multi-agent settings, a priori known symmetries have been shown to address a fundamental coordination failure mode known as mutually incompatible symmetry breaking; e.g. in a game where two independent agents can choose to move \"left\" or \"right\", and where a reward of +1 or -1 is received when the agents choose the same action or different actions, respectively. However, the efficient and automatic discovery of environment symmetries, in particular for decentralized partially observable Markov decision processes, remains an open problem. Furthermore, environmental symmetry breaking constitutes only one type of coordination failure, which motivates the search for a more accessible and broader symmetry class. In this paper, we introduce such a broader group of previously unexplored symmetries, which we call expected return symmetries, which contains environment symmetries as a subgroup. We show that agents trained to be compatible under the group of expected return symmetries achieve better zero-shot coordination results than those using environment symmetries. As an additional benefit, our method makes minimal a priori assumptions about the structure of their environment and does not require access to ground truth symmetries.",
      "authors": [
        "Darius Muglich",
        "Johannes Forkel",
        "Elise van der Pol",
        "Jakob Nicolaus Foerster"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=wFg0shwoRe",
      "cdate": 1727513797668,
      "mdate": 1743358696995,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833351"
    },
    {
      "id": "x83w6yGIWb",
      "title": "Beware of Calibration Data for Pruning Large Language Models",
      "abstract": "As large language models (LLMs) are widely applied across various fields, model\ncompression has become increasingly crucial for reducing costs and improving\ninference efficiency. Post-training pruning is a promising method that does not\nrequire resource-intensive iterative training and only needs a small amount of\ncalibration data to assess the importance of parameters. Recent research has enhanced post-training pruning from different aspects but few of them systematically\nexplore the effects of calibration data, and it is unclear if there exist better calibration data construction strategies. We fill this blank and surprisingly observe that\ncalibration data is also crucial to post-training pruning, especially for high sparsity. Through controlled experiments on important influence factors of calibration\ndata, including the pruning settings, the amount of data, and its similarity with\npre-training data, we observe that a small size of data is adequate, and more similar data to its pre-training stage can yield better performance. As pre-training data\nis usually inaccessible for advanced LLMs, we further provide a self-generating\ncalibration data synthesis strategy to construct feasible calibration data. Experimental results on recent strong open-source LLMs (e.g., DCLM, and LLaMA-3)\nshow that the proposed strategy can enhance the performance of strong pruning\nmethods (e.g., Wanda, DSnoT, OWL) by a large margin (up to 2.68%).",
      "authors": [
        "Yixin Ji",
        "Yang Xiang",
        "Juntao Li",
        "Qingrong Xia",
        "Ping Li",
        "Xinyu Duan",
        "Zhefeng Wang",
        "Min Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=x83w6yGIWb",
      "cdate": 1727513738844,
      "mdate": 1741748958315,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833357"
    },
    {
      "id": "Se6MgCtRhz",
      "title": "Herald: A Natural Language Annotated Lean 4 Dataset",
      "abstract": "Verifiable formal languages like Lean have profoundly impacted mathematical reasoning, particularly through the use of large language models (LLMs) for automated reasoning. A significant challenge in training LLMs for these formal languages is the lack of parallel datasets that align natural language with formal language proofs. To address this challenge, this paper introduces a novel framework for translating the Mathlib4 corpus (a unified library of mathematics in formal language Lean 4) into natural language. Building upon this, we employ a dual augmentation strategy that combines tactic-based and informal-based approaches, leveraging the Lean-jixia system, a Lean 4 analyzer. We present the results of this pipeline on Mathlib4 as Herald (Hierarchy and Retrieval-based Translated Lean Dataset). We also propose the Herald Translator, which is fine-tuned on Herald. Herald translator achieves a 96.7\\% accuracy (Pass@128) on formalizing statements in the miniF2F-test and a 23.5\\% accuracy on our internal graduate-level textbook dataset, outperforming InternLM2-Math-Plus-7B (73.0\\% and 7.5\\%) and TheoremLlama (50.1\\% and 4.0\\%). Furthermore, we propose a section-level translation framework for real-world applications. As a direct application of Herald translator, we have successfully translated a template section in the Stack project, marking a notable progress in the automatic formalization of graduate-level mathematical literature. Our model, along with the datasets, are open-sourced to the public.",
      "authors": [
        "Guoxiong Gao",
        "Yutong Wang",
        "Jiedong Jiang",
        "Qi Gao",
        "Zihan Qin",
        "Tianyi Xu",
        "Bin Dong"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Se6MgCtRhz",
      "cdate": 1727513599527,
      "mdate": 1740639076345,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833363"
    },
    {
      "id": "BUj9VSCoET",
      "title": "Efficient Residual Learning with Mixture-of-Experts for Universal Dexterous Grasping",
      "abstract": "Universal dexterous grasping across diverse objects presents a fundamental yet formidable challenge in robot learning. Existing approaches using reinforcement learning (RL) to develop policies on extensive object datasets face critical limitations, including complex curriculum design for multi-task learning and limited generalization to unseen objects. \nTo overcome these challenges, we introduce ResDex, a novel approach that integrates residual policy learning with a mixture-of-experts (MoE) framework. ResDex is distinguished by its use of geometry-agnostic base policies that are efficiently acquired on individual objects and capable of generalizing across a wide range of unseen objects. Our MoE framework incorporates several base policies to facilitate diverse grasping styles suitable for various objects. By learning residual actions alongside weights that combine these base policies, ResDex enables efficient multi-task RL for universal dexterous grasping.\nResDex achieves state-of-the-art performance on the DexGraspNet dataset comprising 3,200 objects with an 88.8% success rate. It exhibits no generalization gap with unseen objects and demonstrates superior training efficiency, mastering all tasks within only 12 hours on a single GPU. For further details and videos, visit our project page.",
      "authors": [
        "Ziye Huang",
        "Haoqi Yuan",
        "Yuhui Fu",
        "Zongqing Lu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=BUj9VSCoET",
      "cdate": 1727513462501,
      "mdate": 1740571462914,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833368"
    },
    {
      "id": "5z9GjHgerY",
      "title": "DPLM-2: A Multimodal Diffusion Protein Language Model",
      "abstract": "Proteins are essential macromolecules defined by their amino acid sequences, which determine their three-dimensional structures and, consequently, their functions in all living organisms. Therefore, generative protein modeling necessitates a multimodal approach to simultaneously model, understand, and generate both sequences and structures. However, existing methods typically use separate models for each modality, limiting their ability to capture the intricate relationships between sequence and structure. This results in suboptimal performance in tasks that requires joint understanding and generation of both modalities.\nIn this paper, we introduce DPLM-2, a multimodal protein foundation model that extends discrete diffusion protein language model (DPLM) to accommodate both sequences and structures.\nTo enable structural learning with the language model, 3D coordinates are converted to discrete tokens using a lookup-free quantization-based tokenizer.\nBy training on both experimental and high-quality synthetic structures, DPLM-2 learns the joint distribution of sequence and structure, as well as their marginals and conditionals.\nWe also implement an efficient warm-up strategy to exploit the connection between large-scale evolutionary data and structural inductive biases from pre-trained sequence-based protein language models.\nEmpirical evaluation shows that DPLM-2 can simultaneously generate highly compatible amino acid sequences and their corresponding 3D structures eliminating the need for a two-stage generation approach.\nMoreover, DPLM-2 demonstrates competitive performance in various conditional generation tasks, including folding, inverse folding, and scaffolding with multimodal motif inputs.",
      "authors": [
        "Xinyou Wang",
        "Zaixiang Zheng",
        "Fei YE",
        "Dongyu Xue",
        "Shujian Huang",
        "Quanquan Gu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=5z9GjHgerY",
      "cdate": 1727513420999,
      "mdate": 1739546810549,
      "matched_keywords": [
        "foundation model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.833374"
    },
    {
      "id": "moWiYJuSGF",
      "title": "Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation",
      "abstract": "Large language models (LLMs) have recently gained much attention in building autonomous agents. However, performance of current LLM-based web agents in long-horizon tasks is far from optimal, often yielding errors such as repeatedly buying a non-refundable flight ticket. By contrast, humans can avoid such an irreversible mistake, as we have an awareness of the potential outcomes (e.g., losing money) of our actions, also known as the \"world model\". Motivated by this, our study first starts with preliminary analyses, confirming the absence of world models in current LLMs (e.g., GPT-4o, Claude-3.5-Sonnet, etc.). Then, we present a World-model-augmented (WMA) web agent, which simulates the outcomes of its actions for better decision-making. To overcome the challenges in training LLMs as world models predicting next observations, such as repeated elements across observations and long HTML inputs, we propose a transition-focused observation abstraction, where the prediction objectives are free-form natural language descriptions exclusively highlighting important state differences between time steps. Experiments on WebArena and Mind2Web show that our world models improve agents' policy selection without training and demonstrate our agents' cost- and time-efficiency compared to recent tree-search-based agents.",
      "authors": [
        "Hyungjoo Chae",
        "Namyoung Kim",
        "Kai Tzu-iunn Ong",
        "Minju Gwak",
        "Gwanwoo Song",
        "Jihoon Kim",
        "Sunghwan Kim",
        "Dongha Lee",
        "Jinyoung Yeo"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=moWiYJuSGF",
      "cdate": 1727513353073,
      "mdate": 1740906066097,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833379"
    },
    {
      "id": "Kak2ZH5Itp",
      "title": "Language Imbalance Driven Rewarding for Multilingual Self-improving",
      "abstract": "Large Language Models (LLMs) have achieved state-of-the-art performance across numerous tasks. However, these advancements have predominantly benefited \"first-class\" languages such as English and Chinese, leaving many other languages underrepresented. This imbalance, while limiting broader applications, generates a natural preference ranking between languages, offering an opportunity to bootstrap the multilingual capabilities of LLM in a self-improving manner. Thus, we propose $\\textit{Language Imbalance Driven Rewarding}$, where the inherent imbalance between dominant and non-dominant languages within LLMs is leveraged as a reward signal. Iterative DPO training demonstrates that this approach not only enhances LLM performance in non-dominant languages but also improves the dominant language's capacity, thereby yielding an iterative reward signal. Fine-tuning Meta-Llama-3-8B-Instruct over two iterations of this approach results in continuous improvements in multilingual performance across instruction-following and arithmetic reasoning tasks, evidenced by an average improvement of 7.46\\% win rate on the X-AlpacaEval leaderboard and 13.9\\% accuracy on the MGSM benchmark. This work serves as an initial exploration, paving the way for multilingual self-improvement of LLMs.",
      "authors": [
        "Wen Yang",
        "Junhong Wu",
        "Chen Wang",
        "Chengqing Zong",
        "Jiajun Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Kak2ZH5Itp",
      "cdate": 1727513297178,
      "mdate": 1740576059530,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833385"
    },
    {
      "id": "rpwGUtTeA5",
      "title": "UniCBE: An Uniformity-driven Comparing Based Evaluation Framework with Unified Multi-Objective Optimization",
      "abstract": "Human preference plays a significant role in measuring large language models and guiding them to align with human values. Unfortunately, current comparing-based evaluation (CBE) methods typically focus on a single optimization objective, failing to effectively utilize scarce yet valuable preference signals. To address this, we delve into key factors that can enhance the accuracy, convergence, and scalability of CBE: suppressing sampling bias, balancing descending process of uncertainty, and mitigating updating uncertainty.\nFollowing the derived guidelines, we propose UniCBE, a unified uniformity-driven CBE framework which simultaneously optimize these core objectives by constructing and integrating three decoupled sampling probability matrices, each designed to ensure uniformity in specific aspects. We further ablate the optimal tuple sampling and preference aggregation strategies to achieve efficient CBE.\nOn the AlpacaEval benchmark, UniCBE saves over 17% of evaluation budgets while achieving a Pearson correlation with ground truth exceeding 0.995, demonstrating excellent accuracy and convergence. In scenarios where new models are continuously introduced, UniCBE can even save over 50% of evaluation costs, highlighting its improved scalability.",
      "authors": [
        "Peiwen Yuan",
        "Shaoxiong Feng",
        "Yiwei Li",
        "Xinglin Wang",
        "Yueqi Zhang",
        "Jiayi Shi",
        "Chuyi Tan",
        "Boyuan Pan",
        "Yao Hu",
        "Kan Li"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=rpwGUtTeA5",
      "cdate": 1727512820378,
      "mdate": 1740983646525,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833391"
    },
    {
      "id": "UxzKcIZedp",
      "title": "Think Then React: Towards Unconstrained Action-to-Reaction Motion Generation",
      "abstract": "Modeling human-like action-to-reaction generation has significant real-world applications, like human-robot interaction and games.\nDespite recent advancements in single-person motion generation, it is still challenging to well handle action-to-reaction generation, due to the difficulty of directly predicting reaction from action sequence without prompts, and the absence of a unified representation that effectively encodes multi-person motion.\nTo address these challenges, we introduce Think-Then-React (TTR), a large language-model-based framework designed to generate human-like reactions.\nFirst, with our fine-grained multimodal training strategy, TTR is capable to unify two processes during inference: a thinking process that explicitly infers action intentions and reasons corresponding reaction description, which serve as semantic prompts, and a reacting process that predicts reactions based on input action and the inferred semantic prompts.\nSecond, to effectively represent multi-person motion in language models, we propose a unified motion tokenizer by decoupling egocentric pose and absolute space features, which effectively represents action and reaction motion with same encoding.\nExtensive experiments demonstrate that TTR outperforms existing baselines, achieving significant improvements in evaluation metrics, such as reducing FID from 3.988 to 1.942.",
      "authors": [
        "Wenhui Tan",
        "Boyuan Li",
        "Chuhao Jin",
        "Wenbing Huang",
        "Xiting Wang",
        "Ruihua Song"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=UxzKcIZedp",
      "cdate": 1727512714517,
      "mdate": 1740748373470,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.833397"
    },
    {
      "id": "1Iu2Yte5N6",
      "title": "Rapid Selection and Ordering of In-Context Demonstrations via Prompt Embedding Clustering",
      "abstract": "While Large Language Models (LLMs) excel at in-context learning (ICL) using just a few demonstrations, their performances are sensitive to demonstration orders. The reasons behind this sensitivity remain poorly understood. In this paper, we investigate the prompt embedding space to bridge the gap between the order sensitivity of ICL with inner workings of decoder-only LLMs, uncovering the clustering property: prompts sharing the first and last demonstrations have closer embeddings, with first-demonstration clustering usually being stronger in practice. We explain this property through extensive theoretical analyses and empirical evidences. Our finding suggests that the positional encoding and the causal attention mask are key contributors to the clustering phenomenon. Leveraging this clustering insight, we introduce Cluster-based Search, a novel method that accelerates the selection and ordering of demonstrations in self-adaptive ICL settings. Our approach substantially decreases the time complexity from factorial to quadratic, saving 92% to nearly 100% execution time while maintaining comparable performance to exhaustive search.",
      "authors": [
        "Kha Pham",
        "Hung Le",
        "Man Ngo",
        "Truyen Tran"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=1Iu2Yte5N6",
      "cdate": 1727512476359,
      "mdate": 1741274608765,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833408"
    },
    {
      "id": "tNn6Hskmti",
      "title": "Asymptotic Analysis of Two-Layer Neural Networks after One Gradient Step under Gaussian Mixtures Data with Structure",
      "abstract": "In this work, we study the training and generalization performance of two-layer neural networks (NNs) after one gradient descent step under structured data modeled by Gaussian mixtures. While previous research has extensively analyzed this model under isotropic data assumption, such simplifications overlook the complexities inherent in real-world datasets. Our work addresses this limitation by analyzing two-layer NNs under Gaussian mixture data assumption in the asymptotically proportional limit, where the input dimension, number of hidden neurons, and sample size grow with finite ratios. We characterize the training and generalization errors by leveraging recent advancements in Gaussian universality. Specifically, we prove that a high-order polynomial model performs equivalent to the non-linear neural networks under certain conditions. The degree of the equivalent model is intricately linked to both the \"data spread\" and the learning rate employed during one gradient step. Through extensive simulations, we demonstrate the equivalence between the original model and its polynomial counterpart across various regression and classification tasks. Additionally, we explore how different properties of Gaussian mixtures affect learning outcomes. Finally, we illustrate experimental results on Fashion-MNIST classification, indicating that our findings can translate to realistic data.",
      "authors": [
        "Samet Demir",
        "Zafer Dogan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=tNn6Hskmti",
      "cdate": 1727512310171,
      "mdate": 1747498767677,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833418"
    },
    {
      "id": "rlgplAuN2p",
      "title": "OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large Language Models",
      "abstract": "Offline evaluation of LLMs is crucial in understanding their capacities, though current methods remain underexplored in existing research. In this work, we focus on the offline evaluation of the chain-of-thought capabilities and show how to optimize LLMs based on the proposed evaluation method. To enable offline feedback with rich knowledge and reasoning paths, we use knowledge graphs (KGs) (e.g., Wikidata5M) to provide feedback on the generated chain of thoughts. Due to the heterogeneity between LLM reasoning and KG structures, direct interaction and feedback from knowledge graphs on LLM behavior are challenging, as they require accurate entity linking and grounding of LLM-generated chains of thought in the KG. To address the above challenge, we propose an offline chain-of-thought evaluation framework, OCEAN, which models chain-of-thought reasoning in LLMs as a Markov Decision Process (MDP), and evaluate the policy’s alignment with KG preference modeling. To overcome the reasoning heterogeneity and grounding problems, we leverage on-policy KG exploration and reinforcement learning to model a KG policy that generates token-level likelihood distributions for LLM-generated chain-of-thought reasoning paths, simulating KG reasoning preference. Then we incorporate the knowledge-graph feedback on the validity and alignment of the generated reasoning paths into inverse propensity scores and propose KG-IPS estimator. Theoretically, we prove the unbiasedness of the proposed KG-IPS estimator and provide a lower bound on its variance. With the off-policy evaluated value function, we can directly enable off-policy optimization to further enhance chain-of-thought alignment. Our empirical study shows that OCEAN can be efficiently optimized for generating chain-of-thought reasoning paths with higher estimated values without affecting LLMs’ general abilities in downstream tasks or their internal knowledge.",
      "authors": [
        "Junda Wu",
        "Xintong Li",
        "Ruoyu Wang",
        "Yu Xia",
        "Yuxin Xiong",
        "Jianing Wang",
        "Tong Yu",
        "Xiang Chen",
        "Branislav Kveton",
        "Lina Yao",
        "Jingbo Shang",
        "Julian McAuley"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=rlgplAuN2p",
      "cdate": 1727512138580,
      "mdate": 1740996385250,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833424"
    },
    {
      "id": "pDDODPtpx9",
      "title": "Distribution-Free Data Uncertainty for Neural Network Regression",
      "abstract": "Quantifying uncertainty is an essential part of predictive modeling, especially in the context of high-stakes decision-making. While classification output includes data uncertainty by design in the form of class probabilities, the regression task generally aims only to predict the expected value of the target variable. Probabilistic extensions often assume parametric distributions around the expected value, optimizing the likelihood over the resulting explicit densities. However, using parametric distributions can limit practical applicability, making it difficult for models to capture skewed, multi-modal, or otherwise complex distributions. In this paper, we propose optimizing a novel nondeterministic neural network regression architecture for loss functions derived from a sample-based approximation of the continuous ranked probability score (CRPS), enabling a truly distribution-free approach by learning to sample from the target's aleatoric distribution, rather than predicting explicit densities. Our approach allows the model to learn well-calibrated, arbitrary uni- and multivariate output distributions. We evaluate the method on a variety of synthetic and real-world tasks, including uni- and multivariate problems, function inverse approximation, and standard regression uncertainty benchmarks. Finally, we make all experiment code publicly available.",
      "authors": [
        "Domokos M. Kelen",
        "Ádám Jung",
        "Péter Kersch",
        "Andras A Benczur"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=pDDODPtpx9",
      "cdate": 1727511907935,
      "mdate": 1740890361349,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833429"
    },
    {
      "id": "yR47RmND1m",
      "title": "Understanding and Enhancing Safety Mechanisms of LLMs via Safety-Specific Neuron",
      "abstract": "Safety alignment for large language models (LLMs) has become a critical issue due to their rapid progress. However, our understanding of effective safety mechanisms in LLMs remains limited, leading to safety alignment training that mainly focuses on improving optimization, data-level enhancement, or adding extra structures to intentionally block harmful outputs. To address this gap, we develop a neuron detection method to identify safety neurons—those consistently crucial for handling and defending against harmful queries. Our findings reveal that these safety neurons constitute less than $1\\%$ of all parameters, are language-specific and are predominantly located in self-attention layers. Moreover, safety is collectively managed by these neurons in the first several layers. Based on these observations, we introduce a $\\underline{S}$afety $\\underline{N}$euron $\\underline{Tun}$ing method, named $\\texttt{SN-Tune}$, that exclusively tune safety neurons without compromising models' general capabilities. $\\texttt{SN-Tune}$ significantly enhances the safety of instruction-tuned models, notably reducing the harmful scores of Llama3-8B-Instruction from $65.5$ to $2.0$, Mistral-7B-Instruct-v0.2 from $70.8$ to $4.5$, and Vicuna-13B-1.5 from $93.5$ to $3.0$. Moreover, $\\texttt{SN-Tune}$ can be applied to base models on efficiently establishing LLMs' safety mechanism. In addition, we propose $\\underline{R}$obust $\\underline{S}$afety $\\underline{N}$euron $\\underline{Tun}$ing method ($\\texttt{RSN-Tune}$), which preserves the integrity of LLMs' safety mechanisms during downstream task fine-tuning by separating the safety neurons from models' foundation neurons.",
      "authors": [
        "Yiran Zhao",
        "Wenxuan Zhang",
        "Yuxi Xie",
        "Anirudh Goyal",
        "Kenji Kawaguchi",
        "Michael Shieh"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=yR47RmND1m",
      "cdate": 1727511838075,
      "mdate": 1741014826620,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833435"
    },
    {
      "id": "1eQT9OzfNQ",
      "title": "Long Context Compression with Activation Beacon",
      "abstract": "Long context compression is a critical research problem due to its significance in reducing the high computational and memory costs associated with LLMs. In this paper, we propose Activation Beacon, a plug-in module for transformer-based LLMs that targets effective, efficient, and flexible compression of long contexts. To achieve this, our method introduces the following technical designs. \n1) We directly compress the activations (i.e. keys and values at every layer), rather than leveraging soft prompts to relay information (which constitute a major bottleneck to encapsulate the complex information within long contexts).\n2) We tailor the compression workflow, where each fine-grained input unit is progressively compressed, enabling high-quality compression and efficient computation during both training and inference. \n3) We train the model through compression-based auto-regression, making full use of plain texts and instructional data to optimize the model's compression performance.\n4) During training, we randomly sample a compression ratio at each step, teaching the model to support a wide range of compression configurations. \n\nExtensive evaluations are conducted on various long-context tasks whose lengths (e.g., 128K) may far exceed the maximum training length (20K), such as document understanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing methods struggle to handle these challenging tasks, Activation Beacon maintains a comparable performance to the uncompressed baseline across various scenarios, \nachieving a 2x acceleration in inference time and an 8x reduction of memory costs for KV cache.",
      "authors": [
        "Peitian Zhang",
        "Zheng Liu",
        "Shitao Xiao",
        "Ninglu Shao",
        "Qiwei Ye",
        "Zhicheng Dou"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=1eQT9OzfNQ",
      "cdate": 1727511707251,
      "mdate": 1740308537020,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.833441"
    },
    {
      "id": "7mlvOHL6qJ",
      "title": "LASeR: Towards Diversified and Generalizable Robot Design with Large Language Models",
      "abstract": "Recent advances in Large Language Models (LLMs) have stimulated a significant paradigm shift in evolutionary optimization, where hand-crafted search heuristics are gradually replaced with LLMs serving as intelligent search operators. However, these studies still bear some notable limitations, including a challenge to balance exploitation with exploration, often leading to inferior solution diversity, as well as poor generalizability of problem solving across different task settings. These unsolved issues render the prowess of LLMs in robot design automation largely untapped. In this work, we present LASeR -- Large Language Model-Aided Evolutionary Search for Robot Design Automation. Leveraging a novel reflection mechanism termed DiRect, we elicit more knowledgeable exploratory behaviors from LLMs based on past search trajectories, reshaping the exploration-exploitation tradeoff with dual improvements in optimization efficiency and solution diversity. Additionally, with evolution fully grounded in task-related background information, we unprecedentedly uncover the inter-task reasoning capabilities of LLMs, facilitating generalizable design processes that effectively inspire zero-shot robot proposals for new applications. Our simulated experiments on voxel-based soft robots showcase distinct advantages of LASeR over competitive baselines. Code at https://github.com/WoodySJR/LASeR.",
      "authors": [
        "Junru Song",
        "Yang Yang",
        "Huan Xiao",
        "Wei Peng",
        "Wen Yao",
        "Feifei Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=7mlvOHL6qJ",
      "cdate": 1727511555561,
      "mdate": 1743343023096,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833446"
    },
    {
      "id": "L14sqcrUC3",
      "title": "TabReD: Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks",
      "abstract": "Advances in machine learning research drive progress in real-world applications. \nTo ensure this progress, it is important to understand the potential pitfalls on the way from a novel method's success on academic benchmarks to its practical deployment. In this work, we analyze existing tabular deep learning benchmarks and find two common characteristics of tabular data in typical industrial applications that are underrepresented in the datasets usually used for evaluation in the literature.\nFirst, in real-world deployment scenarios, distribution of data often changes over time. To account for this distribution drift, time-based train/test splits should be used in evaluation. However, existing academic tabular datasets often lack timestamp metadata to enable such evaluation.\nSecond, a considerable portion of datasets in production settings stem from extensive data acquisition and feature engineering pipelines. This can have an impact on the absolute and relative number of predictive, uninformative, and correlated features compared to academic datasets.\nIn this work, we aim to understand how recent research advances in tabular deep learning transfer to these underrepresented conditions.\nTo this end, we introduce TabReD -- a collection of eight industry-grade tabular datasets. \nWe reassess a large number of tabular ML models and techniques on TabReD. We demonstrate that evaluation on both time-based data splits and richer feature sets leads to different methods ranking, compared to evaluation on random splits and smaller number of features, which are common in academic benchmarks. Furthermore, simple MLP-like architectures and GBDT show the best results on the TabReD datasets, while other methods are less effective in the new setting.",
      "authors": [
        "Ivan Rubachev",
        "Nikolay Kartashev",
        "Yury Gorishniy",
        "Artem Babenko"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=L14sqcrUC3",
      "cdate": 1727511398935,
      "mdate": 1740890361290,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833456"
    },
    {
      "id": "eIgGesYKLG",
      "title": "Arithmetic Transformers Can Length-Generalize in Both Operand Length and Count",
      "abstract": "Transformers often struggle with *length generalization*, meaning they fail to generalize to sequences longer than those encountered during training. While arithmetic tasks are commonly used to study length generalization, certain tasks are considered notoriously difficult, e.g., multi-operand addition (requiring generalization over both the number of operands and their lengths) and multiplication (requiring generalization over both operand lengths). In this work, we achieve approximately 2–3× length generalization on both tasks, which is the first such achievement in arithmetic Transformers. We design task-specific scratchpads enabling the model to focus on a fixed number of tokens per each next-token prediction step, and apply multi-level versions of *Position Coupling* (Cho et al., 2024; McLeish et al., 2024) to let Transformers know the right position to attend to. On the theory side, we prove that a 1-layer Transformer using our method can solve multi-operand addition, up to operand length and operand count that are exponential in embedding dimension.",
      "authors": [
        "Hanseul Cho",
        "Jaeyoung Cha",
        "Srinadh Bhojanapalli",
        "Chulhee Yun"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=eIgGesYKLG",
      "cdate": 1727511168601,
      "mdate": 1743569469018,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.833461"
    },
    {
      "id": "uMEsKEiB7J",
      "title": "NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens",
      "abstract": "Recent advancements in Large Language Models (LLMs) have pushed the boundaries of natural language processing, especially in long-context understanding. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives. NovelQA, constructed from English novels, offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper details the design and construction of NovelQA, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension.  Our evaluation of long-context LLMs on NovelQA reveals significant insights into their strengths and weaknesses. Notably, the models struggle with multi-hop reasoning, detail-oriented questions, and handling extremely long inputs, averaging over 200,000 tokens. Results highlight the need for substantial advancements in LLMs to enhance their long-context comprehension and contribute effectively to computational literary analysis.",
      "authors": [
        "Cunxiang Wang",
        "Ruoxi Ning",
        "Boqi Pan",
        "Tonghui Wu",
        "Qipeng Guo",
        "Cheng Deng",
        "Guangsheng Bao",
        "Xiangkun Hu",
        "Zheng Zhang",
        "Qian Wang",
        "Yue Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=uMEsKEiB7J",
      "cdate": 1727510936311,
      "mdate": 1743391556065,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833467"
    },
    {
      "id": "KnoS9XxIlK",
      "title": "A Multi-Power Law for Loss Curve Prediction Across Learning Rate Schedules",
      "abstract": "Training large models is both resource-intensive and time-consuming, making it crucial to understand the quantitative relationship between model performance and hyperparameters. In this paper, we derive an empirical law that predicts pretraining loss for large language models for every intermediate training step across various learning rate schedules, including constant, cosine, and step decay schedules. Our proposed law takes a multi-power form, combining a power law based on the sum of learning rates and additional power laws to account for a loss reduction effect as learning rate decays. We validate this law extensively on Llama-2 models of varying sizes and demonstrate that, after fitting on a few learning rate schedules, it accurately predicts the loss curves for unseen schedules of different shapes and horizons. Moreover, by minimizing the predicted final pretraining loss across learning rate schedules, we are able to find a schedule that outperforms the widely-used cosine learning rate schedule. Interestingly, this automatically discovered schedule bears some resemblance to the recently proposed Warmup-Stable-Decay (WSD) schedule (Hu et al, 2024) but achieves a slightly lower final loss. We believe these results could offer valuable insights for understanding the dynamics of pretraining and for designing learning rate schedules to improve efficiency.",
      "authors": [
        "Kairong Luo",
        "Haodong Wen",
        "Shengding Hu",
        "Zhenbo Sun",
        "Zhiyuan Liu",
        "Maosong Sun",
        "Kaifeng Lyu",
        "Wenguang Chen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=KnoS9XxIlK",
      "cdate": 1727510402364,
      "mdate": 1746781510480,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833472"
    },
    {
      "id": "UQJ7CDW8nb",
      "title": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token",
      "abstract": "The advent of real-time large multimodal models (LMMs) like GPT-4o has sparked considerable interest in efficient LMMs. LMM frameworks typically encode visual inputs into vision tokens (continuous representations) and integrate them and textual instructions into the context of large language models (LLMs), where large-scale parameters and numerous context tokens (predominantly vision tokens) result in substantial computational overhead. Previous efforts towards efficient LMMs always focus on replacing the LLM backbone with smaller models, while neglecting the crucial issue of token quantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal vision tokens. To achieve a high compression ratio of vision tokens while preserving visual information, we first analyze how LMMs understand vision tokens and find that most vision tokens only play a crucial role in the early layers of LLM backbone, where they mainly fuse visual information into text tokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to fuse visual information into text tokens in advance, thereby facilitating the extreme compression of vision tokens fed to LLM backbone into one token. LLaVA-Mini is a unified large multimodal model that can support the understanding of images, high-resolution images, and videos in an efficient manner. Experiments across 11 image-based and 7 video-based benchmarks demonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token instead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by 77%, deliver low-latency responses within 40 milliseconds, and process over 10,000 frames of video on the GPU hardware with 24GB of memory.",
      "authors": [
        "Shaolei Zhang",
        "Qingkai Fang",
        "Zhe Yang",
        "Yang Feng"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=UQJ7CDW8nb",
      "cdate": 1727510379548,
      "mdate": 1740930928933,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.833484"
    },
    {
      "id": "MBBRHDuiwM",
      "title": "URLOST: Unsupervised Representation Learning without Stationarity or Topology",
      "abstract": "Unsupervised representation learning has seen tremendous progress. However, it is constrained by its reliance on domain specific stationarity and topology, a limitation not found in biological intelligence systems. For instance, unlike computer vision, human vision can process visual signals sampled from highly irregular and non-stationary sensors. We introduce a novel framework that learns from high-dimensional data without prior knowledge of stationarity and topology. Our model, abbreviated as URLOST, combines a learnable self-organizing layer, spectral clustering, and a masked autoencoder (MAE). We evaluate its effectiveness on three diverse data modalities including simulated biological vision data, neural recordings from the primary visual cortex, and gene expressions. Compared to state-of-the-art unsupervised learning methods like SimCLR and MAE, our model excels at learning meaningful representations across diverse modalities without knowing their stationarity or topology. It also outperforms other methods that are not dependent on these factors, setting a new benchmark in the field. We position this work as a step toward unsupervised learning methods capable of generalizing across diverse high-dimensional data modalities.",
      "authors": [
        "Zeyu Yun",
        "Juexiao Zhang",
        "Yann LeCun",
        "Yubei Chen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=MBBRHDuiwM",
      "cdate": 1727510367055,
      "mdate": 1742542387044,
      "matched_keywords": [
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833490"
    },
    {
      "id": "VnLhUogHYE",
      "title": "K-HALU: Multiple Answer Korean Hallucination Benchmark for Large Language Models",
      "abstract": "Recent researchers and companies have been developing large language models (LLMs) specifically designed for particular purposes and have achieved significant advancements in various natural language processing tasks. However, LLMs are still prone to generating hallucinations—results that are unfaithful or inconsistent with the given input. As a result, the need for datasets to evaluate and demonstrate the hallucination detection capabilities of LLMs is increasingly recognized. Nonetheless, the Korean NLP community lacks publicly available benchmark datasets demonstrating the faithfulness of knowledge-based information. Furthermore, the few existing datasets that evaluate hallucination are limited in their access to the entire dataset, restricting detailed analysis beyond simple scoring, and are based on translated English knowledge. To address these challenges, we introduce K-HALU, a Korean benchmark designed to evaluate LLMs' hallucination detection in Korean. This benchmark contains seven domains, considering the faithfulness of statements based on knowledge documents compiled from Korean news, magazines, and books. For more strict evaluation, 40% of the dataset is structured as multiple-answer questions, requiring models to select all possible correct answers from the given options. Our empirical results show that open-source LLMs still struggle with hallucination detection in Korean knowledge, emphasizing the need for a more detailed analysis of their limitations.",
      "authors": [
        "Jaehyung Seo",
        "Heuiseok Lim"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=VnLhUogHYE",
      "cdate": 1727510295882,
      "mdate": 1741014826480,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833496"
    },
    {
      "id": "aWXnKanInf",
      "title": "TopoLM: brain-like spatio-functional organization in a topographic language model",
      "abstract": "Neurons in the brain are spatially organized such that neighbors on tissue often exhibit similar response profiles. In the human language system, experimental studies have observed clusters for syntactic and semantic categories, but the mechanisms underlying this functional organization remain unclear. Here, building on work from the vision literature, we develop TopoLM, a transformer language model with an explicit two-dimensional spatial representation of model units. By combining a next-token prediction objective with a spatial smoothness loss, representations in this model assemble into clusters that correspond to semantically interpretable groupings of text and closely match the functional organization in the brain's language system. TopoLM successfully predicts the emergence of a spatially organized cortical language system as well as the organization of functional clusters selective for fine-grained linguistic features empirically observed in human cortex. Our results suggest that the functional organization of the human language system is driven by a unified spatial objective, and provide a functionally and spatially aligned model of language processing in the brain.Neurons in the brain are spatially organized such that neighbors on tissue often exhibit similar response profiles. In the human language system, experimental studies have observed clusters for syntactic and semantic categories, but the mechanisms underlying this functional organization remain unclear. Here, building on work from the vision literature, we develop TopoLM, a transformer language model with an explicit two-dimensional spatial representation of model units. By combining a next-token prediction objective with a spatial smoothness loss, representations in this model assemble into clusters that correspond to semantically interpretable groupings of text and closely match the functional organization in the brain's language system. TopoLM successfully predicts the emergence of a spatially organized cortical language system as well as the organization of functional clusters selective for fine-grained linguistic features empirically observed in human cortex. Our results suggest that the functional organization of the human language system is driven by a unified spatial objective, and provide a functionally and spatially aligned model of language processing in the brain.",
      "authors": [
        "Neil Rathi",
        "Johannes Mehrer",
        "Badr AlKhamissi",
        "Taha Osama A Binhuraib",
        "Nicholas Blauch",
        "Martin Schrimpf"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=aWXnKanInf",
      "cdate": 1727509126624,
      "mdate": 1746942416453,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.833501"
    },
    {
      "id": "nCrJD7qPJN",
      "title": "Distilling Dataset into Neural Field",
      "abstract": "Utilizing a large-scale dataset is essential for training high-performance deep learning models, but it also comes with substantial computation and storage costs. To overcome these challenges, dataset distillation has emerged as a promising solution by compressing the large-scale dataset into a smaller synthetic dataset that retains the essential information needed for training. This paper proposes a novel parameterization framework for dataset distillation, coined Distilling Dataset into Neural Field (DDiF), which leverages the neural field to store the necessary information of the large-scale dataset. Due to the unique nature of the neural field, which takes coordinates as input and output quantity, DDiF effectively preserves the information and easily generates various shapes of data. We theoretically confirm that DDiF exhibits greater expressiveness than some previous literature when the utilized budget for a single synthetic instance is the same. Through extensive experiments, we demonstrate that DDiF achieves superior performance on several benchmark datasets, extending beyond the image domain to include video, audio, and 3D voxel. We release the code at \\url{https://github.com/aailab-kaist/DDiF}.",
      "authors": [
        "Donghyeok Shin",
        "HeeSun Bae",
        "Gyuwon Sim",
        "Wanmo Kang",
        "Il-chul Moon"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=nCrJD7qPJN",
      "cdate": 1727508960200,
      "mdate": 1740886962150,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833507"
    },
    {
      "id": "VELhv9BBfn",
      "title": "Neural Dueling Bandits: Preference-Based Optimization with Human Feedback",
      "abstract": "Contextual dueling bandit is used to model the bandit problems, where a learner's goal is to find the best arm for a given context using observed noisy human preference feedback over the selected arms for the past contexts. However, existing algorithms assume the reward function is linear, which can be complex and non-linear in many real-life applications like online recommendations or ranking web search results. To overcome this challenge, we use a neural network to estimate the reward function using preference feedback for the previously selected arms. We propose upper confidence bound- and Thompson sampling-based algorithms with sub-linear regret guarantees that efficiently select arms in each round. We also extend our theoretical results to contextual bandit problems with binary feedback, which is in itself a non-trivial contribution. Experimental results on the problem instances derived from synthetic datasets corroborate our theoretical results.",
      "authors": [
        "Arun Verma",
        "Zhongxiang Dai",
        "Xiaoqiang Lin",
        "Patrick Jaillet",
        "Bryan Kian Hsiang Low"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=VELhv9BBfn",
      "cdate": 1727508650498,
      "mdate": 1741014826364,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833512"
    },
    {
      "id": "x1yOHtFfDh",
      "title": "SPORTU: A Comprehensive Sports Understanding Benchmark for Multimodal Large Language Models",
      "abstract": "Multimodal Large Language Models (MLLMs) are advancing the ability to reason about complex sports scenarios by integrating textual and visual information. To comprehensively evaluate their capabilities, we introduce SPORTU, a benchmark designed to assess MLLMs across multi-level sports reasoning tasks. SPORTU comprises two key components: SPORTU-text, featuring 900 multiple-choice questions with human-annotated explanations for rule comprehension and strategy understanding. This component focuses on testing models' ability to reason about sports solely through question-answering (QA), without requiring visual inputs; SPORTU-video, consisting of 1,701 slow-motion video clips across 7 different sports and 12,048 QA pairs, designed to assess multi-level reasoning, from simple sports recognition to complex tasks like foul detection and rule application. We evaluated four prevalent LLMs mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting on the SPORTU-text part. GPT-4o achieves the highest accuracy of 71\\%, but still falls short of human-level performance, highlighting room for improvement in rule comprehension and reasoning. The evaluation for the SPORTU-video part includes 6 proprietary and 8 open-source MLLMs. Experiments show that models fall short on hard tasks that require deep reasoning and rule-based understanding. GPT-4o performs the best with only 57.8\\% accuracy on the hard task, showing large room for improvement. We hope that SPORTU will serve as a critical step toward evaluating models' capabilities in sports understanding and reasoning. The dataset is available at [https://github.com/chili-lab/SPORTU](https://github.com/chili-lab/SPORTU).",
      "authors": [
        "Haotian Xia",
        "Zhengbang Yang",
        "Junbo Zou",
        "Rhys Tracy",
        "Yuqing Wang",
        "Chi Lu",
        "Christopher Lai",
        "Yanjun He",
        "Xun Shao",
        "Zhuoqing Xie",
        "Yuan-fang Wang",
        "Weining Shen",
        "Hanjie Chen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=x1yOHtFfDh",
      "cdate": 1727508384732,
      "mdate": 1740890361078,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.833518"
    },
    {
      "id": "gcouwCx7dG",
      "title": "Improving the Sparse Structure Learning of Spiking Neural Networks from the View of Compression Efficiency",
      "abstract": "The human brain utilizes spikes for information transmission and dynamically reorganizes its network structure to boost energy efficiency and cognitive capabilities throughout its lifespan. Drawing inspiration from this spike-based computation, Spiking Neural Networks (SNNs) have been developed to construct event-driven models that emulate this efficiency. Despite these advances, deep SNNs continue to suffer from over-parameterization during training and inference, a stark contrast to the brain’s ability to self-organize. Furthermore, existing sparse SNNs are challenged by maintaining optimal pruning levels due to a static pruning ratio, resulting in either under or over-pruning.\nIn this paper, we propose a novel two-stage dynamic structure learning approach for deep SNNs, aimed at maintaining effective sparse training from scratch while optimizing compression efficiency. \nThe first stage evaluates the compressibility of existing sparse subnetworks within SNNs using the PQ index, which facilitates an adaptive determination of the rewiring ratio for synaptic connections based on data compression insights. In the second stage, this rewiring ratio critically informs the dynamic synaptic connection rewiring process, including both pruning and regrowth. This approach significantly improves the exploration of sparse structures training in deep SNNs, adapting sparsity dynamically from the point view of compression efficiency.\nOur experiments demonstrate that this sparse training approach not only aligns with the performance of current deep SNNs models but also significantly improves the efficiency of compressing sparse SNNs. Crucially, it preserves the advantages of initiating training with sparse models and offers a promising solution for implementing Edge AI on neuromorphic hardware.",
      "authors": [
        "Jiangrong Shen",
        "Qi Xu",
        "Gang Pan",
        "Badong Chen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=gcouwCx7dG",
      "cdate": 1727508361685,
      "mdate": 1740840650993,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833523"
    },
    {
      "id": "OJsMGsO6yn",
      "title": "SIM: Surface-based fMRI Analysis for Inter-Subject Multimodal Decoding from Movie-Watching Experiments",
      "abstract": "Current AI frameworks for brain decoding and encoding, typically train and test models within the same datasets. This limits their utility for cognitive training (neurofeedback) for which it would be useful to pool experiences across individuals to better simulate stimuli not sampled during training. A key obstacle to model generalisation is the degree of variability of inter-subject cortical organisation, which makes it difficult to align or compare cortical signals across participants. In this paper we address this through use of surface vision transformers, which build a generalisable model of cortical functional dynamics, through encoding the topography of cortical networks and their interactions as a moving image across a surface. This is then combined with tri-modal self-supervised contrastive (CLIP) alignment of audio, video, and fMRI modalities to enable the retrieval of visual and auditory stimuli from patterns of cortical activity (and vice-versa).  We validate our approach on 7T task-fMRI data from 174 healthy participants engaged in the movie-watching experiment from the Human Connectome Project (HCP). Results show that it is possible to detect which movie clips an individual is watching purely from their brain activity, even for individuals and movies *not seen during training*. Further analysis of attention maps reveals that our model captures individual patterns of brain activity that reflect semantic and visual systems. This opens the door to future personalised simulations of brain function. Code \\& pre-trained models will be made available at https://github.com/metrics-lab/sim.",
      "authors": [
        "Simon Dahan",
        "Gabriel Bénédict",
        "Logan Zane John Williams",
        "Yourong Guo",
        "Daniel Rueckert",
        "Robert Leech",
        "Emma Claire Robinson"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=OJsMGsO6yn",
      "cdate": 1727508359272,
      "mdate": 1740910610623,
      "matched_keywords": [
        "multimodal",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.833528"
    },
    {
      "id": "iLUcsecZJp",
      "title": "Why In-Context Learning Models are Good Few-Shot Learners?",
      "abstract": "We explore in-context learning (ICL) models from a learning-to-learn perspective. Unlike studies that identify specific learning algorithms in ICL models, we compare ICL models with typical meta-learners to understand their superior performance. We theoretically prove the expressiveness of ICL models as learning algorithms and examine their learnability and generalizability. \nOur findings show that ICL with transformers \ncan effectively construct data-dependent learning algorithms instead of directly follow existing ones \n(including gradient-based, metric-based, and amortization-based meta-learners). \nThe construction of such learning algorithm is determined by the pre-training process, as a function fitting the training distribution, which raises generalizability as an important issue.\nWith above understanding, we propose strategies to transfer techniques for classical deep networks to meta-level to further improve ICL. As examples, we implement meta-level meta-learning for domain adaptability with limited data and meta-level curriculum learning for accelerated convergence during pre-training, demonstrating their empirical effectiveness.",
      "authors": [
        "Shiguang Wu",
        "Yaqing Wang",
        "Quanming Yao"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=iLUcsecZJp",
      "cdate": 1727507807980,
      "mdate": 1739261799154,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.833534"
    },
    {
      "id": "E36NHwe7Zc",
      "title": "Evaluating Large Language Models through Role-Guide and Self-Reflection: A Comparative Study",
      "abstract": "Large Language Models fine-tuned with Reinforcement Learning from Human Feedback (RLHF-LLMs) can over-rely on aligned preferences without truly gaining self-knowledge, leading to hallucination and biases. If an LLM can better access its knowledge and know what it knows, it can avoid making false or unsupported claims. Therefore, it is crucial to evaluate whether LLMs have the ability to know what they know, as it can help to ensure accuracy and faithfulness in real-world applications. Inspired by research in Educational Psychology, surface learners who don’t really know are easily affected by teacher and peer guidance, we treat LLM as a student, incorporate role guidance in prompts to explore whether LLMs really know. Specifically, we propose a novel strategy called Role-Guided and Self-Reflection (RoSe) to fully assess whether LLM “knows it knows”. We introduce multiple combinations of different roles and strong reminder in prompts combined with self-reflection to explore what local information in prompt LLMs rely on and whether LLMs remain unaffected by external guidance with varying roles. Our findings reveal that LLMs are very sensitive to the strong reminder information. Role guidance can help LLMs reduce their reliance on strong reminder. Meanwhile, LLMs tend to trust the role of authority more when guided by different roles. Following these findings, we propose a double-calibrated strategy with verbalized confidence to extract well-calibrated data from closed-source LLM and fine-tune open-source LLMs. Extensive experiments conducted on fine-tuning open-source LLMs demonstrate the effectiveness of double-calibrated strategy in mitigating the reliance of LLMs on local information. For a thorough comparison, we not only employ public JEC-QA and openBookQA datasets, but also construct EG-QA which contains English Grammar multiple-choice question-answering and 14 key knowledge points for assessing self-knowledge and logical reasoning.",
      "authors": [
        "Lili Zhao",
        "Yang Wang",
        "Qi Liu",
        "Mengyun Wang",
        "Wei Chen",
        "Zhichao Sheng",
        "Shijin Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=E36NHwe7Zc",
      "cdate": 1727507784279,
      "mdate": 1745916722231,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833542"
    },
    {
      "id": "xsELpEPn4A",
      "title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
      "abstract": "Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, multi-turn chat, etc.",
      "authors": [
        "Lianghui Zhu",
        "Xinggang Wang",
        "Xinlong Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=xsELpEPn4A",
      "cdate": 1727507382974,
      "mdate": 1740822332170,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.833548"
    },
    {
      "id": "5wxCQDtbMo",
      "title": "GotenNet: Rethinking Efficient 3D Equivariant Graph Neural Networks",
      "abstract": "Understanding complex three-dimensional (3D) structures of graphs is essential for accurately modeling various properties, yet many existing approaches struggle with fully capturing the intricate spatial relationships and symmetries inherent in such systems, especially in large-scale, dynamic molecular datasets. These methods often must balance trade-offs between expressiveness and computational efficiency, limiting their scalability. To address this gap, we propose a novel Geometric Tensor Network (GotenNet) that effectively models the geometric intricacies of 3D graphs while ensuring strict equivariance under the Euclidean group E(3). Our approach directly tackles the expressiveness-efficiency trade-off by leveraging effective geometric tensor representations without relying on irreducible representations or Clebsch-Gordan transforms, thereby reducing computational overhead. We introduce a unified structural embedding, incorporating geometry-aware tensor attention and hierarchical tensor refinement that iteratively updates edge representations through inner product operations on high-degree steerable features, allowing for flexible and efficient representations for various tasks. We evaluated models on QM9, rMD17, MD22, and Molecule3D datasets, where the proposed model consistently outperforms state-of-the-art methods in both scalar and high-degree property predictions, demonstrating exceptional robustness across diverse datasets, and establishes GotenNet as a versatile and scalable framework for 3D equivariant Graph Neural Networks.",
      "authors": [
        "Sarp Aykent",
        "Tian Xia"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=5wxCQDtbMo",
      "cdate": 1727506309711,
      "mdate": 1747530740817,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833554"
    },
    {
      "id": "41HlN8XYM5",
      "title": "Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition",
      "abstract": "Automated mechanistic interpretation research has attracted great interest due to its potential to scale explanations of neural network internals to large models. Existing automated circuit discovery work relies on activation patching or its approximations to identify subgraphs in models for specific tasks (circuits). They often suffer from slow runtime, approximation errors, and specific requirements of metrics, such as non-zero gradients.\nIn this work, we introduce contextual decomposition for transformers (CD-T) to build interpretable circuits in large language models. CD-T can produce circuits at any level of abstraction and is the first to efficiently produce circuits as fine-grained as attention heads at specific sequence positions.\nCD-T is compatible to all transformer types, and requires no training or manually-crafted examples.\nCD-T consists of a set of mathematical equations to isolate contribution of model features. Through recursively computing contribution of all nodes in a computational graph of a model using CD-T followed by pruning, we are able to reduce circuit discovery runtime from hours to seconds compared to state-of-the-art baselines.\nOn three standard circuit evaluation datasets (indirect object identification, greater-than comparisons, and docstring completion),\nwe demonstrate that CD-T outperforms ACDC and EAP by better recovering the manual circuits with an average of 97% ROC AUC under low runtimes.\nIn addition, we provide evidence that faithfulness of CD-T circuits is not due to random chance by showing our circuits are 80% more faithful than random circuits of up to 60% of the original model size. \nFinally, we show CD-T circuits are able to perfectly replicate original models' behavior(faithfulness  = 1) using fewer nodes than the baselines for all tasks.\nOur results underscore the great promise of CD-T for efficient automated mechanistic interpretability, paving the way for new insights into the workings of large language models.",
      "authors": [
        "Aliyah R. Hsu",
        "Georgia Zhou",
        "Yeshwanth Cherapanamjeri",
        "Yaxuan Huang",
        "Anobel Odisho",
        "Peter R. Carroll",
        "Bin Yu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=41HlN8XYM5",
      "cdate": 1727505919516,
      "mdate": 1740903838970,
      "matched_keywords": [
        "large language model",
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833560"
    },
    {
      "id": "cbttLtO94Q",
      "title": "How to Evaluate Reward Models for RLHF",
      "abstract": "We introduce a new benchmark for reward models that quantifies their ability to produce strong language models through RLHF (Reinforcement Learning from Human Feedback).\nThe gold-standard approach is to run a full RLHF training pipeline and directly probe downstream LLM performance.\nHowever, this process is prohibitively expensive.\nTo address this, we build a predictive model of downstream LLM performance by evaluating the reward model on proxy tasks. \nThese proxy tasks consist of a large-scale human preference and a verifiable correctness preference dataset, in which we measure 12 metrics across 12 domains.\nTo investigate which reward model metrics are most correlated to gold-standard RLHF outcomes, we launch an end-to-end RLHF experiment on a large-scale crowd-sourced human preference platform to view real reward model downstream performance as ground truth. \nUltimately, we compile our data and findings into Preference Proxy Evaluations (PPE), the first reward model benchmark explicitly linked to post-RLHF real-world human preference performance, which we opensource for public use and further development at https://github.com/lmarena/PPE.",
      "authors": [
        "Evan Frick",
        "Tianle Li",
        "Connor Chen",
        "Wei-Lin Chiang",
        "Anastasios Nikolas Angelopoulos",
        "Jiantao Jiao",
        "Banghua Zhu",
        "Joseph E. Gonzalez",
        "Ion Stoica"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=cbttLtO94Q",
      "cdate": 1727505885110,
      "mdate": 1744050095233,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833566"
    },
    {
      "id": "5RZoYIT3u6",
      "title": "You Only Prune Once: Designing Calibration-Free Model Compression With Policy Learning",
      "abstract": "The ever-increasing size of large language models (LLMs) presents significant challenges for deployment due to their heavy computational and memory requirements. Current model pruning techniques attempt to alleviate these issues by relying heavily on external calibration datasets to determine which parameters to prune or compress, thus limiting their flexibility and scalability across different compression ratios. Moreover, these methods often cause severe performance degradation, particularly in downstream tasks, when subjected to higher compression rates. In this paper, we propose *PruneNet*, a novel model compression method that addresses these limitations by reformulating model pruning as a policy learning process. PruneNet decouples the pruning process from the model architecture, eliminating the need for calibration datasets. It learns a stochastic pruning policy to assess parameter importance solely based on intrinsic model properties while preserving the spectral structure to minimize information loss. PruneNet can compress the LLaMA-2-7B model in just 15 minutes, achieving over 80\\% retention of its zero-shot performance with a 30\\% compression ratio, outperforming existing methods that retain only 75\\% performance. Furthermore, on complex multitask language understanding tasks, PruneNet demonstrates its robustness by preserving up to 80\\% performance of the original model, proving itself a superior alternative to conventional structured compression techniques.",
      "authors": [
        "Ayan Sengupta",
        "Siddhant Chaudhary",
        "Tanmoy Chakraborty"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=5RZoYIT3u6",
      "cdate": 1727505737993,
      "mdate": 1740755206902,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833572"
    },
    {
      "id": "OL44KtasKc",
      "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration",
      "abstract": "The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of $O(N^2)$, compared to $O(N)$ for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although quantization has proven to be an effective method for accelerating model inference, existing quantization methods primarily focus on optimizing the linear layer.\nIn response, we first analyze the feasibility of quantization in attention detailedly. Following that, we propose SageAttention, a highly efficient and accurate quantization method for attention. The OPS (operations per second) of our approach outperforms FlashAttention2 and xformers by about 2.1x and 2.7x, respectively. SageAttention also achieves superior accuracy performance over FlashAttention3. Comprehensive experiments confirm that our approach incurs almost no end-to-end metrics loss across diverse models—including those for large language processing, image generation, and video generation. The code is available at https://github.com/thu-ml/SageAttention.",
      "authors": [
        "Jintao Zhang",
        "Jia wei",
        "Pengle Zhang",
        "Jun Zhu",
        "Jianfei Chen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=OL44KtasKc",
      "cdate": 1727505387570,
      "mdate": 1746001367408,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.833577"
    },
    {
      "id": "wM2sfVgMDH",
      "title": "Diffusion-Based Planning for Autonomous Driving with Flexible Guidance",
      "abstract": "Achieving human-like driving behaviors in complex open-world environments is a critical challenge in autonomous driving. Contemporary learning-based planning approaches such as imitation learning methods often struggle to balance competing objectives and lack of safety assurance,due to limited adaptability and inadequacy in learning complex multi-modal behaviors commonly exhibited in human planning, not to mention their strong reliance on the fallback strategy with predefined rules. We propose a novel transformer-based Diffusion Planner for closed-loop planning, which can effectively model multi-modal driving behavior and ensure trajectory quality without any rule-based refinement. Our model supports joint modeling of both prediction and planning tasks under the same architecture, enabling cooperative behaviors between vehicles. Moreover, by learning the gradient of the trajectory score function and employing a flexible classifier guidance mechanism, Diffusion Planner effectively achieves safe and adaptable planning behaviors. Evaluations on the large-scale real-world autonomous planning benchmark nuPlan and our newly collected 200-hour delivery-vehicle driving dataset demonstrate that Diffusion Planner achieves state-of-the-art closed-loop performance with robust transferability in diverse driving styles.",
      "authors": [
        "Yinan Zheng",
        "Ruiming Liang",
        "Kexin ZHENG",
        "Jinliang Zheng",
        "Liyuan Mao",
        "Jianxiong Li",
        "Weihao Gu",
        "Rui Ai",
        "Shengbo Eben Li",
        "Xianyuan Zhan",
        "Jingjing Liu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=wM2sfVgMDH",
      "cdate": 1727504894623,
      "mdate": 1739261797742,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.833583"
    },
    {
      "id": "4sJ2FYE65U",
      "title": "Neural Multi-Objective Combinatorial Optimization via Graph-Image Multimodal Fusion",
      "abstract": "Existing neural multi-objective combinatorial optimization (MOCO) methods still exhibit an optimality gap since they fail to fully exploit the intrinsic features of problem instances. A significant factor contributing to this shortfall is their reliance solely on graph-modal information. To overcome this, we propose a novel graph-image multimodal fusion (GIMF) framework that enhances neural MOCO methods by integrating graph and image information of the problem instances. Our GIMF framework comprises three key components: (1) a constructed coordinate image to better represent the spatial structure of the problem instance, (2) a problem-size adaptive resolution strategy during the image construction process to improve the cross-size generalization of the model, and (3) a multimodal fusion mechanism with modality-specific bottlenecks to efficiently couple graph and image information. We demonstrate the versatility of our GIMF by implementing it with two state-of-the-art neural MOCO backbones. Experimental results on classic MOCO problems show that our GIMF significantly outperforms state-of-the-art neural MOCO methods and exhibits superior generalization capability.",
      "authors": [
        "Jinbiao Chen",
        "Jiahai Wang",
        "Zhiguang Cao",
        "Yaoxin Wu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=4sJ2FYE65U",
      "cdate": 1727504570309,
      "mdate": 1741921892433,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.833589"
    },
    {
      "id": "bc3sUsS6ck",
      "title": "Generative Adapter: Contextualizing Language Models in Parameters with A Single Forward Pass",
      "abstract": "Large language models (LLMs) acquire substantial knowledge during pretraining but often need adaptation to new contexts, tasks, or domains, typically achieved through fine-tuning or prompting. However, fine-tuning incurs significant training costs, while prompting increases inference overhead. Inspired by fast weight memory, we introduce GenerativeAdapter, an effective and efficient adaptation method that encode test-time context into language model parameters with a single forward pass.\nGenerativeAdapter augments a frozen pretrained LM with a lightweight adapter generator, trained via self-supervised learning, to produce parameter-efficient adapters.\nNotably, our generator is general-purpose, i.e., one generator can adapt the corresponding base model for all langauge processing scenarios.\nWe apply GenerativeAdapter to two pretrained LMs (Mistral-7B-Instruct and Llama2-7B-Chat) and evaluate the adapted models across  knowledge acquisition from documents, learning from demonstrations, and personalization for users.\nIn StreamingQA, our approach is effective in injecting knowledge into the LM's parameters, achieving a 63.5\\% improvement in F1 score over the model with supervised fine-tuning (from $19.5$ to $31.5$) for contexts as long as 32K tokens.\nIn the MetaICL in-context learning evaluation, our method achieves an average accuracy of $44.9$ across 26 tasks, outperforming the base model. \nOn MSC, our method proves to be highly competitive in memorizing user information from conversations with a 4x reduction in computation and memory costs compared to \nprompting with full conversation history.\nOverall, GenerativeAdapter provides a viable solution for adapting large LMs to evolving information and providing tailored user experience, while reducing training and inference costs relative to traditional fine-tuning and prompting techniques.",
      "authors": [
        "Tong Chen",
        "Hao Fang",
        "Patrick Xia",
        "Xiaodong Liu",
        "Benjamin Van Durme",
        "Luke Zettlemoyer",
        "Jianfeng Gao",
        "Hao Cheng"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=bc3sUsS6ck",
      "cdate": 1727504394832,
      "mdate": 1740895699229,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833595"
    },
    {
      "id": "pXlmOmlHJZ",
      "title": "ICLR: In-Context Learning of Representations",
      "abstract": "Recent work demonstrates that structured patterns in pretraining data influence how representations of different concepts are organized in a large language model’s (LLM) internals, with such representations then driving downstream abilities. Given the open-ended nature of LLMs, e.g., their ability to in-context learn novel tasks, we ask whether models can flexibly alter their semantically grounded organization of concepts. Specifically, if we provide in-context exemplars wherein a concept plays a different role than what the pretraining data suggests, can models infer these novel semantics and reorganize representations in accordance with them? To answer this question, we define a toy “graph tracing” task wherein the nodes of the graph are referenced via concepts seen during training (e.g., apple, bird, etc.), and the connectivity of the graph is defined via some predefined structure (e.g., a square grid). Given exemplars that indicate traces of random walks on the graph, we analyze intermediate representations of the model and find that as the amount of context is scaled, there is a sudden re-organization of representations according to the graph’s structure. Further, we find that when reference concepts have correlations in their semantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure is still present in the representations, but is unable to dominate the pretrained structure. To explain these results, we analogize our task to energy minimization for a predefined graph topology, which shows getting non-trivial performance on the task requires for the model to infer a connected component. Overall, our findings indicate context-size may be an underappreciated scaling axis that can flexibly re-organize model representations, unlocking novel capabilities.",
      "authors": [
        "Core Francisco Park",
        "Andrew Lee",
        "Ekdeep Singh Lubana",
        "Yongyi Yang",
        "Maya Okawa",
        "Kento Nishi",
        "Martin Wattenberg",
        "Hidenori Tanaka"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=pXlmOmlHJZ",
      "cdate": 1727504327774,
      "mdate": 1746164411690,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833603"
    },
    {
      "id": "BpyHIrpUOL",
      "title": "PolyhedronNet: Representation Learning for Polyhedra with Surface-attributed Graph",
      "abstract": "Ubiquitous geometric objects can be precisely and efficiently represented as polyhedra. The transformation of a polyhedron into a vector, known as polyhedra representation learning, is crucial for manipulating these shapes with mathematical and statistical tools for tasks like classification, clustering, and generation. Recent years have witnessed significant strides in this domain, yet most efforts focus on the vertex sequence of a polyhedron, neglecting the complex surface modeling crucial in real-world polyhedral objects.\nThis study proposes \\textbf{PolyhedronNet}, a general framework tailored for learning representations of 3D polyhedral objects.  We propose the concept of the surface-attributed graph to seamlessly model the vertices, edges, faces, and their geometric interrelationships within a polyhedron. \nTo effectively learn the representation of the entire surface-attributed graph, we first propose to break it down into local rigid representations to effectively learn each local region's relative positions against the remaining regions without geometric information loss. Subsequently, we propose PolyhedronGNN to hierarchically aggregate the local rigid representation via intra-face and inter-face geometric message passing modules, to obtain a global representation that minimizes information loss while maintaining rotation and translation invariance.\nOur experimental evaluations on four distinct datasets, encompassing both classification and retrieval tasks, substantiate PolyhedronNet's efficacy in capturing comprehensive and informative representations of 3D polyhedral objects.",
      "authors": [
        "Dazhou Yu",
        "Genpei Zhang",
        "Liang Zhao"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=BpyHIrpUOL",
      "cdate": 1727503606259,
      "mdate": 1741911544626,
      "matched_keywords": [
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833609"
    },
    {
      "id": "HN0CYZbAPw",
      "title": "Efficient Online Reinforcement Learning Fine-Tuning Need Not Retain Offline Data",
      "abstract": "The modern paradigm in machine learning involves pre-training on diverse data, followed by task-specific fine-tuning. In reinforcement learning (RL), this translates to learning via offline RL on a diverse historical dataset, followed by rapid online RL fine-tuning using interaction data. Most RL fine-tuning methods require continued training on offline data for stability and performance. However, this is undesirable because training on diverse offline data is slow and expensive for large datasets, and should, in principle, also limit the performance improvement possible because of constraints or pessimism on offline data. In this paper, we show that retaining offline data is unnecessary as long as we use a properly-designed online RL approach for fine-tuning offline RL initializations. To build this approach, we start by analyzing the role of retaining offline data in online fine-tuning. We find that continued training on offline data is mostly useful for preventing a sudden divergence in the value function at the onset of fine-tuning, caused by a distribution mismatch between the offline data and online rollouts. This divergence typically results in unlearning and forgetting the benefits of offline pre-training. Our approach, Warm-start RL (WSRL), mitigates the catastrophic forgetting of pre-trained initializations using a very simple idea. WSRL employs a warmup phase that seeds the online RL run with a very small number of rollouts from the pre-trained policy to do fast online RL. The data collected during warmup bridges the distribution mismatch, and helps ``recalibrate'' the offline Q-function to the online distribution, allowing us to completely discard offline data without destabilizing the online RL fine-tuning. We show that WSRL is able to fine-tune without retaining any offline data, and is able to learn faster and attains higher performance than existing algorithms irrespective of whether they do or do not retain offline data.",
      "authors": [
        "Zhiyuan Zhou",
        "Andy Peng",
        "Qiyang Li",
        "Sergey Levine",
        "Aviral Kumar"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=HN0CYZbAPw",
      "cdate": 1727502875789,
      "mdate": 1740890360635,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833615"
    },
    {
      "id": "wfLuiDjQ0u",
      "title": "Making Text Embedders Few-Shot Learners",
      "abstract": "Large language models (LLMs) with decoder-only architectures have demonstrated exceptional text-generation capabilities across a variety of tasks. Some researchers have also adapted these models for text representation tasks. However, in text representation tasks, these models often face performance degradation on unseen tasks. In-context learning (ICL), which leverages examples provided in the input context, enables LLMs to handle unseen tasks effectively. Inspired by this, we aim to fully utilize the inherent properties of LLMs to enhance text representation performance across different tasks through the ICL approach.\n\nIn this paper, we introduce a simple yet effective training strategy, which significantly improves text representation capabilities. Unlike previous models that prepend task instructions to the text, our method randomly samples a varying number of examples during training, endowing the embedding model with in-context learning abilities while maintaining its zero-shot capabilities. This approach does not require additional data construction or modifications to the model architecture. On the contrary, we find that some popular modifications to the model, such as bidirectional attention, can degrade performance, undermining the inherent characteristics of LLMs. We have publicly released our method at this \\href{https://github.com/FlagOpen/FlagEmbedding}{repo}.",
      "authors": [
        "Chaofan Li",
        "Minghao Qin",
        "Shitao Xiao",
        "Jianlyu Chen",
        "Kun Luo",
        "Defu Lian",
        "Yingxia Shao",
        "Zheng Liu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=wfLuiDjQ0u",
      "cdate": 1727502691598,
      "mdate": 1740890360609,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833620"
    },
    {
      "id": "8UFG9D8xeU",
      "title": "Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Model Using Implicit Feedback from Pre-training Demonstrations",
      "abstract": "Recent advancements in Large Language Models (LLMs) have revolutionized motion generation models in embodied applications such as autonomous driving and robotic manipulation. While LLM-type auto-regressive motion generation models benefit from training scalability, there remains a discrepancy between their token prediction objectives and human preferences. As a result, models pre-trained solely with token-prediction objectives often generate behaviors that deviate from what humans would prefer, making post-training preference alignment crucial for producing human-preferred motions. Unfortunately, post-training alignment requires extensive preference rankings of motions generated by the pre-trained model, which are costly and time-consuming to annotate, especially in multi-agent motion generation settings. Recently, there has been growing interest in leveraging expert demonstrations previously used during pre-training to scalably generate preference data for post-training alignment. However, these methods often adopt an adversarial assumption, treating all pre-trained model-generated samples as unpreferred examples and relying solely on pre-training expert demonstrations to construct preferred examples. This adversarial approach overlooks the valuable signal provided by preference rankings among the model's own generations, ultimately reducing alignment effectiveness and potentially leading to misaligned behaviors. In this work, instead of treating all generated samples as equally bad, we propose a principled approach that leverages implicit preferences encoded in pre-training expert demonstrations to construct preference rankings among the pre-trained model's generations, offering more nuanced preference alignment guidance with zero human cost. We apply our approach to large-scale traffic simulation (more than 100 agents) and demonstrate its effectiveness in improving the realism of pre-trained model's generated behaviors, making a lightweight 1M motion generation model comparable to state-of-the-art large imitation-based models by relying solely on implicit feedback from pre-training demonstrations, without requiring additional post-training human preference annotations or incurring high computational costs. Furthermore, we provide an in-depth analysis of preference data scaling laws and their effects on over-optimization, offering valuable insights for future studies.",
      "authors": [
        "Thomas Tian",
        "Kratarth Goel"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=8UFG9D8xeU",
      "cdate": 1727502632413,
      "mdate": 1742943105634,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833626"
    },
    {
      "id": "e2NRNQ0sZe",
      "title": "Efficient Reinforcement Learning with Large Language Model Priors",
      "abstract": "In sequential decision-making (SDM) tasks, methods like reinforcement learning (RL) and heuristic search have made notable advances in specific cases. However, they often require extensive exploration and face challenges in generalizing across diverse environments due to their limited grasp of the underlying decision dynamics. In contrast, large language models (LLMs) have recently emerged as powerful general-purpose tools, due to their capacity to maintain vast amounts of domain-specific knowledge. To harness this rich prior knowledge for efficiently solving complex SDM tasks, we propose treating LLMs as prior action distributions and integrating them into RL frameworks through Bayesian inference methods, making use of variational inference and direct posterior sampling. The proposed approaches facilitate the seamless incorporation of fixed LLM priors into both policy-based and value-based RL frameworks. Our experiments show that incorporating LLM-based action priors significantly reduces exploration and optimization complexity, substantially improving sample efficiency compared to traditional RL techniques, e.g., using LLM priors decreases the number of required samples by over 90\\% in offline learning scenarios.",
      "authors": [
        "Xue Yan",
        "Yan Song",
        "Xidong Feng",
        "Mengyue Yang",
        "Haifeng Zhang",
        "Haitham Bou Ammar",
        "Jun Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=e2NRNQ0sZe",
      "cdate": 1727501821986,
      "mdate": 1743524286130,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833632"
    },
    {
      "id": "FvQsk3la17",
      "title": "Langevin Soft Actor-Critic: Efficient Exploration through Uncertainty-Driven Critic Learning",
      "abstract": "Existing actor-critic algorithms, which are popular for continuous control reinforcement learning (RL) tasks, suffer from poor sample efficiency due to lack of principled exploration mechanism within them. Motivated by the success of Thompson sampling for efficient exploration in RL, we propose a novel model-free RL algorithm, \\emph{Langevin Soft Actor Critic} (LSAC), which prioritizes enhancing critic learning through uncertainty estimation over policy optimization. LSAC employs three key innovations: approximate Thompson sampling through distributional Langevin Monte Carlo (LMC) based $Q$ updates, parallel tempering for exploring multiple modes of the posterior of the $Q$ function, and diffusion synthesized state-action samples regularized with $Q$ action gradients. Our extensive experiments demonstrate that LSAC outperforms or matches the performance of mainstream model-free RL algorithms for continuous control tasks.\nNotably, LSAC marks the first successful application of an LMC based Thompson sampling in continuous control tasks with continuous action spaces.",
      "authors": [
        "Haque Ishfaq",
        "Guangyuan Wang",
        "Sami Nur Islam",
        "Doina Precup"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=FvQsk3la17",
      "cdate": 1727501687638,
      "mdate": 1739323632868,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833638"
    },
    {
      "id": "cfKZ5VrhXt",
      "title": "Online Preference Alignment for Language Models via Count-based Exploration",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has shown great potential in fine-tuning Large Language Models (LLMs) to align with human preferences. Existing methods perform preference alignment from a fixed dataset, which can be limited in data coverage and the resulting reward model is hard to generalize in out-of-distribution responses. Thus, online RLHF is more desirable to empower the LLM to explore outside the support of the initial dataset by iteratively collecting the prompt-response pairs. In this paper, we study the fundamental problem in online RLHF, i.e., how to explore for LLM. We give a theoretical motivation in linear reward assumption to show that an optimistic reward with an upper confidence bound (UCB) term leads to a provably efficient RLHF policy. Then, we reformulate our objective to direct preference optimization with an exploration term, where the UCB-term can be converted to a count-based exploration bonus. We further propose a practical algorithm, named Count-based Online Preference Optimization (COPO), which leverages a simple coin-flip counting module to estimate the pseudo-count of a prompt-response pair in previously collected data. COPO encourages LLMs to balance exploration and preference optimization in an iterative manner, which enlarges the exploration space and the entire data coverage of iterative LLM policies. We conduct online RLHF experiments on Zephyr and Llama-3 models. The results on instruction-following and standard academic benchmarks show that COPO significantly increases performance.",
      "authors": [
        "Chenjia Bai",
        "Yang Zhang",
        "Shuang Qiu",
        "Qiaosheng Zhang",
        "Kang Xu",
        "Xuelong Li"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=cfKZ5VrhXt",
      "cdate": 1727501615529,
      "mdate": 1740213796940,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833644"
    },
    {
      "id": "xQCXInDq0m",
      "title": "Context Steering: Controllable Personalization at Inference Time",
      "abstract": "To deliver high-quality, personalized responses, large language models (LLMs) must effectively incorporate context — personal, demographic, and cultural information specific to an end-user. For example, asking the model to explain Newton's second law with the context \"I am a toddler'' should produce a response different from when the context is \"I am a physics professor''. However, leveraging the context in practice is a nuanced and challenging task, and is often dependent on the specific situation or user base. The model must strike a balance between providing specific, personalized responses and maintaining general applicability. Current solutions, such as prompt-engineering and fine-tuning, require collection of contextually appropriate responses as examples, making them time-consuming and less flexible to use across different contexts. In this work, we introduce Context Steering (CoS) —a simple, training-free decoding approach that amplifies the influence of the context in next token predictions. CoS computes contextual influence by comparing the output probabilities from two LLM forward passes: one that includes the context and one that does not. By linearly scaling the contextual influence, CoS allows practitioners to flexibly control the degree of personalization for different use cases. We show that CoS can be applied to autoregressive LLMs, and demonstrates strong performance in personalized recommendations. Additionally, we show that CoS can function as a Bayesian Generative model to infer and quantify correlations between open-ended texts, broadening its potential applications.",
      "authors": [
        "Jerry Zhi-Yang He",
        "Sashrika Pandey",
        "Mariah L Schrum",
        "Anca Dragan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=xQCXInDq0m",
      "cdate": 1727501323339,
      "mdate": 1739261795914,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833649"
    },
    {
      "id": "Dem5LyVk8R",
      "title": "Efficient Policy Evaluation with Safety Constraint for Reinforcement Learning",
      "abstract": "In reinforcement learning, classic on-policy evaluation methods often suffer from high variance and require massive online data to attain the desired accuracy. Previous studies attempt to reduce evaluation variance by searching for or designing proper behavior policies to collect data. However, these approaches ignore the safety of such behavior policies---the designed behavior policies have no safety guarantee and may lead to severe damage during online executions. In this paper, to address the challenge of reducing variance while ensuring safety simultaneously, we propose an optimal variance-minimizing behavior policy under safety constraints. Theoretically, while ensuring safety constraints, our evaluation method is unbiased and has lower variance than on-policy evaluation. Empirically, our method is the only existing method to achieve both substantial variance reduction and safety constraint satisfaction. Furthermore, we show our method is even superior to previous methods in both variance reduction and execution safety.",
      "authors": [
        "Claire Chen",
        "Shuze Liu",
        "Shangtong Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Dem5LyVk8R",
      "cdate": 1727500855448,
      "mdate": 1741191098668,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833654"
    },
    {
      "id": "lIVRgt4nLv",
      "title": "Agent S: An Open Agentic Framework that Uses Computers Like a Human",
      "abstract": "We present Agent S, an open agentic framework that enables autonomous interaction with computers through Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks. Agent S addresses three key challenges in automating computer tasks: acquiring domain-specific knowledge, planning over long task horizons, and handling dynamic, non-uniform interfaces. To this end, Agent S introduces experience-augmented hierarchical planning, which learns from external knowledge search and internal experience retrieval at multiple levels, facilitating efficient task planning and subtask execution. \nIn addition, it employs an Agent-Computer Interface (ACI) to better elicit the reasoning and control capabilities of GUI agents based on Multimodal Large Language Models (MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the baseline by 9.37\\% on success rate (an 83.6\\% relative improvement) and achieves a new state-of-the-art. Comprehensive analysis highlights the effectiveness of individual components and provides insights for future improvements. Furthermore, Agent S demonstrates broad generalizability to different operating systems on a newly-released WindowsAgentArena benchmark. Code available at https://github.com/simular-ai/Agent-S.",
      "authors": [
        "Saaket Agashe",
        "Jiuzhou Han",
        "Shuyu Gan",
        "Jiachen Yang",
        "Ang Li",
        "Xin Eric Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=lIVRgt4nLv",
      "cdate": 1727500778722,
      "mdate": 1743457955060,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.833663"
    },
    {
      "id": "kO0DgO07hW",
      "title": "Semantic Loss Guided Data Efficient Supervised Fine Tuning for Safe Responses in LLMs",
      "abstract": "Large Language Models (LLMs) generating unsafe responses to toxic prompts is a significant issue in their applications. While various efforts aim to address this safety concern, previous approaches often demand substantial human data collection or rely on the less dependable option of using another LLM to generate corrective data. In this paper, we aim to take this problem and overcome limitations of requiring significant high-quality human data. Our method requires only a small set of unsafe responses to toxic prompts, easily obtained from the unsafe LLM itself. By employing a semantic cost combined with a negative Earth Mover Distance (EMD) loss, we guide the LLM away from generating unsafe responses. Additionally, we propose a novel lower bound for EMD loss, enabling more efficient optimization. Our results demonstrate superior performance and data efficiency compared to baselines, and we further examine the nuanced effects of over-alignment and potential degradation of language capabilities when using contrastive data.",
      "authors": [
        "Yuxiao Lu",
        "Arunesh Sinha",
        "Pradeep Varakantham"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=kO0DgO07hW",
      "cdate": 1727500777740,
      "mdate": 1740730991177,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833669"
    },
    {
      "id": "o1Et3MogPw",
      "title": "Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence",
      "abstract": "The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents. However, existing multi-agent frameworks often struggle with integrating diverse capable third-party agents due to reliance on agents defined within their own ecosystems. They also face challenges in simulating distributed environments, as most frameworks are limited to single-device setups. Furthermore, these frameworks often rely on hard-coded communication pipelines, limiting their adaptability to dynamic task requirements. Inspired by the concept of the Internet, we propose the Internet of Agents (IoA), a novel framework that addresses these limitations by providing a flexible and scalable platform for LLM-based multi-agent collaboration. IoA introduces an agent integration protocol, an instant-messaging-like architecture design, and dynamic mechanisms for agent teaming and conversation flow control. Through extensive experiments on general assistant tasks, embodied AI tasks, and retrieval-augmented generation benchmarks, we demonstrate that IoA consistently outperforms state-of-the-art baselines, showcasing its ability to facilitate effective collaboration among heterogeneous agents. IoA represents a step towards linking diverse agents in an Internet-like environment, where agents can seamlessly collaborate to achieve greater intelligence and capabilities. We will release our code to facilitate further research.",
      "authors": [
        "Weize Chen",
        "Ziming You",
        "Ran Li",
        "yitong guan",
        "Chen Qian",
        "Chenyang Zhao",
        "Cheng Yang",
        "Ruobing Xie",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=o1Et3MogPw",
      "cdate": 1727500708544,
      "mdate": 1740710384774,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833674"
    },
    {
      "id": "tBom4xOW1H",
      "title": "Adversarial Generative Flow Network for Solving Vehicle Routing Problems",
      "abstract": "Recent research into solving vehicle routing problems (VRPs) has gained significant traction, particularly through the application of deep (reinforcement) learning for end-to-end solution construction. However, many current construction-based neural solvers predominantly utilize Transformer architectures, which can face scalability challenges and struggle to produce diverse solutions. To address these limitations, we introduce a novel framework beyond Transformer-based approaches, i.e., Adversarial Generative Flow Networks (AGFN). This framework integrates the generative flow network (GFlowNet)—a probabilistic model inherently adept at generating diverse solutions (routes)—with a complementary model for discriminating (or evaluating) the solutions. These models are trained alternately in an adversarial manner to improve the overall solution quality, followed by a proposed hybrid decoding method to construct the solution. We apply the AGFN framework to solve the capacitated vehicle routing problem (CVRP) and travelling salesman problem (TSP), and our experimental results demonstrate that AGFN surpasses the popular construction-based neural solvers, showcasing strong generalization capabilities on synthetic and real-world benchmark instances.",
      "authors": [
        "Ni Zhang",
        "Jingfeng Yang",
        "Zhiguang Cao",
        "Xu Chi"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=tBom4xOW1H",
      "cdate": 1727500656059,
      "mdate": 1741140617888,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.833679"
    },
    {
      "id": "v593OaNePQ",
      "title": "Learning to Search from Demonstration Sequences",
      "abstract": "Search and planning are essential for solving many real-world problems. However, in numerous learning scenarios, only action-observation sequences, such as demonstrations or instruction sequences, are available for learning. Relying solely on supervised learning with these sequences can lead to sub-optimal performance due to the vast, unseen search space encountered during training. In this paper, we introduce Differentiable Tree Search Network (D-TSN), a novel neural network architecture that learns to construct search trees from just sequences of demonstrations by performing gradient descent on a best-first search tree construction algorithm. D-TSN enables the joint learning of submodules, including an encoder, value function, and world model, which are essential for planning. To construct the search tree, we employ a stochastic tree expansion policy and formulate it as another decision-making task. Then, we optimize the tree expansion policy via REINFORCE with an effective variance reduction technique for the gradient computation. D-TSN can be applied to problems with a known world model or to scenarios where it needs to jointly learn a world model with a latent state space. We study problems from these two scenarios, including Game of 24, 2D grid navigation, and Procgen games, to understand when D-TSN is more helpful. Through our experiments, we show that D-TSN is effective, especially when the world model with a latent state space is jointly learned. The code is available at https://github.com/dixantmittal/differentiable-tree-search-network.",
      "authors": [
        "Dixant Mittal",
        "Liwei Kang",
        "Wee Sun Lee"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=v593OaNePQ",
      "cdate": 1727500200025,
      "mdate": 1741588784704,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833684"
    },
    {
      "id": "fMbLszVO1H",
      "title": "LongMamba: Enhancing Mamba's Long-Context Capabilities via Training-Free Receptive Field Enlargement",
      "abstract": "State space models (SSMs) have emerged as an efficient alternative to Transformer models for language modeling, offering linear computational complexity and constant memory usage as context length increases. However, despite their efficiency in handling long contexts, recent studies have shown that SSMs, such as Mamba models, generally underperform compared to Transformers in long-context understanding tasks. To address this significant shortfall and achieve both efficient and accurate long-context understanding, we propose LongMamba, a training-free technique that significantly enhances the long-context capabilities of Mamba models. LongMamba builds on our discovery that the hidden channels in Mamba can be categorized into local and global channels based on their receptive field lengths, with global channels primarily responsible for long-context capability. These global channels can become the key bottleneck as the input context lengthens. Specifically, when input lengths largely exceed the training sequence length, global channels exhibit limitations in adaptively extend their receptive fields, leading to Mamba’s poor long-context performance. The key idea of LongMamba is to mitigate the hidden state memory decay in these global channels by preventing the accumulation of unimportant tokens in their memory. This is achieved by first identifying critical tokens in the global channels and then applying token filtering to accumulate only those critical tokens. Through extensive benchmarking across synthetic and real-world long-context scenarios, LongMamba sets a new standard for Mamba’s long-context performance, significantly extending its operational range without requiring additional training. Our code is available at https://github.com/GATECH-EIC/LongMamba.",
      "authors": [
        "Zhifan Ye",
        "Kejing Xia",
        "Yonggan Fu",
        "Xin Dong",
        "Jihoon Hong",
        "Xiangchi Yuan",
        "Shizhe Diao",
        "Jan Kautz",
        "Pavlo Molchanov",
        "Yingyan Celine Lin"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=fMbLszVO1H",
      "cdate": 1727500195434,
      "mdate": 1740914317773,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.833689"
    },
    {
      "id": "Es4RPNDtmq",
      "title": "Robust Weight Initialization for Tanh Neural Networks with Fixed Point Analysis",
      "abstract": "As a neural network's depth increases, it can improve generalization performance. However, training deep networks is challenging due to gradient and signal propagation issues. To address these challenges, extensive theoretical research and various methods have been introduced. Despite these advances, effective weight initialization methods for tanh neural networks remain insufficiently investigated. This paper presents a novel weight initialization method for neural networks with tanh activation function. Based on an analysis of the fixed points of the function $\\tanh(ax)$, the proposed method aims to determine values of $a$ that mitigate activation saturation. A series of experiments on various classification datasets and physics-informed neural networks demonstrates that the proposed method outperforms Xavier initialization methods (with or without normalization) in terms of robustness across different network sizes, data efficiency, and convergence speed. Code is available at https://github.com/1HyunwooLee/Tanh-Init.",
      "authors": [
        "Hyun woo Lee",
        "Hayoung Choi",
        "Hyunju Kim"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Es4RPNDtmq",
      "cdate": 1727500083987,
      "mdate": 1740916526265,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833694"
    },
    {
      "id": "7El7K1DoyX",
      "title": "Lawma: The Power of Specialization for Legal Annotation",
      "abstract": "Annotation and classification of legal text are central components of empirical legal research. Traditionally, these tasks are often delegated to trained research assistants. Motivated by the advances in language modeling, empirical legal scholars are increasingly turning to commercial models, hoping that it will alleviate the significant cost of human annotation. In this work, we present a comprehensive analysis of large language models’ current abilities to perform legal annotation tasks. To do so, we construct CaselawQA, a benchmark comprising 260 legal text classification tasks, nearly all new to the machine learning community. We demonstrate that commercial models, such as GPT-4.5 and Claude 3.7 Sonnet, achieve non-trivial accuracy but generally fall short of the performance required for legal work. We then demonstrate that small, lightly fine-tuned models vastly outperform commercial models. A few dozen to a few hundred labeled examples are usually enough to achieve higher accuracy. Our work points to a viable alternative to the predominant practice of prompting commercial models. For concrete legal annotation tasks with some available labeled data, researchers are likely better off using a fine-tuned open-source model. Code, datasets, and fine-tuned models are available at https://github.com/socialfoundations/lawma.",
      "authors": [
        "Ricardo Dominguez-Olmedo",
        "Vedant Nanda",
        "Rediet Abebe",
        "Stefan Bechtold",
        "Christoph Engel",
        "Jens Frankenreiter",
        "Krishna P. Gummadi",
        "Moritz Hardt",
        "Michael Livermore"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=7El7K1DoyX",
      "cdate": 1727499864064,
      "mdate": 1743524171159,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833700"
    },
    {
      "id": "M4qNIzQYpd",
      "title": "OpenRCA: Can Large Language Models Locate the Root Cause of Software Failures?",
      "abstract": "Large language models (LLMs) are driving substantial advancements in software engineering, with successful applications like Copilot and Cursor transforming real-world development practices. However, current research predominantly focuses on the early stages of development, such as code generation, while overlooking the post-development phases that are crucial to user experience. To explore the potential of LLMs in this direction, we propose OpenRCA, a benchmark dataset and evaluation framework for assessing LLMs’ ability to identify the root cause of software failures. OpenRCA includes 335 failures from three enterprise software systems, along with over 68 GB of telemetry data (logs, metrics, and traces). Given a failure case and its associated telemetry, the LLM is tasked to identify the root cause that triggered the failure, requiring comprehension of software dependencies and reasoning over heterogeneous, long-context telemetry data. Our results show substantial room for improvement, as current models can only handle the simplest cases. Even with the specially designed RCA-agent, the best-performing model, Claude 3.5, solved only 11.34% failure cases. Our work paves the way for future research in this direction.",
      "authors": [
        "Junjielong Xu",
        "Qinan Zhang",
        "Zhiqing Zhong",
        "Shilin He",
        "Chaoyun Zhang",
        "Qingwei Lin",
        "Dan Pei",
        "Pinjia He",
        "Dongmei Zhang",
        "Qi Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=M4qNIzQYpd",
      "cdate": 1727499733238,
      "mdate": 1741493396270,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833708"
    },
    {
      "id": "chfJJYC3iL",
      "title": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code",
      "abstract": "Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEvla, MBPP) are no longer sufficient for assessing their capabilities suffering from data contamination, overfitting, saturation, and focus on merely code generation. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which collects new problems over time from contests across three competition platforms, Leetcode, Atcoder, and Codeforces. Notably, our benchmark also focuses on a broader range of code-related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts over six hundred coding problems that were published between May 2023 and Aug 2024. We evaluate over 50 LLMs on LiveCodeBench (LCB for brevity) presenting the largest evaluation study of code LLMs on competition problems. Based on the study, we present novel empirical findings on contamination, overfitting, and holistic evaluations. We demonstrate that time-segmented evaluations serve as a robust approach to evade contamination; they are successful at detecting contamination across a wide range of open and closed models including GPT-4O, Claude, Deepseek, and Codestral. Next, we highlight overfitting and saturation of traditional coding benchmarks like HumanEvla and demonstrate LCB allows more reliable evaluations. Finally, our holistic evaluation scenarios allow for measuring the different capabilities of programming agents in isolation.",
      "authors": [
        "Naman Jain",
        "King Han",
        "Alex Gu",
        "Wen-Ding Li",
        "Fanjia Yan",
        "Tianjun Zhang",
        "Sida Wang",
        "Armando Solar-Lezama",
        "Koushik Sen",
        "Ion Stoica"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=chfJJYC3iL",
      "cdate": 1727499410494,
      "mdate": 1740708990562,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833713"
    },
    {
      "id": "NUD03NBDOE",
      "title": "ActionReasoningBench: Reasoning about Actions with and without Ramification Constraints",
      "abstract": "Reasoning about Actions and Change (RAC) has historically played a pivotal role in solving foundational AI problems, such as the frame problem. It has driven advancements in AI fields, such as non-monotonic and commonsense reasoning. RAC remains crucial for AI systems that operate in dynamic environments, engage in interactive scenarios, or rely on commonsense reasoning. Despite substantial advances made by Large Language Models (LLMs) in various AI domains, their performance in RAC remains underexplored. To address this gap, we introduce a new diagnostic benchmark, $\\textbf{ActionReasoningBench}$, which encompasses 8 domains and includes questions for up to 19 action sequences. This benchmark rigorously evaluates LLMs across six key RAC dimensions: $\\textit{Fluent Tracking}$, $\\textit{State Tracking}$, $\\textit{Action Executability}$, $\\textit{Effects of Actions}$, $\\textit{Numerical RAC}$, and $\\textit{Composite Questions}$. LLMs demonstrate average accuracy rates of 73.55%, 65.63%, 58.73%, and 62.38% on the former four dimensions, which are frequently discussed in RAC literature. However, the performance on the latter two dimensions, which introduce complex and novel reasoning questions, the average performance of LLMs is lowered to 33.16% and 51.19%, respectively, reflecting a 17.9% performance decline. We also introduce new ramification constraints to capture the indirect effects of actions, providing deeper insights into RAC challenges. Our evaluation of state-of-the-art LLMs, including both open-source and commercial models, reveals challenges across all RAC dimensions, particularly in handling ramifications, with GPT-4o failing to solve any question and o1-preview achieving a score of only 18.4%.",
      "authors": [
        "Divij Handa",
        "Pavel Dolin",
        "Shrinidhi Kumbhar",
        "Tran Cao Son",
        "Chitta Baral"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=NUD03NBDOE",
      "cdate": 1727499380499,
      "mdate": 1740784274523,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833722"
    },
    {
      "id": "A6Y7AqlzLW",
      "title": "Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning",
      "abstract": "A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. With the goal of using PRMs to improve a *base* policy via test-time search and reinforcement learning (RL), we ask: ``How should we design process rewards?'' Our key insight is that, to be effective, the process reward for a step should measure \n *progress*: a change in the likelihood of producing a correct response in the future, before and after taking the step, as measured under a *prover* policy distinct from the base policy. Such progress values can {distinguish} good and bad steps generated by the base policy, even though the base policy itself cannot.  Theoretically, we show that even weaker provers can improve the base policy, as long as they distinguish steps without being too misaligned with the base policy. Our results show that process rewards defined as progress under such provers improve the efficiency of exploration during test-time search and online RL. We empirically validate our claims by training  **process advantage verifiers (PAVs)** to measure progress under such provers and show that compared to ORM, they are >8% more accurate, and 1.5-5x more compute-efficient. Equipped with these insights, our PAVs enable **one of the first results** showing a 6x gain in sample efficiency for a policy trained using online RL with PRMs vs. ORMs.",
      "authors": [
        "Amrith Setlur",
        "Chirag Nagpal",
        "Adam Fisch",
        "Xinyang Geng",
        "Jacob Eisenstein",
        "Rishabh Agarwal",
        "Alekh Agarwal",
        "Jonathan Berant",
        "Aviral Kumar"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=A6Y7AqlzLW",
      "cdate": 1727499238364,
      "mdate": 1740894568044,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833727"
    },
    {
      "id": "CS2JWaziYr",
      "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding",
      "abstract": "Large Language Models (LLMs) have become more prevalent in long-context applications such as interactive chatbots, document analysis, and agent workflows, but it is challenging to serve long-context requests with low latency and high throughput. Speculative decoding (SD) is a widely used technique to reduce latency losslessly, but the conventional wisdom suggests that its efficacy is limited to small batch sizes. In MagicDec, we show that surprisingly SD can achieve speedup even for a high throughput inference regime for moderate to long sequences. More interestingly, an intelligent drafting strategy can achieve better speedup with increasing batch size based on our rigorous analysis. MagicDec first identifies the bottleneck shifts with increasing batch size and sequence length, and uses these insights to deploy SD more effectively for high throughput inference. We leverage draft model with sparse KV cache to address the KV bottleneck, which scales with both sequence length and batch size. Additionally, we propose a theoretical model to select the optimal drafting strategy for maximum speedup. Our work highlights the broad applicability of speculative decoding in long-context serving, as it can enhance throughput and reduce latency without compromising accuracy. For moderate to long sequences, we demonstrate up to 2.51x speedup for LLaMA-3.1-8B when serving batch sizes ranging from 32 to 256 on various types of hardware and tasks.",
      "authors": [
        "Ranajoy Sadhukhan",
        "Jian Chen",
        "Zhuoming Chen",
        "Vashisth Tiwari",
        "Ruihang Lai",
        "Jinyuan Shi",
        "Ian En-Hsu Yen",
        "Avner May",
        "Tianqi Chen",
        "Beidi Chen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=CS2JWaziYr",
      "cdate": 1727499190172,
      "mdate": 1743559306436,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833732"
    },
    {
      "id": "exgLs4snap",
      "title": "Parameter Expanded Stochastic Gradient Markov Chain Monte Carlo",
      "abstract": "Bayesian Neural Networks (BNNs) provide a promising framework for modeling predictive uncertainty and enhancing out-of-distribution robustness (OOD) by estimating the posterior distribution of network parameters. Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) is one of the most powerful methods for scalable posterior sampling in BNNs, achieving efficiency by combining stochastic gradient descent with second-order Langevin dynamics. However, SGMCMC often suffers from limited sample diversity in practice, which affects uncertainty estimation and model performance. We propose a simple yet effective approach to enhance sample diversity in SGMCMC without the need for tempering or running multiple chains. Our approach reparameterizes the neural network by decomposing each of its weight matrices into a product of matrices, resulting in a sampling trajectory that better explores the target parameter space. This approach produces a more diverse set of samples, allowing faster mixing within the same computational budget. Notably, our sampler achieves these improvements without increasing the inference cost compared to the standard SGMCMC. Extensive experiments on image classification tasks, including OOD robustness, diversity, loss surface analyses, and a comparative study with Hamiltonian Monte Carlo, demonstrate the superiority of the proposed approach.",
      "authors": [
        "Hyunsu Kim",
        "Giung Nam",
        "Chulhee Yun",
        "Hongseok Yang",
        "Juho Lee"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=exgLs4snap",
      "cdate": 1727499058731,
      "mdate": 1740881959193,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833738"
    },
    {
      "id": "NkGDNM8LB0",
      "title": "Hyperbolic Genome Embeddings",
      "abstract": "Current approaches to genomic sequence modeling often struggle to align the inductive biases of machine learning models with the evolutionarily-informed structure of biological systems. To this end, we formulate a novel application of hyperbolic CNNs that exploits this structure, enabling more expressive DNA sequence representations. Our strategy circumvents the need for explicit phylogenetic mapping while discerning key properties of sequences pertaining to core functional and regulatory behavior. Across 37 out of 42 genome interpretation benchmark datasets, our hyperbolic models outperform their Euclidean equivalents. Notably, our approach even surpasses state-of-the-art performance on seven GUE benchmark datasets, consistently outperforming many DNA language models while using orders of magnitude fewer parameters and avoiding pretraining. Our results include a novel set of benchmark datasets---the Transposable Elements Benchmark---which explores a major but understudied component of the genome with deep evolutionary significance. We further motivate our work by exploring how our hyperbolic models recognize genomic signal under various data-generating conditions and by constructing an empirical method for interpreting the hyperbolicity of dataset embeddings. Throughout these assessments, we find persistent evidence highlighting the potential of our hyperbolic framework as a robust paradigm for genome representation learning. Our code and benchmark datasets are available at https://github.com/rrkhan/HGE.",
      "authors": [
        "Raiyan R. Khan",
        "Philippe Chlenski",
        "Itsik Pe'er"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=NkGDNM8LB0",
      "cdate": 1727498941946,
      "mdate": 1743531949923,
      "matched_keywords": [
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833743"
    },
    {
      "id": "hzuumhfYSO",
      "title": "Deep Distributed Optimization for Large-Scale Quadratic Programming",
      "abstract": "Quadratic programming (QP) forms a crucial foundation in optimization, appearing in a broad spectrum of domains and serving as the basis for more advanced algorithms. Consequently, as the scale and complexity of modern applications continue to grow, the development of efficient and reliable QP algorithms becomes increasingly vital. In this context, this paper introduces a novel deep learning-aided distributed optimization architecture designed for tackling large-scale QP problems. First, we combine the state-of-the-art Operator Splitting QP (OSQP) method with a consensus approach to derive **DistributedQP**, a new method tailored for network-structured problems, with convergence guarantees to optimality. Subsequently, we unfold this optimizer into a deep learning framework, leading to **DeepDistributedQP**, which leverages learned policies to accelerate reaching to desired accuracy within a restricted amount of iterations. Our approach is also theoretically grounded through Probably Approximately Correct (PAC)-Bayes theory, providing generalization bounds on the expected optimality gap for unseen problems. The proposed framework, as well as its centralized version **DeepQP**, significantly outperform their standard optimization counterparts on a variety of tasks such as randomly generated problems, optimal control, linear regression, transportation networks and others. Notably, DeepDistributedQP demonstrates strong generalization by training on small problems and scaling to solve much larger ones (up to 50K variables and 150K constraints) using the same policy. Moreover, it achieves orders-of-magnitude improvements in wall-clock time compared to OSQP. The certifiable performance guarantees of our approach are also demonstrated, ensuring higher-quality solutions over traditional optimizers.",
      "authors": [
        "Augustinos D Saravanos",
        "Hunter Kuperman",
        "Alex Oshin",
        "Arshiya Taj Abdul",
        "Vincent Pacelli",
        "Evangelos Theodorou"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=hzuumhfYSO",
      "cdate": 1727498453969,
      "mdate": 1743586267422,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833749"
    },
    {
      "id": "gU4ZgQNsOC",
      "title": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining",
      "abstract": "Pretraining large language models (LLMs) on vast and heterogeneous datasets is crucial for achieving state-of-the-art performance across diverse downstream tasks. However, current training paradigms treat all samples equally, overlooking the importance or relevance of individual samples throughout the training process. Existing reweighting strategies, which primarily focus on group-level data importance, fail to leverage fine-grained instance-level information and do not adapt dynamically to individual sample importance as training progresses. In this paper, we introduce novel algorithms for dynamic, instance-level data reweighting aimed at improving both the efficiency and effectiveness of LLM pretraining. Our methods adjust the weight of each training sample based on its loss value in an online fashion, allowing the model to dynamically focus on more informative or important samples at the current training stage. In particular, our framework allows us to systematically devise reweighting strategies deprioritizing redundant or uninformative data, which we find tend to work best. \nFurthermore, we develop a new theoretical framework for analyzing the impact of loss-based reweighting on the convergence of gradient-based optimization, providing the first formal characterization of how these strategies affect convergence bounds. We empirically validate our approach across a spectrum of tasks, from pretraining 7B and 1.4B parameter LLMs to smaller-scale language models and linear regression problems, demonstrating that our loss-based reweighting approach can lead to faster convergence and significantly improved performance.",
      "authors": [
        "Daouda Sow",
        "Herbert Woisetschläger",
        "Saikiran Bulusu",
        "Shiqiang Wang",
        "Hans Arno Jacobsen",
        "Yingbin Liang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=gU4ZgQNsOC",
      "cdate": 1727498257928,
      "mdate": 1742438959716,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833754"
    },
    {
      "id": "m9wG6ai2Xk",
      "title": "MQuAKE-Remastered: Multi-Hop Knowledge Editing Can Only Be Advanced with Reliable Evaluations",
      "abstract": "Large language models (LLMs) can give out erroneous answers to factually rooted questions either as a result of undesired training outcomes or simply because the world has moved on after a certain knowledge cutoff date. Under such scenarios, *knowledge editing* often comes to the rescue by delivering efficient patches for such erroneous answers without significantly altering the rest, where many editing methods have seen reasonable success when the editing targets are simple and direct (e.g., *``what club does Lionel Messi currently play for?''*).\n\nHowever, knowledge fragments like this are often deeply intertwined in the real world, making effectively propagating the editing effect to non-directly related questions a practical challenge (to entertain an extreme example: [*\"What car did the wife of the owner of the club that Messi currently plays for used to get to school in the 80s?\"*](youtube.com/watch?v=DbwiHC1Fu-E\\&t=132s)). Prior arts have coined this task as *multi-hop knowledge editing* with the most popular dataset being MQuAKE, serving as the sole evaluation benchmark for many later proposed editing methods due to the expensive nature of constructing knowledge editing datasets at scale. \n\nIn this work, we reveal that **up to 33\\% or 76\\% of \\mquake{}'s questions and ground truth labels are, in fact, corrupted in various fashions due to some unintentional clerical or procedural oversights**. Our work provides a detailed audit of MQuAKE's error pattern and a comprehensive fix without sacrificing its dataset capacity. Additionally, we benchmarked almost all proposed MQuAKE-evaluated editing methods on our post-fix dataset, **MQuAKE-Remastered**. We observe that many methods try to overfit the original MQuAKE by exploiting some dataset idiosyncrasies of MQuAKE. We provide a guideline on how to approach such datasets faithfully and show that a simple, minimally invasive approach — **GWalk** — can offer beyond SOTA editing performance without such exploitation. The MQuAKE-Remastered datasets and utilities are available at [huggingface.co/datasets/henryzhongsc/MQuAKE-Remastered](https://huggingface.co/datasets/henryzhongsc/MQuAKE-Remastered) and [github.com/henryzhongsc/MQuAKE-Remastered](https://github.com/henryzhongsc/MQuAKE-Remastered), respectively.",
      "authors": [
        "Shaochen Zhong",
        "Yifan Lu",
        "Lize Shao",
        "Bhargav Bhushanam",
        "Xiaocong Du",
        "Yixin Wan",
        "Yucheng Shi",
        "Daochen Zha",
        "Yiwei Wang",
        "Ninghao Liu",
        "Kaixiong Zhou",
        "Shuai Xu",
        "Kai-Wei Chang",
        "Louis Feng",
        "Vipin Chaudhary",
        "Xia Hu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=m9wG6ai2Xk",
      "cdate": 1727498182878,
      "mdate": 1742405340317,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833760"
    },
    {
      "id": "LNL7zKvm7e",
      "title": "Frame-Voyager: Learning to Query Frames for Video Large Language Models",
      "abstract": "Video Large Language Models (Video-LLMs) have made remarkable progress in video understanding tasks. However, they are constrained by the maximum length of input tokens, making it impractical to input entire videos. Existing frame selection approaches, such as uniform frame sampling and text-frame retrieval, fail to account for the information density variations in the videos or the complex instructions in the tasks, leading to sub-optimal performance. In this paper, we propose Frame-Voyager that learns to query informative frame combinations, based on the given textual queries in the task. To train Frame-Voyager, we introduce a new data collection and labeling pipeline, by ranking frame combinations using a pre-trained Video-LLM. Given a video of M frames, we traverse its T-frame combinations, feed them into a Video-LLM, and rank them based on Video-LLM's prediction losses. Using this ranking as supervision, we train Frame-Voyager to query the frame combinations with lower losses. In experiments, we evaluate Frame-Voyager on four Video Question Answering benchmarks by plugging it into two different Video-LLMs. The experimental results demonstrate that Frame-Voyager achieves impressive results in all settings, highlighting its potential as a plug-and-play solution for Video-LLMs.",
      "authors": [
        "Sicheng Yu",
        "CHENGKAI JIN",
        "Huanyu Wang",
        "Zhenghao Chen",
        "Sheng Jin",
        "ZHONGRONG ZUO",
        "XU XIAOLEI",
        "Zhenbang Sun",
        "Bingni Zhang",
        "Jiawei Wu",
        "Hao Zhang",
        "Qianru Sun"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=LNL7zKvm7e",
      "cdate": 1727498052446,
      "mdate": 1743131691837,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833765"
    },
    {
      "id": "XrsOu4KgDE",
      "title": "Attributing Culture-Conditioned Generations to Pretraining Corpora",
      "abstract": "In open-ended generative tasks like narrative writing or dialogue, large language models often exhibit cultural biases, showing limited knowledge and generating templated outputs for less prevalent cultures. Recent works show that these biases may stem from uneven cultural representation in pretraining corpora. This work investigates how pretraining leads to biased culture-conditioned generations\nby analyzing how models associate entities with cultures based on pretraining data patterns. We propose the MEMOED framework (MEMOrization from prEtraining Document) to determine whether a generation for a culture arises from memorization. Using MEMOED on culture-conditioned generations about food and clothing for 110 cultures, we find that high-frequency cultures in pretraining data yield more generations with memorized symbols, while some low-frequency cultures produce none. Additionally, the model favors generating entities with extraordinarily high frequency regardless of the conditioned culture, reflecting biases toward frequent pretraining terms irrespective of relevance. We hope that the MEMOED framework and our insights will inspire more works on attributing model performance on pretraining data.",
      "authors": [
        "Huihan Li",
        "Arnav Goel",
        "Keyu He",
        "Xiang Ren"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=XrsOu4KgDE",
      "cdate": 1727497944412,
      "mdate": 1742410869095,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833770"
    },
    {
      "id": "PHg4rAXFVH",
      "title": "RTop-K: Ultra-Fast Row-Wise Top-K Selection for Neural Network Acceleration on GPUs",
      "abstract": "Abstract Top-k selection algorithms are fundamental in a wide range of applications, including high-performance computing, information retrieval, big data processing, and neural network model training. In this paper, we present RTop-K, a highly efficient parallel row-wise top-k selection algorithm specifically designed for GPUs. RTop-K leverages a binary search-based approach to optimize row-wise top-k selection, providing a scalable and accelerated solution.\nWe conduct a detailed analysis of early stopping in our algorithm, showing that it effectively maintains the testing accuracy of neural network models while substantially improving performance. Our GPU implementation of RTop-K demonstrates superior performance over state-of-the-art row-wise top-k GPU implementations, achieving an average speed-up of up to 11.49× with early stopping and 7.29× without early stopping. Moreover, RTop-K accelerates the overall training workflow of MaxK-GNNs, delivering speed-ups ranging from 11.97% to 33.29% across different models and datasets.",
      "authors": [
        "Xi Xie",
        "Yuebo Luo",
        "Hongwu Peng",
        "Caiwen Ding"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=PHg4rAXFVH",
      "cdate": 1727497840105,
      "mdate": 1743572490949,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833782"
    },
    {
      "id": "hoYFLRNbhc",
      "title": "DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory",
      "abstract": "Large language models (LLMs) have achieved reasonable quality improvements in machine translation (MT).\nHowever, most current research on MT-LLMs still faces significant challenges in maintaining translation consistency and accuracy when processing entire documents.\nIn this paper, we introduce DelTA, a Document-levEL Translation Agent designed to overcome these limitations.\nDelTA features a multi-level memory structure that stores information across various granularities and spans, including Proper Noun Records, Bilingual Summary, Long-Term Memory, and Short-Term Memory, which are continuously retrieved and updated by auxiliary LLM-based components.\nExperimental results indicate that DelTA significantly outperforms strong baselines in terms of translation consistency and quality across four open/closed-source LLMs and two representative document translation datasets, achieving an increase in consistency scores by up to 4.58 percentage points and in COMET scores by up to 3.16 points on average.\nDelTA employs a sentence-by-sentence translation strategy, ensuring no sentence omissions and offering a memory-efficient solution compared to the mainstream method.\nFurthermore, DelTA improves pronoun and context-dependent translation accuracy, and the summary component of the agent also shows promise as a tool for query-based summarization tasks.\nThe code and data of our approach are released at https://github.com/YutongWang1216/DocMTAgent.",
      "authors": [
        "Yutong Wang",
        "Jiali Zeng",
        "Xuebo Liu",
        "Derek F. Wong",
        "Fandong Meng",
        "Jie Zhou",
        "Min Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=hoYFLRNbhc",
      "cdate": 1727497687674,
      "mdate": 1740629243523,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833788"
    },
    {
      "id": "eENHKMTOfW",
      "title": "Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs",
      "abstract": "The rise of large language models (LLMs) has created a significant disparity: industrial research labs with their computational resources, expert teams, and advanced infrastructures, can effectively fine-tune LLMs, while individual developers and small organizations face barriers due to limited resources to effectively explore the experiment space. In this paper, we aim to bridge this gap by presenting a comprehensive study on supervised fine-tuning of LLMs using instruction-tuning datasets spanning diverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B parameters) for their cost-efficiency and accessibility. We explore various training configurations and strategies across four open-source pre-trained models. We provide detailed documentation of these configurations, revealing findings that challenge several common training practices, including hyperparameter recommendations from TULU and phased training recommended by Orca. The code used for the experiments can be found here: https://github.com/instructlab/training.\n\nKey insights from our work include: (i) larger batch sizes paired with lower learning rates lead to improved model performance on benchmarks such as MMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics, such as lower gradient norms and higher loss values, are strong indicators of better final model performance, allowing for early termination of sub-optimal runs and significant computational savings; (iii) through a thorough exploration of hyperparameters like warmup steps and learning rate schedules, we provide guidance for practitioners and find that certain simplifications do not compromise performance; and (iv) we observe no significant difference in performance between phased (sequentially training on data divided into phases) and stacked (training on the entire dataset at once) strategies, but stacked training is simpler and more sample efficient. With these findings holding robustly across datasets as well as model families and sizes, we hope this study serves as a guide for practitioners fine-tuning small LLMs and promotes a more inclusive research environment for LLM development.",
      "authors": [
        "Aldo Pareja",
        "Nikhil Shivakumar Nayak",
        "Hao Wang",
        "Krishnateja Killamsetty",
        "Shivchander Sudalairaj",
        "Wenlong Zhao",
        "Seungwook Han",
        "Abhishek Bhandwaldar",
        "Guangxuan Xu",
        "Kai Xu",
        "Ligong Han",
        "Luke Inglis",
        "Akash Srivastava"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=eENHKMTOfW",
      "cdate": 1727497678450,
      "mdate": 1740890360059,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833796"
    },
    {
      "id": "6bKEWevgSd",
      "title": "ManiSkill-HAB: A Benchmark for Low-Level Manipulation in Home Rearrangement Tasks",
      "abstract": "High-quality benchmarks are the foundation for embodied AI research, enabling significant advancements in long-horizon navigation, manipulation and rearrangement tasks. However, as frontier tasks in robotics get more advanced, they require faster simulation speed, more intricate test environments, and larger demonstration datasets. To this end, we present MS-HAB, a holistic benchmark for low-level manipulation and in-home object rearrangement. First, we provide a GPU-accelerated implementation of the Home Assistant Benchmark (HAB). We support realistic low-level control and achieve over 3x the speed of prior magical grasp implementations at a fraction of the GPU memory usage. Second, we train extensive reinforcement learning (RL) and imitation learning (IL) baselines for future work to compare against. Finally, we develop a rule-based trajectory filtering system to sample specific demonstrations from our RL policies which match predefined criteria for robot behavior and safety. Combining demonstration filtering with our fast environments enables efficient, controlled data generation at scale.",
      "authors": [
        "Arth Shukla",
        "Stone Tao",
        "Hao Su"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=6bKEWevgSd",
      "cdate": 1727497153651,
      "mdate": 1740736899557,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833801"
    },
    {
      "id": "25j2ZEgwTj",
      "title": "How Gradient descent balances features: A dynamical analysis for two-layer neural networks",
      "abstract": "This paper investigates the fundamental regression task of learning $k$ neurons (\\emph{a.k.a.} teachers) from Gaussian input, using two-layer ReLU neural networks with width $m$ (\\emph{a.k.a.} students) and $m, k= \\mathcal{O}(1)$, trained via gradient descent under proper initialization and a small step-size. Our analysis follows a three-phase structure: \\emph{alignment} after weak recovery, \\emph{tangential growth}, and \\emph{local convergence}, providing deeper insights into the learning dynamics of gradient descent (GD). We prove the global convergence at the rate of $\\mathcal{O}(T^{-3})$ for the zero loss of excess risk. Additionally, our results show that GD automatically groups and balances student neurons, revealing an implicit bias toward achieving the minimum ``balanced'' $\\ell_2$-norm in the solution. Our work extends beyond previous studies in exact-parameterization setting ($m = k = 1$, (Yehudai and Ohad, 2020)) and single-neuron setting ($m \\geq k = 1$, (Xu and Du, 2023)). The key technical challenge lies in handling the interactions between multiple teachers and students during training, which we address by refining the alignment analysis in Phase 1 and introducing a new dynamic system analysis for tangential components in Phase 2. Our results pave the way for further research on optimizing neural network training dynamics and understanding implicit biases in more complex architectures.",
      "authors": [
        "Zhenyu Zhu",
        "Fanghui Liu",
        "Volkan Cevher"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=25j2ZEgwTj",
      "cdate": 1727497093433,
      "mdate": 1740890360053,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833807"
    },
    {
      "id": "YaBiGjuDiC",
      "title": "A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become the predominant approach for aligning language models (LMs) to be more helpful and less harmful. \nAt its core, RLHF uses a margin-based loss for preference optimization, which specifies the ideal LM behavior only in terms of the difference between preferred and dispreferred responses. In this paper, we identify a common pitfall of margin-based methods---the under-specification of ideal LM behavior on preferred and dispreferred responses individually, which results in two unintended consequences as the margin increases:\n(1) The probability of dispreferred (e.g., unsafe) responses may increase, resulting in potential safety alignment failures.\n(2) The probability of preferred responses may decrease, even when those responses are ideal.\nWe demystify the reasons behind these problematic behaviors: margin-based losses couple the change in the preferred probability with the gradient of the dispreferred one, and vice versa, often preventing the preferred probability from increasing while the dispreferred one decreases, and thus causing a synchronized increase or decrease in both probabilities. We term this effect, inherent in margin-based objectives, gradient entanglement. \nFormally, we derive conditions for general margin-based alignment objectives under which gradient entanglement becomes concerning: the inner product between the gradient of preferred log-probability and the gradient of dispreferred log-probability is large relative to the individual gradient norms. Furthermore, we theoretically investigate why such inner products can be large when aligning language models and empirically validate our findings. Empirical implications of our framework further extend to explaining important differences in the training dynamics of various preference optimization algorithms and suggesting future directions for improvement.",
      "authors": [
        "Hui Yuan",
        "Yifan Zeng",
        "Yue Wu",
        "Huazheng Wang",
        "Mengdi Wang",
        "Liu Leqi"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=YaBiGjuDiC",
      "cdate": 1727497044261,
      "mdate": 1741014825552,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833812"
    },
    {
      "id": "lOi6FtIwR8",
      "title": "Model Editing as a Robust and Denoised variant of DPO: A Case Study on Toxicity",
      "abstract": "Recent alignment algorithms such as direct preference optimization (DPO) have been developed to improve the safety of large language models (LLMs) by training these models to match human behaviors exemplified by preference data. However, these methods are \nboth computationally intensive and lacking in controllability and transparency, inhibiting their widespread use. Furthermore, these tuning-based methods require large-scale preference data for training and are susceptible to noisy preference data. In this paper, we introduce a tuning-free alignment alternative, ProFS (Projection Filter for Subspaces), and demonstrate its effectiveness under the use case of toxicity reduction. Grounded on theory from factor analysis, ProFS is a sample-efficient model editing approach that identifies a toxic subspace in the model parameter space and reduces model toxicity by projecting away the detected subspace. The toxic subspace is identified by extracting preference data embeddings from the language model, and removing non-toxic information from these embeddings. We show that ProFS is more sample-efficient than DPO, further showcasing greater robustness to noisy data. Finally, we attempt to connect tuning based alignment with editing, by establishing both theoretical and empirical connections between ProFS and DPO, showing that ProFS can be interpreted as a denoised version of a single DPO step.",
      "authors": [
        "Rheeya Uppaal",
        "Apratim Dey",
        "Yiting He",
        "Yiqiao Zhong",
        "Junjie Hu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=lOi6FtIwR8",
      "cdate": 1727496848210,
      "mdate": 1740791892337,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833817"
    },
    {
      "id": "Mv3GAYJGcW",
      "title": "MetaDesigner: Advancing Artistic Typography through AI-Driven, User-Centric, and Multilingual WordArt Synthesis",
      "abstract": "MetaDesigner introduces a transformative framework for artistic typography synthesis, powered by Large Language Models (LLMs) and grounded in a user-centric design paradigm. Its foundation is a multi-agent system comprising the Pipeline, Glyph, and Texture agents, which collectively orchestrate the creation of customizable WordArt, ranging from semantic enhancements to intricate textural elements. A central feedback mechanism leverages insights from both multimodal models and user evaluations, enabling iterative refinement of design parameters. Through this iterative process, MetaDesigner dynamically adjusts hyperparameters to align with user-defined stylistic and thematic preferences, consistently delivering WordArt that excels in visual quality and contextual resonance. Empirical evaluations underscore the system's versatility and effectiveness across diverse WordArt applications, yielding outputs that are both aesthetically compelling and context-sensitive.",
      "authors": [
        "Jun-Yan He",
        "Zhi-Qi Cheng",
        "Chenyang Li",
        "Jingdong Sun",
        "Qi He",
        "Wangmeng Xiang",
        "Hanyuan Chen",
        "Jin-Peng Lan",
        "Xianhui Lin",
        "kang zhu",
        "Bin Luo",
        "Yifeng Geng",
        "Xuansong Xie",
        "Alexander G Hauptmann"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Mv3GAYJGcW",
      "cdate": 1727496682998,
      "mdate": 1744352801722,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.833822"
    },
    {
      "id": "uAtDga3q0r",
      "title": "Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA Adapters",
      "abstract": "Large Language Models (LLMs) are computationally intensive, particularly during inference. Neuron-adaptive techniques, which selectively activate neurons in Multi-Layer Perceptron (MLP) layers, offer some speedups but suffer from limitations in modern Transformers. These include reliance on sparse activations, incompatibility with attention layers, and the use of costly neuron masking techniques. To address these issues, we propose the Adaptive Rank Allocation framework and introduce the Rank and Neuron Allocator (RaNA) adapter. RaNA adapters leverage rank adapters, which operate on linear layers by applying both low-rank matrix decompositions and adaptive masking to efficiently allocate compute without depending on activation sparsity. This enables RaNA to be generally applied to MLPs and linear components of attention modules, while eliminating the need for expensive maskers found in neuron-adaptive methods. Notably, when compared to neuron adapters, RaNA improves perplexity by up to 7 points and increases accuracy by up to 8 percentage-points when reducing FLOPs by $\\sim$44\\% in state-of-the-art Transformer architectures. These results position RaNA as a robust solution for improving inference efficiency in  modern Transformer architectures.",
      "authors": [
        "Roberto Garcia",
        "Jerry Weihong Liu",
        "Daniel Sorvisto",
        "Sabri Eyuboglu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=uAtDga3q0r",
      "cdate": 1727496659445,
      "mdate": 1742766006283,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.833828"
    },
    {
      "id": "jCDF7G3LpF",
      "title": "EFFICIENT JAILBREAK ATTACK SEQUENCES ON LARGE LANGUAGE MODELS VIA MULTI-ARMED BANDIT-BASED CONTEXT SWITCHING",
      "abstract": "Content warning: This paper contains examples of harmful language and content.\nRecent advances in large language models (LLMs) have made them increasingly vulnerable to jailbreaking attempts, where malicious users manipulate models into generating harmful content. While existing approaches rely on either single-step attacks that trigger immediate safety responses or multi-step methods that inefficiently iterate prompts using other LLMs, we introduce ``Sequence of Context\" (SoC) attacks that systematically alter conversational context through strategically crafted context-switching queries (CSQs). We formulate this as a multi-armed bandit (MAB) optimization problem, automatically learning optimal sequences of CSQs that gradually weaken the model's safety boundaries. Our theoretical analysis provides tight bounds on both the expected sequence length until successful jailbreak and the convergence of cumulative rewards. Empirically, our method achieves a 95\\% attack success rate, surpassing PAIR by 63.15\\%, AutoDAN by 60\\%, and ReNeLLM by 50\\%. We evaluate our attack across multiple open-source LLMs including Llama and Mistral variants. Our findings highlight critical vulnerabilities in current LLM safeguards and emphasize the need for defenses that consider sequential attack patterns rather than relying solely on static prompt filtering or iterative refinement.",
      "authors": [
        "Aditya Ramesh",
        "Shivam Bhardwaj",
        "Aditya Saibewar",
        "Manohar Kaul"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=jCDF7G3LpF",
      "cdate": 1727496626094,
      "mdate": 1740377978713,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833833"
    },
    {
      "id": "TrKRpaOk8y",
      "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference with Partial Contexts",
      "abstract": "Training and serving long-context large language models (LLMs) incurs substantial overhead. \nTo address this, two critical steps are often required: a pretrained LLM typically undergoes a separate stage for context length extension by training on long-context data, followed by architectural modifications to reduce the overhead of KV cache during serving. \nThis paper argues that integrating length extension with a GPU-friendly KV cache reduction architecture not only reduces training overhead during length extension, but also achieves better long-context performance. \nThis leads to our proposed LongGen, which finetunes a pretrained LLM into an efficient architecture during length extension. \nLongGen builds on three key insights: \n(1) Sparse attention patterns, such as window attention (attending to recent tokens), attention sink (initial ones), and blockwise sparse attention (strided token blocks) are well-suited for building efficient long-context models, primarily due to their GPU-friendly memory access patterns, enabling efficiency gains not just theoretically but in practice as well. \n(2) It is essential for the model to have direct access to all tokens. \nA hybrid architecture with 1/3 full attention layers and 2/3 efficient ones achieves a balanced trade-off between efficiency and long-context performance.\n(3) Lightweight training on 5B long-context data is sufficient to extend the hybrid model's context length from 4K to 128K.\n\nWe evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its effectiveness across different scales. \nDuring training with 128K-long contexts, LongGen achieves 1.55x training speedup and reduces wall-clock time by 36%, compared to a full-attention baseline. \nDuring inference, LongGen reduces KV cache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding speedup.\nCompared to baselines that apply KV-cache reduction techniques to full-attention long-context LLMs, LongGen achieves substantially stronger performance not only on the Needle-in-a-Haystack retrieval task, but also on more challenging long-context reasoning tasks, including BABILong and RULER.",
      "authors": [
        "Suyu Ge",
        "Xihui Lin",
        "Yunan Zhang",
        "Jiawei Han",
        "Hao Peng"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=TrKRpaOk8y",
      "cdate": 1727496604031,
      "mdate": 1740897391707,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833841"
    },
    {
      "id": "etif9j1CnG",
      "title": "What Secrets Do Your Manifolds Hold? Understanding the Local Geometry of Generative Models",
      "abstract": "Deep Generative Models are frequently used to learn continuous representations of complex data distributions by training on a finite number of samples. For any generative model, including pre-trained foundation models with Diffusion or Transformer architectures, generation performance can significantly vary across the learned data manifold. In this paper, we study the local geometry of the learned manifold and its relationship to generation outcomes for a wide range of generative models, including DDPM, Diffusion Transformer (DiT), and Stable Diffusion 1.4. Building on the theory of continuous piecewise-linear (CPWL) generators, we characterize the local geometry in terms of three geometric descriptors - scaling ($\\psi$), rank ($\\nu$), and complexity/un-smoothness ($\\delta$). We provide quantitative and qualitative evidence showing that for a given latent vector, the local descriptors are indicative of post-generation aesthetics, generation diversity, and memorization by the generative model. Finally, we demonstrate that by training a reward model on the 'local scaling' for Stable Diffusion, we can self-improve both generation aesthetics and diversity using geometry sensitive guidance during denoising. Website: https://imtiazhumayun.github.io/generative_geometry.",
      "authors": [
        "Ahmed Imtiaz Humayun",
        "Ibtihel Amara",
        "Cristina Nader Vasconcelos",
        "Deepak Ramachandran",
        "Candice Schumann",
        "Junfeng He",
        "Katherine A Heller",
        "Golnoosh Farnadi",
        "Negar Rostamzadeh",
        "Mohammad Havaei"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=etif9j1CnG",
      "cdate": 1727496555864,
      "mdate": 1744637289029,
      "matched_keywords": [
        "foundation model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.833847"
    },
    {
      "id": "AoIKgHu9Si",
      "title": "L-WISE: Boosting Human Visual Category Learning Through Model-Based Image Selection and Enhancement",
      "abstract": "The currently leading artificial neural network models of the visual ventral stream - which are derived from a combination of performance optimization and robustification methods - have demonstrated a remarkable degree of behavioral alignment with humans on visual categorization tasks. We show that image perturbations generated by these models can enhance the ability of humans to accurately report the ground truth class. Furthermore, we find that the same models can also be used out-of-the-box to predict the proportion of correct human responses to individual images, providing a simple, human-aligned estimator of the relative difficulty of each image. Motivated by these observations, we propose to augment visual learning in humans in a way that improves human categorization accuracy at test time. Our learning augmentation approach consists of (i) selecting images based on their model-estimated recognition difficulty, and (ii) applying image perturbations that aid recognition for novice learners. We find that combining these model-based strategies leads to categorization accuracy gains of 33-72% relative to control subjects without these interventions, on unmodified, randomly selected held-out test images. Beyond the accuracy gain, the training time for the augmented learning group was also shortened by 20-23%, despite both groups completing the same number of training trials. We demonstrate the efficacy of our approach in a fine-grained categorization task with natural images, as well as two tasks in clinically relevant image domains - histology and dermoscopy - where visual learning is notoriously challenging. To the best of our knowledge, our work is the first application of artificial neural networks to increase visual learning performance in humans by enhancing category-specific image features.",
      "authors": [
        "Morgan Bruce Talbot",
        "Gabriel Kreiman",
        "James J. DiCarlo",
        "Guy Gaziv"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=AoIKgHu9Si",
      "cdate": 1727496457599,
      "mdate": 1747412173736,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833853"
    },
    {
      "id": "k3y0oyK7sn",
      "title": "Predictive Uncertainty Quantification for Bird's Eye View Segmentation: A Benchmark and Novel Loss Function",
      "abstract": "The fusion of raw sensor data to create a Bird's Eye View (BEV) representation is critical for autonomous vehicle planning and control. Despite the growing interest in using deep learning models for BEV semantic segmentation, anticipating segmentation errors and enhancing the explainability of these models remain underexplored. This paper introduces a comprehensive benchmark for predictive uncertainty quantification in BEV segmentation, evaluating multiple uncertainty quantification methods across three popular datasets with three representative network architectures. Our study focuses on the effectiveness of quantified uncertainty in detecting misclassified and out-of-distribution (OOD) pixels while also improving model calibration. Through empirical analysis, we uncover challenges in existing uncertainty quantification methods and demonstrate the potential of evidential deep learning techniques, which capture both aleatoric and epistemic uncertainty. To address these challenges, we propose a novel loss function, Uncertainty-Focal-Cross-Entropy (UFCE), specifically designed for highly imbalanced data, along with a simple uncertainty-scaling regularization term that improves both uncertainty quantification and model calibration for BEV segmentation.",
      "authors": [
        "Linlin Yu",
        "Bowen Yang",
        "Tianhao Wang",
        "Kangshuo Li",
        "Feng Chen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=k3y0oyK7sn",
      "cdate": 1727496413679,
      "mdate": 1740899363828,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833858"
    },
    {
      "id": "vyflgpwfJW",
      "title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
      "abstract": "Can the rapid advances in code generation, function calling, and data analysis using large language models (LLMs) help automate the search and verification of hypotheses purely from a set of provided datasets? To evaluate this question, we present DiscoveryBench, the first comprehensive benchmark that formalizes the multi-step process of data-driven discovery. The benchmark is designed to systematically assess current model capabilities in discovery tasks and provide a useful resource for improving them. Our benchmark contains 264 tasks collected across 6 diverse domains, such as sociology and engineering, by manually deriving discovery workflows from published papers to approximate the real-world challenges faced by researchers, where each task is defined by a dataset, its metadata, and a discovery goal in natural language. We additionally provide 903 synthetic tasks to conduct controlled evaluations on data-driven workflows that are not covered in the manually collected split. Furthermore, our structured formalism of data-driven discovery enables a facet-based evaluation that provides useful insights into different failure modes. We evaluate several popular LLM-based reasoning frameworks using both open and closed LLMs as baselines on DiscoveryBench and find that even the best system scores only 25%. Our benchmark, thus, illustrates the challenges in autonomous data-driven discovery and serves as a valuable resource for the community to make progress.",
      "authors": [
        "Bodhisattwa Prasad Majumder",
        "Harshit Surana",
        "Dhruv Agarwal",
        "Bhavana Dalvi Mishra",
        "Abhijeetsingh Meena",
        "Aryan Prakhar",
        "Tirth Vora",
        "Tushar Khot",
        "Ashish Sabharwal",
        "Peter Clark"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=vyflgpwfJW",
      "cdate": 1727496380162,
      "mdate": 1741014825402,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833863"
    },
    {
      "id": "XgH1wfHSX8",
      "title": "Competition Dynamics Shape Algorithmic Phases of In-Context Learning",
      "abstract": "In-Context Learning (ICL) has significantly expanded the general-purpose nature of large language models, allowing them to adapt to novel tasks using merely the inputted context. This has motivated a series of papers that analyze tractable synthetic domains and postulate precise mechanisms that may underlie ICL. However, the use of relatively distinct setups that often lack a sequence modeling nature to them makes it unclear how general the reported insights from such studies are. Motivated by this, we propose a synthetic sequence modeling task that involves learning to simulate a finite mixture of Markov chains. As we show, models trained on this task reproduce most well-known results on ICL, hence offering a unified setting for studying the concept. Building on this setup, we demonstrate we can explain a model’s behavior by decomposing it into four broad algorithms that combine a fuzzy retrieval vs. inference approach with either unigram or bigram statistics of the context. These algorithms engage in a competitive dynamics to dominate model behavior, with the precise experimental conditions dictating which algorithm ends up superseding others: e.g., we find merely varying context size or amount of training yields (at times sharp) transitions between which algorithm dictates the model behavior, revealing a mechanism that explains the transient nature of ICL. In this sense, we argue ICL is best thought of as a mixture of different algorithms, each with its own peculiarities, instead of a monolithic capability. This also implies that making general claims about ICL that hold universally across all settings may be infeasible.",
      "authors": [
        "Core Francisco Park",
        "Ekdeep Singh Lubana",
        "Hidenori Tanaka"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=XgH1wfHSX8",
      "cdate": 1727496207680,
      "mdate": 1746164639321,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833868"
    },
    {
      "id": "MF7ljU8xcf",
      "title": "Compute-Optimal LLMs Provably Generalize Better with Scale",
      "abstract": "Why do larger language models generalize better? To explore this question, we develop generalization bounds on the pretraining objective of large language models (LLMs) in the compute-optimal regime, as described by the Chinchilla scaling laws. We introduce a novel, fully empirical Freedman-type martingale concentration inequality that tightens existing bounds by accounting for the variance of the loss function. The generalization bound can be broken into three contributions: the number of parameters per token, the loss variance, and the quantization error at a fixed bitrate. As language models are scaled up, the number of parameters per data point stays constant; however, both the loss variance and the quantization error decrease, implying that larger models should have \\emph{smaller} generalization gaps. We examine why larger models tend to be more quantizable from an information theoretic perspective, showing that the rate at which they can integrate new information grows slower than their capacity on the compute optimal frontier. From these findings we produce a scaling law for the generalization gap, showing that our bounds decrease in a predictable way.",
      "authors": [
        "Marc Anton Finzi",
        "Sanyam Kapoor",
        "Diego Granziol",
        "Anming Gu",
        "Christopher De Sa",
        "J Zico Kolter",
        "Andrew Gordon Wilson"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=MF7ljU8xcf",
      "cdate": 1727496132078,
      "mdate": 1742089643224,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833873"
    },
    {
      "id": "f3QR9TEERH",
      "title": "Safety-Prioritizing Curricula for Constrained Reinforcement Learning",
      "abstract": "Curriculum learning aims to accelerate reinforcement learning (RL) by generating curricula, i.e., sequences of tasks of increasing difficulty. \nAlthough existing curriculum generation approaches provide benefits in sample efficiency, they overlook safety-critical settings where an RL agent must adhere to safety constraints.\nThus, these approaches may generate tasks that cause RL agents to violate safety constraints during training and behave suboptimally after. \nWe develop a safe curriculum generation approach (SCG) that aligns the objectives of constrained RL and curriculum learning: improving safety during training and boosting sample efficiency.\nSCG generates sequences of tasks where the RL agent can be safe and performant by initially generating tasks with minimum safety violations over high-reward ones.\nWe empirically show that compared to the state-of-the-art curriculum learning approaches and their naively modified safe versions, SCG achieves optimal performance and the lowest amount of constraint violations during training.",
      "authors": [
        "Cevahir Koprulu",
        "Thiago D. Simão",
        "Nils Jansen",
        "ufuk topcu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=f3QR9TEERH",
      "cdate": 1727495954306,
      "mdate": 1739866419305,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833878"
    },
    {
      "id": "10JOlFIPjt",
      "title": "In vivo cell-type and brain region classification via multimodal contrastive learning",
      "abstract": "Current electrophysiological approaches can track the activity of many neurons, yet it is usually unknown which cell-types or brain areas are being recorded without further molecular or histological analysis. Developing accurate and scalable algorithms for identifying the cell-type and brain region of recorded neurons is thus crucial for improving our understanding of neural computation. In this work, we develop a multimodal contrastive learning approach for neural data that can be fine-tuned for different downstream tasks, including inference of cell-type and brain location. We utilize multimodal contrastive learning to jointly embed the activity autocorrelations and extracellular waveforms of individual neurons. We demonstrate that our embedding approach, Neuronal Embeddings via MultimOdal Contrastive Learning (NEMO), paired with supervised fine-tuning, achieves state-of-the-art cell-type classification for two opto-tagged datasets and brain region classification for the public International Brain Laboratory Brain-wide Map dataset. Our method represents a promising step towards accurate cell-type and brain region classification from electrophysiological recordings.",
      "authors": [
        "Han Yu",
        "Hanrui Lyu",
        "YiXun Xu",
        "Charlie Windolf",
        "Eric Kenji Lee",
        "Fan Yang",
        "Andrew M Shelton",
        "Olivier Winter",
        "International Brain Laboratory",
        "Eva L Dyer",
        "Chandramouli Chandrasekaran",
        "Nicholas A. Steinmetz",
        "Liam Paninski",
        "Cole Lincoln Hurwitz"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=10JOlFIPjt",
      "cdate": 1727495929127,
      "mdate": 1743556534985,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.833883"
    },
    {
      "id": "hxUMQ4fic3",
      "title": "Neural Stochastic Differential Equations for Uncertainty-Aware Offline RL",
      "abstract": "Offline model-based reinforcement learning (RL) offers a principled approach to using a learned dynamics model as a simulator to optimize a control policy. \nDespite the near-optimal performance of existing approaches on benchmarks with high-quality datasets, most struggle on datasets with low state-action space coverage or suboptimal demonstrations.\nWe develop a novel offline model-based RL approach that particularly shines in low-quality data regimes while maintaining competitive performance on high-quality datasets.\nNeural Stochastic Differential Equations for Uncertainty-aware, Offline RL (NUNO) learns a dynamics model as neural stochastic differential equations (SDE), \nwhere its drift term can leverage prior physics knowledge as inductive bias.\nIn parallel, its diffusion term provides distance-aware estimates of model uncertainty by matching the dynamics' underlying stochasticity near the training data regime while providing high but bounded estimates beyond it.\nTo address the so-called model exploitation problem in offline model-based RL, NUNO builds on existing studies by penalizing and adaptively truncating neural SDE's rollouts according to uncertainty estimates.\nOur empirical results in D4RL and NeoRL MuJoCo benchmarks evidence that NUNO outperforms state-of-the-art methods in low-quality datasets by up to 93% while matching or surpassing their performance by up to 55% in some high-quality counterparts.",
      "authors": [
        "Cevahir Koprulu",
        "Franck Djeumou",
        "ufuk topcu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=hxUMQ4fic3",
      "cdate": 1727495799218,
      "mdate": 1744577153180,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833888"
    },
    {
      "id": "9KxnxWOBA5",
      "title": "Towards Optimal Multi-draft Speculative Decoding",
      "abstract": "Large Language Models (LLMs) have become an indispensable part of natural language processing tasks. However, autoregressive sampling has become an efficiency bottleneck. Multi-Draft Speculative Decoding (MDSD) is a recent approach where, when generating each token, a small draft model generates multiple drafts, and the target LLM verifies them in parallel, ensuring that the final output conforms to the target model distribution. The two main design choices in MDSD are the draft sampling method and the verification algorithm. For a fixed draft sampling method, the optimal acceptance rate is a solution to an optimal transport problem, but the complexity of this problem makes it difficult to solve for the optimal acceptance rate and measure the gap between existing verification algorithms and the theoretical upper bound. This paper discusses the dual of the optimal transport problem, providing a way to efficiently compute the optimal acceptance rate. For the first time, we measure the theoretical upper bound of MDSD efficiency for vocabulary sizes in the thousands and quantify the gap between existing verification algorithms and this bound. We also compare different draft sampling methods based on their optimal acceptance rates. Our results show that the draft sampling method strongly influences the optimal acceptance rate, with sampling without replacement outperforming sampling with replacement. Additionally, existing verification algorithms do not reach the theoretical upper bound for both without replacement and with replacement sampling. Our findings suggest that carefully designed draft sampling methods can potentially improve the optimal acceptance rate and enable the development of verification algorithms that closely match the theoretical upper bound.",
      "authors": [
        "Zhengmian Hu",
        "Tong Zheng",
        "Vignesh Viswanathan",
        "Ziyi Chen",
        "Ryan A. Rossi",
        "Yihan Wu",
        "Dinesh Manocha",
        "Heng Huang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=9KxnxWOBA5",
      "cdate": 1727495545684,
      "mdate": 1744045355975,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833896"
    },
    {
      "id": "mqNKiEB6pd",
      "title": "Towards Federated RLHF with Aggregated Client Preference for LLMs",
      "abstract": "Reinforcement learning with human feedback (RLHF) fine-tunes a pretrained large language model (LLM) using user preference data, enabling it to generate content aligned with human preferences. However, due to privacy concerns, users may be reluctant to share sensitive preference data. To address this, we propose utilizing Federated Learning (FL) techniques, allowing large-scale preference collection from diverse real-world users without requiring them to transmit data to a central server. Our federated RLHF methods (i.e., FedBis and FedBiscuit) encode each client’s preferences into binary selectors and aggregate them to capture common preferences. In particular, FedBiscuit overcomes key challenges, such as preference heterogeneity and reward hacking, through innovative solutions like grouping clients with similar preferences to reduce heterogeneity and using multiple binary selectors to enhance LLM output quality. To evaluate the performance of the proposed methods, we establish the first federated RLHF benchmark with a heterogeneous human preference dataset. Experimental results show that by integrating the LLM with aggregated client preferences, FedBis and FedBiscuit significantly enhance the professionalism and readability of the generated content.",
      "authors": [
        "Feijie Wu",
        "Xiaoze Liu",
        "Haoyu Wang",
        "Xingchen Wang",
        "Lu Su",
        "Jing Gao"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=mqNKiEB6pd",
      "cdate": 1727495136896,
      "mdate": 1740934144158,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833902"
    },
    {
      "id": "w3iM4WLuvy",
      "title": "Overcoming Slow Decision Frequencies in Continuous Control: Model-Based Sequence Reinforcement Learning for Model-Free Control",
      "abstract": "Reinforcement learning (RL) is rapidly reaching and surpassing human-level control capabilities. However, state-of-the-art RL algorithms often require timesteps and reaction times significantly faster than human capabilities, which is impractical in real-world settings and typically necessitates specialized hardware. We introduce Sequence Reinforcement Learning (SRL), an RL algorithm designed to produce a sequence of actions for a given input state, enabling effective control at lower decision frequencies. SRL addresses the challenges of learning action sequences by employing both a model and an actor-critic architecture operating at different temporal scales. We propose a \"temporal recall\" mechanism, where the critic uses the model to estimate intermediate states between primitive actions, providing a learning signal for each individual action within the sequence. Once training is complete, the actor can generate action sequences independently of the model, achieving model-free control at a slower frequency. We evaluate SRL on a suite of continuous control tasks, demonstrating that it achieves performance comparable to state-of-the-art algorithms while significantly reducing actor sample complexity. To better assess performance across varying decision frequencies, we introduce the Frequency-Averaged Score (FAS) metric. Our results show that SRL significantly outperforms traditional RL algorithms in terms of FAS, making it particularly suitable for applications requiring variable decision frequencies. Furthermore, we compare SRL with model-based online planning, showing that SRL achieves comparable FAS while leveraging the same model during training that online planners use for planning.",
      "authors": [
        "Devdhar Patel",
        "Hava T Siegelmann"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=w3iM4WLuvy",
      "cdate": 1727495021125,
      "mdate": 1747261379003,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833907"
    },
    {
      "id": "vJgJSrYPe1",
      "title": "Logic-Logit: A Logic-Based Approach to Choice Modeling",
      "abstract": "In this study, we propose a novel rule-based interpretable choice model, {\\bf Logic-Logit}, designed to effectively learn and explain human choices. Choice models have been widely applied across various domains—such as commercial demand forecasting, recommendation systems, and consumer behavior analysis—typically categorized as parametric, nonparametric, or deep network-based. While recent innovations have favored neural network approaches for their computational power, these flexible models often involve large parameter sets and lack interpretability, limiting their effectiveness in contexts where transparency is essential.\n\nPrevious empirical evidence shows that individuals usually use {\\it heuristic decision rules} to form their consideration sets, from which they then choose. These rules are often represented as {\\it disjunctions of conjunctions} (i.e., OR-of-ANDs). These rules-driven, {\\it consider-then-choose} decision processes enable people to quickly screen numerous alternatives while reducing cognitive and search costs. Motivated by this insight, our approach leverages logic rules to elucidate human choices, providing a fresh perspective on preference modeling. We introduce a unique combination of column generation techniques and the Frank-Wolfe algorithm to facilitate efficient rule extraction for preference modeling—a process recognized as NP-hard. Our empirical evaluation, conducted on both synthetic datasets and real-world data from commercial and healthcare domains, demonstrates that Logic-Logit significantly outperforms baseline models in terms of interpretability and accuracy.",
      "authors": [
        "Shuhan Zhang",
        "Wendi Ren",
        "Shuang Li"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=vJgJSrYPe1",
      "cdate": 1727494912726,
      "mdate": 1741774511828,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833912"
    },
    {
      "id": "wUbum0nd9N",
      "title": "On Calibration of LLM-based Guard Models for Reliable Content Moderation",
      "abstract": "Large language models (LLMs) pose significant risks due to the potential for generating harmful content or users attempting to evade guardrails. Existing studies have developed LLM-based guard models designed to moderate the input and output of threat LLMs, ensuring adherence to safety policies by blocking content that violates these protocols upon deployment. However, limited attention has been given to the reliability and calibration of such guard models. In this work, we empirically conduct comprehensive investigations of confidence calibration for 9 existing LLM-based guard models on 12 benchmarks in both user input and model output classification. Our findings reveal that current LLM-based guard models tend to 1) produce overconfident predictions, 2) exhibit significant miscalibration when subjected to jailbreak attacks, and 3) demonstrate limited robustness to the outputs generated by different types of response models. Additionally, we assess the effectiveness of post-hoc calibration methods to mitigate miscalibration. We demonstrate the efficacy of temperature scaling and, for the first time, highlight the benefits of contextual calibration for confidence calibration of guard models, particularly in the absence of validation sets. Our analysis and experiments underscore the limitations of current LLM-based guard models and provide valuable insights for the future development of well-calibrated guard models toward more reliable content moderation. We also advocate for incorporating reliability evaluation of confidence calibration when releasing future LLM-based guard models.",
      "authors": [
        "Hongfu Liu",
        "Hengguan Huang",
        "Xiangming Gu",
        "Hao Wang",
        "Ye Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=wUbum0nd9N",
      "cdate": 1727494764294,
      "mdate": 1740890359711,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833918"
    },
    {
      "id": "1H90Gb9rJ9",
      "title": "Optimizing Neural Network Representations of Boolean Networks",
      "abstract": "Neural networks are known to be universal computers for Boolean functions. Recent advancements in hardware have significantly reduced matrix multiplication times, making neural network simulation both fast and efficient. Consequently, functions defined by complex Boolean networks are increasingly viable candidates for simulation through their neural network representation. Prior research has introduced a general method for deriving neural network representations of Boolean networks. However, the resulting neural networks are often suboptimal in terms of the number of neurons and connections, leading to slower simulation performance. Optimizing them while preserving functional equivalence --lossless optimization-- is an NP-hard problem, and current methods only provide lossy solutions. In this paper, we present a deterministic algorithm to optimize such neural networks in terms of neurons and connections while preserving functional equivalence. Moreover, to accelerate the compression of the neural network, we introduce an objective-aware algorithm that exploits representations that are shared among subproblems of the overall optimization. We demonstrate experimentally that we are able to reduce connections and neurons by up to 70% and 60%, respectively, in comparison to state-of-the-art. We also find that our objective-aware algorithm results in consistent speedups in optimization time, achieving up to 34.3x and 5.9x speedup relative to naive and caching solutions, respectively. Our methods are of practical relevance to applications such as high-throughput circuit simulation and placing neurosymbolic systems on the same hardware architecture.",
      "authors": [
        "Joshua Russell",
        "Ignacio Gavier",
        "Devdhar Patel",
        "Edward Rietman",
        "Hava T Siegelmann"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=1H90Gb9rJ9",
      "cdate": 1727494748681,
      "mdate": 1740680018599,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833923"
    },
    {
      "id": "9juyeCqL0u",
      "title": "Causal Order: The Key to Leveraging Imperfect Experts in Causal Inference",
      "abstract": "Large Language Models (LLMs) have recently been used as experts to infer causal graphs, often by repeatedly applying a pairwise prompt that asks about the causal relationship of each variable pair. However, such experts, including human domain experts, cannot distinguish between direct and indirect effects given a pairwise prompt. Therefore, instead of the graph, we propose that causal order be used as a more stable output interface for utilizing expert knowledge. When querying a perfect expert with a pairwise prompt, we show that the inferred graph can have significant errors whereas the causal order is always correct. In practice, however, LLMs are imperfect experts and we find that pairwise prompts lead to multiple cycles and do not yield a valid order. Hence, we propose a prompting strategy that introduces an auxiliary variable for every variable pair and instructs the LLM to avoid cycles within this triplet. We show, both theoretically and empirically, that such a triplet prompt leads to fewer cycles than the pairwise prompt. Across multiple real-world graphs, the triplet prompt yields a more accurate order using both LLMs and human annotators as experts. By querying the expert with different auxiliary variables for the same variable pair, it also increases robustness---triplet method with much smaller models such as Phi-3 and Llama-3 8B outperforms a pairwise prompt with GPT-4. For practical usage, we show how the estimated causal order from the triplet method  can be used to reduce error in downstream discovery and effect inference tasks.",
      "authors": [
        "Aniket Vashishtha",
        "Abbavaram Gowtham Reddy",
        "Abhinav Kumar",
        "Saketh Bachu",
        "Vineeth N. Balasubramanian",
        "Amit Sharma"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=9juyeCqL0u",
      "cdate": 1727494741389,
      "mdate": 1741549982274,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833929"
    },
    {
      "id": "rTCJ29pkuA",
      "title": "Reasoning of Large Language Models over Knowledge Graphs with Super-Relations",
      "abstract": "While large language models (LLMs) have made significant progress in processing and reasoning over knowledge graphs, current methods suffer from a high non-retrieval rate. This limitation reduces the accuracy of answering questions based on these graphs. Our analysis reveals that the combination of greedy search and forward reasoning is a major contributor to this issue. To overcome these challenges, we introduce the concept of super-relations, which enables both forward and backward reasoning by summarizing and connecting various relational paths within the graph. This holistic approach not only expands the search space, but also significantly improves retrieval efficiency. In this paper, we propose the ReKnoS framework, which aims to Reason over Knowledge Graphs with Super-Relations. Our framework’s key advantages include the inclusion of multiple relation paths through super-relations, enhanced forward and backward reasoning capabilities, and increased efficiency in querying LLMs. These enhancements collectively lead to a substantial improvement in the successful retrieval rate and overall reasoning performance. We conduct extensive experiments on a variety of datasets to evaluate ReKnoS, and the results demonstrate the superior performance of ReKnoS over existing state-of-the-art baselines, with an average accuracy gain of 2.92% across nine real-world datasets.",
      "authors": [
        "Song Wang",
        "Junhong Lin",
        "Xiaojie Guo",
        "Julian Shun",
        "Jundong Li",
        "Yada Zhu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=rTCJ29pkuA",
      "cdate": 1727494479340,
      "mdate": 1742111800557,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833934"
    },
    {
      "id": "zg3ec1TdAP",
      "title": "Context Clues: Evaluating Long Context Models for Clinical Prediction Tasks on EHR Data",
      "abstract": "Foundation Models (FMs) trained on Electronic Health Records (EHRs) have achieved state-of-the-art results on numerous clinical prediction tasks. However, prior EHR FMs typically have context windows of $<$1k tokens, which prevents them from modeling full patient EHRs which can exceed 10k's of events. For making clinical predictions, both model performance and robustness to the unique properties of EHR data are crucial. Recent advancements in subquadratic long-context architectures (e.g. Mamba) offer a promising solution. However, their application to EHR data has not been well-studied. We address this gap by presenting the first systematic evaluation of the effect of context length on modeling EHR data. We find that longer context models improve predictive performance -- our Mamba-based model surpasses the prior state-of-the-art on 9/14 tasks on the EHRSHOT prediction benchmark. Additionally, we measure robustness to three unique, previously underexplored properties of EHR data: (1) the prevalence of ``copy-forwarded\" diagnoses which create artificial token repetition in EHR sequences; (2) the irregular time intervals between EHR events which can lead to a wide range of timespans within a context window; and (3) the natural increase in disease complexity over time which makes later tokens in the EHR harder to predict than earlier ones. Stratifying our EHRSHOT results, we find that higher levels of each property correlate negatively with model performance (e.g., a 14% higher Brier loss between the least and most irregular patients), but that longer context models are more robust to more extreme levels of these properties. Our work highlights the potential for using long-context architectures to model EHR data, and offers a case study on how to identify and quantify new challenges in modeling sequential data motivated by domains outside of natural language. We release all of our model checkpoints and code.",
      "authors": [
        "Michael Wornow",
        "Suhana Bedi",
        "Miguel Angel Fuentes Hernandez",
        "Ethan Steinberg",
        "Jason Alan Fries",
        "Christopher Re",
        "Sanmi Koyejo",
        "Nigam Shah"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=zg3ec1TdAP",
      "cdate": 1727494114066,
      "mdate": 1739482923736,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833940"
    },
    {
      "id": "NIkfix2eDQ",
      "title": "Plastic Learning with Deep Fourier Features",
      "abstract": "Deep neural networks can struggle to learn continually in the face of non-stationarity, a\n  phenomenon known as loss of plasticity.\n  In this paper, we identify underlying principles that lead to plastic algorithms.\n  We provide theoretical results showing that linear function approximation, as well as a special case of deep linear networks, do not suffer from loss of plasticity.\n  We then propose deep Fourier features, which are the concatenation of a sine and cosine in every layer, and we show that this combination provides a dynamic balance between the trainability obtained through linearity and the effectiveness obtained through the nonlinearity of neural networks.\n  Deep networks composed entirely of deep Fourier features are highly trainable and sustain their trainability over the course of learning.\n  Our empirical results show that continual learning performance can be improved by replacing ReLU activations with deep Fourier features combined with regularization.\n  These results hold for different continual learning scenarios (e.g., label noise, class incremental learning, pixel permutations)\n  on all major supervised learning datasets used for continual learning research, such as CIFAR10, CIFAR100, and tiny-ImageNet.",
      "authors": [
        "Alex Lewandowski",
        "Dale Schuurmans",
        "Marlos C. Machado"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=NIkfix2eDQ",
      "cdate": 1727493938521,
      "mdate": 1744084402369,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.833946"
    },
    {
      "id": "FAfxvdv1Dy",
      "title": "STAFF: Speculative Coreset Selection for Task-Specific Fine-tuning",
      "abstract": "Task-specific fine-tuning is essential for the deployment of large language models (LLMs), but it requires significant computational resources and time. Existing solutions have proposed coreset selection methods to improve data efficiency and reduce model training overhead, but they still have limitations: ❶ Overlooking valuable samples at high pruning rates, which degrades the coreset’s performance.\n❷ Requiring high time overhead during coreset selection to fine-tune and evaluate the target LLM. In this paper, we introduce STAFF, a speculative coreset selection method. STAFF leverages a small model from the same family as the target LLM to efficiently estimate data scores and then verifies the scores on the target LLM to accurately identify and allocate more selection budget to important regions while maintaining coverage of easy regions. We evaluate STAFF on three LLMs and three downstream tasks and show that STAFF improves the performance of SOTA methods by up to 54.3% and reduces selection overhead by up to 70.5% at different pruning rates. Furthermore, we observe that the coreset selected by STAFF at low pruning rates (i.e., 20%) can even obtain better fine-tuning performance than the full dataset.",
      "authors": [
        "Xiaoyu Zhang",
        "Juan Zhai",
        "Shiqing Ma",
        "Chao Shen",
        "Tianlin Li",
        "Weipeng Jiang",
        "Yang Liu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=FAfxvdv1Dy",
      "cdate": 1727493903640,
      "mdate": 1739359372659,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833956"
    },
    {
      "id": "zcTLpIfj9u",
      "title": "Time-to-Event Pretraining for 3D Medical Imaging",
      "abstract": "With the rise of medical foundation models and the growing availability of imaging data, scalable pretraining techniques offer a promising way to identify imaging biomarkers predictive of future disease risk. While current self-supervised methods for 3D medical imaging models capture local structural features like organ morphology, they fail to link pixel biomarkers with long-term health outcomes due to a missing context problem. Current approaches lack the temporal context necessary to identify biomarkers correlated with disease progression, as they rely on supervision derived only from images and concurrent text descriptions. To address this, we introduce time-to-event pretraining, a pretraining framework for 3D medical imaging models that leverages large-scale temporal supervision from paired, longitudinal electronic health records (EHRs). Using a dataset of 18,945 CT scans (4.2 million 2D images) and time-to-event distributions across thousands of EHR-derived tasks, our method improves outcome prediction, achieving an average AUROC increase of 23.7% and a 29.4% gain in Harrell’s C-index across 8 benchmark tasks. Importantly, these gains are achieved without sacrificing diagnostic classification performance. This study lays the foundation for integrating longitudinal EHR and 3D imaging data to advance clinical risk prediction.",
      "authors": [
        "Zepeng Frazier Huo",
        "Jason Alan Fries",
        "Alejandro Lozano",
        "Jeya Maria Jose Valanarasu",
        "Ethan Steinberg",
        "Louis Blankemeier",
        "Akshay S Chaudhari",
        "Curtis Langlotz",
        "Nigam Shah"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=zcTLpIfj9u",
      "cdate": 1727493779580,
      "mdate": 1742369779497,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833965"
    },
    {
      "id": "fCi4o83Mfs",
      "title": "TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models",
      "abstract": "Existing benchmarks often highlight the remarkable performance achieved by state-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal context for video understanding.\nHowever, *how well do the models truly perform visual temporal reasoning?*\nOur study of existing benchmarks shows that this capability of MFMs is likely overestimated as many questions can be solved by using a single, few, or out-of-order frames. \nTo systematically examine current visual temporal reasoning tasks, we propose three principles with corresponding metrics:\n(1) *Multi-Frame Gain*,\n(2) *Frame Order Sensitivity*,\nand (3) *Frame Information Disparity*.\nFollowing these principles, we introduce **TOMATO**, **T**emp**O**ral Reasoning **M**ultimod**A**l Evalua**T**i**O**n, a novel benchmark crafted to rigorously assess MFMs' temporal reasoning capabilities in video understanding.\nTOMATO comprises 1,484 carefully curated, *human-annotated* questions spanning *six* tasks (i.e. *action count, direction, rotation, shape & trend, velocity & frequency, and visual cues*), applied to 1,417 videos, including 805 self-recorded and -generated videos, that encompass human-centric, real-world, and simulated scenarios. \nOur comprehensive evaluation reveals a human-model performance gap of 57.3% with the best-performing model.\nMoreover, our in-depth analysis uncovers more fundamental limitations beyond this gap in current MFMs. While they can accurately recognize events in isolated frames, they fail to interpret these frames as a continuous sequence.\nWe believe TOMATO will serve as a crucial testbed for evaluating the next-generation MFMs and as a call to the community to develop AI systems capable of comprehending the human world dynamics through the video modality.",
      "authors": [
        "Ziyao Shangguan",
        "Chuhan Li",
        "Yuxuan Ding",
        "Yanan Zheng",
        "Yilun Zhao",
        "Tesca Fitzgerald",
        "Arman Cohan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=fCi4o83Mfs",
      "cdate": 1727493628832,
      "mdate": 1743477663218,
      "matched_keywords": [
        "foundation model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.833971"
    },
    {
      "id": "yzloNYH3QN",
      "title": "Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers",
      "abstract": "Information retrieval (IR) systems have played a vital role in modern digital life and have cemented their continued usefulness in this new era of generative AI via retrieval-augmented generation. With strong language processing capabilities and remarkable versatility, large language models (LLMs) have become popular choices for zero-shot re-ranking in IR systems. So far, LLM-based re-ranking methods rely on strong generative capabilities, which restricts their use to either specialized or powerful proprietary models. Given these restrictions, we ask: is autoregressive generation necessary and optimal for LLMs to perform re-ranking? We hypothesize that there are abundant signals relevant to re-ranking within LLMs that might not be used to their full potential via generation. To more  directly leverage such signals, we propose in-context re-ranking (ICR), a novel method  that leverages the change in attention pattern caused by the search query for accurate and efficient re-ranking. We assume that more relevant documents should receive more attention weights when an LLM is processing the query tokens, and leverage such signals for re-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration method using a content-free query. Due to the absence of generation, ICR only requires two ($O(1)$) forward passes to re-rank $N$ documents, making it substantially more efficient than generative re-ranking methods that require at least $O(N)$ forward passes. Our novel design also enables ICR to be applied to any LLM without specialized training while guaranteeing a well-formed ranking. Extensive experiments with two popular open-weight LLMs on standard single-hop and multi-hop information retrieval benchmarks show that ICR outperforms RankGPT while cutting the latency by more than 60% in practice. Through detailed analyses, we show that ICR's performance is specially strong on tasks that require more complex re-ranking signals, such as handling contextualization and contradiction between the query and passages, as well as information integration across multiple passages. Our findings call for further exploration on novel ways of utilizing open-weight LLMs beyond text generation.",
      "authors": [
        "Shijie Chen",
        "Bernal Jimenez Gutierrez",
        "Yu Su"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=yzloNYH3QN",
      "cdate": 1727493602932,
      "mdate": 1740772041218,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833977"
    },
    {
      "id": "snocoXIQXz",
      "title": "Towards Learning High-Precision Least Squares Algorithms with Sequence Models",
      "abstract": "This paper investigates whether sequence models can learn to perform numerical algorithms, e.g. gradient descent, on the fundamental problem of least squares. Our goal is to inherit two properties of standard algorithms from numerical analysis: (1) machine precision, i.e. we want to obtain solutions that are accurate to near floating point error, and (2) numerical generality, i.e. we want them to apply broadly across problem instances. We find that prior approaches using Transformers fail to meet these criteria, and identify limitations present in existing architectures and training procedures. First, we show that softmax Transformers struggle to perform high-precision multiplications, which prevents them from precisely learning numerical algorithms. Second, we identify an alternate class of architectures, comprised entirely of polynomials, that can efficiently represent high-precision gradient descent iterates. Finally, we investigate precision bottlenecks during training and address them via a high-precision training recipe that reduces stochastic gradient noise. Our recipe enables us to train two polynomial architectures, gated convolutions and linear attention, to perform gradient descent iterates on least squares problems. For the first time, we demonstrate the ability to train to near machine precision. Applied iteratively, our models obtain $100,000\\times$ lower MSE than standard Transformers trained end-to-end and they incur a $10,000\\times$ smaller generalization gap on out-of-distribution problems. We make progress towards end-to-end learning of numerical algorithms for least squares.",
      "authors": [
        "Jerry Weihong Liu",
        "Jessica Grogan",
        "Owen M Dugan",
        "Ashish Rao",
        "Simran Arora",
        "Atri Rudra",
        "Christopher Re"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=snocoXIQXz",
      "cdate": 1727493470412,
      "mdate": 1740869360497,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.833982"
    },
    {
      "id": "AWg2tkbydO",
      "title": "Learning Efficient Positional Encodings with Graph Neural Networks",
      "abstract": "Positional encodings (PEs) are essential for effective graph representation learning because they provide position awareness in inherently position-agnostic transformer architectures and increase the expressive capacity of Graph Neural Networks (GNNs). However, designing powerful and efficient PEs for graphs poses significant challenges due to the absence of canonical node ordering and the scale of the graph. In this work, we identify four key properties that graph PEs should satisfy: stability, expressive power, scalability, and genericness. We find that existing eigenvector-based PE methods often fall short of jointly satisfying these criteria. To address this gap, we introduce PEARL, a novel framework of learnable PEs for graphs. Our primary insight is that message-passing GNNs function as nonlinear mappings of eigenvectors, enabling the design of GNN architectures for generating powerful and efficient PEs. A crucial challenge lies in initializing node features in a manner that is both expressive and permutation equivariant. We tackle this by initializing GNNs with random node inputs or standard basis vectors, thereby unlocking the expressive power of message-passing operations, while employing statistical pooling functions to maintain permutation equivariance. Our analysis demonstrates that PEARL approximates equivariant functions of eigenvectors with linear complexity, while rigorously establishing its stability and high expressive power. Experimental evaluations show that PEARL outperforms lightweight versions of eigenvector-based PEs and achieves comparable performance to full eigenvector-based PEs, but with one or two orders of magnitude lower complexity. Our code is available at https://github.com/ehejin/Pearl-PE.",
      "authors": [
        "Charilaos Kanatsoulis",
        "Evelyn Choi",
        "Stefanie Jegelka",
        "Jure Leskovec",
        "Alejandro Ribeiro"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=AWg2tkbydO",
      "cdate": 1727493445482,
      "mdate": 1743538918732,
      "matched_keywords": [
        "transformer",
        "neural network",
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833988"
    },
    {
      "id": "HNOo4UNPBF",
      "title": "Scale-Aware Contrastive Reverse Distillation for Unsupervised Medical Anomaly Detection",
      "abstract": "Unsupervised anomaly detection using deep learning has garnered significant research attention due to its broad applicability, particularly in medical imaging where labeled anomalous data are scarce. While earlier approaches leverage generative models like autoencoders and generative adversarial networks (GANs), they often fall short due to overgeneralization. Recent methods explore various strategies, including memory banks, normalizing flows, self-supervised learning, and knowledge distillation, to enhance discrimination. Among these, knowledge distillation, particularly reverse distillation, has shown promise. Following this paradigm, we propose a novel scale-aware contrastive reverse distillation model that addresses two key limitations of existing reverse distillation methods: insufficient feature discriminability and inability to handle anomaly scale variations. Specifically, we introduce a contrastive student-teacher learning approach to derive more discriminative representations by generating and exploring out-of-normal distributions. Further, we design a scale adaptation mechanism to softly weight contrastive distillation losses at different scales to account for the scale variation issue. Extensive experiments on benchmark datasets demonstrate state-of-the-art performance, validating the efficacy of the proposed method. The code will be made publicly available.",
      "authors": [
        "Chunlei Li",
        "Yilei Shi",
        "Jingliang Hu",
        "Xiao Xiang Zhu",
        "Lichao Mou"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=HNOo4UNPBF",
      "cdate": 1727493022331,
      "mdate": 1740890359465,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.833994"
    },
    {
      "id": "gLa96FlWwn",
      "title": "Scalable Influence and Fact Tracing for Large Language Model Pretraining",
      "abstract": "Training data attribution (TDA) methods aim to attribute model outputs back to specific training examples, and the application of these methods to large language model (LLM) outputs could significantly advance model transparency and data curation. However, it has been challenging to date to apply these methods to the full scale of LLM pretraining. In this paper, we refine existing gradient-based methods to work effectively at scale, allowing us to retrieve influential examples for an 8B-parameter language model from a pretraining corpus of over 160B tokens with no need for subsampling or pre-filtering. Our method combines several techniques, including optimizer state correction, a task-specific Hessian approximation, and normalized encodings, which we find to be critical for performance at scale. In quantitative evaluations on a fact tracing task, our method performs best at identifying examples that influence model predictions, but classical, model-agnostic retrieval methods such as BM25 still perform better at finding passages which explicitly contain relevant facts. These results demonstrate a misalignment between factual *attribution* and causal *influence*. With increasing model size and training tokens, we find that influence more closely aligns with factual attribution. Finally, we examine different types of examples identified as influential by our method, finding that while many directly entail a particular fact, others support the same output by reinforcing priors on relation types, common entities, and names. We release our prompt set and model outputs, along with a web-based visualization tool to explore influential examples for factual predictions, commonsense reasoning, arithmetic, and open-ended generation for an 8B-parameter LLM.",
      "authors": [
        "Tyler A. Chang",
        "Dheeraj Rajagopal",
        "Tolga Bolukbasi",
        "Lucas Dixon",
        "Ian Tenney"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=gLa96FlWwn",
      "cdate": 1727492975559,
      "mdate": 1740169600080,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.833999"
    },
    {
      "id": "BLWaTeucYX",
      "title": "Generating CAD Code with Vision-Language Models for 3D Designs",
      "abstract": "Generative AI has transformed the fields of Design and Manufacturing by providing\nefficient and automated methods for generating and modifying 3D objects. One\napproach involves using Large Language Models (LLMs) to generate Computer-\nAided Design (CAD) scripting code, which can then be executed to render a 3D\nobject; however, the resulting 3D object may not meet the specified requirements.\nTesting the correctness of CAD generated code is challenging due to the complexity\nand structure of 3D objects (e.g., shapes, surfaces, and dimensions) that are not\nfeasible in code. In this paper, we introduce CADCodeVerify, a novel approach to\niteratively verify and improve 3D objects generated from CAD code. Our approach\nworks by producing ameliorative feedback by prompting a Vision-Language Model\n(VLM) to generate and answer a set of validation questions to verify the generated\nobject and prompt the VLM to correct deviations. To evaluate CADCodeVerify, we\nintroduce, CADPrompt, the first benchmark for CAD code generation, consisting of\n200 natural language prompts paired with expert-annotated scripting code for 3D\nobjects to benchmark progress. Our findings show that CADCodeVerify improves\nVLM performance by providing visual feedback, enhancing the structure of the 3D\nobjects, and increasing the success rate of the compiled program. When applied to\nGPT-4, CADCodeVerify achieved a 7.30% reduction in Point Cloud distance and a\n5.0% improvement in success rate compared to prior work.",
      "authors": [
        "Kamel Alrashedy",
        "Pradyumna Tambwekar",
        "Zulfiqar Haider Zaidi",
        "Megan Langwasser",
        "Wei Xu",
        "Matthew Gombolay"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=BLWaTeucYX",
      "cdate": 1727492886440,
      "mdate": 1740717030181,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834005"
    },
    {
      "id": "U49N5V51rU",
      "title": "A Formal Framework for Understanding Length Generalization in Transformers",
      "abstract": "A major challenge for transformers is generalizing to sequences longer than those observed during training. While previous works have empirically shown that transformers can either succeed or fail at length generalization depending on the task, theoretical understanding of this phenomenon remains limited. In this work, we introduce a rigorous theoretical framework to analyze length generalization in causal transformers with learnable absolute positional encodings. In particular, we characterize those functions that are identifiable in the limit from sufficiently long inputs with absolute positional encodings under an idealized inference scheme using a norm-based regularizer. This enables us to prove the possibility of length generalization for a rich family of problems. We experimentally validate the theory as a predictor of success and failure of length generalization across a range of algorithmic and formal language tasks. Our theory not only explains a broad set of empirical observations but also opens the way to provably predicting length generalization capabilities in transformers.",
      "authors": [
        "Xinting Huang",
        "Andy Yang",
        "Satwik Bhattamishra",
        "Yash Sarrof",
        "Andreas Krebs",
        "Hattie Zhou",
        "Preetum Nakkiran",
        "Michael Hahn"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=U49N5V51rU",
      "cdate": 1727492878334,
      "mdate": 1746004671351,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834011"
    },
    {
      "id": "xzSUdw6s76",
      "title": "PALMBENCH: A COMPREHENSIVE BENCHMARK OF COMPRESSED LARGE LANGUAGE MODELS ON MOBILE PLATFORMS",
      "abstract": "Deploying large language models (LLMs) locally on mobile devices is advantageous in scenarios where transmitting data to remote cloud servers is either undesirable due to privacy concerns or impractical due to network connection. Recent advancements have facilitated the local deployment of LLMs. However, local deployment also presents challenges, particularly in balancing quality (generative performance), latency, and throughput within the hardware constraints of mobile devices. In this paper, we introduce our lightweight, all-in-one automated benchmarking framework that allows users to evaluate LLMs on mobile devices. We provide a comprehensive benchmark of various popular LLMs with different quantization configurations (both weights and activations) across multiple mobile platforms with varying hardware capabilities. Unlike traditional benchmarks that assess full-scale models on high-end GPU clusters, we focus on evaluating resource efficiency (memory and power consumption) and harmful output for compressed models on mobile devices. Our key observations include: i) differences in energy efficiency and throughput across mobile platforms; ii) the impact of quantization on memory usage, GPU execution time, and power consumption; and iii) accuracy and performance degradation of quantized models compared to their non-quantized counterparts; and iv) the frequency of hallucinations and toxic content generated by compressed LLMs on\nmobile devices.",
      "authors": [
        "Yilong Li",
        "Jingyu Liu",
        "Hao Zhang",
        "M Badri Narayanan",
        "Utkarsh Sharma",
        "Shuai Zhang",
        "Yijing Zeng",
        "Jayaram Raghuram",
        "Suman Banerjee"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=xzSUdw6s76",
      "cdate": 1727492825914,
      "mdate": 1743326160597,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834019"
    },
    {
      "id": "JBXO05r4AV",
      "title": "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation",
      "abstract": "Recent advances in long-context large language models (LLMs) have led to the emerging paradigm of many-shot in-context learning (ICL), where it is observed that scaling many more demonstrating examples beyond the conventional few-shot setup in the context can lead to performance benefits. However, despite its promise, it is unclear what aspects dominate the benefits and whether simply scaling to more examples is the most effective way of improving many-shot ICL. In this work, we first provide an analysis on the factors driving many-shot ICL, and we find that 1) many-shot performance can still be attributed to often a few disproportionately influential examples and 2) identifying such influential examples (\"optimize\") and using them as demonstrations to regenerate new examples (\"generate\") can lead to further improvements. Inspired by the findings, we propose BRIDGE, an algorithm that alternates between the optimize step with Bayesian optimization to discover the influential sets of examples and the generate step to reuse this set to expand the reasoning paths of the examples back to the many-shot regime automatically. On Gemini, Claude, and Mistral LLMs of different sizes, we show BRIDGE led to significant improvements across a diverse set of tasks including symbolic reasoning, numerical reasoning and code generation.",
      "authors": [
        "Xingchen Wan",
        "Han Zhou",
        "Ruoxi Sun",
        "Sercan O Arik"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=JBXO05r4AV",
      "cdate": 1727492720212,
      "mdate": 1740890359384,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834024"
    },
    {
      "id": "ed7zI29lRF",
      "title": "Learning Neural Networks with Distribution Shift: Efficiently Certifiable Guarantees",
      "abstract": "We give the first provably efficient algorithms for learning neural networks with respect to distribution shift. We work in the Testable Learning with Distribution Shift  framework (TDS learning) of Klivans et al. (2024), where the learner receives labeled examples from a training distribution and unlabeled examples from a test distribution and must either output a hypothesis with low test error or reject if distribution shift is detected.  No assumptions are made on the test distribution. \n\nAll prior work in TDS learning focuses on classification, while here we must handle the setting of nonconvex regression. Our results apply to real-valued networks with arbitrary Lipschitz activations and work whenever the training distribution has strictly sub-exponential tails. For training distributions that are bounded and hypercontractive, we give a fully polynomial-time algorithm for TDS learning one hidden-layer networks with sigmoid activations. We achieve this by importing classical kernel methods into the TDS framework using data-dependent feature maps and a type of kernel matrix that couples samples from both train and test distributions.",
      "authors": [
        "Gautam Chandrasekaran",
        "Adam Klivans",
        "Lin Lin Lee",
        "Konstantinos Stavropoulos"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=ed7zI29lRF",
      "cdate": 1727492629436,
      "mdate": 1743119959369,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834030"
    },
    {
      "id": "q2Lnyegkr8",
      "title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
      "abstract": "An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings.  Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer's superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet.  We also introduce a \"Pro\" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer.\nOur code is available at [`https://github.com/zhixuan-lin/forgetting-transformer`](https://github.com/zhixuan-lin/forgetting-transformer).",
      "authors": [
        "Zhixuan Lin",
        "Evgenii Nikishin",
        "Xu He",
        "Aaron Courville"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=q2Lnyegkr8",
      "cdate": 1727492569125,
      "mdate": 1743449132929,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834035"
    },
    {
      "id": "R4q3cY3kQf",
      "title": "MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization",
      "abstract": "Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards. Most common RL algorithms use undirected exploration, i.e., select random sequences of actions.\nExploration can also be directed using intrinsic rewards, such as curiosity or model epistemic uncertainty. However, effectively balancing task and intrinsic rewards is challenging and often task-dependent. In this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and extrinsic exploration. MaxInfoRL steers exploration towards informative transitions, by maximizing intrinsic rewards such as the information gain about the underlying task. \nWhen combined with Boltzmann exploration, this approach naturally trades off maximization of the value function with that of the entropy over states, rewards, and actions. We show that our approach achieves sublinear regret in the simplified setting of multi-armed bandits. We then apply this general formulation to a variety of off-policy model-free RL methods for continuous state-action spaces, yielding novel algorithms that achieve superior performance across hard exploration problems and complex scenarios such as visual control tasks.",
      "authors": [
        "Bhavya Sukhija",
        "Stelian Coros",
        "Andreas Krause",
        "Pieter Abbeel",
        "Carmelo Sferrazza"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=R4q3cY3kQf",
      "cdate": 1727492527484,
      "mdate": 1743071594994,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834041"
    },
    {
      "id": "wLmJIs1uqG",
      "title": "LancBiO: Dynamic Lanczos-aided Bilevel Optimization via Krylov Subspace",
      "abstract": "Bilevel optimization, with broad applications in machine learning, has an intricate hierarchical structure. Gradient-based methods have emerged as a common approach to large-scale bilevel problems. However, the computation of the hyper-gradient, which involves a Hessian inverse vector product, confines the efficiency and is regarded as a bottleneck. To circumvent the inverse, we construct a sequence of low-dimensional approximate Krylov subspaces with the aid of the Lanczos process. As a result, the constructed subspace is able to dynamically and incrementally approximate the Hessian inverse vector product with less effort and thus leads to a favorable estimate of the hyper-gradient. Moreover, we propose a provable subspace-based framework for bilevel problems where one central step is to solve a small-size tridiagonal linear system. To the best of our knowledge, this is the first time that subspace techniques are incorporated into bilevel optimization. This successful trial not only enjoys $\\mathcal{O}(\\epsilon^{-1})$ convergence rate but also demonstrates efficiency in a synthetic problem and two deep learning tasks.",
      "authors": [
        "Yan Yang",
        "Bin Gao",
        "Ya-xiang Yuan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=wLmJIs1uqG",
      "cdate": 1727492483257,
      "mdate": 1743322212455,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834046"
    },
    {
      "id": "aKJr5NnN8U",
      "title": "Toward Understanding In-context vs. In-weight Learning",
      "abstract": "It has recently been demonstrated empirically that in-context learning emerges in transformers when certain distributional properties are present in the training data, but this ability can also diminish upon further training. We provide a new theoretical understanding of these phenomena by identifying simplified distributional properties that give rise to the emergence and eventual disappearance of in-context learning. We do so by first analyzing a simplified model that uses a gating mechanism to choose between an in-weight and an in-context predictor. Through a combination of a generalization error and regret analysis we identify conditions where in-context and in-weight learning emerge. These theoretical findings are then corroborated experimentally by comparing the behaviour of a full transformer on the simplified distributions to that of the stylized model, demonstrating aligned results. We then extend the study to a full large language model, showing how fine-tuning on various collections of natural language prompts can elicit similar in-context and in-weight learning behaviour.",
      "authors": [
        "Bryan Chan",
        "Xinyi Chen",
        "András György",
        "Dale Schuurmans"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=aKJr5NnN8U",
      "cdate": 1727492473044,
      "mdate": 1745680118089,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834051"
    },
    {
      "id": "is4nCVkSFA",
      "title": "Can Neural Networks Achieve Optimal Computational-statistical Tradeoff? An Analysis on Single-Index Model",
      "abstract": "In this work, we tackle the following question: Can neural networks trained with gradient-based methods achieve the optimal statistical-computational tradeoff in learning Gaussian single-index models? \nPrior research has shown that any polynomial-time algorithm under the statistical query (SQ) framework requires $\\Omega(d^{s^\\star/2}\\lor d)$ samples, where $s^\\star$ is the generative exponent representing the intrinsic difficulty of learning the underlying model.\nHowever, it remains unknown whether neural networks can achieve this sample complexity. \nInspired by prior techniques such as label transformation and landscape smoothing for learning single-index models, we propose a unified gradient-based algorithm for training a two-layer neural network in polynomial time.\nOur method is adaptable to a variety of loss and activation functions, covering a broad class of existing approaches.\nWe show that our algorithm learns a feature representation that strongly aligns with the unknown signal $\\theta^\\star$, with sample complexity $\\tilde O (d^{s^\\star/2} \\lor d)$, matching the SQ lower bound up to a polylogarithmic factor for all generative exponents $s^\\star\\geq 1$.\nFurthermore, we extend our approach to the setting where $\\theta^\\star$ is $k$-sparse for $k = o(\\sqrt{d})$ by introducing a novel weight perturbation technique that leverages the sparsity structure. \nWe derive a corresponding SQ lower bound \nof order $\\tilde\\Omega(k^{s^\\star})$, matched by our method up to a polylogarithmic factor.\nOur framework, especially the weight perturbation technique, is of independent interest, and suggests potential gradient-based solutions to other problems such as sparse tensor PCA.",
      "authors": [
        "Siyu Chen",
        "Beining Wu",
        "Miao Lu",
        "Zhuoran Yang",
        "Tianhao Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=is4nCVkSFA",
      "cdate": 1727492429971,
      "mdate": 1740791443662,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834056"
    },
    {
      "id": "8sSqNntaMr",
      "title": "RouteLLM: Learning to Route LLMs from Preference Data",
      "abstract": "Large language models (LLMs) excel at a wide range of tasks, but choosing the right model often involves balancing performance and cost. Powerful models offer better results but are expensive, while smaller models are more cost-effective but less capable. To address this trade-off, we introduce a training framework for learning efficient router models that dynamically select between a stronger and weaker LLM during inference. Our framework leverages human preference data and employs data augmentation techniques to enhance performance. Evaluations on public benchmarks show that our approach can reduce costs by over 2 times without sacrificing response quality. Moreover, our routers exhibit strong generalization capabilities, maintaining performance even when routing between LLMs not included in training. This highlights the potential of our framework to deliver cost-effective, high-performance LLM solutions.",
      "authors": [
        "Isaac Ong",
        "Amjad Almahairi",
        "Vincent Wu",
        "Wei-Lin Chiang",
        "Tianhao Wu",
        "Joseph E. Gonzalez",
        "M Waleed Kadous",
        "Ion Stoica"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=8sSqNntaMr",
      "cdate": 1727492428126,
      "mdate": 1740299879943,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834062"
    },
    {
      "id": "6fDjUoEQvm",
      "title": "HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks",
      "abstract": "Mechanistic interpretability has made great strides in identifying neural network features (e.g., directions in hidden activation space) that mediate concepts (e.g., *the birth year of a Nobel laureate*) and enable predictable manipulation. Distributed alignment search (DAS) leverages supervision from counterfactual data to learn concept features within hidden states, but DAS assumes we can afford to conduct a brute force search over potential feature locations. To address this, we present HyperDAS, a transformer-based hypernetwork architecture that (1) automatically locates the token-positions of the residual stream that a concept is realized in and (2) learns features of those residual stream vectors for the concept. In experiments with Llama3-8B, HyperDAS achieves state-of-the-art performance on the RAVEL benchmark for disentangling concepts in hidden states. In addition, we review the design decisions we made to mitigate the concern that HyperDAS (like all powerful interpretabilty methods) might inject new information into the target model rather than faithfully interpreting it.",
      "authors": [
        "Jiuding Sun",
        "Jing Huang",
        "Sidharth Baskaran",
        "Karel D'Oosterlinck",
        "Christopher Potts",
        "Michael Sklar",
        "Atticus Geiger"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=6fDjUoEQvm",
      "cdate": 1727491679049,
      "mdate": 1740807787814,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834067"
    },
    {
      "id": "AjXkRZIvjB",
      "title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models",
      "abstract": "Recent advancements in Large Language Models (LLMs) have sparked interest in their mathematical reasoning capabilities. While performance on the widely popular GSM8K benchmark has improved, questions remain about whether reported evaluation metrics are reliable, and reasoning abilities of LLMs have advanced. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models. \nOur findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and demonstrate that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is due to the fact that current LLMs are not capable of genuine logical reasoning; instead, they attempt to replicate the reasoning steps observed in their training data. When we add a single clause that appears relevant to the question, we observe significant performance drops (up to 65%) across all state-of-the-art models, even though the added clause does not contribute to the reasoning chain needed to reach the final answer. Overall, our work provides a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.",
      "authors": [
        "Seyed Iman Mirzadeh",
        "Keivan Alizadeh",
        "Hooman Shahrokhi",
        "Oncel Tuzel",
        "Samy Bengio",
        "Mehrdad Farajtabar"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=AjXkRZIvjB",
      "cdate": 1727491525272,
      "mdate": 1740885836622,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834076"
    },
    {
      "id": "rJ5g8ueQaI",
      "title": "SEMDICE: Off-policy State Entropy Maximization via Stationary Distribution Correction Estimation",
      "abstract": "In the unsupervised pre-training for reinforcement learning, the agent aims to learn a prior policy for downstream tasks without relying on task-specific reward functions. We focus on state entropy maximization (SEM), where the goal is to learn a policy that maximizes the entropy of the state's stationary distribution. In this paper, we introduce SEMDICE, a principled off-policy algorithm that computes an SEM policy from an arbitrary off-policy dataset, which optimizes the policy directly within the space of stationary distributions. SEMDICE computes a single, stationary Markov state-entropy-maximizing policy from an arbitrary off-policy dataset. Experimental results demonstrate that SEMDICE outperforms baseline algorithms in maximizing state entropy while achieving the best adaptation efficiency for downstream tasks among SEM-based unsupervised RL pre-training methods.",
      "authors": [
        "Jongmin Lee",
        "Meiqi Sun",
        "Pieter Abbeel"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=rJ5g8ueQaI",
      "cdate": 1727491353281,
      "mdate": 1747241205348,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834084"
    },
    {
      "id": "Tz8Li6G2xU",
      "title": "Discovering Group Structures via Unitary Representation Learning",
      "abstract": "Discovering group structures within data poses a fundamental challenge across diverse scientific domains. A key obstacle is the non-differentiable nature of group axioms, hindering their integration into deep learning frameworks. To address this, we introduce a novel differentiable approach leveraging the representation theory of finite groups. Our method features a unique network architecture that models interactions between group elements via matrix multiplication of their representations, along with a regularizer promoting the unitarity of these representations. The interplay between the network architecture and the unitarity condition implicitly encourages the emergence of valid group structures. Evaluations demonstrate our method's ability to accurately recover group operations and their unitary representations from partial observations, achieving significant improvements in sample efficiency and a $\\times 1000$ speedup over the state of the art. This work lays the foundation for a promising new paradigm in automated algebraic structure discovery, with potential applications across various domains, including automatic symmetry discovery for geometric deep learning.",
      "authors": [
        "Dongsung Huh"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Tz8Li6G2xU",
      "cdate": 1727491288023,
      "mdate": 1744656647772,
      "matched_keywords": [
        "deep learning",
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834089"
    },
    {
      "id": "Antib6Uovh",
      "title": "A Theoretical Analysis of Self-Supervised Learning for Vision Transformers",
      "abstract": "Self-supervised learning has become a cornerstone in computer vision, primarily divided into reconstruction-based methods like masked autoencoders (MAE) and discriminative methods such as contrastive learning (CL).  Recent empirical observations reveal that MAE and CL capture different types of representations: CL tends to focus on global patterns, while MAE adeptly captures  **both global and subtle local** information simultaneously. Despite a flurry of recent empirical investigations to shed light on this difference, theoretical understanding remains limited, especially on the dominant architecture **vision  transformers** (ViTs). In this paper, to provide rigorous insights, we model the visual data distribution by considering two types of spatial features: dominant global features and comparatively minuscule local features, and study the impact of imbalance among these features.  We analyze the training dynamics of one-layer softmax-based ViTs on both MAE and CL objectives using gradient descent. Our analysis shows that as the degree of feature imbalance varies, ViTs trained with the MAE objective effectively learn both global and local features to achieve near-optimal reconstruction, while the CL-trained ViTs favor predominantly global features, even under mild imbalance. These results provide a theoretical explanation for distinct behaviors of MAE and CL observed in empirical studies.",
      "authors": [
        "Yu Huang",
        "Zixin Wen",
        "Yuejie Chi",
        "Yingbin Liang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Antib6Uovh",
      "cdate": 1727491069708,
      "mdate": 1739261785742,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834094"
    },
    {
      "id": "BksqWM8737",
      "title": "ProteinBench: A Holistic Evaluation of Protein Foundation Models",
      "abstract": "Recent years have witnessed a surge in the development of protein foundation models, significantly improving performance in protein prediction and generative tasks ranging from 3D structure prediction and protein design to conformational dynamics. However, the capabilities and limitations associated with these models remain poorly understood due to the absence of a unified evaluation framework. To fill this gap, we introduce ProteinBench, a holistic evaluation framework designed to enhance the transparency of protein foundation models. Our approach consists of three key components: (i) A taxonomic classification of tasks that broadly encompass the main challenges in the protein domain, based on the relationships between different protein modalities; (ii) A multi-metric evaluation approach that assesses performance across four key dimensions: quality, novelty, diversity, and robustness; and (iii) In-depth analyses from various user objectives, providing a holistic view of model performance. Our comprehensive evaluation of protein foundation models reveals several key findings that shed light on their current capabilities and limitations. To promote transparency and facilitate further research, we release the evaluation dataset, code, and a public leaderboard publicly for further analysis and a general modular toolkit. We intend for ProteinBench to be a living benchmark for establishing a standardized, in-depth evaluation framework for protein foundation models, driving their development and application while fostering collaboration within the field.",
      "authors": [
        "Fei YE",
        "Zaixiang Zheng",
        "Dongyu Xue",
        "Yuning Shen",
        "Lihao Wang",
        "Yiming Ma",
        "Yan Wang",
        "Xinyou Wang",
        "Xiangxin Zhou",
        "Quanquan Gu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=BksqWM8737",
      "cdate": 1727491007569,
      "mdate": 1740737626812,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834100"
    },
    {
      "id": "hwnObmOTrV",
      "title": "Modeling Complex System Dynamics with Flow Matching Across Time and Conditions",
      "abstract": "Modeling the dynamics of complex real-world systems from temporal snapshot data is crucial for understanding phenomena such as gene regulation, climate change, and financial market fluctuations. Researchers have recently proposed a few methods based either on the Schroedinger Bridge or Flow Matching to tackle this problem, but these approaches remain limited in their ability to effectively combine data from multiple time points and different experimental settings. This integration is essential in real-world scenarios where observations from certain combinations of time points and experimental conditions are missing, either because of experimental costs or sensory failure. To address this challenge, we propose a novel method named Multi-Marginal Flow Matching (MMFM). MMFM first constructs a flow using smooth spline-based interpolation across time points and conditions and regresses it with a neural network using the classifier-free guided Flow Matching framework. This framework allows for the sharing of contextual information about the dynamics across multiple trajectories. We demonstrate the effectiveness of our method on both synthetic and real-world datasets, including a recent single-cell genomics data set with around a hundred chemical perturbations across time points. Our results show that MMFM significantly outperforms existing methods at imputing data at missing time points.",
      "authors": [
        "Martin Rohbeck",
        "Edward De Brouwer",
        "Charlotte Bunne",
        "Jan-Christian Huetter",
        "Anne Biton",
        "Kelvin Y. Chen",
        "Aviv Regev",
        "Romain Lopez"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=hwnObmOTrV",
      "cdate": 1727490833025,
      "mdate": 1741122454761,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834105"
    },
    {
      "id": "VCbqXtS5YY",
      "title": "Joint Reward and Policy Learning with Demonstrations and Human Feedback Improves Alignment",
      "abstract": "Aligning to human preferences and/or intentions is an important requirement for contemporary foundation models. To ensure alignment, popular approaches such as reinforcement learning with human feedback (RLHF) break down the task into three stages: (i) a model is computed with supervised fine-tuning (SFT) based upon large demonstrations data, (ii) a reward model (RM) is estimated based upon human feedback data, and (iii) reinforcement learning (RL) is used to further refine the SFT model by optimizing the estimated reward model.  Demonstrations and human feedback data reflect human user preferences in different ways. As a result, the reward model estimate obtained from only human feedback data is likely not as accurate as a reward model estimate obtained from both demonstration and human feedback data. A policy model that optimizes the reward model estimate obtained from both demonstration and human feedback data will likely exhibit better alignment performance. We introduce a tractable algorithm for finding the reward and policy models and provide a finite-time performance guarantee. Additionally, we demonstrate the efficiency of the proposed solution with extensive experiments including alignment problems in LLMs and robotic control problems in MuJoCo. We observe that the proposed solutions outperform the existing alignment algorithm by large margins, especially when the amounts of demonstration and preference data are unbalanced.",
      "authors": [
        "Chenliang Li",
        "Siliang Zeng",
        "Zeyi Liao",
        "Jiaxiang Li",
        "Dongyeop Kang",
        "Alfredo Garcia",
        "Mingyi Hong"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=VCbqXtS5YY",
      "cdate": 1727490809056,
      "mdate": 1740774956207,
      "matched_keywords": [
        "foundation model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834109"
    },
    {
      "id": "ISqx8giekS",
      "title": "LeanQuant: Accurate and Scalable Large Language Model Quantization with Loss-error-aware Grid",
      "abstract": "Large language models (LLMs) have shown immense potential across various domains, but their high memory requirements and inference costs remain critical challenges for deployment. Post-training quantization (PTQ) has emerged as a promising technique to reduce memory requirements and decoding latency. However, recent accurate quantization methods often depend on specialized computations or custom data formats to achieve better model quality, which limits their compatibility with popular frameworks, as they require dedicated inference kernels tailored to specific hardware and software platforms, hindering wider adoption. Furthermore, many competitive methods have high resource requirements and computational overhead for quantizing models, making it challenging to scale them to hundreds of billions of parameters. In response to these challenges, we propose LeanQuant (Loss-error-aware network Quantization), a novel quantization method that is accurate, versatile, and scalable. In the existing popular iterative loss-error-based quantization framework, we identify a critical limitation in prior methods: the min-max affine quantization grid fails to preserve model quality due to outliers in inverse Hessian diagonals. To overcome this fundamental issue, we propose learning loss-error-aware grids, instead of using non-adaptive min-max affine grids. Our approach not only produces quantized models that are more accurate but also generalizes to a wider range of quantization types, including affine and non-uniform quantization, enhancing compatibility with more frameworks. Extensive experiments with recent LLMs demonstrate that LeanQuant is highly accurate, comparing favorably against competitive baselines in model quality, and scalable, achieving very accurate quantization of Llama-3.1 405B, one of the largest open-source LLMs to date, using two Quadro RTX 8000-48GB GPUs in 21 hours. Our code is available at https://github.com/LeanModels/LeanQuant.",
      "authors": [
        "Tianyi Zhang",
        "Anshumali Shrivastava"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=ISqx8giekS",
      "cdate": 1727490568402,
      "mdate": 1742009512815,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834115"
    },
    {
      "id": "gfI9v7AbFg",
      "title": "Strategist: Self-improvement of LLM Decision Making via Bi-Level Tree Search",
      "abstract": "Traditional reinforcement learning and planning require a lot of data and training to develop effective strategies. On the other hand, large language models (LLMs) can generalize well and perform tasks without prior training but struggle with complex planning and decision-making. We introduce **STRATEGIST**, a new approach that combines the strengths of both methods. It uses LLMs to generate and update high-level strategies in text form, while a Monte Carlo Tree Search (MCTS) algorithm refines and executes them. STRATEGIST is a general framework that optimizes strategies through self-play simulations without requiring any training data. We test STRATEGIST in competitive, multi-turn games with partial information, such as **Game of Pure Strategy (GOPS)** and **The Resistance: Avalon**, a multi-agent hidden-identity discussion game. Our results show that STRATEGIST-based agents outperform traditional reinforcement learning models, other LLM-based methods, and existing LLM agents while achieving performance levels comparable to human players.",
      "authors": [
        "Jonathan Light",
        "Min Cai",
        "Weiqin Chen",
        "Guanzhi Wang",
        "Xiusi Chen",
        "Wei Cheng",
        "Yisong Yue",
        "Ziniu Hu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=gfI9v7AbFg",
      "cdate": 1727490414172,
      "mdate": 1740897480151,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834119"
    },
    {
      "id": "eNbA8Fqir4",
      "title": "DataMan: Data Manager for Pre-training Large Language Models",
      "abstract": "The performance emergence of large language models (LLMs) driven by data scaling laws makes the selection of pre-training data increasingly important. \nHowever, existing methods rely on limited heuristics and human intuition, lacking comprehensive and clear guidelines.\nTo address this, we are inspired by *``reverse thinking''* -- prompting LLMs to self-identify which criteria benefit its performance. \nAs its pre-training capabilities are related to perplexity (PPL), we derive 14 quality criteria from the causes of text perplexity anomalies and introduce 15 common application domains to support domain mixing.\nIn this paper, we train a **Data** **Man**ager (**DataMan**) to learn quality ratings and domain recognition from pointwise rating, and use it to annotate a 447B token pre-training corpus with 14 quality ratings and domain type.\nOur experiments validate our approach, using DataMan to select 30B tokens to train a 1.3B-parameter language model, demonstrating significant improvements in in-context learning (ICL), perplexity, and instruction-following ability over the state-of-the-art baseline. \nThe best-performing model, based on the *Overall Score l=5* surpasses a model trained with 50% more data using uniform sampling. \nWe continue pre-training with high-rated, domain-specific data annotated by DataMan to enhance domain-specific ICL performance and thus verify DataMan's domain mixing ability. \nOur findings emphasize the importance of quality ranking, the complementary nature of quality criteria, and their low correlation with perplexity, analyzing misalignment between PPL and ICL performance. \nWe also thoroughly analyzed our pre-training dataset, examining its composition, the distribution of quality ratings, and the original document sources.",
      "authors": [
        "Ru Peng",
        "Kexin Yang",
        "Yawen Zeng",
        "Junyang Lin",
        "Dayiheng Liu",
        "Junbo Zhao"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=eNbA8Fqir4",
      "cdate": 1727490386040,
      "mdate": 1741773233608,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834125"
    },
    {
      "id": "Y2Dh8rWwlb",
      "title": "EditRoom: LLM-parameterized Graph Diffusion for Composable 3D Room Layout Editing",
      "abstract": "Given the steep learning curve of professional 3D software and the time-\nconsuming process of managing large 3D assets, language-guided 3D scene editing has significant potential in fields such as virtual reality, augmented reality, and\ngaming. However, recent approaches to language-guided 3D scene editing either\nrequire manual interventions or focus only on appearance modifications without\nsupporting comprehensive scene layout changes. In response, we propose EditRoom, a unified framework capable of executing a variety of layout edits through\nnatural language commands, without requiring manual intervention. Specifically,\nEditRoom leverages Large Language Models (LLMs) for command planning and\ngenerates target scenes using a diffusion-based method, enabling six types of edits: rotate, translate, scale, replace, add, and remove. To address\nthe lack of data for language-guided 3D scene editing, we have developed an automatic pipeline to augment existing 3D scene synthesis datasets and introduced\nEditRoom-DB, a large-scale dataset with 83k editing pairs, for training and evaluation. Our experiments demonstrate that our approach consistently outperforms\nother baselines across all metrics, indicating higher accuracy and coherence in\nlanguage-guided scene layout editing.",
      "authors": [
        "Kaizhi Zheng",
        "Xiaotong Chen",
        "Xuehai He",
        "Jing Gu",
        "Linjie Li",
        "Zhengyuan Yang",
        "Kevin Lin",
        "Jianfeng Wang",
        "Lijuan Wang",
        "Xin Eric Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Y2Dh8rWwlb",
      "cdate": 1727490299760,
      "mdate": 1740713714956,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834133"
    },
    {
      "id": "7GKbQ1WT1C",
      "title": "Prompting Fairness: Integrating Causality to Debias Large Language Models",
      "abstract": "Large language models (LLMs), despite their remarkable capabilities, are susceptible to generating biased and discriminatory responses. As LLMs increasingly influence high-stakes decision-making (e.g., hiring and healthcare), mitigating these biases becomes critical. In this work, we propose a causality-guided debiasing framework to tackle social biases, aiming to reduce the objectionable dependence between LLMs' decisions and the social information in the input. Our framework introduces a novel perspective to identify how social information can affect an LLM's decision through different causal pathways. Leveraging these causal insights, we outline principled prompting strategies that regulate these pathways through selection mechanisms. This framework not only unifies existing prompting-based debiasing techniques, but also opens up new directions for reducing bias by encouraging the model to prioritize fact-based reasoning over reliance on biased social cues. We validate our framework through extensive experiments on real-world datasets across multiple domains, demonstrating its effectiveness in debiasing LLM decisions, even with only black-box access to the model.",
      "authors": [
        "Jingling Li",
        "Zeyu Tang",
        "Xiaoyu Liu",
        "Peter Spirtes",
        "Kun Zhang",
        "Liu Leqi",
        "Yang Liu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=7GKbQ1WT1C",
      "cdate": 1727490092866,
      "mdate": 1740864125400,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834139"
    },
    {
      "id": "fWRBheSJth",
      "title": "GReaTer: Gradients Over Reasoning Makes Smaller Language Models Strong Prompt Optimizers",
      "abstract": "The effectiveness of large language models (LLMs) is closely tied to the design of prompts, making prompt optimization essential for enhancing their performance across a wide range of tasks. Although recent advancements have focused on automating prompt engineering, many existing approaches rely exclusively on textual feedback, refining prompts based solely on inference errors identified by large, computationally expensive LLMs. Unfortunately, smaller models struggle to generate high-quality feedback, resulting in complete dependence on large LLM judgment. Moreover, these methods fail to leverage more direct and finer-grained information, such as gradients, due to operating purely in text space. To this end, we introduce, we introduce *GReaTer*, a novel prompt optimization technique that directly incorporates *gradient information over task-specific reasoning*. By utilizing task loss gradients, *GReaTer* enables self-optimization of prompts for smaller, lightweight language models (LM) without the need for costly closed-source LLMs, while maintaining reasonable prompt structures. This allows high-performance prompt optimization without dependence on massive LLMs, closing the gap between smaller models and the sophisticated reasoning often needed for prompt refinement. Extensive evaluations across diverse tasks demonstrate that \\ours consistently outperforms previous methods, even those reliant on powerful LLMs. Additionally, *GReaTer*-optimized prompts frequently exhibit better transferability and, in some cases, boost task performance to levels comparable to or surpassing those achieved by larger language models, highlighting the effectiveness of *\"gradient over reasoning\"*-based prompt optimization. Code of *GReaTer* is available at: https://github.com/psunlpgroup/GreaTer",
      "authors": [
        "Sarkar Snigdha Sarathi Das",
        "Ryo Kamoi",
        "Bo Pang",
        "Yusen Zhang",
        "Caiming Xiong",
        "Rui Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=fWRBheSJth",
      "cdate": 1727490050348,
      "mdate": 1740716178801,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834144"
    },
    {
      "id": "yu1vqQqKkx",
      "title": "LICO: Large Language Models for In-Context Molecular Optimization",
      "abstract": "Optimizing black-box functions is a fundamental problem in science and engineering. To solve this problem, many approaches learn a surrogate function that estimates the underlying objective from limited historical evaluations. Large Language Models (LLMs), with their strong pattern-matching capabilities via pretraining on vast amounts of data, stand out as a potential candidate for surrogate modeling. However, directly prompting a pretrained language model to produce predictions is not feasible in many scientific domains due to the scarcity of domain-specific data in the pretraining corpora and the challenges of articulating complex problems in natural language. In this work, we introduce LICO, a general-purpose model that extends arbitrary base LLMs for black-box optimization, with a particular application to the molecular domain. To achieve this, we equip the language model with a separate embedding layer and prediction layer, and train the model to perform in-context predictions on a diverse set of functions defined over the domain. Once trained, LICO can generalize to unseen molecule properties simply via in-context prompting. LICO performs competitively on PMO, a challenging molecular optimization benchmark comprising 23 objective functions, and achieves state-of-the-art performance on its low-budget version PMO-1K.",
      "authors": [
        "Tung Nguyen",
        "Aditya Grover"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=yu1vqQqKkx",
      "cdate": 1727489475342,
      "mdate": 1739521943171,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834149"
    },
    {
      "id": "BPgK5XW1Nb",
      "title": "Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment",
      "abstract": "Aligning large language models (LLMs) with human preferences becomes a key component to obtaining state-of-the-art performance, but it yields a huge cost to construct a large human-annotated preference dataset. To tackle this problem, we propose a new framework, Spread Preference Annotation with direct preference judgment (SPA), that boosts the alignment of LLMs using only a very small amount of human-annotated preference data.\nOur key idea is leveraging the human prior knowledge within the small (seed) data and progressively improving the alignment of LLM, by iteratively generating the responses and learning from them with the self-annotated preference data.\nTo be specific, we propose to derive the preference label from the logits of LLM to explicitly extract the model's inherent preference. \nCompared to the previous approaches using external reward models or implicit in-context learning, we observe that the proposed approach is significantly more effective.\nIn addition, we introduce a noise-aware preference learning algorithm to mitigate the risk of low quality within generated preference data.\nOur experimental results demonstrate that the proposed framework significantly boosts the alignment of LLMs.\nFor example, we achieve superior alignment performance on AlpacaEval 2.0 with only 3.3% of the ground-truth preference labels in the Ultrafeedback data compared to the cases using the entire data or state-of-the-art baselines.",
      "authors": [
        "Dongyoung Kim",
        "Kimin Lee",
        "Jinwoo Shin",
        "Jaehyung Kim"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=BPgK5XW1Nb",
      "cdate": 1727489436707,
      "mdate": 1740890359123,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834154"
    },
    {
      "id": "uuriavczkL",
      "title": "Counterfactual Realizability",
      "abstract": "It is commonly believed that, in a real-world environment, samples can only be drawn from observational and interventional distributions, corresponding to Layers 1 and 2 of the *Pearl Causal Hierarchy*. Layer 3, representing counterfactual distributions, is believed to be inaccessible by definition. However, Bareinboim, Forney, and Pearl (2015) introduced a procedure that allows an agent to sample directly from a counterfactual distribution, leaving open the question of what other counterfactual quantities can be estimated directly via physical experimentation. We resolve this by introducing a formal definition of realizability, the ability to draw samples from a distribution, and then developing a complete algorithm to determine whether an arbitrary counterfactual distribution is realizable given fundamental physical constraints, such as the inability to go back in time and subject the same unit to a different experimental condition. We illustrate the implications of this new framework for counterfactual data collection using motivating examples from causal fairness and causal reinforcement learning. While the baseline approach in these motivating settings typically follows an interventional or observational strategy, we show that a counterfactual strategy provably dominates both.",
      "authors": [
        "Arvind Raghavan",
        "Elias Bareinboim"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=uuriavczkL",
      "cdate": 1727489430364,
      "mdate": 1741971009848,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834160"
    },
    {
      "id": "8RCmNLeeXx",
      "title": "Forking Paths in Neural Text Generation",
      "abstract": "Estimating uncertainty in Large Language Models (LLMs) is important for properly evaluating LLMs, and ensuring safety for users. However, prior approaches to uncertainty estimation focus on the final answer in generated text, ignoring intermediate steps that might dramatically impact the outcome. We hypothesize that there exist key forking tokens, such that re-sampling the system at those specific tokens, but not others, leads to very different outcomes. To test this empirically, we develop a novel approach to representing uncertainty dynamics across individual tokens of text generation, and applying statistical models to test our hypothesis. Our approach is highly flexible: it can be applied to any dataset and any LLM, without fine tuning or accessing model weights. We use our method to analyze LLM responses on 7 different tasks across 4 domains,  spanning a wide range of typical use cases. We find many examples of forking tokens, including surprising ones such as a space character instead of a colon, suggesting that LLMs are often just a single token away from saying something very different.",
      "authors": [
        "Eric J Bigelow",
        "Ari Holtzman",
        "Hidenori Tanaka",
        "Tomer Ullman"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=8RCmNLeeXx",
      "cdate": 1727489425137,
      "mdate": 1740877376035,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834168"
    },
    {
      "id": "SuH5SdOXpe",
      "title": "Robustness Reprogramming for Representation Learning",
      "abstract": "This work tackles an intriguing and fundamental open challenge in representation learning: Given a well-trained deep learning model, can it be reprogrammed to enhance its robustness against adversarial or noisy input perturbations without altering its parameters?\nTo explore this, we revisit the core feature transformation mechanism in representation learning and propose a novel non-linear robust pattern matching technique as a robust alternative. Furthermore, we introduce three model reprogramming paradigms to offer flexible control of robustness under different efficiency requirements. Comprehensive experiments and ablation studies across diverse learning models ranging from basic linear model and MLPs to shallow and modern deep ConvNets demonstrate the effectiveness \nof our approaches.\nThis work not only opens a promising and orthogonal direction for improving adversarial defenses in deep learning beyond existing methods but also provides new insights into designing more resilient AI systems with robust statistics. \nOur implementation is available at https://github.com/chris-hzc/Robustness-Reprogramming.",
      "authors": [
        "Zhichao Hou",
        "MohamadAli Torkamani",
        "Hamid Krim",
        "Xiaorui Liu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=SuH5SdOXpe",
      "cdate": 1727488981489,
      "mdate": 1740807206845,
      "matched_keywords": [
        "deep learning",
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834173"
    },
    {
      "id": "3usdM1AuI3",
      "title": "BRAID: Input-driven Nonlinear Dynamical Modeling of Neural-Behavioral Data",
      "abstract": "Neural populations exhibit complex recurrent structures that drive behavior, while continuously receiving and integrating external inputs from sensory stimuli, upstream regions, and neurostimulation. However, neural populations are often modeled as autonomous dynamical systems, with little consideration given to the influence of external inputs that shape the population activity and behavioral outcomes. Here, we introduce BRAID, a deep learning framework that models nonlinear neural dynamics underlying behavior while explicitly incorporating any measured external inputs. Our method disentangles intrinsic recurrent neural population dynamics from the effects of inputs by including a forecasting objective within input-driven recurrent neural networks. BRAID further prioritizes the learning of intrinsic dynamics that are related to a behavior of interest by using a multi-stage optimization scheme. We validate BRAID with nonlinear simulations, showing that it can accurately learn the intrinsic dynamics shared between neural and behavioral modalities. We then apply BRAID to motor cortical activity recorded during a motor task and demonstrate that our method more accurately fits the neural-behavioral data by incorporating measured sensory stimuli into the model and improves the forecasting of neural-behavioral data compared with various baseline methods, whether input-driven or not.",
      "authors": [
        "Parsa Vahidi",
        "Omid G. Sani",
        "Maryam Shanechi"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=3usdM1AuI3",
      "cdate": 1727488967324,
      "mdate": 1740883809487,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834178"
    },
    {
      "id": "wNobG8bV5Q",
      "title": "LLM-based Typed Hyperresolution for Commonsense Reasoning with Knowledge Bases",
      "abstract": "Large language models (LLM) are being increasingly applied to tasks requiring commonsense reasoning. Despite their outstanding potential, the reasoning process of LLMs is prone to errors and hallucinations that hinder their applicability, especially in high-stakes scenarios. Several works have attempted to enhance commonsense reasoning performance of LLMs by (i) using prompting styles that elicit more accurate reasoning, (ii) utilizing the LLM as a semantic parser for a symbolic reasoner, or (iii) enforcing the LLM to simulate a logical inference rule.  However, all these solutions have critical limitations: they are unable to leverage the internal commonsense knowledge of the LLM in tandem with an axiomatic knowledge base, they lack a mechanism to reliably repair erroneous inference steps, and their application is restricted to small knowledge bases that fit the context limit of the LLM. In this work, we present LLM-based Typed Hyperresolution (LLM-TH), a logical commonsense reasoning framework that leverages \"theory resolution\", a concept from classical logical inference which enables integrating LLMs into the \"resolution\" inference rule, thus mitigating reasoning errors and hallucinations and enabling verification of the reasoning procedure. LLM-TH is also equipped with a mechanism for repairing erroneous inference steps supported by theoretical guarantees. Using \"Hyperresolution\" and \"Typed inference\" schemes, we show that LLM-TH can efficiently reason over large knowledge bases consisting of tens of thousands of rules with arbitrary predicate arities. Our experiments on three diverse language-based reasoning tasks—preference reasoning, multi-domain deductive reasoning, and geographical question answering—showcase that LLM-TH, using merely a BART 406M parameter NLI entailment model, significantly reduces reasoning errors compared to baselines using Llama3-70B, Gemini1.5-Flash, GPT-3.5-Turbo, and Mixtral-46.7B.",
      "authors": [
        "Armin Toroghi",
        "Ali Pesaranghader",
        "Tanmana Sadhu",
        "Scott Sanner"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=wNobG8bV5Q",
      "cdate": 1727488522652,
      "mdate": 1743517587740,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834183"
    },
    {
      "id": "r0pLGGcuY6",
      "title": "What's the Move? Hybrid Imitation Learning via Salient Points",
      "abstract": "While imitation learning (IL) offers a promising framework for teaching robots various behaviors, learning complex tasks remains challenging. Existing IL policies struggle to generalize effectively across visual and spatial variations even for simple tasks. In this work, we introduce **SPHINX**: **S**alient **P**oint-based **H**ybrid **I**mitatio**N** and e**X**ecution, a flexible IL policy that leverages multimodal observations (point clouds and wrist images), along with a hybrid action space of low-frequency, sparse waypoints and high-frequency, dense end effector movements. Given 3D point cloud observations, SPHINX learns to infer task-relevant points within a point cloud, or *salient points*, which support spatial generalization by focusing on semantically meaningful features. These salient points serve as anchor points to predict waypoints for long-range movement, such as reaching target poses in free-space. Once near a salient point, SPHINX learns to switch to predicting dense end-effector movements given close-up wrist images for precise phases of a task. By exploiting the strengths of different input modalities and action representations for different manipulation phases, SPHINX tackles complex tasks in a sample-efficient, generalizable manner. Our method achieves **86.7%**  success across 4 real-world and 2 simulated tasks, outperforming the next best state-of-the-art IL baseline by **41.1%** on average across **440** real world trials. SPHINX additionally generalizes to novel viewpoints, visual distractors, spatial arrangements, and execution speeds with a **1.7x** speedup over the most competitive baseline. Our website (http://sphinx-manip.github.io) provides open-sourced code for data collection, training, and evaluation, along with supplementary videos.",
      "authors": [
        "Priya Sundaresan",
        "Hengyuan Hu",
        "Quan Vuong",
        "Jeannette Bohg",
        "Dorsa Sadigh"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=r0pLGGcuY6",
      "cdate": 1727488473248,
      "mdate": 1740809024577,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.834191"
    },
    {
      "id": "txoJvjfI9w",
      "title": "PEARL: Towards Permutation-Resilient LLMs",
      "abstract": "The in-context learning (ICL) capability of large language models (LLMs) enables them to perform challenging tasks using provided demonstrations. However, ICL is highly sensitive to the ordering of demonstrations, leading to instability in predictions. This paper shows that this vulnerability can be exploited to design a natural attack—difficult for model providers to detect—that achieves nearly 80% success rate on LLaMA-3 by simply permuting the demonstrations. Existing mitigation methods primarily rely on post-processing and fail to enhance the model's inherent robustness to input permutations, raising concerns about safety and reliability of LLMs. To address this issue, we propose Permutation-resilient learning (PEARL), a novel framework based on distributionally robust optimization (DRO), which optimizes model performance against the worst-case input permutation. Specifically, PEARL consists of a permutation-proposal network (P-Net) and the LLM. The P-Net generates the most challenging permutations by treating it as an optimal transport problem, which is solved using an entropy-constrained Sinkhorn algorithm. Through minimax optimization, the P-Net and the LLM iteratively optimize against each other, progressively improving the LLM's robustness. Experiments on synthetic pre-training and real-world instruction tuning tasks demonstrate that PEARL effectively mitigates permutation attacks and enhances performance. Notably, despite being trained on fewer shots and shorter contexts, PEARL achieves performance gains of up to 40% when scaled to many-shot and long-context scenarios, highlighting its efficiency and generalization capabilities.",
      "authors": [
        "Liang CHEN",
        "Li Shen",
        "Yang Deng",
        "Xiaoyan Zhao",
        "Bin Liang",
        "Kam-Fai Wong"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=txoJvjfI9w",
      "cdate": 1727488411966,
      "mdate": 1740922363000,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834196"
    },
    {
      "id": "riTiq3i21b",
      "title": "SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?",
      "abstract": "Autonomous systems for software engineering are now capable of fixing bugs and developing features. These systems are commonly evaluated on SWE-bench (Jimenez et al., 2024a), which assesses their ability to solve software issues from GitHub repositories. However, SWE-bench uses only Python repositories, with problem statements presented predominantly as text and lacking visual elements such as images. This limited coverage motivates our inquiry into how existing systems might perform on unrepresented software engineering domains\n(e.g., front-end, game development, DevOps), which use different programming languages and paradigms. Therefore, we propose SWE-bench Multimodal (SWE-bench M), to evaluate systems on their ability to fix bugs in visual, user-facing JavaScript software. SWE-bench M features 617 task instances collected from 17 JavaScript libraries used for web interface design, diagramming, data visualization, syntax highlighting, and interactive mapping. Each SWE-bench M task instance contains at least one image in its problem statement or unit tests. Our analysis finds that top-performing SWE-bench systems struggle with SWE-bench M, revealing limitations in visual problem-solving and cross-language generalization. Lastly, we show that SWE-agent’s flexible language-agnostic features enable it to substantially outperform alternatives on SWE-bench M, resolving 12% of task\ninstances compared to 6% for the next best system.",
      "authors": [
        "John Yang",
        "Carlos E Jimenez",
        "Alex L Zhang",
        "Kilian Lieret",
        "Joyce Yang",
        "Xindi Wu",
        "Ori Press",
        "Niklas Muennighoff",
        "Gabriel Synnaeve",
        "Karthik R Narasimhan",
        "Diyi Yang",
        "Sida Wang",
        "Ofir Press"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=riTiq3i21b",
      "cdate": 1727488121369,
      "mdate": 1744129290093,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.834201"
    },
    {
      "id": "7ohlQUbTpp",
      "title": "Collab: Controlled Decoding using Mixture of Agents for LLM Alignment",
      "abstract": "Alignment of Large Language models (LLMs) is crucial for safe and trustworthy deployment in applications. Reinforcement learning from human feedback (RLHF) has emerged as an effective technique to align LLMs to human preferences, and broader utilities, but it requires updating billions of model parameters which is computationally expensive. Controlled Decoding, by contrast, provides a mechanism for aligning a model at inference time without retraining. However, single-agent decoding approaches often struggle to adapt to diverse tasks due to the complexity and variability inherent in these tasks. To strengthen the test-time performance w.r.t the target task, we propose a mixture of agents-based decoding strategies leveraging the existing off-the-shelf aligned LLM policies. Treating each prior policy as an agent in the spirit of mixture of agent collaboration, we develop a decoding method that allows for inference-time alignment through a token-level selection strategy among multiple agents. For each token, the most suitable LLM is dynamically chosen from a pool of models based on a long-term utility metric. This policy-switching mechanism ensures optimal model selection at each step, enabling efficient collaboration and alignment among LLMs during decoding. Theoretical analysis of our proposed algorithm establishes optimal performance with respect to the target task represented via a target reward, for the given off-the-shelf models. We conduct comprehensive empirical evaluations with open-source aligned models on diverse tasks and preferences, which demonstrates the merits of this approach over single-agent decoding baselines. Notably, COLLAB surpasses the current SoTA decoding strategy, achieving an improvement of {up to 1.56x} in average reward and $71.89\\%$ in GPT-4 based win-tie rate.",
      "authors": [
        "Souradip Chakraborty",
        "Sujay Bhatt",
        "Udari Madhushani Sehwag",
        "Soumya Suvra Ghosal",
        "Jiahao Qiu",
        "Mengdi Wang",
        "Dinesh Manocha",
        "Furong Huang",
        "Alec Koppel",
        "Sumitra Ganesh"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=7ohlQUbTpp",
      "cdate": 1727487797370,
      "mdate": 1740825806057,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834207"
    },
    {
      "id": "lOfuvmi2HT",
      "title": "Efficient Top-m Data Values Identification for Data Selection",
      "abstract": "Data valuation has found many real-world applications, e.g., data pricing and data selection. However, the most adopted approach -- Shapley value (SV) -- is computationally expensive due to the large number of model trainings required. Fortunately, most applications (e.g., data selection) require only knowing the $m$ data points with the highest data values (i.e., top-$m$ data values), which implies the potential for fewer model trainings as exact data values are not required. Existing work formulates top-$m$ Shapley value identification as top-$m$ arms identification in multi-armed bandits (MAB). However, the proposed approach falls short because it does not utilize data features to predict data values, a method that has been shown empirically to be effective. A recent top-$m$ arms identification work does consider the use of arm features while assuming a linear relationship between arm features and rewards, which is often not satisfied in data valuation. To this end, we propose the GPGapE algorithm that uses the Gaussian process to model the \\emph{non-linear} mapping from data features to data values, removing the linear assumption. We theoretically analyze the correctness and stopping iteration of GPGapE in finding an $(\\epsilon, \\delta)$-approximation to the top-$m$ data values. We further improve the computational efficiency, by calculating data values using small data subsets to reduce the computation cost of model training. We empirically demonstrate that GPGapE outperforms other baselines in top-$m$ data values identification, noisy data detection, and data subset selection on real-world datasets. We also demonstrate the efficiency of our GPGapE in data selection for large language model fine-tuning.",
      "authors": [
        "Xiaoqiang Lin",
        "Xinyi Xu",
        "See-Kiong Ng",
        "Bryan Kian Hsiang Low"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=lOfuvmi2HT",
      "cdate": 1727487580920,
      "mdate": 1740971308690,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834216"
    },
    {
      "id": "tkiZQlL04w",
      "title": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads",
      "abstract": "The memory and computational demands of Key-Value (KV) cache present significant challenges for deploying long-context language models. Previous approaches attempt to mitigate this issue by selectively dropping tokens, which irreversibly erases critical information that might be needed for future queries. In this paper, we propose a novel compression technique for KV cache that preserves all token information. Our investigation reveals that: i) Most attention heads primarily focus on the local context; ii) Only a few heads, denoted as retrieval heads, can essentially pay attention to all input tokens. These key observations motivate us to use separate caching strategy for attention heads.Therefore, we propose RazorAttention, a training-free KV cache compression algorithm, which maintains a full cache for these crucial retrieval heads and discards the remote tokens in non-retrieval heads. Furthermore, we introduce a novel mechanism involving a “compensation token” to further recover the information in the dropped tokens. Extensive evaluations across a diverse set of large language models (LLMs) demonstrate that RazorAttention achieves a reduction in KV cache size by over 70% without noticeable impacts on performance. Additionally, RazorAttention is compatible with FlashAttention, rendering it an efficient and plug-and-play solution that enhances LLM inference efficiency without overhead or retraining of the original model.",
      "authors": [
        "Hanlin Tang",
        "Yang Lin",
        "Jing Lin",
        "Qingsen Han",
        "Danning Ke",
        "Shikuan Hong",
        "Yiwu Yao",
        "Gongyi Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=tkiZQlL04w",
      "cdate": 1727487518567,
      "mdate": 1740890358895,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834221"
    },
    {
      "id": "JvkuZZ04O7",
      "title": "Simple is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation",
      "abstract": "Large Language Models (LLMs) demonstrate strong reasoning abilities but face limitations such as hallucinations and outdated knowledge. Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by grounding LLM outputs in structured external knowledge from KGs. However, current KG-based RAG frameworks still struggle to optimize the trade-off between retrieval effectiveness and efficiency in identifying a suitable amount of relevant graph information for the LLM to digest. We introduce SubgraphRAG, extending the KG-based RAG framework that retrieves subgraphs and leverages LLMs for reasoning and answer prediction. Our approach innovatively integrates a lightweight multilayer perceptron (MLP) with a parallel triple-scoring mechanism for efficient and flexible subgraph retrieval while encoding directional structural distances to enhance retrieval effectiveness. The size of retrieved subgraphs can be flexibly adjusted to match the query's needs and the downstream LLM's capabilities. This design strikes a balance between model complexity and reasoning power, enabling scalable and generalizable retrieval processes. Notably, based on our retrieved subgraphs, smaller LLMs like Llama3.1-8B-Instruct deliver competitive results with explainable reasoning, while larger models like GPT-4o achieve state-of-the-art accuracy compared with previous baselines—all without fine-tuning. Extensive evaluations on the WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency, accuracy, and reliability by reducing hallucinations and improving response grounding.",
      "authors": [
        "Mufei Li",
        "Siqi Miao",
        "Pan Li"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=JvkuZZ04O7",
      "cdate": 1727487462341,
      "mdate": 1740890358890,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834227"
    },
    {
      "id": "5xSRg3eYZz",
      "title": "VVC-Gym: A Fixed-Wing UAV Reinforcement Learning Environment for Multi-Goal Long-Horizon Problems",
      "abstract": "Multi-goal long-horizon problems are prevalent in real-world applications. The additional goal space introduced by multi-goal problems intensifies the spatial complexity of exploration; meanwhile, the long interaction sequences in long-horizon problems exacerbate the temporal complexity of exploration. Addressing the great exploration challenge posed by multi-goal long-horizon problems depends not only on the design of algorithms but also on the design of environments and the availability of demonstrations to assist in training. To facilitate the above research, we propose a multi-goal long-horizon Reinforcement Learning (RL) environment based on realistic fixed-wing UAV's velocity vector control, named VVC-Gym, and generate multiple demonstration sets of various quality. Through experimentation, we analyze the impact of different environment designs on training, assess the quantity and quality of demonstrations and their influence on training, and assess the effectiveness of various RL algorithms, providing baselines on VVC-Gym and its corresponding demonstrations. The results suggest that VVC-Gym is suitable for studying: (1) the influence of environment designs on addressing multi-goal long-horizon problems with RL. (2) the assistance that demonstrations can provide in overcoming the exploration challenges of multi-goal long-horizon problems. (3) the RL algorithm designs with the least possible impact from environment designs on the efficiency and effectiveness of training.",
      "authors": [
        "Xudong Gong",
        "Feng Dawei",
        "Kele Xu",
        "weijia wang",
        "Zhangjun Sun",
        "Xing Zhou",
        "Si Zheng",
        "Bo Ding",
        "Huaimin Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=5xSRg3eYZz",
      "cdate": 1727487294991,
      "mdate": 1741229267494,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834232"
    },
    {
      "id": "fv9XU7CyN2",
      "title": "CL-MFAP: A Contrastive Learning-Based Multimodal Foundation Model for Molecular Property Prediction and Antibiotic Screening",
      "abstract": "Due to the rise in antimicrobial resistance, identifying novel compounds with antibiotic potential is crucial for combatting this global health issue. However, traditional drug development methods are costly and inefficient. Recognizing the pressing need for more effective solutions, researchers have turned to machine learning techniques to streamline the prediction and development of novel antibiotic compounds. While foundation models have shown promise in antibiotic discovery, current mainstream efforts still fall short of fully leveraging the potential of multimodal molecular data. Recent studies suggest that contrastive learning frameworks utilizing multimodal data exhibit excellent performance in representation learning across various domains. Building upon this, we introduce CL-MFAP, an unsupervised contrastive learning (CL)-based multimodal foundation (MF) model specifically tailored for discovering small molecules with potential antibiotic properties (AP) using three types of molecular data. This model employs 1.6 million bioactive molecules with drug-like properties from the ChEMBL dataset to jointly pretrain three encoders: (1) a transformer-based encoder with rotary position embedding for processing SMILES strings; (2) another transformer-based encoder, incorporating a novel bi-level routing attention mechanism to handle molecular graph representations; and (3) a Morgan fingerprint encoder using a multilayer perceptron, to achieve the contrastive learning purpose. The CL-MFAP outperforms baseline models in antibiotic property prediction by effectively utilizing different molecular modalities and demonstrates superior domain-specific performance when fine-tuned for antibiotic-related property prediction tasks.",
      "authors": [
        "Gen Zhou",
        "Sugitha Janarthanan",
        "Yutong Lu",
        "Pingzhao Hu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=fv9XU7CyN2",
      "cdate": 1727487199164,
      "mdate": 1740890358807,
      "matched_keywords": [
        "foundation model",
        "multimodal",
        "transformer",
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834238"
    },
    {
      "id": "u3TL0qxLWf",
      "title": "SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators",
      "abstract": "Large Language Models (LLMs) have transformed natural language processing, but face significant challenges in widespread deployment due to their high runtime cost. In this paper, we introduce SeedLM, a novel post-training compression method that uses seeds of a pseudo-random generator to encode and compress model weights. Specifically, for each block of weights, we find a seed that is fed into a Linear Feedback Shift Register (LFSR) during inference to efficiently generate a random matrix. This matrix is then linearly combined with compressed coefficients to reconstruct the weight block. SeedLM reduces memory access and leverages idle compute cycles during inference, effectively speeding up memory-bound tasks by trading compute for fewer memory accesses. Unlike state-of-the-art methods that rely on calibration data, our approach is data-free and generalizes well across diverse tasks. Our experiments with Llama3 70B, which is particularly challenging, show zero-shot accuracy retention at 4- and 3-bit compression to be on par with or better than state-of-the-art methods, while maintaining performance comparable to FP16 baselines. Additionally, FPGA-based tests demonstrate that 4-bit SeedLM, as model size increases, approaches a 4x speed-up over an FP16 Llama 2/3 baseline.",
      "authors": [
        "Rasoul Shafipour",
        "David Harrison",
        "Maxwell Horton",
        "JEFFREY MARKER",
        "Houman Bedayat",
        "Sachin Mehta",
        "Mohammad Rastegari",
        "Mahyar Najibi",
        "Saman Naderiparizi"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=u3TL0qxLWf",
      "cdate": 1727487106854,
      "mdate": 1744831306307,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834243"
    },
    {
      "id": "YauQYh2k1g",
      "title": "Dissecting Adversarial Robustness of Multimodal LM Agents",
      "abstract": "As language models (LMs) are used to build autonomous agents in real environments, ensuring their adversarial robustness becomes a critical challenge. Unlike chatbots, agents are compound systems with multiple components taking actions, which existing LMs safety evaluations do not adequately address. To bridge this gap, we manually create 200 targeted adversarial tasks and evaluation scripts in a realistic threat model on top of VisualWebArena, a real environment for web agents. To systematically examine the robustness of agents, we propose the Agent Robustness Evaluation (ARE) framework. ARE views the agent as a graph showing the flow of intermediate outputs between components and decomposes robustness as the flow of adversarial information on the graph. We find that we can successfully break latest agents that use black-box frontier LMs, including those that perform reflection and tree search. With imperceptible perturbations to a single image (less than 5% of total web page pixels), an attacker can hijack these agents to execute targeted adversarial goals with success rates up to 67%. We also use ARE to rigorously evaluate how the robustness changes as new components are added. We find that inference-time compute that typically improves benign performance can open up new vulnerabilities and harm robustness. An attacker can compromise the evaluator used by the reflexion agent and the value function of the tree search agent, which increases the attack success relatively by 15% and 20%. Our data and code for attacks, defenses, and evaluation are at https://github.com/ChenWu98/agent-attack",
      "authors": [
        "Chen Henry Wu",
        "Rishi Rajesh Shah",
        "Jing Yu Koh",
        "Russ Salakhutdinov",
        "Daniel Fried",
        "Aditi Raghunathan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=YauQYh2k1g",
      "cdate": 1727487090607,
      "mdate": 1740677534425,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.834251"
    },
    {
      "id": "nfKfAzkiez",
      "title": "ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration",
      "abstract": "Large language models (LLMs) have demonstrated a remarkable ability to serve as general-purpose tools for various language-based tasks. \n  Recent works have demonstrated that the efficacy of such models can be improved through iterative dialog between multiple models.\n  While these \n  paradigms show promise \n  in\n  improving model efficacy, most works in this area treat collaboration as an emergent behavior, rather than a learned behavior. \n  In doing so, current multi-agent frameworks rely on collaborative behaviors to have been sufficiently trained into off-the-shelf models. \n  To address this limitation, we propose ACC-Collab, an **A**ctor-**C**riti**c** based learning framework to produce a two-agent team (an actor-agent and a critic-agent) specialized in collaboration.\n  We demonstrate that ACC-Collab outperforms SotA multi-agent techniques on a wide array of benchmarks.",
      "authors": [
        "Andrew Estornell",
        "Jean-Francois Ton",
        "Yuanshun Yao",
        "Yang Liu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=nfKfAzkiez",
      "cdate": 1727486990136,
      "mdate": 1740890358781,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834257"
    },
    {
      "id": "rEQqBZIz49",
      "title": "Greener GRASS: Enhancing GNNs with Encoding, Rewiring, and Attention",
      "abstract": "Graph Neural Networks (GNNs) have become important tools for machine learning on graph-structured data. In this paper, we explore the synergistic combination of graph encoding, graph rewiring, and graph attention, by introducing Graph Attention with Stochastic Structures (GRASS), a novel GNN architecture. GRASS utilizes relative random walk probabilities (RRWP) encoding and a novel decomposed variant (D-RRWP) to efficiently capture structural information. It rewires the input graph by superimposing a random regular graph to enhance long-range information propagation. It also employs a novel additive attention mechanism tailored for graph-structured data. Our empirical evaluations demonstrate that GRASS achieves state-of-the-art performance on multiple benchmark datasets, including a 20.3% reduction in mean absolute error on the ZINC dataset.",
      "authors": [
        "Tongzhou Liao",
        "Barnabas Poczos"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=rEQqBZIz49",
      "cdate": 1727486878166,
      "mdate": 1741995860252,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834262"
    },
    {
      "id": "VoayJihXra",
      "title": "NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks In Open Domains",
      "abstract": "We explore neuro-symbolic approaches to generalize actionable knowledge, enabling embodied agents to tackle complex tasks more effectively in open-domain environments. A key challenge for embodied agents is the generalization of knowledge across diverse environments and situations, as limited experiences often confine them to their prior knowledge. To address this issue, we introduce a novel framework, NeSyC, a neuro-symbolic continual learner that emulates the hypothetico-deductive model by continually formulating and validating knowledge from limited experiences through the combined use of Large Language Models (LLMs) and symbolic tools. Specifically, we devise a contrastive generality improvement scheme within NeSyC, which iteratively generates hypotheses using LLMs and conducts contrastive validation via symbolic tools. This scheme reinforces the justification for admissible actions while minimizing the inference of inadmissible ones. Additionally, we incorporate a memory-based monitoring scheme that efficiently detects action errors and triggers the knowledge refinement process across domains. Experiments conducted on diverse embodied task benchmarks—including ALFWorld, VirtualHome, Minecraft, RLBench, and a real-world robotic scenario—demonstrate that NeSyC is highly effective in solving complex embodied tasks across a range of open-domain environments.",
      "authors": [
        "Wonje Choi",
        "Jinwoo Park",
        "Sanghyun Ahn",
        "Daehee Lee",
        "Honguk Woo"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=VoayJihXra",
      "cdate": 1727486873878,
      "mdate": 1747446552037,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834267"
    },
    {
      "id": "nrRkAAAufl",
      "title": "Constraint-Conditioned Actor-Critic for Offline Safe Reinforcement Learning",
      "abstract": "Offline safe reinforcement learning (OSRL) aims to learn policies with high rewards while satisfying safety constraints solely from data collected offline. However, the learned policies often struggle to handle states and actions that are not present or out-of-distribution (OOD) from the offline dataset, which can result in violation of the safety constraints or overly conservative behaviors during their online deployment. Moreover, many existing methods are unable to learn policies that can adapt to varying constraint thresholds. To address these challenges, we propose constraint-conditioned actor-critic (CCAC), a novel OSRL method that models the relationship between state-action distributions and safety constraints, and leverages this relationship to regularize critics and policy learning. CCAC learns policies that can effectively handle OOD data and adapt to varying constraint thresholds. Empirical evaluations on the $\\texttt{DSRL}$ benchmarks show that CCAC significantly outperforms existing methods for learning adaptive, safe, and high-reward policies.",
      "authors": [
        "Zijian Guo",
        "Weichao Zhou",
        "Shengao Wang",
        "Wenchao Li"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=nrRkAAAufl",
      "cdate": 1727486569977,
      "mdate": 1741201088089,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834272"
    },
    {
      "id": "JYTQ6ELUVO",
      "title": "Specialized Foundation Models Struggle to Beat Supervised Baselines",
      "abstract": "Following its success for vision and text, the \"foundation model\" (FM) paradigm&#151;pretraining large models on massive data, then fine-tuning on target tasks&#151;has rapidly expanded to domains in the sciences, engineering, healthcare, and beyond. Has this achieved what the original FMs accomplished, i.e. the supplanting of traditional supervised learning in their domains? To answer we look at three modalities&#151;genomics, satellite imaging, and time series&#151;with multiple recent FMs and compare them to a standard supervised learning workflow: model development, hyperparameter tuning, and training, all using only data from the target task. Across these three specialized domains, we find that it is consistently possible to train simple supervised models&#151;no more complicated than a lightly modified wide ResNet or UNet&#151;that match or even outperform the latest foundation models. Our work demonstrates that the benefits of large-scale pretraining have yet to be realized in many specialized areas, reinforces the need to compare new FMs to strong, well-tuned baselines, and introduces two new, easy-to-use, open-source, and automated workflows for doing so.",
      "authors": [
        "Zongzhe Xu",
        "Ritvik Gupta",
        "Wenduo Cheng",
        "Alexander Shen",
        "Junhong Shen",
        "Ameet Talwalkar",
        "Mikhail Khodak"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=JYTQ6ELUVO",
      "cdate": 1727486534988,
      "mdate": 1741021485596,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834278"
    },
    {
      "id": "dCcY2pyNIO",
      "title": "In-context Time Series Predictor",
      "abstract": "Recent Transformer-based large language models (LLMs) demonstrate in-context learning ability to perform various functions based solely on the provided context, without updating model parameters. To fully utilize the in-context capabilities in time series forecasting (TSF) problems, unlike previous Transformer-based or LLM-based time series forecasting methods, we reformulate \"time series forecasting tasks\" as input tokens by constructing a series of (lookback, future) pairs within the tokens. This method aligns more closely with the inherent in-context mechanisms and is more parameter-efficient without the need of using pre-trained LLM parameters. Furthermore, it addresses issues such as overfitting in existing Transformer-based TSF models, consistently achieving better performance across full-data, few-shot, and zero-shot settings compared to previous architectures.",
      "authors": [
        "Jiecheng Lu",
        "Yan Sun",
        "Shihao Yang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=dCcY2pyNIO",
      "cdate": 1727486244979,
      "mdate": 1740805163125,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834283"
    },
    {
      "id": "1qq1QJKM5q",
      "title": "More Experts Than Galaxies: Conditionally-Overlapping Experts with Biologically-Inspired Fixed Routing",
      "abstract": "The evolution of biological neural systems has led to both modularity and sparse coding, which enables energy efficiency and robustness across the diversity of tasks in the lifespan. In contrast, standard neural networks rely on dense, non-specialized architectures, where all model parameters are simultaneously updated to learn multiple tasks, leading to interference. Current sparse neural network approaches aim to alleviate this issue but are hindered by limitations such as 1) trainable gating functions that cause representation collapse, 2) disjoint experts that result in redundant computation and slow learning, and 3) reliance on explicit input or task IDs that limit flexibility and scalability.\nIn this paper we propose Conditionally Overlapping Mixture of ExperTs (COMET), a general deep learning method that addresses these challenges by inducing a modular, sparse architecture with an exponential number of overlapping experts. COMET replaces the trainable gating function used in Sparse Mixture of Experts with a fixed, biologically inspired random projection applied to individual input representations. This design causes the degree of expert overlap to depend on input similarity, so that similar inputs tend to share more parameters. This results in faster learning per update step and improved out-of-sample generalization. \nWe demonstrate the effectiveness of COMET on a range of tasks, including image classification, language modeling, and regression, using several popular deep learning architectures.",
      "authors": [
        "Sagi Shaier",
        "Francisco Pereira",
        "Katharina von der Wense",
        "Lawrence Hunter",
        "Matt Jones"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=1qq1QJKM5q",
      "cdate": 1727486180458,
      "mdate": 1739261781154,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834288"
    },
    {
      "id": "WQQyJbr5Lh",
      "title": "Discovering Influential Neuron Path in Vision Transformers",
      "abstract": "Vision Transformer models exhibit immense power yet remain opaque to human understanding, posing challenges and risks for practical applications. \nWhile prior research has attempted to demystify these models through input attribution and neuron role analysis,\nthere's been a notable gap in considering layer-level information and the holistic path of information flow across layers.\nIn this paper, we investigate the significance of influential neuron paths within vision Transformers, which is a path of neurons from the model input to output that impacts the model inference most significantly.\nWe first propose a joint influence measure to assess the contribution of a set of neurons to the model outcome.\nAnd we further provide a \nlayer-progressive neuron locating\napproach that efficiently selects the most influential neuron at each layer trying to discover the crucial neuron path from input to output within the target model.\nOur experiments demonstrate the superiority of our method finding the most influential neuron path along which the information flows, over the existing baseline solutions.\nAdditionally, the neuron paths have illustrated that vision Transformers exhibit some specific inner working mechanism for processing the visual information within the same image category. \nWe further analyze the key effects of these neurons on the image classification task, showcasing that the found neuron paths have already preserved the model capability on downstream tasks, which may also shed some lights on real-world applications like model pruning.\nThe project website including implementation code is available at https://foundation-model-research.github.io/NeuronPath/.",
      "authors": [
        "Yifan Wang",
        "Yifei Liu",
        "Yingdong Shi",
        "Changming Li",
        "Anqi Pang",
        "Sibei Yang",
        "Jingyi Yu",
        "Kan Ren"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=WQQyJbr5Lh",
      "cdate": 1727486053841,
      "mdate": 1745670716371,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834294"
    },
    {
      "id": "RWJX5F5I9g",
      "title": "Brain Bandit: A Biologically Grounded Neural Network for Efficient Control of Exploration",
      "abstract": "How to balance between exploration and exploitation in an uncertain environment is a central challenge in reinforcement learning. In contrast, humans and animals have demonstrated superior exploration efficiency in novel environments. To understand how the brain’s neural network controls exploration under uncertainty, we analyzed the dynamical systems model of a biological neural network that controls explore-exploit decisions during foraging. Mathematically, this model (named the Brain Bandit Net, or BBN) is a special type of stochastic continuous Hopfield network. We show through theory and simulation that BBN can perform posterior sampling of action values with a tunable bias towards or against uncertain options. We then demonstrate that, in multi-armed bandit (MAB) tasks, BBN can generate probabilistic choice behavior with a flexible uncertainty bias resembling human and animal choice patterns. In addition to its high efficiency in MAB tasks, BBN can also be embedded with reinforcement learning algorithms to accelerate learning in MDP tasks. Altogether, our findings reveal the theoretical foundation for efficient exploration in biological neural networks and propose a general, brain-inspired algorithm for enhancing exploration in RL.",
      "authors": [
        "Chen Jiang",
        "Jiahui An",
        "Yating Liu",
        "Ni Ji"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=RWJX5F5I9g",
      "cdate": 1727485965939,
      "mdate": 1747381781590,
      "matched_keywords": [
        "reinforcement learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834299"
    },
    {
      "id": "CkUHtnyhpY",
      "title": "When narrower is better: the narrow width limit of Bayesian parallel branching neural networks",
      "abstract": "The infinite width limit of random neural networks is known to result in Neural Networks as Gaussian Process (NNGP) (Lee et al. (2018)), characterized by task-independent kernels. It is widely accepted that larger network widths contribute to improved generalization (Park et al. (2019)). However, this work challenges this notion by investigating the narrow width limit of the Bayesian Parallel Branching\nNeural Network (BPB-NN), an architecture that resembles neural networks with residual blocks. We demonstrate that when the width of a BPB-NN is significantly smaller compared to the number of training examples, each branch exhibits more robust learning due to a symmetry breaking of branches in kernel renormalization. Surprisingly, the performance of a BPB-NN in the narrow width limit is generally superior to or comparable to that achieved in the wide width limit in bias-limited scenarios. Furthermore, the readout norms of each branch in the narrow width limit are mostly independent of the architectural hyperparameters but generally reflective of the nature of the data. We demonstrate such phenomenon primarily in the branching graph neural networks, where each branch represents a different order of convolutions of the graph; we also extend the results to other more general architectures such as the residual-MLP and demonstrate that the narrow width effect is a general feature of the branching networks. Our results characterize a newly defined narrow-width regime for parallel branching networks in general.",
      "authors": [
        "Zechen Zhang",
        "Haim Sompolinsky"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=CkUHtnyhpY",
      "cdate": 1727485950604,
      "mdate": 1741014824181,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834307"
    },
    {
      "id": "goBaGHLAdP",
      "title": "Directional Gradient Projection for Robust Fine-Tuning of Foundation Models",
      "abstract": "Robust fine-tuning aims to adapt large foundation models to downstream tasks while preserving their robustness to distribution shifts. Existing methods primarily focus on constraining and projecting current model towards the pre-trained initialization based on the magnitudes between fine-tuned and pre-trained weights, which often require extensive hyper-parameter tuning and can sometimes result in underfitting. In this work, we propose $\\textbf{Di}$rectional $\\textbf{Gra}$dient $\\textbf{P}$rojection (DiGraP), a novel layer-wise trainable method that incorporates directional information from gradients to bridge regularization and multi-objective optimization. Besides demonstrating our method on image classification, as another contribution we generalize this area to the multi-modal evaluation settings for robust fine-tuning. Specifically, we first bridge the uni-modal and multi-modal gap by performing analysis on Image Classification reformulated Visual Question Answering (VQA) benchmarks and further categorize ten out-of-distribution (OOD) VQA datasets by distribution shift types and degree (i.e. near versus far OOD). Experimental results show that DiGraP consistently outperforms existing baselines across Image Classfication and VQA tasks with discriminative and generative backbones, improving both in-distribution (ID) generalization and OOD robustness.",
      "authors": [
        "Chengyue Huang",
        "Junjiao Tian",
        "Brisa Maneechotesuwan",
        "Shivang Chopra",
        "Zsolt Kira"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=goBaGHLAdP",
      "cdate": 1727485915796,
      "mdate": 1740890358569,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834312"
    },
    {
      "id": "6HcnC3pPkp",
      "title": "Token-Supervised Value Models for Enhancing Mathematical Problem-Solving Capabilities of Large Language Models",
      "abstract": "With the rapid advancement of test-time compute search strategies to improve the mathematical problem-solving capabilities of large language models (LLMs), the need for building robust verifiers has become increasingly important. However, all these inference strategies rely on existing verifiers originally designed for Best-of-N search, which makes them sub-optimal for tree search techniques at test time. During tree search, existing verifiers can only offer indirect and implicit assessments of partial solutions or under-value prospective intermediate steps, thus resulting in the premature pruning of promising intermediate steps. To overcome these limitations, we propose token-supervised value models (TVMs) -- a new class of verifiers that assign each token a probability that reflects the likelihood of reaching the correct final answer. This new token-level supervision enables TVMs to directly and explicitly evaluate partial solutions, effectively distinguishing between promising and incorrect intermediate steps during tree search at test time. Experimental results demonstrate that combining tree-search-based inference strategies with TVMs significantly improves the accuracy of LLMs in mathematical problem-solving tasks, surpassing the performance of existing verifiers.",
      "authors": [
        "Jung Hyun Lee",
        "June Yong Yang",
        "Byeongho Heo",
        "Dongyoon Han",
        "Kyungsu Kim",
        "Eunho Yang",
        "Kang Min Yoo"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=6HcnC3pPkp",
      "cdate": 1727485731654,
      "mdate": 1740909600832,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834318"
    },
    {
      "id": "dWsdJAXjQD",
      "title": "ImProver: Agent-Based Automated Proof Optimization",
      "abstract": "Large language models (LLMs) have been used to generate formal proofs of mathematical theorems in proofs assistants such as Lean.\nHowever, we often want to optimize a formal proof with respect to various criteria, depending on its downstream use.\nFor example, we may want a proof to adhere to a certain style, be declaratively structured, or concise. Having suitably optimized proofs is also important for learning tasks, especially since human-written proofs may not optimal for that purpose.\nTo this end, we study a new problem of automated proof optimization: rewriting a proof so that it is correct and optimizes for an arbitrary criterion, such as length or declarativity.\nAs a first method for automated proof optimization, we present ImProver, a large-language-model agent that rewrites proofs to optimize arbitrary user-defined metrics in Lean.\nWe find that naively applying LLMs to proof optimization falls short, and we incorporate various improvements into ImProver, such as the use of symbolic Lean context in a novel Chain-of-States technique, as well as error-correction and retrieval. We test ImProver on rewriting real-world undergraduate, competition, and research-level mathematics theorems, finding that ImProver is capable of rewriting proofs so that they are substantially shorter and more declarative in structure.",
      "authors": [
        "Riyaz Ahuja",
        "Jeremy Avigad",
        "Prasad Tetali",
        "Sean Welleck"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=dWsdJAXjQD",
      "cdate": 1727485671636,
      "mdate": 1740886700946,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834323"
    },
    {
      "id": "Aly68Y5Es0",
      "title": "Learning-Guided Rolling Horizon Optimization for Long-Horizon Flexible Job-Shop Scheduling",
      "abstract": "Long-horizon combinatorial optimization problems (COPs), such as the Flexible Job-Shop Scheduling Problem (FJSP), often involve complex, interdependent decisions over extended time frames, posing significant challenges for existing solvers. While Rolling Horizon Optimization (RHO) addresses this by decomposing problems into overlapping shorter-horizon subproblems, such overlap often involves redundant computations. In this paper, we present L-RHO, the first learning-guided RHO framework for COPs. L-RHO employs a neural network to intelligently fix variables that in hindsight did not need to be re-optimized, resulting in smaller and thus easier-to-solve subproblems. For FJSP, this means identifying operations with unchanged machine assignments between consecutive subproblems. Applied to FJSP, L-RHO accelerates RHO by up to 54\\% while significantly improving solution quality, outperforming other heuristic and learning-based baselines. We also provide in-depth discussions and verify the desirable adaptability and generalization of L-RHO across numerous FJSP variates, distributions, online scenarios and benchmark instances. Moreover, we provide a theoretical analysis to elucidate the conditions under which learning is beneficial.",
      "authors": [
        "Sirui Li",
        "Wenbin Ouyang",
        "Yining Ma",
        "Cathy Wu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Aly68Y5Es0",
      "cdate": 1727485486887,
      "mdate": 1746562306255,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834328"
    },
    {
      "id": "PIpGN5Ko3v",
      "title": "Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors",
      "abstract": "The advent of large language models (LLMs) has revolutionized the field of text generation, producing outputs that closely mimic human-like writing. Although academic and industrial institutions have developed detectors to prevent the malicious usage of LLM-generated texts, other research has doubt about the robustness of these systems. To stress test these detectors, we introduce a humanized proxy-attack (HUMPA) strategy that effortlessly compromises LLMs, causing them to produce outputs that align with human-written text and mislead detection systems. Our method attacks the source model by leveraging a reinforcement learning (RL) fine-tuned humanized small language model (SLM) in the decoding phase. Through an in-depth analysis, we demonstrate that our attack strategy is capable of generating responses that are indistinguishable to detectors, preventing them from differentiating between machine-generated and human-written text. We conduct systematic evaluations on extensive datasets using proxy-attacked open-source models, including Llama2-13B, Llama3-70B, and Mixtral-8x7B in both white- and black-box settings. Our findings show that the proxy-attack strategy effectively deceives the leading detectors, resulting in an average AUROC drop of 70.4% across multiple datasets, with a maximum drop of 95.0% on a single dataset. Furthermore, in cross-discipline scenarios, our strategy also bypasses these detectors, leading to a significant relative decrease of up to 90.9%, while in cross-language scenario, the drop reaches 91.3%. Despite our proxy-attack strategy successfully bypassing the detectors with such significant relative drops, we find that the generation quality of the attacked models remains preserved, even within a modest utility budget, when compared to the text produced by the original, unattacked source model.",
      "authors": [
        "Tianchun Wang",
        "Yuanzhou Chen",
        "Zichuan Liu",
        "Zhanwen Chen",
        "Haifeng Chen",
        "Xiang Zhang",
        "Wei Cheng"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=PIpGN5Ko3v",
      "cdate": 1727485210040,
      "mdate": 1740266189983,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834334"
    },
    {
      "id": "WOt1owGfuN",
      "title": "Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing",
      "abstract": "We introduce Probe Pruning (PP), a novel framework for online, dynamic, structured pruning of Large Language Models (LLMs) applied in a batch-wise manner. PP leverages the insight that not all samples and tokens contribute equally to the model's output, and probing a small portion of each batch effectively identifies crucial weights, enabling tailored dynamic pruning for different batches. It comprises three main stages: probing, history-informed pruning, and full inference. In the probing stage, PP selects a small yet crucial set of hidden states, based on residual importance, to run a few model layers ahead. During the history-informed pruning stage, PP strategically integrates the probing states with historical states. Subsequently, it structurally prunes weights based on the integrated states and the PP importance score, a metric developed specifically to assess the importance of each weight channel in maintaining performance. In the final stage, full inference is conducted on the remaining weights. A major advantage of PP is its compatibility with existing models, as it operates without requiring additional neural network modules or fine-tuning. Comprehensive evaluations of PP on LLaMA-2/3 and OPT models reveal that even minimal probing—using just 1.5% of FLOPs—can substantially enhance the efficiency of structured pruning of LLMs. For instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56 times lower ratio of performance degradation per unit of latency reduction compared to the state-of-the-art method at a 40\\% pruning ratio.",
      "authors": [
        "Qi Le",
        "Enmao Diao",
        "Ziyan Wang",
        "Xinran Wang",
        "Jie Ding",
        "Li Yang",
        "Ali Anwar"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=WOt1owGfuN",
      "cdate": 1727485123802,
      "mdate": 1743445910129,
      "matched_keywords": [
        "large language model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834339"
    },
    {
      "id": "or8mMhmyRV",
      "title": "MaestroMotif: Skill Design from Artificial Intelligence Feedback",
      "abstract": "Describing skills in natural language has the potential to provide an accessible way to inject human knowledge about decision-making into an AI system. We present MaestroMotif, a method for AI-assisted skill design, which yields high-performing and adaptable agents. MaestroMotif leverages the capabilities of Large Language Models (LLMs) to effectively create and reuse skills. It first uses an LLM's feedback to automatically design rewards corresponding to each skill, starting from their natural language description. Then, it employs an LLM's code generation abilities, together with reinforcement learning, for training the skills and combining them to implement complex behaviors specified in language. We evaluate MaestroMotif using a suite of complex tasks in the NetHack Learning Environment (NLE), demonstrating that it surpasses existing approaches in both performance and usability.",
      "authors": [
        "Martin Klissarov",
        "Mikael Henaff",
        "Roberta Raileanu",
        "Shagun Sodhani",
        "Pascal Vincent",
        "Amy Zhang",
        "Pierre-Luc Bacon",
        "Doina Precup",
        "Marlos C. Machado",
        "Pierluca D'Oro"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=or8mMhmyRV",
      "cdate": 1727485052980,
      "mdate": 1741190876833,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834344"
    },
    {
      "id": "oWdzUpOlkX",
      "title": "AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents",
      "abstract": "Autonomy via agents based on large language models (LLMs) that can carry out personalized yet standardized tasks presents a significant opportunity to drive human efficiency. There is an emerging need and interest in automating web tasks  (e.g., booking a hotel for a given date within a budget). Being a practical use case itself, the web agent also serves as an important proof-of-concept example for various agent grounding scenarios, with its success promising advancements in many future applications. Meanwhile, much prior research focuses on handcrafting their web agent strategies (e.g., agent's prompting templates, reflective workflow, role-play and multi-agent systems, search or sampling methods, etc.) and the corresponding in-context examples. However, these custom strategies often struggle with generalizability across all potential real-world applications. On the other hand, there has been limited study on the misalignment between a web agent's observation and action representation, and the data on which the agent's underlying LLM has been pre-trained. This discrepancy is especially notable when LLMs are primarily trained for language completion rather than tasks involving embodied navigation actions and symbolic web elements. In our study, we enhance an LLM-based web agent by simply refining its observation and action space, aligning these more closely with the LLM's capabilities. This approach enables our base agent to significantly outperform previous methods on a wide variety of web tasks. Specifically, on WebArena, a benchmark featuring general-purpose web interaction tasks, our agent AgentOccam surpasses the previous state-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute points respectively, and boosts the success rate by 26.6 points (+161%) over similar plain web agents with its observation and action space alignment. Furthermore, on WebVoyager benchmark comprising tasks defined on real-world websites, AgentOccam exceeds the former best agent by 2.4 points (+4.6%) on tasks with deterministic answers. We achieve this without using in-context examples, new agent roles, online feedback or search strategies. AgentOccam's simple design highlights LLMs' impressive zero-shot performance on web tasks, and underlines the critical role of carefully tuning observation and action spaces for LLM-based agents.",
      "authors": [
        "Ke Yang",
        "Yao Liu",
        "Sapana Chaudhary",
        "Rasool Fakoor",
        "Pratik Chaudhari",
        "George Karypis",
        "Huzefa Rangwala"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=oWdzUpOlkX",
      "cdate": 1727485035797,
      "mdate": 1740782048078,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834350"
    },
    {
      "id": "SQnitDuow6",
      "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF",
      "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference. Depending on the availability of preference data, both online and offline RLHF are active areas of investigation. A key bottleneck is understanding how to incorporate uncertainty estimation in the reward function learned from the preference data for RLHF, regardless of how the preference data is collected. While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations.\n\nIn this paper, we introduce a unified approach to online and offline RLHF --- value-incentivized preference optimization (VPO) --- which regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, modulated by a sign to indicate whether the optimism or pessimism is chosen. VPO also directly optimizes the policy with implicit reward modeling, and therefore shares a simpler RLHF pipeline similar to direct preference optimization. Theoretical guarantees of VPO are provided for both online and offline settings, matching the rates of their standard RL counterparts. Moreover, experiments on text summarization, dialogue, and standard benchmarks verify the practicality and effectiveness of VPO.",
      "authors": [
        "Shicong Cen",
        "Jincheng Mei",
        "Katayoon Goshvadi",
        "Hanjun Dai",
        "Tong Yang",
        "Sherry Yang",
        "Dale Schuurmans",
        "Yuejie Chi",
        "Bo Dai"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=SQnitDuow6",
      "cdate": 1727484980732,
      "mdate": 1739925805253,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834355"
    },
    {
      "id": "FviefuxmeW",
      "title": "Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration",
      "abstract": "Imitation learning is a central problem in reinforcement learning where the goal is to learn a policy that mimics the expert's behavior. In practice, it is often challenging to learn the expert policy from a limited number of demonstrations accurately due to the complexity of the state space. Moreover, it is essential to explore the environment and collect data to achieve beyond-expert performance. To overcome these challenges, we propose a novel imitation learning algorithm called Imitation Learning with Double Exploration (ILDE), which implements exploration in two aspects: (1) optimistic policy optimization via an exploration bonus that rewards state-action pairs with high uncertainty to potentially improve the convergence to the expert policy, and (2) curiosity-driven exploration of the states that deviate from the demonstration trajectories to potentially yield beyond-expert performance. Empirically, we demonstrate that ILDE outperforms the state-of-the-art imitation learning algorithms in terms of sample efficiency and achieves beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations than in previous work. We also provide a theoretical justification of ILDE as an uncertainty-regularized policy optimization method with optimistic exploration, leading to a regret growing sublinearly in the number of episodes.",
      "authors": [
        "Heyang Zhao",
        "Xingrui Yu",
        "David Mark Bossens",
        "Ivor Tsang",
        "Quanquan Gu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=FviefuxmeW",
      "cdate": 1727484968029,
      "mdate": 1743008116740,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834368"
    },
    {
      "id": "0uRc3CfJIQ",
      "title": "ORSO: Accelerating Reward Design via Online Reward Selection and Policy Optimization",
      "abstract": "Reward shaping is critical in reinforcement learning (RL), particularly for complex tasks where sparse rewards can hinder learning. However, choosing effective shaping rewards from a set of reward functions in a computationally efficient manner remains an open challenge. We propose Online Reward Selection and Policy Optimization (ORSO), a novel approach that frames the selection of shaping reward function as an online model selection problem. ORSO automatically identifies performant shaping reward functions without human intervention with provable regret guarantees. We demonstrate ORSO's effectiveness across various continuous control tasks. Compared to prior approaches, ORSO significantly reduces the amount of data required to evaluate a shaping reward function, resulting in superior data efficiency and a significant reduction in computational time (up to 8×). ORSO consistently identifies high-quality reward functions outperforming prior methods by more than 50% and on average identifies policies as performant as the ones learned using manually engineered reward functions by domain experts.",
      "authors": [
        "Chen Bo Calvin Zhang",
        "Zhang-Wei Hong",
        "Aldo Pacchiano",
        "Pulkit Agrawal"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=0uRc3CfJIQ",
      "cdate": 1727484798370,
      "mdate": 1740465286482,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834374"
    },
    {
      "id": "irrtPRFksw",
      "title": "Risk-Sensitive Variational Actor-Critic: A Model-Based Approach",
      "abstract": "Risk-sensitive reinforcement learning (RL) with an entropic risk measure typically requires knowledge of the transition kernel or performs unstable updates w.r.t. exponential Bellman equations. As a consequence, algorithms that optimize this objective have been restricted to tabular or low-dimensional continuous environments. In this work we leverage the connection between the entropic risk measure and the RL-as-inference framework to develop a risk-sensitive variational actor-critic algorithm (rsVAC). Our work extends the variational framework to incorporate stochastic rewards and proposes a variational model-based actor-critic approach that modulates policy risk via a risk parameter.  We consider, both, the risk-seeking and risk-averse regimes and present rsVAC learning variants for each setting.  Our experiments demonstrate that this approach produces risk-sensitive policies and yields improvements in both tabular and risk-aware variants of complex continuous control tasks in MuJoCo.",
      "authors": [
        "Alonso Granados",
        "Reza Ebrahimi",
        "Jason Pacheco"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=irrtPRFksw",
      "cdate": 1727484730065,
      "mdate": 1740782607615,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834382"
    },
    {
      "id": "ULGbw2URE3",
      "title": "L3Ms — Lagrange Large Language Models",
      "abstract": "Supervised fine-tuning (SFT) and alignment of large language models (LLMs) are key steps in providing a good user experience. However, the concept of an appropriate alignment is inherently application-dependent, and current methods often rely on heuristic choices to drive optimization. In this work, we formulate SFT and alignment as a constrained optimization problem: the LLM is fine-tuned on a task while being required to meet application-specific requirements, without resorting to heuristics. To solve this, we propose Lagrange Large Language Models (L3Ms), which employ logarithmic barriers to enforce the constraints. This approach allows for the customization of L3Ms across diverse applications while avoiding heuristic-driven processes. We experimentally demonstrate the versatility and efficacy of L3Ms in achieving tailored alignments for various applications.",
      "authors": [
        "Guneet S. Dhillon",
        "Xingjian Shi",
        "Yee Whye Teh",
        "Alex Smola"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=ULGbw2URE3",
      "cdate": 1727484513257,
      "mdate": 1742118507119,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834388"
    },
    {
      "id": "qssVptHTPN",
      "title": "Locality Alignment Improves Vision-Language Models",
      "abstract": "Vision language models (VLMs) have seen growing adoption in recent years, but many still struggle with basic spatial reasoning errors. We hypothesize that this is due to VLMs adopting pre-trained vision backbones, specifically vision transformers (ViTs) trained with image-level supervision and minimal inductive biases. Such models may fail to encode the class contents at each position in the image, and our goal is to resolve this with a vision backbone that effectively captures both local and global image semantics. Our main insight is that we do not require new supervision to learn this capability – pre-trained models contain significant knowledge of local semantics that we can extract and use for scalable self-supervision. We propose a new efficient post-training stage for ViTs called locality alignment and a novel fine-tuning procedure called MaskEmbed that uses a masked reconstruction loss to learn semantic contributions for each image patch. We first evaluate locality alignment with a vision-only benchmark, finding that it improves a model’s performance at patch-level semantic segmentation, especially for strong backbones trained with image-caption pairs (e.g., CLIP and SigLIP). We then train a series of VLMs with and without locality alignment, and show that locality-aligned backbones improve performance across a range of benchmarks, particularly ones that involve spatial understanding (e.g., RefCOCO, OCID-Ref, TallyQA, VSR, AI2D). Overall, we demonstrate that we can efficiently learn local semantic extraction via a locality alignment stage, and that this procedure benefits VLM training recipes that use off-the-shelf vision backbones.",
      "authors": [
        "Ian Connick Covert",
        "Tony Sun",
        "James Zou",
        "Tatsunori Hashimoto"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=qssVptHTPN",
      "cdate": 1727484421247,
      "mdate": 1741057033233,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834393"
    },
    {
      "id": "goFpCuJalN",
      "title": "ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models",
      "abstract": "The use of Large Language Models (LLMs) in climate science has recently gained significant attention. However, a critical issue remains: the lack of a comprehensive evaluation framework capable of assessing the quality and scientific validity of model outputs. To address this issue, we develop *ClimaGen* (Climate QA Generator), an adaptive learning framework that generates question-answer pairs from graduate textbooks with climate scientists in the loop. As a result, we present *ClimaQA-Gold*, an expert-annotated benchmark dataset alongside *ClimaQA-Silver*, a large-scale, comprehensive synthetic QA dataset for climate science. Finally, we develop evaluation strategies and compare different LLMs on our benchmarks. Our results offer novel insights into various approaches used to enhance knowledge of climate LLMs. ClimaQA’s source code is publicly available at https://github.com/Rose-STL-Lab/genie-climaqa",
      "authors": [
        "Veeramakali Vignesh Manivannan",
        "Yasaman Jafari",
        "Srikar Eranky",
        "Spencer Ho",
        "Rose Yu",
        "Duncan Watson-Parris",
        "Yian Ma",
        "Leon Bergen",
        "Taylor Berg-Kirkpatrick"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=goFpCuJalN",
      "cdate": 1727484067547,
      "mdate": 1740890358413,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834399"
    },
    {
      "id": "DC8bsa9bzY",
      "title": "Estimating the Probabilities of Rare Outputs in Language Models",
      "abstract": "We consider the problem of *low probability estimation*: given a machine learning model and a formally-specified input distribution, how can we estimate the probability of a binary property of the model's output, even when that probability is too small to estimate by random sampling? This problem is motivated by the need to improve worst-case performance, which distribution shift can make much more likely. We study low probability estimation in the context of argmax sampling from small transformer language models. We compare two types of methods: importance sampling, which involves searching for inputs giving rise to the rare output, and activation extrapolation, which involves extrapolating a probability distribution fit to the model's logits. We find that importance sampling outperforms activation extrapolation, but both outperform naive sampling. Finally, we explain how minimizing the probability estimate of an undesirable behavior generalizes adversarial training, and argue that new methods for low probability estimation are needed to provide stronger guarantees about worst-case performance.",
      "authors": [
        "Gabriel Wu",
        "Jacob Hilton"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=DC8bsa9bzY",
      "cdate": 1727483831321,
      "mdate": 1739261778839,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834404"
    },
    {
      "id": "BI2int5SAC",
      "title": "Human-inspired Episodic Memory for Infinite Context LLMs",
      "abstract": "Large language models (LLMs) have shown remarkable capabilities, but still struggle with processing extensive contexts, limiting their ability to maintain coherence and accuracy over long sequences. In contrast, the human brain excels at organising and retrieving episodic experiences across vast temporal scales, spanning a lifetime. In this work, we introduce EM-LLM, a novel approach that integrates key aspects of human episodic memory and event cognition into LLMs with no fine-tuning, enabling them to handle practically infinite context lengths while maintaining computational efficiency. EM-LLM organises sequences of tokens into coherent episodic events using a combination of Bayesian surprise and graph-theoretic boundary refinement in an online fashion. When needed, these events are retrieved through a two-stage memory process, combining similarity-based and temporally contiguous retrieval for efficient, human-inspired access to relevant information. Experiments on the LongBench and $\\infty$-Bench benchmarks demonstrate EM-LLM's superior performance, consistently outperforming the state-of-the-art retrieval model InfLLM across various baseline LLMs. In addition, EM-LLM outperforms its popular counterpart, RAG, in a wide range of tasks, while requiring similar resources. Notably, EM-LLM's performance even surpasses full-context models in most tasks, while successfully performing retrieval across 10 million tokens -- a scale computationally infeasible for such models. Finally, our analysis reveals strong correlations between EM-LLM's event segmentation and human-perceived events, suggesting parallels between this artificial system and its biological counterpart, thereby offering a novel computational framework for exploring human memory mechanisms.",
      "authors": [
        "Zafeirios Fountas",
        "Martin Benfeghoul",
        "Adnan Oomerjee",
        "Fenia Christopoulou",
        "Gerasimos Lampouras",
        "Haitham Bou Ammar",
        "Jun Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=BI2int5SAC",
      "cdate": 1727483533974,
      "mdate": 1742391258986,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834409"
    },
    {
      "id": "XwUrzurG94",
      "title": "Rapidly Adapting Policies to the Real-World via Simulation-Guided Fine-Tuning",
      "abstract": "Robot learning requires a considerable amount of high-quality data to realize the promise of generalization. However, large data sets are costly to collect in the real world. Physics simulators can cheaply generate vast data sets with broad coverage over states, actions, and environments. However, physics engines are fundamentally misspecified approximations to reality. This makes direct zero-shot transfer from simulation to reality challenging, especially in tasks where precise and force-sensitive manipulation is necessary. Thus, fine-tuning these policies with small real-world data sets is an appealing pathway for scaling robot learning. However, current reinforcement learning fine-tuning frameworks leverage general, unstructured exploration strategies which are too inefficient to make real-world adaptation practical. This paper introduces the \\emph{Simulation-Guided Fine-tuning} (SGFT) framework, which demonstrates how to extract structural priors from physics simulators to substantially accelerate real-world adaptation. Specifically, our approach uses a value function learned in simulation to guide real-world exploration. We demonstrate this approach across five real-world dexterous manipulation tasks where zero-shot sim-to-real transfer fails. We further demonstrate our framework substantially outperforms baseline fine-tuning methods, requiring up to an order of magnitude fewer real-world samples and succeeding at difficult tasks where prior approaches fail entirely. Last but not least, we provide theoretical justification for this new paradigm which underpins how SGFT can rapidly learn high-performance policies in the face of large sim-to-real dynamics gaps.",
      "authors": [
        "Patrick Yin",
        "Tyler Westenbroek",
        "Ching-An Cheng",
        "Andrey Kolobov",
        "Abhishek Gupta"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=XwUrzurG94",
      "cdate": 1727482983962,
      "mdate": 1740433340202,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834415"
    },
    {
      "id": "FEpAUnS7f7",
      "title": "Empowering Users in Digital Privacy Management through Interactive LLM-Based Agents",
      "abstract": "This paper presents a novel application of large language models (LLMs) to enhance user comprehension of privacy policies through an interactive dialogue agent. We demonstrate that LLMs significantly outperform traditional models in tasks like Data Practice Identification, Choice Identification, Policy Summarization, and Privacy Question Answering, setting new benchmarks in privacy policy analysis. Building on these findings, we introduce an innovative LLM-based agent that functions as an expert system for processing website privacy policies, guiding users through complex legal language without requiring them to pose specific questions. A user study with 100 participants showed that users assisted by the agent had higher comprehension levels (mean score of 2.6 out of 3 vs. 1.8 in the control group), reduced cognitive load (task difficulty ratings of 3.2 out of 10 vs. 7.8), increased confidence in managing privacy, and completed tasks in less time (5.5 minutes vs. 15.8 minutes). This work highlights the potential of LLM-based agents to transform user interaction with privacy policies, leading to more informed consent and empowering users in the digital services landscape.",
      "authors": [
        "BOLUN SUN",
        "Yifan Zhou",
        "Haiyun Jiang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=FEpAUnS7f7",
      "cdate": 1727482810826,
      "mdate": 1740816850088,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834420"
    },
    {
      "id": "7d2JwGbxhA",
      "title": "Object-Centric Pretraining via Target Encoder Bootstrapping",
      "abstract": "Object-centric representation learning has recently been successfully applied to real-world datasets. This success can be attributed to pretrained non-object-centric foundation models, whose features serve as reconstruction targets for slot attention. However, targets must remain frozen throughout the training, which sets an upper bound on the performance object-centric models can attain. Attempts to update the target encoder by bootstrapping result in large performance drops, which can be attributed to its lack of object-centric inductive biases, causing the object-centric model’s encoder to drift away from representations useful as reconstruction targets. To address these limitations, we propose **O**bject-**CE**ntric Pretraining by Target Encoder **BO**otstrapping, a self-distillation setup for training object-centric models from scratch, on real-world data, for the first time ever. In OCEBO, the target encoder is updated as an exponential moving average of the object-centric model, thus explicitly being enriched with object-centric inductive biases introduced by slot attention while removing the upper bound on performance present in other models. We mitigate the slot collapse caused by random initialization of the target encoder by introducing a novel cross-view patch filtering approach that limits the supervision to sufficiently informative patches. When pretrained on 241k images from COCO, OCEBO achieves unsupervised object discovery performance comparable to that of object-centric models with frozen non-object-centric target encoders pretrained on hundreds of millions of images. The code and pretrained models are publicly available at https://github.com/djukicn/ocebo.",
      "authors": [
        "Nikola Đukić",
        "Tim Lebailly",
        "Tinne Tuytelaars"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=7d2JwGbxhA",
      "cdate": 1727482792187,
      "mdate": 1740890358325,
      "matched_keywords": [
        "foundation model",
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834428"
    },
    {
      "id": "G82uQztzxl",
      "title": "Self-Normalized Resets for Plasticity in Continual Learning",
      "abstract": "Plasticity Loss is an increasingly important phenomenon that refers to the empirical observation that as a neural network is continually trained on a sequence of changing tasks, its ability to adapt to a new task diminishes over time. We introduce Self-Normalized Resets (SNR), a simple adaptive algorithm that mitigates plasticity loss by resetting a neuron’s weights when evidence suggests its firing rate has effectively dropped to zero. Across a battery of continual learning problems and network architectures, we demonstrate that SNR consistently attains superior performance compared to its competitor algorithms. We also demonstrate that SNR is robust to its sole hyperparameter, its rejection percentile threshold, while competitor algorithms show significant sensitivity. SNR’s threshold-based reset mechanism is motivated by a simple hypothesis test we derive. Seen through the lens of this hypothesis test, competing reset proposals yield suboptimal error rates in correctly detecting inactive neurons, potentially explaining our experimental observations. We also conduct a theoretical investigation of the optimization landscape for the problem of learning a single ReLU. We show that even when initialized adversarially, an idealized version of SNR learns the target ReLU, while regularization based approaches can fail to learn.",
      "authors": [
        "Vivek Farias",
        "Adam Daniel Jozefiak"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=G82uQztzxl",
      "cdate": 1727482345471,
      "mdate": 1740851160050,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834434"
    },
    {
      "id": "qFw2RFJS5g",
      "title": "Homomorphism Counts as Structural Encodings for Graph Learning",
      "abstract": "Graph Transformers are popular neural networks that extend the well-known Transformer architecture to the graph domain. These architectures operate by applying self-attention on graph nodes and incorporating graph structure through the use of positional encodings (e.g., Laplacian positional encoding) or structural encodings (e.g., random-walk structural encoding). The quality of such encodings is critical, since they provide the necessary \\emph{graph inductive biases} to condition the model on graph structure. In this work, we propose \\emph{motif structural encoding} (MoSE) as a flexible and powerful structural encoding framework based on counting graph homomorphisms. Theoretically, we compare the expressive power of MoSE to random-walk structural encoding and relate both encodings to the expressive power of standard message passing neural networks. Empirically, we observe that MoSE outperforms other well-known positional and structural encodings across a range of architectures, and it achieves state-of-the-art performance on a widely studied molecular property prediction dataset.",
      "authors": [
        "Linus Bao",
        "Emily Jin",
        "Michael M. Bronstein",
        "Ismail Ilkan Ceylan",
        "Matthias Lanzinger"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=qFw2RFJS5g",
      "cdate": 1727482323045,
      "mdate": 1739261777762,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834439"
    },
    {
      "id": "msEr27EejF",
      "title": "Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking",
      "abstract": "Because it is difficult to precisely specify complex objectives, reinforcement learning policies are often optimized using proxy reward functions that only approximate the true goal. However, optimizing proxy rewards frequently leads to reward hacking: the optimized reward function ceases to be a good proxy and the resulting policy performs poorly with respect to the unspecified true reward. Principled solutions to reward hacking have been impeded by the lack of a good definition for the problem. To address this gap, we introduce a definition of reward hacking based on the correlation between proxy and true rewards for states and actions seen by a “reference policy” that breaks down under optimization. We show that this definition captures reward hacking behavior across several realistic settings, including in reinforcement learning from human feedback (RLHF). Using our formulation, we show theoretically that regularization to the reference policy can effectively prevent reward hacking. While the current practice in RLHF applies a KL penalty between action distributions for this purpose, our theory suggests regularizing the χ2 divergence between the policies’ occupancy measures can be more effective. We intuitively show the benefits of this type of regularization and demonstrate that it better mitigates reward hacking in practice across four realistic settings, including RLHF. Our code is available at https://github.com/cassidylaidlaw/orpo.",
      "authors": [
        "Cassidy Laidlaw",
        "Shivam Singhal",
        "Anca Dragan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=msEr27EejF",
      "cdate": 1727482228814,
      "mdate": 1741886535977,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834444"
    },
    {
      "id": "CAssIgPN4I",
      "title": "Real2Code: Reconstruct Articulated Objects via Code Generation",
      "abstract": "We present Real2Code, a novel approach to reconstructing articulated objects via code generation. Given visual observations of an object, we first reconstruct its part geometry using image segmentation and shape completion. We represent these object parts with oriented bounding boxes, from which a fine-tuned large language model (LLM) predicts joint articulation as code. By leveraging pre-trained vision and language models, our approach scales elegantly with the number of articulated parts, and generalizes from synthetic training data to real world objects in unstructured environments. Experimental results demonstrate that Real2Code significantly outperforms the previous state-of-the-art in terms of reconstruction accuracy, and is the first approach to extrapolate beyond objects' structural complexity in the training set, as we show for objects with up to 10 articulated parts. When incorporated with a stereo reconstruction model, Real2Code moreover generalizes to real-world objects, given only a handful of multi-view RGB images and without the need for depth or camera information.",
      "authors": [
        "Zhao Mandi",
        "Yijia Weng",
        "Dominik Bauer",
        "Shuran Song"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=CAssIgPN4I",
      "cdate": 1727482082898,
      "mdate": 1740884951874,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834451"
    },
    {
      "id": "HPSAkIHRbb",
      "title": "BingoGuard: LLM Content Moderation Tools with Risk Levels",
      "abstract": "Malicious content generated by large language models (LLMs) can pose varying degrees of harm. \nAlthough existing LLM-based moderators can detect harmful content, they struggle to assess risk levels and may miss lower-risk outputs. \nAccurate risk assessment allows platforms with different safety thresholds to tailor content filtering and rejection. In this paper, we introduce per-topic severity rubrics for 11 harmful topics and build BingoGuard, an LLM-based moderation system designed to predict both binary safety labels and severity levels. \nTo address the lack of annotations on levels of severity, we propose a scalable generate-then-filter framework that first generates responses across different severity levels and then filters out low-quality responses. Using this framework, we create BingoGuardTrain, a training dataset with 54,897 examples covering a variety of topics, response severity, styles, and BingoGuardTest, a test set with 988 examples explicitly labeled based on our severity rubrics that enables fine-grained analysis on model behaviors on different severity levels. Our BingoGuard-8B, trained on BingoGuardTrain, achieves the state-of-the-art performance on several moderation benchmarks, including WildGuardTest and HarmBench, as well as BingoGuardTest, outperforming best public models, WildGuard, by 4.3\\%. Our analysis demonstrates that incorporating severity levels into training significantly enhances detection performance and enables the model to effectively gauge the severity of harmful responses. Warning: this paper includes red-teaming examples that may be harmful in nature.",
      "authors": [
        "Fan Yin",
        "Philippe Laban",
        "XIANGYU PENG",
        "Yilun Zhou",
        "Yixin Mao",
        "Vaibhav Vats",
        "Linnea Ross",
        "Divyansh Agarwal",
        "Caiming Xiong",
        "Chien-Sheng Wu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=HPSAkIHRbb",
      "cdate": 1727481786492,
      "mdate": 1740901046627,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834457"
    },
    {
      "id": "din0lGfZFd",
      "title": "Reasoning with Latent Thoughts: On the Power of Looped Transformers",
      "abstract": "Large language models have shown remarkable reasoning abilities and scaling laws suggest that large parameter count, especially along the depth axis, is the primary driver. In this work, we make a stronger claim --- many reasoning problems require a large depth but not necessarily many parameters. This unlocks a novel application of looped models for reasoning. Firstly, we show that for many synthetic reasoning problems like addition, $p$-hop induction, and math problems, a $k$-layer transformer looped $L$ times nearly matches the performance of a $kL$-layer non-looped model, and is significantly better than a $k$-layer model. This is further corroborated by theoretical results showing that many such reasoning problems can be solved via iterative algorithms, and thus, can be solved effectively using looped models with nearly optimal depth. Perhaps surprisingly, these benefits also translate to practical settings of language modeling --- on many downstream reasoning tasks, a language model with $k$-layers looped $L$ times can be competitive to, if not better than, a $kL$-layer language model. In fact, our empirical analysis reveals an intriguing phenomenon: looped and non-looped models exhibit scaling behavior that depends on their effective depth, akin to the inference-time scaling of chain-of-thought (CoT) reasoning. We further elucidate the connection to CoT reasoning by proving that looped models implicitly generate latent thoughts and can simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we also present an interesting dichotomy between reasoning and memorization, and design a looping-based regularization that is effective on both fronts.",
      "authors": [
        "Nikunj Saunshi",
        "Nishanth Dikkala",
        "Zhiyuan Li",
        "Sanjiv Kumar",
        "Sashank J. Reddi"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=din0lGfZFd",
      "cdate": 1727481454487,
      "mdate": 1740890358216,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834465"
    },
    {
      "id": "SThJXvucjQ",
      "title": "Conservative Contextual Bandits: Beyond Linear Representations",
      "abstract": "Conservative Contextual Bandits (CCBs) address safety in sequential decision making by requiring that an agent's policy, along with minimizing regret, also satisfies a safety constraint: the performance is not worse than a baseline policy (e.g., the policy that the company has in production) by more than $(1+\\alpha)$ factor. \nPrior work developed UCB-style\nalgorithms for this problem in the multi-armed (Wu et al., 2016)  and contextual\nlinear (Kazerouni et al., 2017) settings.\nHowever, in practice the cost of the arms\nis often a non-linear function, and therefore existing UCB algorithms are ineffective in such settings. \nIn this paper, we consider CCBs beyond the linear case and develop two algorithms $\\mathtt{C\\text{-}SquareCB}$ and $\\mathtt{C\\text{-}FastCB}$, using Inverse Gap Weighting (IGW) based exploration and an online regression oracle. \nWe show that the safety constraint is satisfied in high probability and that the regret for $\\mathtt{C\\text{-}SquareCB}$ is sub-linear in horizon $T$, while the the regret for $\\mathtt{C\\text{-}FastCB}$ is first-order and is sub-linear in $L^*$, the cumulative loss of the optimal policy. \nSubsequently, we use a neural network for function approximation and online gradient descent as the regression oracle to provide $\\tilde{\\mathcal{O}}\\big(\\sqrt{KT} + K/\\alpha\\big) $ and $\\tilde{\\mathcal{O}}\\big(\\sqrt{KL^*} + K (1 + 1/\\alpha)\\big)$ regret bounds respectively. \nFinally, we demonstrate the efficacy of our algorithms on real world data, and show that they significantly outperform the existing baseline while maintaining the performance guarantee.",
      "authors": [
        "Rohan Deb",
        "Mohammad Ghavamzadeh",
        "Arindam Banerjee"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=SThJXvucjQ",
      "cdate": 1727481350145,
      "mdate": 1743483480715,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834470"
    },
    {
      "id": "jpSLXoRKnH",
      "title": "Quantifying Generalization Complexity for Large Language Models",
      "abstract": "While large language models (LLMs) have shown exceptional capabilities in understanding complex queries\nand performing sophisticated tasks, their generalization abilities are often deeply entangled with \nmemorization, necessitating more precise evaluation.\nTo address this challenge, we introduce Scylla, a dynamic evaluation framework that quantitatively measures the generalization abilities of LLMs. Scylla disentangles generalization from memorization via assessing model performance on both in-distribution (ID) and out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity.\nThrough extensive experiments, we uncover a non-monotonic relationship between task complexity and the performance gap between ID and OOD\ndata, which we term the generalization valley.\nSpecifically, this phenomenon reveals a critical threshold---referred to\nas critical complexity---where reliance on non-generalizable behavior peaks, indicating the\nupper bound of LLMs' generalization capabilities.\nAs model size increases, the critical complexity shifts toward higher levels of task complexity, \nsuggesting that larger models can handle more complex reasoning tasks before over-relying on\nmemorization.\nLeveraging Scylla and the concept of critical complexity, we benchmark 28 LLMs including \nboth open-sourced models such as LLaMA and Qwen families, and closed-sourced models like Claude and \nGPT, providing a more robust evaluation and establishing a clearer \nunderstanding of LLMs' generalization capabilities.",
      "authors": [
        "Zhenting Qi",
        "Hongyin Luo",
        "Xuliang Huang",
        "Zhuokai Zhao",
        "Yibo Jiang",
        "Xiangjun Fan",
        "Himabindu Lakkaraju",
        "James R. Glass"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=jpSLXoRKnH",
      "cdate": 1727481350057,
      "mdate": 1740287747799,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834476"
    },
    {
      "id": "9HK2rHNAhd",
      "title": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget",
      "abstract": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has been considered critical to saving the cost of inference. Most of the existing KV-cache compression algorithms attempted to sparsify the sequence of tokens by taking advantage of the different importance of tokens. However, most of these methods treat all layers equally, allocating the same KV budget to each layer. This approach is suboptimal, as some layers may be less sensitive to input tokens yet still receive the same budget as others. In this work, we found that by identifying the importance of attention layers, we could optimize the KV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based on our observations regarding layer-wise importance in inference, we propose \\sys to precisely optimize the allocation of KV-cache budget among layers on-the-fly and then incorporate three representative sequence-wise algorithms to compress the KV-cache for each layer with its very own budget. Specifically, we first measure each layer's importance by calculating the cosine similarity of the input prompt differences before and after the self-attention layers. Based on this similarity, we then categorize the layers into two groups and adjust their KV budgets accordingly. By optimizing the KV-cache from both sequence's and layer's dimensions, \\sys achieves around 30\\% to 70\\% of the memory reductions and up to 2.2 $\\times$ of throughput improvements in a wide range of LLMs and benchmarks. The code is available at https://github.com/hetailang/SqueezeAttention.",
      "authors": [
        "Zihao Wang",
        "Bin CUI",
        "Shaoduo Gan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=9HK2rHNAhd",
      "cdate": 1727481227546,
      "mdate": 1740750554620,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834481"
    },
    {
      "id": "h0ZfDIrj7T",
      "title": "Mixture-of-Agents Enhances Large Language Model Capabilities",
      "abstract": "Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, Arena-Hard, MT-Bench, and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs achieves a score of 65.1% on AlpacaEval 2.0 compared to 57.5% by GPT-4 Omni.",
      "authors": [
        "Junlin Wang",
        "Jue WANG",
        "Ben Athiwaratkun",
        "Ce Zhang",
        "James Zou"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=h0ZfDIrj7T",
      "cdate": 1727480698855,
      "mdate": 1740766464539,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834491"
    },
    {
      "id": "DpLFmc09pC",
      "title": "DEPfold: RNA Secondary Structure Prediction as Dependency Parsing.",
      "abstract": "RNA secondary structure prediction is critical for understanding RNA function\nbut remains challenging due to complex structural elements like pseudoknots and\nlimited training data. We introduce DEPfold, a novel deep learning approach that\nre-frames RNA secondary structure prediction as a dependency parsing problem.\nDEPfold presents three key innovations: (1) a biologically motivated transformation of RNA structures into labeled dependency trees, (2) a biaffine attention\nmechanism for joint prediction of base pairings and their types, and (3) an optimal\ntree decoding algorithm that enforces valid RNA structural constraints. Unlike traditional energy-based methods, DEPfold learns directly from annotated data and\nleverages pretrained language models to predict RNA structure. We evaluate DEPfold on both within-family and cross-family RNA datasets, demonstrating significant performance improvements over existing methods. DEPfold shows strong\nperformance in cross-family generalization when trained on data augmented by\ntraditional energy-based models, outperforming existing methods on the bpRNAnew dataset. This demonstrates DEPfold’s ability to effectively learn structural\ninformation beyond what traditional methods capture. Our approach bridges natural language processing (NLP) with RNA biology, providing a computationally\nefficient and adaptable tool for advancing RNA structure prediction and analysis",
      "authors": [
        "KE WANG",
        "Shay B Cohen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=DpLFmc09pC",
      "cdate": 1727480656687,
      "mdate": 1741271677433,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834497"
    },
    {
      "id": "xoXn62FzD0",
      "title": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo",
      "abstract": "A wide range of LM applications require generating text that conforms to syntactic or semantic constraints. Imposing such constraints can be naturally framed as _probabilistic conditioning_, but exact generation from the resulting distribution—which can differ substantially from the LM’s base distribution—is generally intractable. In this work,\nwe develop an architecture for controlled LM generation based on sequential Monte Carlo (SMC). Our SMC framework allows us to flexibly incorporate domain- and problem-specific constraints at inference time, and efficiently reallocate computational resources in light of new information during the course of generation. By comparing to a number of alternatives and ablations on four challenging domains---Python code generation for data science, text-to-SQL, goal inference, and molecule synthesis—we demonstrate that, with little overhead, our approach allows small open-source language models to outperform models over 8$\\times$ larger, as well as closed-source, fine-tuned ones. \nIn support of the probabilistic perspective, we show that these performance improvements are driven by better approximation to the posterior distribution. \n[Our system](https://github.com/probcomp/genlm-control) builds on the framework of Lew et al. (2023) and integrates with its _language model probabilistic programming language_, giving users a simple, programmable way to apply SMC to a broad variety of controlled generation problems.",
      "authors": [
        "João Loula",
        "Benjamin LeBrun",
        "Li Du",
        "Ben Lipkin",
        "Clemente Pasti",
        "Gabriel Grand",
        "Tianyu Liu",
        "Yahya Emara",
        "Marjorie Freedman",
        "Jason Eisner",
        "Ryan Cotterell",
        "Vikash Mansinghka",
        "Alexander K. Lew",
        "Tim Vieira",
        "Timothy J. O'Donnell"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=xoXn62FzD0",
      "cdate": 1727480499375,
      "mdate": 1743525137095,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834502"
    },
    {
      "id": "6NNA0MxhCH",
      "title": "Answer, Assemble, Ace: Understanding How LMs Answer Multiple Choice Questions",
      "abstract": "Multiple-choice question answering (MCQA) is a key competence of performant transformer language models that is tested by mainstream benchmarks. However, recent evidence shows that models can have quite a range of performance, particularly when the task format is diversified slightly (such as by shuffling answer choice order). In this work we ask: how do successful models perform formatted MCQA? We employ vocabulary projection and activation patching methods to localize key hidden states that encode relevant information for predicting the correct answer. We find that prediction of a specific answer symbol is causally attributed to a few middle layers, and specifically their multi-head self-attention mechanisms. We show that subsequent layers increase the probability of the predicted answer symbol in vocabulary space, and that this probability increase is associated with a sparse set of attention heads with unique roles. We additionally uncover differences in how different models adjust to alternative symbols. Finally, we demonstrate that a synthetic task can disentangle sources of model error to pinpoint when a model has learned formatted MCQA, and show that logit differences between answer choice tokens continue to grow over the course of training.",
      "authors": [
        "Sarah Wiegreffe",
        "Oyvind Tafjord",
        "Yonatan Belinkov",
        "Hannaneh Hajishirzi",
        "Ashish Sabharwal"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=6NNA0MxhCH",
      "cdate": 1727480464630,
      "mdate": 1740899276836,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834507"
    },
    {
      "id": "SPS6HzVzyt",
      "title": "Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance",
      "abstract": "Large Language Model's are instruction-finetuned to enhance their ability to follow user instructions and better comprehend input context. Still, they often struggle to follow the input context, especially when it contradicts model's parametric knowledge. This manifests as various failures, such as hallucinations where a model inserts outdated or unwarranted facts into its response. In this work, we observe an intriguing phenomenon: the context reliance of the model decreases as instruction finetuning progresses, $\\textit{despite an initial expected increase}$. We call this phenomenon as the $\\textbf{context-parametric inversion}$. This is surprising, as one would expect instruction tuning to improve the model's ability to follow input instructions.  We observe this behavior on multiple general purpose instruction tuning datasets such as TULU, Alpaca and Ultrachat, across multiple model families like Llama, Mistral and Pythia.  We perform various controlled studies to eliminate some simple hypothesis for this observed behavior and isolate what datapoints cause this counter-intuitive behavior. We then analyze the phenomenon theoretically, to explain why context reliance varies across the trajectory of finetuning. \nWe tie the observed context-parametric inversion to the properties of the finetuning data, which provides us with some potential mitigation strategies that provide limited but insightful gains.",
      "authors": [
        "Sachin Goyal",
        "Christina Baek",
        "J Zico Kolter",
        "Aditi Raghunathan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=SPS6HzVzyt",
      "cdate": 1727479627580,
      "mdate": 1740871130514,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834513"
    },
    {
      "id": "Sh4FOyZRpv",
      "title": "CTSyn: A Foundation Model for Cross Tabular Data Generation",
      "abstract": "Generative Foundation Models (GFMs) have achieved remarkable success in producing high-quality synthetic data for images and text. However, their application to tabular data presents significant challenges due to the heterogeneous nature of table features. Current cross-table learning frameworks struggle because they lack a generative model backbone and an effective mechanism to decode heterogeneous feature values. To address these challenges, we propose the Cross-Table Synthesizer (CTSyn), a diffusion-based generative foundation model for tabular data generation. CTSyn comprises two key components. The first is an autoencoder network that consolidates diverse tables into a unified latent space. It dynamically reconstructs table values using a table schema embedding, allowing adaptation to heterogeneous datasets. The second is a conditional latent diffusion model that generates samples from the learned latent space, conditioned on the table schema. Through large-scale pre-training, CTSyn outperforms existing table synthesizers on standard benchmarks in both utility and diversity.  These results position CTSyn as a promising framework for synthetic table generation and lay the groundwork for developing large-scale tabular foundation models.",
      "authors": [
        "Xiaofeng Lin",
        "Chenheng Xu",
        "Matthew Yang",
        "Guang Cheng"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Sh4FOyZRpv",
      "cdate": 1727479475877,
      "mdate": 1741014823510,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834518"
    },
    {
      "id": "4ZX2a3OKEV",
      "title": "Solving hidden monotone variational inequalities with surrogate losses",
      "abstract": "Deep learning has proven to be effective in a wide variety of loss minimization problems.\nHowever, many applications of interest, like minimizing projected Bellman error and min-max optimization, cannot be modelled as minimizing a scalar loss function but instead correspond to solving a variational inequality (VI) problem.\nThis difference in setting has caused many practical challenges as naive gradient-based approaches from supervised learning tend to diverge and cycle in the VI case.\nIn this work, we propose a principled surrogate-based approach compatible with deep learning to solve VIs.\nWe show that our surrogate-based approach has three main benefits: (1) under assumptions that are realistic in practice (when hidden monotone structure is present, interpolation, and sufficient optimization of the surrogates), it guarantees convergence, (2) it provides a unifying perspective of existing methods, and (3) is amenable to existing deep learning optimizers like ADAM.\nExperimentally, we demonstrate our surrogate-based approach is effective in min-max optimization and minimizing projected Bellman error. Furthermore, in the deep reinforcement learning case, we propose a novel variant of TD(0) which is more compute and sample efficient.",
      "authors": [
        "Ryan D'Orazio",
        "Danilo Vucetic",
        "Zichu Liu",
        "Junhyung Lyle Kim",
        "Ioannis Mitliagkas",
        "Gauthier Gidel"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=4ZX2a3OKEV",
      "cdate": 1727479432285,
      "mdate": 1740759322044,
      "matched_keywords": [
        "reinforcement learning",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834524"
    },
    {
      "id": "SFN6Wm7YBI",
      "title": "TorchTitan: One-stop PyTorch native solution for production ready LLM pretraining",
      "abstract": "The development of large language models (LLMs) has been instrumental in advancing state-of-the-art natural language processing applications. Training LLMs with billions of parameters and trillions of tokens requires sophisticated distributed systems that enable composing and comparing several state-of-the-art techniques in order to efficiently scale across thousands of accelerators. However, existing solutions are complex, scattered across multiple libraries/repositories, lack interoperability, and are cumbersome to maintain. Thus, curating and empirically comparing training recipes requires non-trivial engineering effort.\n\nThis paper introduces **TORCHTITAN**$^1$, a PyTorch-native distributed training system that unifies and advances state-of-the-art techniques, streamlining integration and reducing engineering overhead. TORCHTITAN enables seamless application of 4D parallelism in a modular and composable manner, while featuring elastic scaling to adapt to changing computational requirements. The system provides comprehensive logging, efficient checkpointing, and debugging tools, ensuring production-ready training. Moreover, TORCHTITAN incorporates innovative hardware-software co-designed solutions, leveraging cutting-edge features like Float8 training and SymmetricMemory to maximize hardware utilization.\n\nAs a flexible experimental test bed, TORCHTITAN facilitates the curation and comparison of custom recipes for diverse training contexts. By leveraging TORCHTITAN, we developed optimized training recipes for the Llama 3.1 family and provide actionable guidance on selecting and combining distributed training techniques to maximize training efficiency, based on our hands-on experiences.\n\nWe thoroughly assess TORCHTITAN on the Llama 3.1 family of LLMs, spanning 8 billion to 405 billion parameters, and showcase its exceptional performance, modular composability, and elastic scalability. By stacking training optimizations, we demonstrate accelerations ranging from 65.08% on Llama 3.1 8B at 128 GPU scale (1D), 12.59% on Llama 3.1 70B at 256 GPU scale (2D), to 30% on Llama 3.1 405B at 512 GPU scale (3D) on NVIDIA H100 GPUs over optimized baselines. We also demonstrate the effectiveness of 4D parallelism in enabling long context training.\n\n$^1$ GitHub: [https://github.com/pytorch/torchtitan](https://github.com/pytorch/torchtitan)",
      "authors": [
        "Wanchao Liang",
        "Tianyu Liu",
        "Less Wright",
        "Will Constable",
        "Andrew Gu",
        "Chien-Chin Huang",
        "Iris Zhang",
        "Wei Feng",
        "Howard Huang",
        "Junjie Wang",
        "Sanket Purandare",
        "Gokul Nadathur",
        "Stratos Idreos"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=SFN6Wm7YBI",
      "cdate": 1727479133253,
      "mdate": 1743485688414,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834529"
    },
    {
      "id": "nrvoWOWcyg",
      "title": "Chunk-Distilled Language Modeling",
      "abstract": "We introduce Chunk-Distilled Language Modeling (CD-LM), an approach to text generation that addresses two challenges in current large language models (LLMs): the inefficiency of token-level generation, and the difficulty of adapting to new data and knowledge. Our method combines deep network-based LLMs with a straightforward retrieval module, which allows the generation of multi-token text chunks at a single decoding step. Our retrieval framework enables flexible construction of model- or domain-specific datastores, either leveraging the internal knowledge of existing models, or incorporating expert insights from human-annotated corpora. This adaptability allows for enhanced control over the language model's distribution without necessitating additional training. We present the CD-LM formulation along with performance metrics demonstrating its ability to improve language model performance and efficiency across a diverse set of downstream applications. Code and data will be made publicly available.",
      "authors": [
        "Yanhong Li",
        "Karen Livescu",
        "Jiawei Zhou"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=nrvoWOWcyg",
      "cdate": 1727479002726,
      "mdate": 1742128128434,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834534"
    },
    {
      "id": "FPBce2P1er",
      "title": "When does compositional structure yield compositional generalization? A kernel theory.",
      "abstract": "Compositional generalization (the ability to respond correctly to novel combinations of familiar components) is thought to be a cornerstone of intelligent behavior. Compositionally structured (e.g. disentangled) representations support this ability; however, the conditions under which they are sufficient for the emergence of compositional generalization remain unclear. To address this gap, we present a theory of compositional generalization in kernel models with fixed, compositionally structured representations. This provides a tractable framework for characterizing the impact of training data statistics on generalization. We find that these models are limited to functions that assign values to each combination of components seen during training, and then sum up these values (\"conjunction-wise additivity\"). This imposes fundamental restrictions on the set of tasks compositionally structured kernel models can learn, in particular preventing them from transitively generalizing equivalence relations. Even for compositional tasks that they can learn in principle, we identify novel failure modes in compositional generalization (memorization leak and shortcut bias) that arise from biases in the training data. Finally, we empirically validate our theory, showing that it captures the behavior of deep neural networks (convolutional networks, residual networks, and Vision Transformers) trained on a set of compositional tasks with similarly structured data. Ultimately, this work examines how statistical structure in the training data can affect compositional generalization, with implications for how to identify and remedy failure modes in deep learning models.",
      "authors": [
        "Samuel Lippl",
        "Kim Stachenfeld"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=FPBce2P1er",
      "cdate": 1727478997459,
      "mdate": 1743487225449,
      "matched_keywords": [
        "transformer",
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834539"
    },
    {
      "id": "RnJY9WcpA3",
      "title": "Sensor-Invariant Tactile Representation",
      "abstract": "High-resolution tactile sensors have become critical for embodied perception and robotic manipulation. \nHowever, a key challenge in the field is the lack of transferability between sensors due to design and manufacturing variations, which result in significant differences in tactile signals. \nThis limitation hinders the ability to transfer models or knowledge learned from one sensor to another. \nTo address this, we introduce a novel method for extracting Sensor-Invariant Tactile Representations (SITR), enabling zero-shot transfer across optical tactile sensors. \nOur approach utilizes a transformer-based architecture trained on a diverse dataset of simulated sensor designs, allowing it to generalize to new sensors in the real world with minimal calibration. \nExperimental results demonstrate the method’s effectiveness across various tactile sensing applications, facilitating data and model transferability for future advancements in the field.",
      "authors": [
        "Harsh Gupta",
        "Yuchen Mo",
        "Shengmiao Jin",
        "Wenzhen Yuan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=RnJY9WcpA3",
      "cdate": 1727478775295,
      "mdate": 1741829757379,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834550"
    },
    {
      "id": "6LtdZCyuZR",
      "title": "NutriBench: A Dataset for Evaluating Large Language Models in Nutrition Estimation from Meal Descriptions",
      "abstract": "Accurate nutrition estimation helps people make informed dietary choices and is essential in the prevention of serious health complications. We present NutriBench, the first publicly available natural language meal description nutrition benchmark. NutriBench consists of 11,857 meal descriptions generated from real-world global dietary intake data. The data is human-verified and annotated with macro-nutrient labels, including carbohydrates, proteins, fats, and calories. We conduct an extensive evaluation of Nutribench on the task of carbohydrate estimation, testing twelve leading Large Language Models (LLMs), including GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using standard, Chain-of-Thought and Retrieval-Augmented Generation strategies. Additionally, we present a study involving professional nutritionists, finding that LLMs can provide comparable but significantly faster estimates. Finally, we perform a real-world risk assessment by simulating the effect of carbohydrate predictions on the blood glucose levels of individuals with type 1 diabetes. Our work highlights the opportunities and challenges of using LLMs for nutrition estimation, demonstrating their potential to aid professionals and laypersons and improve health outcomes. Our benchmark is publicly available at: https://mehak126.github.io/nutribench.html",
      "authors": [
        "Mehak Preet Dhaliwal",
        "Andong Hua",
        "Laya Pullela",
        "Ryan Burke",
        "Yao Qin"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=6LtdZCyuZR",
      "cdate": 1727478771104,
      "mdate": 1740817458859,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834556"
    },
    {
      "id": "YbURbViE7l",
      "title": "GOttack: Universal Adversarial Attacks on Graph Neural Networks via Graph Orbits Learning",
      "abstract": "Graph Neural Networks (GNNs) have demonstrated superior performance in node classification tasks across diverse applications. However, their vulnerability to adversarial attacks, where minor perturbations can mislead model predictions, poses significant challenges. This study introduces GOttack, a novel adversarial attack framework that exploits the topological structure of graphs to undermine the integrity of GNN predictions systematically. \n\nBy defining a topology-aware method to manipulate graph orbits, our approach generates adversarial modifications that are both subtle and effective, posing a severe test to the robustness of GNNs. We evaluate the efficacy of GOttack across multiple prominent GNN architectures using standard benchmark datasets. Our results show that GOttack outperforms existing state-of-the-art adversarial techniques and completes training in approximately 55% of the time required by the fastest competing model, achieving the highest average misclassification rate in 155 tasks. \nThis work not only sheds light on the susceptibility of GNNs to structured adversarial attacks \nbut also shows that certain topological patterns may play a significant role in the underlying robustness of the GNNs. Our Python implementation is shared at https://github.com/cakcora/GOttack.",
      "authors": [
        "Zulfikar Alom",
        "Tran Gia Bao Ngo",
        "Murat Kantarcioglu",
        "Cuneyt Gurcan Akcora"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=YbURbViE7l",
      "cdate": 1727478737812,
      "mdate": 1740711686040,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834561"
    },
    {
      "id": "hJVdwBpWjt",
      "title": "NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics",
      "abstract": "Large language models (LLMs) prompted with text and audio have achieved state-of-the-art performance across various auditory tasks, including speech, music, and general audio, showing emergent abilities on unseen tasks. However, their potential has yet to be fully demonstrated in bioacoustics tasks, such as detecting animal vocalizations in large recordings, classifying rare and endangered species, and labeling context and behavior—tasks that are crucial for conservation, biodiversity monitoring, and animal behavior studies. In this work, we present NatureLM-audio, the first audio-language foundation model specifically designed for bioacoustics. Our training dataset consists of carefully curated text-audio pairs spanning bioacoustics, speech, and music, designed to address the field's limited availability of annotated data. We demonstrate successful transfer of learned representations from music and speech to bioacoustics, and our model shows promising generalization to unseen taxa and tasks. We evaluate NatureLM-audio on a novel benchmark (BEANS-Zero) and it sets a new state of the art on several bioacoustics tasks, including zero-shot classification of unseen species. To advance bioacoustics research, we release our model weights, benchmark data, and open-source the code for training and benchmark data generation and model training.",
      "authors": [
        "David Robinson",
        "Marius Miron",
        "Masato Hagiwara",
        "Olivier Pietquin"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=hJVdwBpWjt",
      "cdate": 1727478411825,
      "mdate": 1747375960233,
      "matched_keywords": [
        "large language model",
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834567"
    },
    {
      "id": "IqHeDe2lbl",
      "title": "Sparse components distinguish visual pathways & their alignment to neural networks",
      "abstract": "The ventral, dorsal, and lateral streams in high-level human visual cortex are implicated in distinct functional processes. Yet, deep neural networks (DNNs) trained on a single task model the entire visual system surprisingly well, hinting at common computational principles across these pathways. To explore this inconsistency, we applied a novel sparse decomposition approach to identify the dominant components of visual representations within each stream. Consistent with traditional neuroscience research, we find a clear difference in component response profiles across the three visual streams—identifying components selective for faces, places, bodies, text, and food in the ventral stream; social interactions, implied motion, and hand actions in the lateral stream; and some less interpretable components in the dorsal stream. Building on this, we introduce Sparse Component Alignment (SCA), a new method for measuring representational alignment between brains and machines that better captures the latent neural tuning of these two visual systems. We find that standard visual DNNs are more aligned with ventral than either dorsal or lateral representations. SCA reveals these distinctions with greater resolution than conventional population-level geometry, offering a measure of representational alignment that is sensitive to a system’s underlying axes of neural tuning.",
      "authors": [
        "Ammar I Marvi",
        "Nancy Kanwisher",
        "Meenakshi Khosla"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=IqHeDe2lbl",
      "cdate": 1727478409257,
      "mdate": 1740847620248,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834572"
    },
    {
      "id": "Equ277PBN0",
      "title": "Privacy-Preserving Personalized Federated Prompt Learning for Multimodal Large Language Models",
      "abstract": "Multimodal Large Language Models (LLMs) are pivotal in revolutionizing customer support and operations by integrating multiple modalities such as text, images, and audio. Federated Prompt Learning (FPL) is a recently proposed approach that combines pre-trained multimodal LLMs such as vision-language models with federated learning to create personalized, privacy-preserving AI systems. However, balancing the competing goals of personalization, generalization, and privacy remains a significant challenge. Over-personalization can lead to overfitting, reducing generalizability, while stringent privacy measures, such as differential privacy, can hinder both personalization and generalization. In this paper, we propose a Differentially Private Federated Prompt Learning (DP-FPL) approach to tackle this challenge by leveraging a low-rank factorization scheme to capture generalization while maintaining a residual term that preserves expressiveness for personalization. To ensure privacy, we introduce a novel method where we apply local differential privacy to the two low-rank components of the local prompt, and global differential privacy to the global prompt. Our approach mitigates the impact of privacy noise on the model performance while balancing the tradeoff between personalization and generalization. Extensive experiments demonstrate the effectiveness of our approach over other benchmarks.",
      "authors": [
        "Linh Tran",
        "Wei Sun",
        "Stacy Patterson",
        "Ana Milanova"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Equ277PBN0",
      "cdate": 1727478363570,
      "mdate": 1747327169340,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.834577"
    },
    {
      "id": "246rHKUnnf",
      "title": "Explore Theory of Mind: program-guided adversarial data generation for theory of mind reasoning",
      "abstract": "Do large language models (LLMs) have theory of mind? A plethora of papers and benchmarks have been introduced to evaluate if current models have been able to develop this key ability of social intelligence. However, all rely on limited datasets with simple patterns that can potentially lead to problematic blind spots in evaluation and an overestimation of model capabilities. We introduce ExploreToM, the first framework to allow large-scale generation of diverse and challenging theory of mind data for robust training and evaluation. Our approach leverages an A* search over a custom domain-specific language to produce complex story structures and novel, diverse, yet plausible scenarios to stress test the limits of LLMs. Our evaluation reveals that state-of-the-art LLMs, such as Llama-3.1-70B and GPT-4o, show accuracies as low as 0% and 9% on ExploreToM-generated data, highlighting the need for more robust theory of mind evaluation. As our generations are a conceptual superset of prior work, fine-tuning on our data yields a 27-point accuracy improvement on the classic ToMi benchmark (Le et al., 2019). ExploreToM also enables uncovering underlying skills and factors missing for models to show theory of mind, such as unreliable state tracking or data imbalances, which may contribute to models' poor performance on benchmarks.",
      "authors": [
        "Melanie Sclar",
        "Jane Dwivedi-Yu",
        "Maryam Fazel-Zarandi",
        "Yulia Tsvetkov",
        "Yonatan Bisk",
        "Yejin Choi",
        "Asli Celikyilmaz"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=246rHKUnnf",
      "cdate": 1727478160517,
      "mdate": 1740890706013,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834583"
    },
    {
      "id": "UIFAJZ22ZF",
      "title": "The 3D-PC: a benchmark for visual perspective taking in humans and machines",
      "abstract": "Visual perspective taking (VPT) is the ability to perceive and reason about the perspectives of others. It is an essential feature of human intelligence, which develops over the first decade of life and requires an ability to process the 3D structure of visual scenes. A growing number of reports have indicated that deep neural networks (DNNs) become capable of analyzing 3D scenes after training on large image datasets. We investigated if this emergent ability for 3D analysis in DNNs is sufficient for VPT with the 3D perception challenge (3D-PC): a novel benchmark for 3D perception in humans and DNNs. The 3D-PC is comprised of three 3D-analysis tasks posed within natural scene images: (i.) a simple test of object depth order, (ii.) a basic VPT task (VPT-basic), and (iii.) a more challenging version of VPT (VPT-perturb) designed to limit the effectiveness of \"shortcut\" visual strategies. We tested human participants (N=33) and linearly probed or text-prompted over 300 DNNs on the challenge and found that nearly all of the DNNs approached or exceeded human accuracy in analyzing object depth order. Surprisingly, DNN accuracy on this task correlated with their object recognition performance. In contrast, there was an extraordinary gap between DNNs and humans on VPT-basic. Humans were nearly perfect, whereas most DNNs were near chance. Fine-tuning DNNs on VPT-basic brought them close to human performance, but they, unlike humans, dropped back to chance when tested on VPT-perturb. Our challenge demonstrates that the training routines and architectures of today's DNNs are well-suited for learning basic 3D properties of scenes and objects but are ill-suited for reasoning about these properties like humans do. We release our 3D-PC datasets and code to help bridge this gap in 3D perception between humans and machines.",
      "authors": [
        "Drew Linsley",
        "Peisen Zhou",
        "Alekh Karkada Ashok",
        "Akash Nagaraj",
        "Gaurav Gaonkar",
        "Francis E Lewis",
        "Zygmunt Pizlo",
        "Thomas Serre"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=UIFAJZ22ZF",
      "cdate": 1727478044214,
      "mdate": 1740756186232,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834588"
    },
    {
      "id": "bIlnpVM4bc",
      "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling",
      "abstract": "Efficiently modeling sequences with infinite context length has long been a challenging problem. Previous approaches have either suffered from quadratic computational complexity or limited extrapolation ability in length generalization. In this\nwork, we present Samba, a simple hybrid architecture that layer-wise combines\nMamba, a selective State Space Model (SSM), with Sliding Window Attention\n(SWA). Samba selectively compresses a given sequence into recurrent hidden\nstates while still maintaining the ability to precisely recall recent memories with the\nattention mechanism. We scale Samba up to 3.8B parameters with 3.2T training\ntokens and demonstrate that it significantly outperforms state-of-the-art models\nacross a variety of benchmarks. Pretrained on sequences of 4K length, Samba\nshows improved perplexity in context lengths of up to 1M in zero-shot. When\nfinetuned on 4K-length sequences, Samba efficiently extrapolates to a 256K context length with perfect memory recall on the Passkey Retrieval task, and exhibits\nsuperior retrieval extrapolation on the challenging Phonebook task compared to\nfull-attention models. As a linear-time sequence model, Samba achieves a 3.73×\nhigher throughput compared to Transformers with grouped-query attention for user\nprompts of 128K length, and a 3.64× speedup when generating 64K tokens with\nunlimited streaming.",
      "authors": [
        "Liliang Ren",
        "Yang Liu",
        "Yadong Lu",
        "yelong shen",
        "Chen Liang",
        "Weizhu Chen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=bIlnpVM4bc",
      "cdate": 1727477465161,
      "mdate": 1740708685829,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834594"
    },
    {
      "id": "4ytRL3HJrq",
      "title": "Nova: Generative Language Models for Assembly Code with Hierarchical Attention and Contrastive Learning",
      "abstract": "Binary code analysis is the foundation of crucial tasks in the security domain; thus building effective binary analysis techniques is more important than ever. Large language models (LLMs) although have brought impressive improvement to source code tasks, do not directly generalize to assembly code due to the unique challenges of assembly: (1) the low information density of assembly and (2) the diverse optimizations in assembly code. To overcome these challenges, this work proposes a hierarchical attention mechanism that builds attention summaries to capture the semantics more effectively and designs contrastive learning objectives to train LLMs to learn assembly optimization. Equipped with these techniques, this work develops Nova, a generative LLM for assembly code. Nova outperforms existing techniques on binary code decompilation by up to 14.84 -- 21.58% higher Pass@1 and Pass@10, and outperforms the latest binary code similarity detection techniques by up to 6.17% Recall@1, showing promising abilities on both assembly generation and understanding tasks.",
      "authors": [
        "Nan Jiang",
        "Chengxiao Wang",
        "Kevin Liu",
        "Xiangzhe Xu",
        "Lin Tan",
        "Xiangyu Zhang",
        "Petr Babkin"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=4ytRL3HJrq",
      "cdate": 1727477117750,
      "mdate": 1740869218264,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834599"
    },
    {
      "id": "LBl7Hez0fF",
      "title": "Reducing Hallucinations in Large Vision-Language Models via Latent Space Steering",
      "abstract": "Hallucination poses a challenge to the deployment of large vision-language models (LVLMs) in applications. Unlike in large language models (LLMs), hallucination in LVLMs often arises from misalignments between visual inputs and textual outputs. This paper investigates the underlying mechanisms of hallucination, focusing on the unique structure of LVLMs that distinguishes them from LLMs. We identify that hallucinations often arise from the sensitivity of text decoders to vision inputs, a natural phenomenon when image encoders and text decoders are pre-trained separately. Inspired by this, we introduce Visual and Textual Intervention (VTI), a novel technique designed to reduce hallucinations by steering latent space representations during inference to enhance the stability of vision features. As a task-agnostic test-time intervention, VTI can be easily applied to any problem without additional training costs. Extensive experiments demonstrate that it can effectively reduce hallucinations and outperform baseline methods across multiple metrics, highlighting the critical role of vision feature stability in LVLMs.",
      "authors": [
        "Sheng Liu",
        "Haotian Ye",
        "James Zou"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=LBl7Hez0fF",
      "cdate": 1727476893205,
      "mdate": 1740897316434,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834607"
    },
    {
      "id": "Ian00SaFHg",
      "title": "Efficient Model-Based Reinforcement Learning Through Optimistic Thompson Sampling",
      "abstract": "Learning complex robot behavior through interactions with the environment necessitates principled exploration. Effective strategies should prioritize exploring regions of the state-action space that maximize rewards, with optimistic exploration emerging as a promising direction aligned with this idea and enabling sample-efficient reinforcement learning. However, existing methods overlook a crucial aspect: the need for optimism to be informed by a belief connecting the reward and state. To address this, we propose a practical, theoretically grounded approach to optimistic exploration based on Thompson sampling.  Our approach is the first that allows for reasoning about _joint_ uncertainty over transitions and rewards for optimistic exploration. We apply our method on a set of MuJoCo and VMAS continuous control tasks. Our experiments demonstrate that optimistic exploration significantly accelerates learning in environments with sparse rewards, action penalties, and difficult-to-explore regions. Furthermore, we provide insights into when optimism is beneficial and emphasize the critical role of model uncertainty in guiding exploration.",
      "authors": [
        "Jasmine Bayrooti",
        "Carl Henrik Ek",
        "Amanda Prorok"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Ian00SaFHg",
      "cdate": 1727476815228,
      "mdate": 1740795970743,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834612"
    },
    {
      "id": "bDt5qc7TfO",
      "title": "Latent Safety-Constrained Policy Approach for Safe Offline Reinforcement Learning",
      "abstract": "In safe offline reinforcement learning, the objective is to develop a policy that maximizes cumulative rewards while strictly adhering to safety constraints, utilizing only offline data. Traditional methods often face difficulties in balancing these constraints, leading to either diminished performance or increased safety risks. We address these issues with a novel approach that begins by learning a conservatively safe policy through the use of Conditional Variational Autoencoders, which model the latent safety constraints. Subsequently, we frame this as a Constrained Reward-Return Maximization problem, wherein the policy aims to optimize rewards while complying with the inferred latent safety constraints. This is achieved by training an encoder with a reward-Advantage Weighted Regression objective within the latent constraint space. Our methodology is supported by theoretical analysis, including bounds on policy performance and sample complexity. Extensive empirical evaluation on benchmark datasets, including challenging autonomous driving scenarios, demonstrates that our approach not only maintains safety compliance but also excels in cumulative reward optimization, surpassing existing methods. Additional visualizations provide further insights into the effectiveness and underlying mechanisms of our approach.",
      "authors": [
        "Prajwal Koirala",
        "Zhanhong Jiang",
        "Soumik Sarkar",
        "Cody Fleming"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=bDt5qc7TfO",
      "cdate": 1727476658180,
      "mdate": 1746053210638,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834618"
    },
    {
      "id": "K3KrOsR6y9",
      "title": "LLMs Can Plan Only If We Tell Them",
      "abstract": "Large language models (LLMs) have demonstrated significant capabilities in natural language processing and reasoning, yet their effectiveness in autonomous planning has been under debate. While existing studies have utilized LLMs with external feedback mechanisms or in controlled environments for planning, these approaches often involve substantial computational and development resources due to the requirement for careful design and iterative backprompting. Moreover, even the most advanced LLMs like GPT-4 struggle to match human performance on standard planning benchmarks, such as the Blocksworld, without additional support. This paper investigates whether LLMs can independently generate long-horizon plans that rival human baselines. Our novel enhancements to Algorithm-of-Thoughts (AoT), which we dub AoT+, help achieve state-of-the-art results in planning benchmarks out-competing prior methods and human baselines all autonomously.",
      "authors": [
        "Bilgehan Sel",
        "Ruoxi Jia",
        "Ming Jin"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=K3KrOsR6y9",
      "cdate": 1727476480868,
      "mdate": 1740868020008,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834623"
    },
    {
      "id": "ZAyuwJYN8N",
      "title": "InterMask: 3D Human Interaction Generation via Collaborative Masked Modeling",
      "abstract": "Generating realistic 3D human-human interactions from textual descriptions remains a challenging task. Existing approaches, typically based on diffusion models, often produce results lacking realism and fidelity. In this work, we introduce *InterMask*, a novel framework for generating human interactions using collaborative masked modeling in discrete space. InterMask first employs a VQ-VAE to transform each motion sequence into a 2D discrete motion token map. Unlike traditional 1D VQ token maps, it better preserves fine-grained spatio-temporal details and promotes *spatial awareness* within each token. Building on this representation, InterMask utilizes a generative masked modeling framework to collaboratively model the tokens of two interacting individuals. This is achieved by employing a transformer architecture specifically designed to capture complex spatio-temporal inter-dependencies. During training, it randomly masks the motion tokens of both individuals and learns to predict them. For inference, starting from fully masked sequences, it progressively fills in the tokens for both individuals. With its enhanced motion representation, dedicated architecture, and effective learning strategy, InterMask achieves state-of-the-art results, producing high-fidelity and diverse human interactions. It outperforms previous methods, achieving an FID of $5.154$ (vs $5.535$ of in2IN) on the InterHuman dataset and $0.399$ (vs $5.207$ of InterGen) on the InterX dataset. Additionally, InterMask seamlessly supports reaction generation without the need for model redesign or fine-tuning.",
      "authors": [
        "Muhammad Gohar Javed",
        "chuan guo",
        "Li cheng",
        "Xingyu Li"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=ZAyuwJYN8N",
      "cdate": 1727476457197,
      "mdate": 1741014823148,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834629"
    },
    {
      "id": "amDkNPVWcn",
      "title": "Denoising Autoregressive Transformers for Scalable Text-to-Image Generation",
      "abstract": "Diffusion models have become the dominant approach for visual generation. They are trained by denoising a Markovian process which gradually adds noise to the input. We argue that the Markovian property limits the model’s ability to fully utilize the generation trajectory, leading to inefficiencies during training and inference. In this paper, we propose DART, a transformer-based model that unifies autoregressive (AR) and diffusion within a non-Markovian framework.  DART iteratively denoises image patches spatially and spectrally using an AR model that has the same architecture as standard language models. DART does not rely on image quantization, which enables more effective image modeling while maintaining flexibility. Furthermore, DART seamlessly trains with both text and image data in a unified model. Our approach demonstrates competitive performance on class-conditioned and text-to-image generation tasks, offering a scalable, efficient alternative to traditional diffusion models. Through this unified framework, DART sets a new benchmark for scalable, high-quality image synthesis.",
      "authors": [
        "Jiatao Gu",
        "Yuyang Wang",
        "Yizhe Zhang",
        "Qihang Zhang",
        "Dinghuai Zhang",
        "Navdeep Jaitly",
        "Joshua M. Susskind",
        "Shuangfei Zhai"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=amDkNPVWcn",
      "cdate": 1727476381730,
      "mdate": 1740605844572,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834634"
    },
    {
      "id": "ZTpWOwMrzQ",
      "title": "Radar: Fast Long-Context Decoding for Any Transformer",
      "abstract": "Transformer models have demonstrated exceptional performance across a wide range of applications. Though forming the foundation of Transformer models, the dot-product attention does not scale well to long-context data since its time requirement grows quadratically with context length. In this work, we propose Radar, a training-free approach that accelerates inference by dynamically searching for the most important context tokens. For any pre-trained Transformer, Radar can reduce the decoding time complexity without training or heuristically evicting tokens. Moreover, we provide theoretical justification for our approach, demonstrating that Radar can reliably identify the most important tokens with high probability. We conduct extensive comparisons with the previous methods on a wide range of tasks. The results demonstrate that Radar achieves the state-of-the-art performance across different architectures with reduced time complexity, offering a practical solution for efficient long-context processing of Transformers. The code is publicly available at https://github.com/BorealisAI/radar-decoding.",
      "authors": [
        "Yongchang Hao",
        "Mengyao Zhai",
        "Hossein Hajimirsadeghi",
        "Sepidehsadat Hosseini",
        "Frederick Tung"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=ZTpWOwMrzQ",
      "cdate": 1727475843443,
      "mdate": 1740709298382,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834639"
    },
    {
      "id": "hmDt068MoZ",
      "title": "Can Knowledge Editing Really Correct Hallucinations?",
      "abstract": "Large Language Models (LLMs) suffer from hallucinations, referring to the non-factual information in generated content, despite their superior capacities across tasks. Meanwhile, knowledge editing has been developed as a new popular paradigm to correct  erroneous factual knowledge encoded in LLMs with the advantage of avoiding retraining from scratch. However, a common issue of existing evaluation datasets for knowledge editing is that they do not ensure that LLMs actually generate hallucinated answers to the evaluation questions before editing. When LLMs are evaluated on such datasets after being edited by different techniques, it is hard to directly adopt the performance to assess the effectiveness of different knowledge editing methods in correcting hallucinations. Thus, the fundamental question remains insufficiently validated: Can knowledge editing really correct hallucinations in LLMs? We proposed HalluEditBench to holistically benchmark knowledge editing methods in correcting real-world hallucinations. First, we rigorously construct a massive hallucination dataset with 9 domains, 26 topics and more than 6,000 hallucinations. Then, we assess the performance of knowledge editing methods in a holistic way on five dimensions including Efficacy, Generalization, Portability, Locality, and Robustness. Through HalluEditBench, we have provided new insights into the potentials and limitations of different knowledge editing methods in correcting hallucinations, which could inspire future improvements and facilitate progress in the field of knowledge editing.",
      "authors": [
        "Baixiang Huang",
        "Canyu Chen",
        "Xiongxiao Xu",
        "Ali Payani",
        "Kai Shu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=hmDt068MoZ",
      "cdate": 1727475753363,
      "mdate": 1741014664228,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834644"
    },
    {
      "id": "k3gCieTXeY",
      "title": "INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge",
      "abstract": "The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (i.e., multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts.\nOur novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed.",
      "authors": [
        "Angelika Romanou",
        "Negar Foroutan",
        "Anna Sotnikova",
        "Sree Harsha Nelaturu",
        "Shivalika Singh",
        "Rishabh Maheshwary",
        "Micol Altomare",
        "Zeming Chen",
        "Mohamed A. Haggag",
        "Snegha A",
        "Alfonso Amayuelas",
        "Azril Hafizi Amirudin",
        "Danylo Boiko",
        "Michael Chang",
        "Jenny Chim",
        "Gal Cohen",
        "Aditya Kumar Dalmia",
        "Abraham Diress",
        "Sharad Duwal",
        "Daniil Dzenhaliou",
        "Daniel Fernando Erazo Florez",
        "Fabian Farestam",
        "Joseph Marvin Imperial",
        "Shayekh Bin Islam",
        "Perttu Isotalo",
        "Maral Jabbarishiviari",
        "Börje F. Karlsson",
        "Eldar Khalilov",
        "Christopher Klamm",
        "Fajri Koto",
        "Dominik Krzemiński",
        "Gabriel Adriano de Melo",
        "Syrielle Montariol",
        "Yiyang Nan",
        "Joel Niklaus",
        "Jekaterina Novikova",
        "Johan Samir Obando Ceron",
        "Debjit Paul",
        "Esther Ploeger",
        "Jebish Purbey",
        "Swati Rajwal",
        "Selvan Sunitha Ravi",
        "Sara Rydell",
        "Roshan Santhosh",
        "Drishti Sharma",
        "Marjana Prifti Skenduli",
        "Arshia Soltani Moakhar",
        "Bardia soltani moakhar",
        "Ayush Kumar Tarun",
        "Azmine Toushik Wasi",
        "Thenuka Ovin Weerasinghe",
        "Serhan Yilmaz",
        "Mike Zhang",
        "Imanol Schlag",
        "Marzieh Fadaee",
        "Sara Hooker",
        "Antoine Bosselut"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=k3gCieTXeY",
      "cdate": 1727475660540,
      "mdate": 1741707820473,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834650"
    },
    {
      "id": "1R5BcYS8EC",
      "title": "SysCaps: Language Interfaces for Simulation Surrogates of Complex Systems",
      "abstract": "Surrogate models are used to predict the behavior of complex energy systems that are too expensive to simulate with traditional numerical methods. \nOur work introduces the use of language descriptions, which we call \"system captions\" or SysCaps, to interface with such surrogates. \nWe argue that interacting with surrogates through text, particularly natural language, makes these models more accessible for both experts and non-experts.\nWe introduce a lightweight multimodal text and timeseries regression model and a training pipeline that uses large language models (LLMs) to synthesize high-quality captions from simulation metadata. \nOur experiments on two real-world simulators of buildings and wind farms show that our SysCaps-augmented surrogates have better accuracy on held-out systems than traditional methods while enjoying new generalization abilities, such as handling semantically related descriptions of the same test system.\nAdditional experiments also highlight the potential of SysCaps to unlock language-driven design space exploration and to regularize training through prompt augmentation.",
      "authors": [
        "Patrick Emami",
        "Zhaonan Li",
        "Saumya Sinha",
        "Truc Nguyen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=1R5BcYS8EC",
      "cdate": 1727475616637,
      "mdate": 1744045141712,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.834655"
    },
    {
      "id": "vaJ4FObpXN",
      "title": "Learning to Explore and Exploit with GNNs for Unsupervised Combinatorial Optimization",
      "abstract": "Combinatorial optimization (CO) problems are pervasive\nacross various domains, but their NP-hard nature often necessitates problem-specific\nheuristic algorithms. Recent advancements in deep learning have led to the development of learning-based heuristics, yet these approaches often struggle with limited search capabilities.\nWe introduce  Explore-and-Exploit GNN ($X^2$GNN, pronounced x-squared GNN), \na novel unsupervised neural framework that combines exploration and exploitation for combinatorial search optimization:\ni) Exploration - $X^2$GNN generates multiple  solutions simultaneously, promoting diversity in the search space; \n(ii) Exploitation - $X^2$GNN  employs neural stochastic iterative refinement to exploit partial existing solutions, guiding the search toward promising regions and helping escape local optima.\nBy balancing exploration and exploitation, $X^2$GNN achieves superior performance and generalization on several graph CO problems including Max Cut, Max Independent Set, and Max Clique. Notably, for large Max Clique problems, $X^2$GNN consistently generates solutions within 1.2\\% of optimality, while other state-of-the-art learning-based approaches struggle to reach within 22\\% of optimal. Moreover, $X^2$GNN consistently generates better solutions than Gurobi on large graphs for all three problems under reasonable time budgets. Furthermore, $X^2$GNN exhibits exceptional generalization capabilities. For the Maximum Independent Set problem, $X^2$GNN outperforms state-of-the-art methods even when trained on smaller or out-of-distribution graphs compared to the test set.  Our framework offers a more effective and flexible approach to neural combinatorial optimization, addressing a key challenge in the field and providing a promising direction for future research in learning-based heuristics for combinatorial optimization.",
      "authors": [
        "Utku Umur ACIKALIN",
        "Aaron M Ferber",
        "Carla P Gomes"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=vaJ4FObpXN",
      "cdate": 1727475377326,
      "mdate": 1740860860175,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834664"
    },
    {
      "id": "qn9tBYQHGi",
      "title": "Do LLM Agents  Have Regret? A Case Study in Online Learning and Games",
      "abstract": "Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of regret. We first empirically study the no-regret behaviors of LLMs in canonical non-stochastic online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behaviors of LLM agents, under certain assumptions on the supervised pre-training and the rationality model of human decision-makers who generate the data. Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To further promote the no-regret behaviors, we propose a novel unsupervised training loss of regret-loss, which, in contrast to the supervised pre-training loss, does not require the labels of (optimal) actions. Finally, we establish the statistical guarantee of generalization bound for regret-loss minimization, and more importantly, the optimization guarantee that minimizing such a loss may automatically lead to known no-regret learning algorithms, when single-layer self-attention models are used. Our further experiments demonstrate the effectiveness of our regret-loss, especially in addressing the above “regrettable” cases.",
      "authors": [
        "Chanwoo Park",
        "Xiangyu Liu",
        "Asuman E. Ozdaglar",
        "Kaiqing Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=qn9tBYQHGi",
      "cdate": 1727475052674,
      "mdate": 1741237564929,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834669"
    },
    {
      "id": "30oIfmrcFO",
      "title": "Seq-VCR: Preventing  Collapse in Intermediate Transformer Representations for Enhanced Reasoning",
      "abstract": "Decoder-only Transformers often struggle with complex reasoning tasks, particularly arithmetic reasoning requiring multiple sequential operations. In this work, we identify representation collapse in the model’s intermediate layers as a key factor limiting their reasoning capabilities. To address this, we propose Sequential Variance-Covariance Regularization (Seq-VCR), which enhances the entropy of intermediate representations and prevents collapse. Combined with dummy pause tokens as substitutes for chain-of-thought (CoT) tokens, our method significantly improves performance in arithmetic reasoning problems. In the challenging 5 × 5 integer multiplication task, our approach achieves 99.5% exact match accuracy, outperforming models of the same size (which yield 0% accuracy) and GPT-4 with five-shot CoT prompting (44%). We also demonstrate superior results on arithmetic expression and longest increasing subsequence (LIS) datasets. Our findings highlight the importance of preventing intermediate layer representation collapse to enhance the reasoning capabilities of Transformers and show that Seq-VCR offers an effective solution without requiring explicit CoT supervision.",
      "authors": [
        "Md Rifat Arefin",
        "Gopeshh Subbaraj",
        "Nicolas Gontier",
        "Yann LeCun",
        "Irina Rish",
        "Ravid Shwartz-Ziv",
        "Christopher Pal"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=30oIfmrcFO",
      "cdate": 1727474983524,
      "mdate": 1740871683755,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834674"
    },
    {
      "id": "YH4M1Tbxfz",
      "title": "BoneMet: An Open Large-Scale Multi-Modal Murine Dataset for Breast Cancer Bone Metastasis Diagnosis and Prognosis",
      "abstract": "Breast cancer bone metastasis (BCBM) affects women’s health globally, calling\n for the development of effective diagnosis and prognosis solutions. While deep\n learning has exhibited impressive capacities across various healthcare domains, its\n applicability in BCBM diseases is consistently hindered by the lack of an open,\n large-scale, deep learning-ready dataset. As such, we introduce the Bone Metastasis\n (BoneMet) dataset, the first large-scale, publicly available, high-resolution medical\n resource, which is derived from a well-accepted murine BCBM model. The unique\n advantage of BoneMet over existing human datasets is repeated sequential scans\n per subject over the entire disease development phases. The dataset consists of\n over 67 terabytes of multi-modal medical data, including 2D X-ray images, 3D\n CT scans, and detailed biological data (e.g., medical records and bone quantitative\n analysis), collected from more than five hundreds mice spanning from 2019 to\n 2024. Our BoneMet dataset is well-organized into six components, i.e., Rotation\nX-Ray, Recon-CT, Seg-CT, Regist-CT, RoI-CT, and MiceMediRec. We further\n show that BoneMet can be readily adopted to build versatile, large-scale AI models\n for managing BCBM diseases in terms of diagnosis using 2D or 3D images, prognosis of bone deterioration, and sparse-angle 3D reconstruction for safe long-term\n disease monitoring. Our preliminary results demonstrate that BoneMet has the\n potentials to jump-start the development and fine-tuning of AI-driven solutions\n prior to their applications to human patients. To facilitate its easy access and\n wide dissemination, we have created the BoneMet package, providing three APIs\n that enable researchers to (i) flexibly process and download the BoneMet data\n filtered by specific time frames; and (ii) develop and train large-scale AI models for\n precise BCBM diagnosis and prognosis. The BoneMet dataset is officially available on Hugging Face Datasets at https://huggingface.co/datasets/BoneMet/BoneMet. The BoneMet package is available on the Python Package Index (PyPI) at https://pypi.org/project/BoneMet. Code and tutorials are available at https://github.com/Tiankuo528/BoneMet.",
      "authors": [
        "Tiankuo Chu",
        "Fudong Lin",
        "Shubo Wang",
        "Jason Jiang",
        "Wiley Jia-Wei Gong",
        "Xu Yuan",
        "Liyun Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=YH4M1Tbxfz",
      "cdate": 1727474663468,
      "mdate": 1740890357567,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834680"
    },
    {
      "id": "5yDS32hKJc",
      "title": "Time After Time: Deep-Q Effect Estimation for Interventions on When and What to do",
      "abstract": "Problems in fields such as healthcare, robotics, and finance requires reasoning about the value both of what decision or action to take and when to take it. The prevailing hope is that artificial intelligence will support such decisions by estimating the causal effect of policies such as how to treat patients or how to allocate resources over time. However, existing methods for estimating the effect of a policy struggle with \\emph{irregular time}.  They either discretize time, or disregard the effect of timing policies.\n\nWe present a new deep-Q algorithm that estimates the effect of both when and what to do called Earliest Disagreement Q-Evaluation (EDQ). EDQ makes use of recursion for the Q-function that is compatible with flexible sequence models, such as transformers. EDQ provides accurate estimates under standard assumptions. We validate the approach through experiments on survival time and tumor growth tasks.",
      "authors": [
        "Yoav Wald",
        "Mark Goldstein",
        "Yonathan Efroni",
        "Wouter A.C. van Amsterdam",
        "Rajesh Ranganath"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=5yDS32hKJc",
      "cdate": 1727474638777,
      "mdate": 1742365030242,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834688"
    },
    {
      "id": "SyVPiehSbg",
      "title": "Deep Learning Alternatives Of The Kolmogorov Superposition Theorem",
      "abstract": "This paper explores alternative formulations of the Kolmogorov Superposition Theorem (KST) as a foundation for neural network design. The original KST formulation, while mathematically elegant, presents practical challenges due to its limited insight into the structure of inner and outer functions and the large number of unknown variables it introduces. Kolmogorov-Arnold Networks (KANs) leverage KST for function approximation, but they have faced scrutiny due to mixed results compared to traditional multilayer perceptrons (MLPs) and practical limitations imposed by the original KST formulation. To address these issues, we introduce ActNet, a scalable deep learning model that builds on the KST and overcomes some of the drawbacks of Kolmogorov's original formulation. We evaluate ActNet in the context of Physics-Informed Neural Networks (PINNs), a framework well-suited for leveraging KST's strengths in low-dimensional function approximation, particularly for simulating partial differential equations (PDEs). In this challenging setting, where models must learn latent functions without direct measurements, ActNet consistently outperforms KANs across multiple benchmarks and is competitive against the current best MLP-based approaches. These results present ActNet as a promising new direction for KST-based deep learning applications, particularly in scientific computing and PDE simulation tasks.",
      "authors": [
        "Leonardo Ferreira Guilhoto",
        "Paris Perdikaris"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=SyVPiehSbg",
      "cdate": 1727474554139,
      "mdate": 1740800347303,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834694"
    },
    {
      "id": "MscdsFVZrN",
      "title": "ALLaM: Large Language Models for Arabic and English",
      "abstract": "In this work, we present ALLaM: Arabic Large Language Model, a series of large language models to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is carefully trained, considering the values of language alignment and transferability of knowledge at scale. The models are based on an autoregressive decoder-only architecture and are pretrained on a mixture of Arabic and English texts. We illustrate how the second-language acquisition via vocabulary expansion can help steer a language model towards a new language without any major catastrophic forgetting in English. Furthermore, we highlight the effectiveness of using translation data and the process of knowledge encoding within the language model's latent space. Finally, we show that effective alignment with human preferences can significantly enhance the performance of a large language model (LLM) compared to less aligned models of a larger scale. Our methodology enables us to achieve state-of-the-art performance in various Arabic benchmarks, including MMLU Arabic, ACVA, and Arabic Exams. Our aligned models improve both in Arabic and English from its base aligned models.",
      "authors": [
        "M Saiful Bari",
        "Yazeed Alnumay",
        "Norah A. Alzahrani",
        "Nouf M. Alotaibi",
        "Hisham Abdullah Alyahya",
        "Sultan AlRashed",
        "Faisal Abdulrahman Mirza",
        "Shaykhah Z. Alsubaie",
        "Hassan A. Alahmed",
        "Ghadah Alabduljabbar",
        "Raghad Alkhathran",
        "Yousef Almushayqih",
        "Raneem Alnajim",
        "Salman Alsubaihi",
        "Maryam Al Mansour",
        "Saad Amin Hassan",
        "Dr. Majed Alrubaian",
        "Ali Alammari",
        "Zaki Alawami",
        "Abdulmohsen Al-Thubaity",
        "Ahmed Abdelali",
        "Jeril Kuriakose",
        "Abdalghani Abujabal",
        "Nora Al-Twairesh",
        "Areeb Alowisheq",
        "Haidar Khan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=MscdsFVZrN",
      "cdate": 1727474528443,
      "mdate": 1741951681755,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834700"
    },
    {
      "id": "PY56Wur7S0",
      "title": "Execution-guided within-prompt search for programming-by-example",
      "abstract": "Large language models (LLMs) can generate code from examples without being limited to a DSL, but they lack search, as sampled programs are independent.\nIn this paper, we use an LLM as a policy that generates lines of code and then join these lines of code to let the LLM implicitly estimate the value of each of these lines in its next iteration.\nWe further guide the policy and value estimation by executing each line and annotating it with its results on the given examples. \nThis allows us to search for programs within a single (expanding) prompt until a sound program is found, by letting the policy reason in both the syntactic (code) and semantic (execution) space.\nWe evaluate within-prompt search on straight-line Python code generation using five benchmarks across different domains (strings, lists, and arbitrary Python programming problems).\nWe show that the model uses the execution results to guide the search and that within-prompt search performs well at low token budgets.\nWe also analyze how the model behaves as a policy and value, show that it can parallelize the search, and that it can implicitly backtrack over earlier generations.",
      "authors": [
        "Gust Verbruggen",
        "Ashish Tiwari",
        "Mukul Singh",
        "Vu Le",
        "Sumit Gulwani"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=PY56Wur7S0",
      "cdate": 1727474521018,
      "mdate": 1740890357475,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834705"
    },
    {
      "id": "7PLpiVdnUC",
      "title": "Lie Algebra Canonicalization: Equivariant Neural Operators under arbitrary Lie Groups",
      "abstract": "The quest for robust and generalizable machine learning models has driven recent interest in exploiting symmetries through equivariant neural networks. In the context of PDE solvers, recent works have shown that Lie point symmetries can be a useful inductive bias for Physics-Informed Neural Networks (PINNs) through data and loss augmentation. Despite this, directly enforcing equivariance within the model architecture for these problems remains elusive. This is because many PDEs admit non-compact symmetry groups, oftentimes not studied beyond their infinitesimal generators, making them incompatible with most existing equivariant architectures. In this work, we propose Lie aLgebrA Canonicalization (LieLAC), a novel approach that exploits only the action of infinitesimal generators of the symmetry group, circumventing the need for knowledge of the full group structure. To achieve this, we address existing theoretical issues in the canonicalization literature, establishing connections with frame averaging in the case of continuous non-compact groups. Operating within the framework of canonicalization, LieLAC can easily be integrated with unconstrained pre-trained models, transforming inputs to a canonical form before feeding them into the existing model, effectively aligning the input for model inference according to allowed symmetries. LieLAC utilizes standard Lie group descent schemes, achieving equivariance in pre-trained models. Finally, we showcase LieLAC's efficacy on tasks of invariant image classification and Lie point symmetry equivariant neural PDE solvers using pre-trained models.",
      "authors": [
        "Zakhar Shumaylov",
        "Peter Zaika",
        "James Rowbottom",
        "Ferdia Sherry",
        "Melanie Weber",
        "Carola-Bibiane Schönlieb"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=7PLpiVdnUC",
      "cdate": 1727474465376,
      "mdate": 1741114606829,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834713"
    },
    {
      "id": "iAmR7FfMmq",
      "title": "Improving Graph Neural Networks by Learning Continuous Edge Directions",
      "abstract": "Graph Neural Networks (GNNs) traditionally employ a message-passing mechanism that resembles diffusion over undirected graphs, which often leads to homogenization of node features and reduced discriminative power in tasks such as node classification. Our key insight for addressing this limitation is to assign fuzzy edge directions---that can vary continuously from node $i$ pointing to node $j$ to vice versa---to the edges of a graph so that features can preferentially flow in one direction between nodes to enable long-range information transmission across the graph. We also introduce a novel complex-valued Laplacian for directed graphs with fuzzy edges where the real and imaginary parts represent information flow in opposite directions. Using this Laplacian, we propose a general framework, called Continuous Edge Direction (CoED) GNN, for learning on graphs with fuzzy edges and prove its expressivity limits using a generalization of the Weisfeiler-Leman (WL) graph isomorphism test for directed graphs with fuzzy edges. Our architecture aggregates neighbor features scaled by the learned edge directions and processes the aggregated messages from in-neighbors and out-neighbors separately alongside the self-features of the nodes. Since continuous edge directions are differentiable, they can be learned jointly with the GNN weights via gradient-based optimization. CoED GNN is particularly well-suited for graph ensemble data where the graph structure remains fixed but multiple realizations of node features are available, such as in gene regulatory networks, web connectivity graphs, and power grids. We demonstrate through extensive experiments on both synthetic and real graph ensemble datasets that learning continuous edge directions significantly improves performance both for undirected and directed graphs compared with existing methods.",
      "authors": [
        "Seong Ho Pahng",
        "Sahand Hormoz"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=iAmR7FfMmq",
      "cdate": 1727474196766,
      "mdate": 1740774688358,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834718"
    },
    {
      "id": "KfeRfxTemB",
      "title": "Accelerating Task Generalisation with Multi-Level Skill Hierarchies",
      "abstract": "Developing reinforcement learning agents that can generalise effectively to new tasks is one of the main challenges in AI research. This paper introduces Fracture Cluster Options (FraCOs), a multi-level hierarchical reinforcement learning method designed to improve generalisation performance. FraCOs identifies patterns in agent behaviour and forms temporally-extended actions (options) based on the expected future usefulness of those patterns, enabling rapid adaptation to new tasks. In tabular settings, FraCOs demonstrates effective transfer and improves performance as the depth of the hierarchy increases. In several complex procedurally-generated environments, FraCOs consistently outperforms state-of-the-art deep reinforcement learning algorithms, achieving superior results in both in-distribution and out-of-distribution scenarios.",
      "authors": [
        "Thomas P Cannon",
        "Özgür Şimşek"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=KfeRfxTemB",
      "cdate": 1727473986069,
      "mdate": 1742070212729,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834726"
    },
    {
      "id": "27SSnLl85x",
      "title": "Make Haste Slowly: A Theory of Emergent Structured Mixed Selectivity in Feature Learning ReLU Networks",
      "abstract": "In spite of finite dimension ReLU neural networks being a consistent factor behind recent deep learning successes, a theory of feature learning in these models remains elusive. Currently, insightful theories still rely on assumptions including the linearity of the network computations, unstructured input data and architectural constraints such as infinite width or a single hidden layer. To begin to address this gap we establish an equivalence between ReLU networks and Gated Deep Linear Networks, and use their greater tractability to derive dynamics of learning. We then consider multiple variants of a core task reminiscent of multi-task learning or contextual control which requires both feature learning and nonlinearity. We make explicit that, for these tasks, the ReLU networks possess an inductive bias towards latent representations which are *not* strictly modular or disentangled but are still highly structured and reusable between contexts. This effect is amplified with the addition of more contexts and hidden layers. Thus, we take a step towards a theory of feature learning in finite ReLU networks and shed light on how structured mixed-selective latent representations can emerge due to a bias for node-reuse and learning speed.",
      "authors": [
        "Devon Jarvis",
        "Richard Klein",
        "Benjamin Rosman",
        "Andrew M Saxe"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=27SSnLl85x",
      "cdate": 1727473823817,
      "mdate": 1743348142871,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834732"
    },
    {
      "id": "i1NNCrRxdM",
      "title": "SymDiff: Equivariant Diffusion via Stochastic Symmetrisation",
      "abstract": "We propose SymDiff, a method for constructing equivariant diffusion models using the framework of stochastic symmetrisation. SymDiff resembles a learned data augmentation that is deployed at sampling time, and is lightweight, computationally efficient, and easy to implement on top of arbitrary off-the-shelf models. In contrast to previous work, SymDiff typically does not require any neural network components that are intrinsically equivariant, avoiding the need for complex parameterisations or the use of higher-order geometric features. Instead, our method can leverage highly scalable modern architectures as drop-in replacements for these more constrained alternatives. We show that this additional flexibility yields significant empirical benefit for E(3)-equivariant molecular generation. To the best of our knowledge, this is the first application of symmetrisation to generative modelling, suggesting its potential in this domain more generally.",
      "authors": [
        "Leo Zhang",
        "Kianoosh Ashouritaklimi",
        "Yee Whye Teh",
        "Rob Cornish"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=i1NNCrRxdM",
      "cdate": 1727473731419,
      "mdate": 1740891050038,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834737"
    },
    {
      "id": "NEu8wgPctU",
      "title": "AdaWM: Adaptive World Model based Planning for Autonomous Driving",
      "abstract": "World model based reinforcement learning (RL) has emerged as a promising approach for autonomous driving, which learns a latent dynamics model and uses it to train a   planning policy. To speed up the learning process, the pretrain-finetune paradigm is often used, where online RL is initialized by a pretrained model and a policy learned offline. However, naively performing such initialization in RL may result in dramatic performance degradation during the online interactions in the new task. To tackle this challenge, we first analyze the  performance degradation and identify two primary root causes therein: the mismatch of the planning policy and the mismatch of the dynamics model,  due to distribution shift. We further analyze the effects of these factors  on performance degradation during finetuning, and our findings reveal that the choice of finetuning strategies plays a pivotal role in mitigating these effects. We then introduce AdaWM, an Adaptive World Model based planning method, featuring two key steps: (a) mismatch identification, which quantifies the mismatches and informs the finetuning strategy, and (b) alignment-driven finetuning, which selectively updates either the policy or the model as needed  using efficient low-rank updates. Extensive experiments  on the challenging CARLA driving tasks demonstrate that AdaWM significantly improves the finetuning process, resulting in more robust and efficient performance in autonomous driving systems.",
      "authors": [
        "Hang Wang",
        "Xin Ye",
        "Feng Tao",
        "Chenbin Pan",
        "Abhirup Mallik",
        "Burhaneddin Yaman",
        "Liu Ren",
        "Junshan Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=NEu8wgPctU",
      "cdate": 1727473693577,
      "mdate": 1740704230627,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834742"
    },
    {
      "id": "FSjIrOm1vz",
      "title": "Inference Scaling for Long-Context Retrieval Augmented Generation",
      "abstract": "The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge.  However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs’ ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this,  we further develop the computation allocation model to estimate RAG performance across different inference configurations.  The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.",
      "authors": [
        "Zhenrui Yue",
        "Honglei Zhuang",
        "Aijun Bai",
        "Kai Hui",
        "Rolf Jagerman",
        "Hansi Zeng",
        "Zhen Qin",
        "Dong Wang",
        "Xuanhui Wang",
        "Michael Bendersky"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=FSjIrOm1vz",
      "cdate": 1727473624297,
      "mdate": 1742445038563,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834748"
    },
    {
      "id": "AZR4R3lw7y",
      "title": "Boosting Multiple Views for pretrained-based Continual Learning",
      "abstract": "Recent research has shown that Random Projection (RP) can effectively improve the performance of pre-trained models in Continual learning (CL). The authors hypothesized that using RP to map features onto a higher-dimensional space can make them more linearly separable. In this work, we theoretically analyze the role of RP and present its benefits for improving the model’s generalization ability\nin each task and facilitating CL overall. Additionally, we take this result to the next level by proposing a Multi-View Random Projection scheme for a stronger ensemble classifier. In particular, we train a set of linear experts, among which diversity is encouraged based on the principle of AdaBoost, which was initially very challenging to apply to CL. Moreover, we employ a task-based adaptive backbone\nwith distinct prompts dedicated to each task for better representation learning. To properly select these task-specific components and mitigate potential feature shifts caused by misprediction, we introduce a simple yet effective technique called the self-improvement process. Experimentally, our method consistently outperforms state-of-the-art baselines across a wide range of datasets.",
      "authors": [
        "Quyen Tran",
        "Tung Lam Tran",
        "Khanh Doan",
        "Toan Tran",
        "Dinh Phung",
        "Khoat Than",
        "Trung Le"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=AZR4R3lw7y",
      "cdate": 1727473546599,
      "mdate": 1743568157118,
      "matched_keywords": [
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834753"
    },
    {
      "id": "eHfq8Q3LeD",
      "title": "Matrix Product Sketching via Coordinated Sampling",
      "abstract": "We revisit the well-studied problem of approximating a matrix product, $\\bv{A}^T\\bv{B}$, based on small space sketches $\\mathcal{S}(\\bv{A})$ and  $\\mathcal{S}(\\bv{B})$ of $\\bv{A} \\in \\R^{n \\times d}$ and $\\bv{B}\\in \\R^{n \\times m}$. We are interested in the setting where the sketches must be computed independently of each other, except for the use of a shared random seed. We prove that, when $\\bv{A}$ and $\\bv{B}$ are sparse, methods based on \\emph{coordinated random sampling} can outperform classical linear sketching approaches, like Johnson-Lindenstrauss Projection or CountSketch. For example, to obtain Frobenius norm error $\\epsilon\\|\\bv{A}\\|_F\\|\\bv{B}\\|_F$, coordinated sampling requires sketches of size $O(s/\\epsilon^2)$ when $\\bv{A}$ and $\\bv{B}$ have at most $s \\leq d,m$ non-zeros per row. In contrast, linear sketching leads to sketches of size $O(d/\\epsilon^2)$ and $O(m/\\epsilon^2)$ for $\\bv{A}$ and $\\bv{B}$. We empirically evaluate our approach on two applications: 1) distributed linear regression in databases, a problem motivated by tasks like dataset discovery and augmentation, and 2) approximating attention matrices in transformer-based language models. In both cases, our sampling algorithms yield an order of magnitude improvement over linear sketching.",
      "authors": [
        "Majid Daliri",
        "Juliana Freire",
        "Danrong Li",
        "Christopher Musco"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=eHfq8Q3LeD",
      "cdate": 1727473227524,
      "mdate": 1740914625167,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834758"
    },
    {
      "id": "eNQp79A5Oz",
      "title": "Preserving Deep Representations in One-Shot Pruning: A Hessian-Free Second-Order Optimization Framework",
      "abstract": "We present SNOWS, a one-shot post-training pruning framework aimed at reducing the cost of vision network inference without retraining. Current leading one-shot pruning methods minimize layer-wise least squares reconstruction error which does not take into account deeper network representations. We propose to optimize a more global reconstruction objective. This objective accounts for nonlinear activations deep in the network to obtain a better proxy for the network loss. This nonlinear objective leads to a more challenging optimization problem---we demonstrate it can be solved efficiently using a specialized second-order optimization framework. A key innovation of our framework is the use of Hessian-free optimization to compute exact Newton descent steps without needing to compute or store the full Hessian matrix. A distinct advantage of SNOWS is that it can be readily applied on top of any sparse mask derived from prior methods, readjusting their weights to preserve deep feature representations. SNOWS obtains state-of-the-art results on various one-shot pruning benchmarks including residual networks and Vision Transformers (ViT/B-16 and ViT/L-16, 86m and 304m parameters respectively). Our open-source implementation is available at https://github.com/mazumder-lab/SNOWS.",
      "authors": [
        "Ryan Lucas",
        "Rahul Mazumder"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=eNQp79A5Oz",
      "cdate": 1727473089308,
      "mdate": 1741274138730,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834763"
    },
    {
      "id": "uHLgDEgiS5",
      "title": "Capturing the Temporal Dependence of Training Data Influence",
      "abstract": "Traditional data influence estimation methods, like influence function, assume that learning algorithms are permutation-invariant with respect to training data. However, modern training paradigms—especially for foundation models using stochastic algorithms and non-convergent, multi-stage curricula—are sensitive to data ordering, thus violating this assumption. This mismatch renders influence functions inadequate for answering some critical questions in current machine learning: How can we differentiate the influence of the same data contributing at different stages of training? More generally, how can we capture the dependence of data influence on the optimization trajectory during training? To address this gap, we formalize the concept of \\emph{trajectory-specific leave-one-out (LOO) influence}, which quantifies the impact of removing a data point from a specific iteration during training, accounting for the exact sequence of data encountered and the model's optimization trajectory. However, exactly evaluating the trajectory-specific LOO presents a significant computational challenge. To address this, we propose \\emph{data value embedding}, a novel technique enabling efficient approximation of trajectory-specific LOO. Specifically, we compute a training data embedding that encapsulates the cumulative interactions between data and the evolving model parameters. The LOO can then be efficiently approximated through a simple dot-product between the data value embedding and the gradient of the given test data. As data value embedding captures training data ordering, it offers valuable insights into model training dynamics. In particular, we uncover distinct phases of data influence, revealing that data points in the early and late stages of training exert a greater impact on the final model. These insights translate into actionable strategies for managing the computational overhead of data selection by strategically timing the selection process, potentially opening new avenues in data curation research.",
      "authors": [
        "Jiachen T. Wang",
        "Dawn Song",
        "James Zou",
        "Prateek Mittal",
        "Ruoxi Jia"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=uHLgDEgiS5",
      "cdate": 1727472987554,
      "mdate": 1741980380611,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834768"
    },
    {
      "id": "cnKhHxN3xj",
      "title": "Wasserstein Distances, Neuronal Entanglement, and Sparsity",
      "abstract": "Disentangling polysemantic neurons is at the core of many current approaches to interpretability of large language models. Here we attempt to study how disentanglement can be used to understand performance, particularly under weight sparsity, a leading post-training optimization technique. We suggest a novel measure for estimating neuronal entanglement: the Wasserstein distance of a neuron's output distribution to a Gaussian. Moreover, we show the existence of a small number of highly entangled \"Wasserstein Neurons\" in each linear layer of an LLM, characterized by their highly non-Gaussian output distributions, their role in mapping similar inputs to dissimilar outputs, and their significant impact on model accuracy. To study these phenomena, we propose a new experimental framework for disentangling polysemantic neurons. Our framework separates each layer's inputs to create a mixture of experts where each neuron's output is computed by a mixture of neurons of lower Wasserstein distance, each better at maintaining accuracy when sparsified without retraining. We provide strong evidence that this is because the mixture of sparse experts is effectively disentangling the input-output relationship of individual neurons, in particular the difficult Wasserstein neurons.",
      "authors": [
        "Shashata Sawmya",
        "Linghao Kong",
        "Ilia Markov",
        "Dan Alistarh",
        "Nir N Shavit"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=cnKhHxN3xj",
      "cdate": 1727472939179,
      "mdate": 1743174794507,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834773"
    },
    {
      "id": "4dAgG8ma3B",
      "title": "Chemistry-Inspired Diffusion with Non-Differentiable Guidance",
      "abstract": "Recent advances in diffusion models have shown remarkable potential in the conditional generation of novel molecules. These models can be guided in two ways: (i) explicitly, through additional features representing the condition, or (ii) implicitly, using a property predictor. However, training property predictors or conditional diffusion models requires an abundance of labeled data and is inherently challenging in real-world applications. We propose a novel approach that attenuates the limitations of acquiring large labeled datasets by leveraging domain knowledge from quantum chemistry as a non-differentiable oracle to guide an unconditional diffusion model. Instead of relying on neural networks, the oracle provides accurate guidance in the form of estimated gradients, allowing the diffusion process to sample from a conditional distribution specified by quantum chemistry. We show that this results in more precise conditional generation of novel and stable molecular structures. Our experiments demonstrate that our method: (1) significantly reduces atomic forces, enhancing the validity of generated molecules when used for stability optimization; (2) is compatible with both explicit and implicit guidance in diffusion models, enabling joint optimization of molecular properties and stability; and (3) generalizes effectively to molecular optimization tasks beyond stability optimization. Our implementation is available at https://github.com/A-Chicharito-S/ChemGuide.",
      "authors": [
        "Yuchen Shen",
        "Chenhao Zhang",
        "Sijie Fu",
        "Chenghui Zhou",
        "Newell Washburn",
        "Barnabas Poczos"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=4dAgG8ma3B",
      "cdate": 1727472903771,
      "mdate": 1740798816118,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834858"
    },
    {
      "id": "nDmwloEl3N",
      "title": "Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning",
      "abstract": "Diffusion Policies have become widely used in Imitation Learning, offering several appealing properties, such as generating multimodal and discontinuous behavior.\nAs models are becoming larger to capture more complex capabilities, their computational demands increase, as shown by recent scaling laws. \nTherefore, continuing with the current architectures will present a computational roadblock. \nTo address this gap, we propose Mixture-of-Denoising Experts (MoDE) as a novel policy for Imitation Learning.\nMoDE surpasses current state-of-the-art Transformer-based Diffusion Policies while enabling parameter-efficient scaling through sparse experts and noise-conditioned routing, reducing both active parameters by 40\\% and inference costs by 90\\% via expert caching.\nOur architecture combines this efficient scaling with noise-conditioned self-attention mechanism, enabling more effective denoising across different noise levels. \nMoDE achieves state-of-the-art performance on 134 tasks in four established imitation learning benchmarks (CALVIN and LIBERO). \nNotably, by pretraining MoDE on diverse robotics data, we achieve 4.01 on CALVIN ABC and 0.95 on LIBERO-90. \nIt surpasses both CNN-based and Transformer Diffusion Policies by an average of $57\\%$ across 4 benchmarks, while using 90\\% fewer FLOPs and fewer active parameters compared to default Diffusion Transformer architectures. Furthermore, we conduct comprehensive ablations on MoDE's components, providing insights for designing efficient and scalable Transformer architectures for Diffusion Policies. \nCode and demonstrations are available at https://mbreuss.github.io/MoDE_Diffusion_Policy.",
      "authors": [
        "Moritz Reuss",
        "Jyothish Pari",
        "Pulkit Agrawal",
        "Rudolf Lioutikov"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=nDmwloEl3N",
      "cdate": 1727472887971,
      "mdate": 1740768783538,
      "matched_keywords": [
        "multimodal",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834866"
    },
    {
      "id": "2TasVD7FXp",
      "title": "InvestESG: A multi-agent reinforcement learning benchmark for studying climate investment as a social dilemma",
      "abstract": "**InvestESG** is a novel multi-agent reinforcement learning (MARL) benchmark designed to study the impact of Environmental, Social, and Governance (ESG) disclosure mandates on corporate climate investments. The benchmark models an intertemporal social dilemma where companies balance short-term profit losses from climate mitigation efforts and long-term benefits from reducing climate risk, while ESG-conscious investors attempt to influence corporate behavior through their investment decisions. Companies allocate capital across mitigation, greenwashing, and resilience, with varying strategies influencing climate outcomes and investor preferences. We are releasing open-source versions of InvestESG in both PyTorch and JAX, which enable scalable and hardware-accelerated simulations for investigating competing incentives in mitigate climate change. Our experiments show that without ESG-conscious investors with sufficient capital, corporate mitigation efforts remain limited under the disclosure mandate. However, when a critical mass of investors prioritizes ESG, corporate cooperation increases, which in turn reduces climate risks and enhances long-term financial stability. Additionally, providing more information about global climate risks encourages companies to invest more in mitigation, even without investor involvement. Our findings align with empirical research using real-world data, highlighting MARL's potential to inform policy by providing insights into large-scale socio-economic challenges through efficient testing of alternative policy and market designs.",
      "authors": [
        "Xiaoxuan Hou",
        "Jiayi Yuan",
        "Joel Z Leibo",
        "Natasha Jaques"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=2TasVD7FXp",
      "cdate": 1727472834833,
      "mdate": 1739261766747,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834871"
    },
    {
      "id": "oYemKnlIrO",
      "title": "Do Mice Grok? Glimpses of Hidden Progress in Sensory Cortex",
      "abstract": "Does learning of task-relevant representations stop when behavior stops changing? Motivated by recent work in machine learning and the intuitive observation that human experts continue to learn after mastery, we hypothesize that task-specific representation learning in cortex can continue, even when behavior saturates. In a novel reanalysis of recently published neural data, we find evidence for such learning in posterior piriform cortex of mice following continued training on a task, long after behavior saturates at near-ceiling performance (\"overtraining\"). We demonstrate that class representations in cortex continue to separate during overtraining, so that examples that were incorrectly classified at the beginning of overtraining can abruptly be correctly classified later on, despite no changes in behavior during that time. We hypothesize this hidden learning takes the form of approximate margin maximization; we validate this and other predictions in the neural data, as well as build and interpret a simple synthetic model that recapitulates these phenomena. We conclude by demonstrating how this model of late-time feature learning implies an explanation for the empirical puzzle of overtraining reversal in animal learning, where task-specific representations are more robust to particular task changes because the learned features can be reused.",
      "authors": [
        "Tanishq Kumar",
        "Blake Bordelon",
        "Cengiz Pehlevan",
        "Venkatesh N Murthy",
        "Samuel J. Gershman"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=oYemKnlIrO",
      "cdate": 1727472526128,
      "mdate": 1740759805701,
      "matched_keywords": [
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834877"
    },
    {
      "id": "4ub9gpx9xw",
      "title": "Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations",
      "abstract": "Large language models (LLMs) are capable of generating *plausible* explanations of how they arrived at an answer to a question. However, these explanations can misrepresent the model's \"reasoning\" process, i.e., they can be *unfaithful*. This, in turn, can lead to over-trust and misuse. We introduce a new approach for measuring the faithfulness of LLM explanations. First, we provide a rigorous definition of faithfulness. Since LLM explanations mimic human explanations, they often reference high-level *concepts* in the input question that purportedly influenced the model. We define faithfulness in terms of the difference between the set of concepts that the LLM's *explanations imply* are influential and the set that *truly* are. Second, we present a novel method for estimating faithfulness that is based on: (1) using an auxiliary LLM to modify the values of concepts within model inputs to create realistic counterfactuals, and (2) using a hierarchical Bayesian model to quantify the causal effects of concepts at both the example- and dataset-level. Our experiments show that our method can be used to quantify and discover interpretable patterns of unfaithfulness. On a social bias task, we uncover cases where LLM explanations hide the influence of social bias. On a medical question answering task, we uncover cases where LLM explanations provide misleading claims about which pieces of evidence influenced the model's decisions.",
      "authors": [
        "Katie Matton",
        "Robert Ness",
        "John Guttag",
        "Emre Kiciman"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=4ub9gpx9xw",
      "cdate": 1727472237170,
      "mdate": 1747548051046,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834882"
    },
    {
      "id": "IF0Q9KY3p2",
      "title": "Implicit Bias of Mirror Flow for Shallow Neural Networks in Univariate Regression",
      "abstract": "We examine the implicit bias of mirror flow in least squares error regression with wide and shallow neural networks. For a broad class of potential functions, we show that mirror flow exhibits lazy training and has the same implicit bias as ordinary gradient flow when the network width tends to infinity. For univariate ReLU networks, we characterize this bias through a variational problem in function space. Our analysis includes prior results for ordinary gradient flow as a special case and lifts limitations which required either an intractable adjustment of the training data or networks with skip connections. We further introduce \\emph{scaled potentials} and show that for these, mirror flow still exhibits lazy training but is not in the kernel regime. For univariate networks with absolute value activations, we show that mirror flow with scaled potentials induces a rich class of biases, which generally cannot be captured by an RKHS norm. A takeaway is that whereas the parameter initialization determines how strongly the curvature of the learned function is penalized at different locations of the input space, the scaled potential determines how the different magnitudes of the curvature are penalized.",
      "authors": [
        "Shuang Liang",
        "Guido Montufar"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=IF0Q9KY3p2",
      "cdate": 1727472032349,
      "mdate": 1739261766191,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834888"
    },
    {
      "id": "uE84MGbKD7",
      "title": "Benchmarking LLMs' Judgments with No Gold Standard",
      "abstract": "We introduce the GEM (Generative Estimator for Mutual Information), an evaluation metric for assessing language generation by large language models (LLMs), particularly in generating informative judgments, without the need for a gold standard reference. GEM broadens the scenarios where we can benchmark LLM generation performance-from traditional ones, like machine translation and summarization, where gold standard references are readily available, to subjective tasks without clear gold standards, such as academic peer review.\n\nGEM uses a generative model to estimate mutual information between candidate and reference responses, without requiring the reference to be a gold standard. In experiments on two human-annotated datasets, GEM demonstrates competitive correlations with human scores compared to the state-of-the-art GPT-4o Examiner, and outperforms all other baselines. Additionally, GEM is more robust against strategic manipulation, such as rephrasing or elongation, which can artificially inflate scores under a GPT-4o Examiner. \n\nWe also present GRE-bench (Generating Review Evaluation Benchmark) which evaluates LLMs based on how well they can generate high-quality peer reviews for academic research papers.  Because GRE-bench is based upon GEM, it inherits its robustness properties.  Additionally, GRE-bench circumvents data contamination problems (or data leakage) by using the continuous influx of new open-access research papers and peer reviews each year. We show GRE-bench results of various popular LLMs on their peer review capabilities using the ICLR2023 dataset.",
      "authors": [
        "Shengwei Xu",
        "Yuxuan Lu",
        "Grant Schoenebeck",
        "Yuqing Kong"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=uE84MGbKD7",
      "cdate": 1727471938171,
      "mdate": 1740796898434,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834893"
    },
    {
      "id": "vVhZh9ZpIM",
      "title": "The Pitfalls of Memorization: When Memorization Hurts Generalization",
      "abstract": "Neural networks often learn simple explanations that fit the majority of the data while memorizing exceptions that deviate from these explanations. This behavior leads to poor generalization when the learned explanations rely on spurious correlations. In this work, we formalize $\\textit{the interplay between memorization and generalization}$, showing that spurious correlations would particularly lead to poor generalization when are combined with memorization. Memorization can reduce training loss to zero, leaving no incentive to learn robust, generalizable patterns. To address this, we propose $\\textit{memorization-aware training}$ (MAT), which uses held-out predictions as a signal of memorization to shift a model's logits. MAT encourages learning robust patterns invariant across distributions, improving generalization under distribution shifts.",
      "authors": [
        "Reza Bayat",
        "Mohammad Pezeshki",
        "Elvis Dohmatob",
        "David Lopez-Paz",
        "Pascal Vincent"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=vVhZh9ZpIM",
      "cdate": 1727471862377,
      "mdate": 1743485479236,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834901"
    },
    {
      "id": "WfxPVtYRlL",
      "title": "Graph Neural Networks Gone Hogwild",
      "abstract": "Graph neural networks (GNNs) appear to be powerful tools to learn state representations for agents in distributed, decentralized multi-agent systems, but generate catastrophically incorrect predictions when nodes update asynchronously during inference.\n  This failure under asynchrony effectively excludes these architectures from many potential applications where synchrony is difficult or impossible to enforce, e.g., robotic swarms or sensor networks.\n  In this work we identify ''implicitly-defined'' GNNs as a class of architectures which is provably robust to asynchronous ''hogwild'' inference, adapting convergence guarantees from work in asynchronous and distributed optimization. \n  We then propose a novel implicitly-defined GNN architecture, which we call an energy GNN. \n  We show that this architecture outperforms other GNNs from this class on a variety of synthetic tasks inspired by multi-agent systems.",
      "authors": [
        "Olga Solodova",
        "Nick Richardson",
        "Deniz Oktay",
        "Ryan P Adams"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=WfxPVtYRlL",
      "cdate": 1727471851177,
      "mdate": 1744812384818,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834910"
    },
    {
      "id": "MKEHCx25xp",
      "title": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild",
      "abstract": "We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries. WildBench consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs. For automated evaluation with WildBench, we have developed two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo. WildBench evaluation uses task-specific checklists to evaluate model outputs systematically and provides structured explanations that justify the scores and comparisons, resulting in more reliable and interpretable automatic judgments. WB-Reward employs fine-grained pairwise comparisons between model responses, generating five potential outcomes: much better, slightly better, slightly worse, much worse, or a tie. Unlike previous evaluations that employed a single baseline model, we selected three baseline models at varying performance levels to ensure a comprehensive pairwise evaluation. Additionally, we propose a simple method to mitigate length bias, by converting outcomes of “slightly better/worse” to “tie” if the winner response exceeds the loser one by more than K characters. WB-Score evaluates the quality of model outputs individually, making it a fast and cost-efficient evaluation metric. WildBench results demonstrate a strong correlation with the human-voted Elo ratings from Chatbot Arena on hard tasks. Specifically, WB-Reward achieves a Pearson correlation of 0.98 with top-ranking models. Additionally, WB-Score reaches 0.95, surpassing both ArenaHard’s 0.91 and AlpacaEval2.0’s 0.89 for length-controlled win rates, as well as the 0.87 for regular win rates.",
      "authors": [
        "Bill Yuchen Lin",
        "Yuntian Deng",
        "Khyathi Chandu",
        "Abhilasha Ravichander",
        "Valentina Pyatkin",
        "Nouha Dziri",
        "Ronan Le Bras",
        "Yejin Choi"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=MKEHCx25xp",
      "cdate": 1727471657442,
      "mdate": 1740860842372,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834918"
    },
    {
      "id": "HD6bWcj87Y",
      "title": "Data Shapley in One Training Run",
      "abstract": "Data Shapley offers a principled framework for attributing the contribution of data within machine learning contexts. However, the traditional notion of Data Shapley requires re-training models on various data subsets, which becomes computationally infeasible for large-scale models. Additionally, this retraining-based definition cannot evaluate the contribution of data for a specific model training run, which may often be of interest in practice. This paper introduces a novel concept, In-Run Data Shapley, which eliminates the need for model retraining and is specifically designed for assessing data contribution for a particular model of interest. In-Run Data Shapley calculates the Shapley value for each gradient update iteration and accumulates these values throughout the training process. We present several techniques that allow the efficient scaling of In-Run Data Shapley to the size of foundation models. In its most optimized implementation, our method adds negligible runtime overhead compared to standard model training. This dramatic efficiency improvement makes it possible to perform data attribution for the foundation model pretraining stage. We present several case studies that offer fresh insights into pretraining data's contribution and discuss their implications for copyright in generative AI and pretraining data curation.",
      "authors": [
        "Jiachen T. Wang",
        "Prateek Mittal",
        "Dawn Song",
        "Ruoxi Jia"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=HD6bWcj87Y",
      "cdate": 1727471609170,
      "mdate": 1747565588596,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834931"
    },
    {
      "id": "T2d0geb6y0",
      "title": "Fundamental Limitations on Subquadratic Alternatives to Transformers",
      "abstract": "The Transformer architecture is widely deployed in many popular and impactful Large Language Models. At its core is the attention mechanism for calculating correlations between pairs of tokens. Performing an attention computation takes quadratic time in the input size, and had become the time bottleneck for transformer operations. In order to circumvent this, researchers have used a variety of approaches, including designing heuristic algorithms for performing attention computations faster, and proposing alternatives to the attention mechanism which can be computed more quickly. For instance, state space models  such as Mamba were designed to replace attention with an almost linear time alternative.\n\nIn this paper, we prove that any such approach cannot perform important tasks that Transformer is able to perform (assuming a popular conjecture from fine-grained complexity theory). We focus on document similarity tasks, where one is given as input many documents and would like to find a pair which is (approximately) the most similar. We prove that Transformer is able to perform this task, and we prove that this task cannot be performed in truly subquadratic time by any algorithm. Thus, any model which can be evaluated in subquadratic time – whether because of subquadratic-time heuristics for attention, faster attention replacements like Mamba, or any other reason – cannot perform this task. In other words, in order to perform tasks that (implicitly or explicitly) involve document similarity, one may as well use Transformer and cannot avoid its quadratic running time.",
      "authors": [
        "Josh Alman",
        "Hantao Yu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=T2d0geb6y0",
      "cdate": 1727471605511,
      "mdate": 1740890357093,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.834939"
    },
    {
      "id": "UyhRtB4hjN",
      "title": "Decision Tree Induction Through LLMs via Semantically-Aware Evolution",
      "abstract": "Decision trees are a crucial class of models offering robust predictive performance and inherent interpretability across various domains, including healthcare, finance, and logistics. However, current tree induction methods often face limitations such as suboptimal solutions from greedy methods or prohibitive computational costs and limited applicability of exact optimization approaches.\nTo address these challenges, we propose an evolutionary optimization method for decision tree induction based on genetic programming (GP). Our key innovation is the integration of semantic priors and domain-specific knowledge about the search space into the optimization algorithm.\nTo this end, we introduce $\\texttt{LLEGO}$, a framework that incorporates semantic priors into genetic search operators through the use of Large Language Models (LLMs), thereby enhancing search efficiency and targeting regions of the search space that yield decision trees with superior generalization performance. This is operationalized through novel genetic operators that work with structured natural language prompts, effectively utilizing LLMs as conditional generative models and sources of semantic knowledge. Specifically, we introduce $\\textit{fitness-guided}$ crossover to exploit high-performing regions, and $\\textit{diversity-guided}$ mutation for efficient global exploration of the search space. These operators are controlled by corresponding hyperparameters that enable a more nuanced balance between exploration and exploitation across the search space. Empirically, we demonstrate across various benchmarks that $\\texttt{LLEGO}$ evolves superior-performing trees compared to existing tree induction methods, and exhibits significantly more efficient search performance compared to conventional GP approaches.",
      "authors": [
        "Tennison Liu",
        "Nicolas Huynh",
        "Mihaela van der Schaar"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=UyhRtB4hjN",
      "cdate": 1727471460166,
      "mdate": 1740890357159,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834944"
    },
    {
      "id": "fn36V5qsCw",
      "title": "Efficient Imitation under Misspecification",
      "abstract": "We consider the problem of imitation learning under misspecification: settings where the learner is fundamentally unable to replicate expert behavior everywhere. This is often true in practice due to differences in observation space and action space expressiveness (e.g. perceptual or morphological differences between robots and humans). Given the learner must make some mistakes in the misspecified setting, interaction with the environment is fundamentally required to figure out which mistakes are particularly costly and lead to compounding errors. However, given the computational cost and safety concerns inherent in interaction, we'd like to perform as little of it as possible while ensuring we've learned a strong policy. Accordingly, prior work has proposed a flavor of efficient inverse reinforcement learning algorithms that merely perform a computationally efficient local search procedure with strong guarantees in the realizable setting. We first prove that under a novel structural condition we term reward-agnostic policy completeness, these sorts of local-search based IRL algorithms are able to avoid compounding errors. We then consider the question of where we should perform local search in the first place, given the learner may not be able to \"walk on a tightrope\" as well as the expert in the misspecified setting. We prove that in the misspecified setting, it is beneficial to broaden the set of states on which local search is performed to include those reachable by good policies the learner can actually play. We then experimentally explore a variety of sources of misspecification and how offline data can be used to effectively broaden where we perform local search from.",
      "authors": [
        "Nicolas Espinosa-Dice",
        "Sanjiban Choudhury",
        "Wen Sun",
        "Gokul Swamy"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=fn36V5qsCw",
      "cdate": 1727471455657,
      "mdate": 1743592476242,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834949"
    },
    {
      "id": "FDaHjwInXO",
      "title": "SV-RAG: LoRA-Contextualizing Adaptation of  MLLMs for Long Document Understanding",
      "abstract": "Multimodal large language models (MLLMs) have recently shown great progress in text-rich image understanding, yet they still struggle with complex, multi-page visually-rich documents. Traditional methods using document parsers for retrieval-augmented generation suffer from performance and efficiency limitations, while directly presenting all pages to MLLMs leads to inefficiencies, especially with lengthy ones.  In this work, we present a novel framework named **S**elf-**V**isual **R**etrieval-**A**ugmented **G**eneration (SV-RAG), which can broaden horizons of *any* MLLM to support long-document understanding. We demonstrate that **MLLMs themselves can be an effective multimodal retriever** to fetch relevant pages and then answer user questions based on these pages. SV-RAG is implemented with two specific MLLM adapters, one for evidence page retrieval and the other for question answering. Empirical results show state-of-the-art performance on public benchmarks, demonstrating the effectiveness of SV-RAG.",
      "authors": [
        "Jian Chen",
        "Ruiyi Zhang",
        "Yufan Zhou",
        "Tong Yu",
        "Franck Dernoncourt",
        "Jiuxiang Gu",
        "Ryan A. Rossi",
        "Changyou Chen",
        "Tong Sun"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=FDaHjwInXO",
      "cdate": 1727471287062,
      "mdate": 1741014822216,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.834954"
    },
    {
      "id": "IeRcpsdY7P",
      "title": "Intelligence at the Edge of Chaos",
      "abstract": "We explore the emergence of intelligent behavior in artificial systems by investigating how the complexity of rule-based systems influences the capabilities of models trained to predict these rules. Our study focuses on elementary cellular automata (ECA), simple yet powerful one-dimensional systems that generate behaviors ranging from trivial to highly complex. By training distinct Large Language Models (LLMs) on different ECAs, we evaluated the relationship between the complexity of the rules' behavior and the intelligence exhibited by the LLMs, as reflected in their performance on downstream tasks. Our findings reveal that rules with higher complexity lead to models exhibiting greater intelligence, as demonstrated by their performance on reasoning and chess move prediction tasks. Both uniform and periodic systems, and often also highly chaotic systems, resulted in poorer downstream performance, highlighting a sweet spot of complexity conducive to intelligence. We conjecture that intelligence arises from the ability to predict complexity and that creating intelligence may require only exposure to complexity.",
      "authors": [
        "Shiyang Zhang",
        "Aakash Patel",
        "Syed A Rizvi",
        "Nianchen Liu",
        "Sizhuang He",
        "Amin Karbasi",
        "Emanuele Zappala",
        "David van Dijk"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=IeRcpsdY7P",
      "cdate": 1727471187500,
      "mdate": 1740755685446,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834959"
    },
    {
      "id": "yIlyHJdYV3",
      "title": "A Unified Framework for Forward and Inverse Problems in Subsurface Imaging using Latent Space Translations",
      "abstract": "In subsurface imaging, learning the mapping from velocity maps to seismic waveforms (forward problem) and waveforms to velocity (inverse problem) is important for several applications. While traditional techniques for solving forward and inverse problems are computationally prohibitive, there is a growing interest to leverage recent advances in deep learning to learn the mapping between velocity maps and seismic waveform images directly from data. Despite the variety of architectures explored in previous works, several open questions still remain unanswered such as the effect of latent space sizes, the importance of manifold learning, the complexity of translation models, and the value of jointly solving forward and inverse problems. We propose a unified framework to systematically characterize prior research in this area termed the Generalized Forward-Inverse (GFI) framework, building on the assumption of manifolds and latent space translations. We show that GFI encompasses previous works in deep learning for subsurface imaging, which can be viewed as specific instantiations of GFI. We also propose two new model architectures within the framework of GFI: Latent U-Net and Invertible X-Net, leveraging the power of U-Nets for domain translation and the ability of IU-Nets to simultaneously learn forward and inverse translations, respectively. We show that our proposed models achieve state-of-the-art (SOTA) performance for forward and inverse problems on a wide range of synthetic datasets, and also investigate their zero-shot effectiveness on two real-world-like datasets. The code is available at https://github.com/KGML-lab/Generalized-Forward-Inverse-Framework-for-DL4SI",
      "authors": [
        "Naveen Gupta",
        "Medha Sawhney",
        "Arka Daw",
        "Youzuo Lin",
        "Anuj Karpatne"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=yIlyHJdYV3",
      "cdate": 1727470997743,
      "mdate": 1740785286002,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834965"
    },
    {
      "id": "KZu3xhPhke",
      "title": "Cauchy-Schwarz Regularizers",
      "abstract": "We introduce a novel class of regularization functions, called Cauchy–Schwarz (CS) regularizers, which can be designed to induce a wide range of properties in solution vectors of optimization problems. To demonstrate the versatility of CS regularizers, we derive regularization functions that promote discrete-valued vectors, eigenvectors of a given matrix, and orthogonal matrices. The resulting CS regularizers are simple, differentiable, and can be free of spurious stationary points, making them suitable for gradient-based solvers and large-scale optimization problems. In addition, CS regularizers automatically adapt to the appropriate scale, which is, for example, beneficial when discretizing the weights of neural networks. To demonstrate the efficacy of CS regularizers, we provide results for solving underdetermined systems of linear equations and weight quantization in neural networks. Furthermore, we discuss specializations, variations, and generalizations, which lead to an even broader class of new and possibly more powerful regularizers.",
      "authors": [
        "Sueda Taner",
        "Ziyi Wang",
        "Christoph Studer"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=KZu3xhPhke",
      "cdate": 1727470787860,
      "mdate": 1741963381633,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834970"
    },
    {
      "id": "tGYFikNONB",
      "title": "Holographic Node Representations: Pre-training Task-Agnostic Node Embeddings",
      "abstract": "Large general purpose pre-trained models have revolutionized computer vision and natural language understanding. However, the development of general purpose pre-trained Graph Neural Networks (GNNs) lags behind other domains due to the lack of suitable generalist node representations. Existing GNN architectures are often tailored to specific task orders, such as node-level, link-level, or higher-order tasks, because different tasks require distinct permutation symmetries, which are difficult to reconcile within a single model. In this paper, we propose _holographic node representations_, a new blueprint for node representations capable of solving tasks of any order. Holographic node representations have two key components: (1) a task-agnostic expansion map, which produces highly expressive, high-dimensional embeddings, free from node-permutation symmetries, to be fed into (2) a reduction map that carefully reintroduces the relevant permutation symmetries to produce low-dimensional, task-specific embeddings. We show that well-constructed expansion maps enable simple and efficient reduction maps, which can be adapted for any task order. Empirical results show that holographic node representations can be effectively pre-trained and reused across tasks of varying orders, yielding up to 100% relative performance improvement, including in cases where prior methods fail entirely.",
      "authors": [
        "Beatrice Bevilacqua",
        "Joshua Robinson",
        "Jure Leskovec",
        "Bruno Ribeiro"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=tGYFikNONB",
      "cdate": 1727470714942,
      "mdate": 1740781776728,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.834975"
    },
    {
      "id": "hyfe5q5TD0",
      "title": "Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics",
      "abstract": "We study computationally and statistically efficient Reinforcement Learning algorithms for the *linear Bellman Complete* setting. This setting uses linear function approximation to capture value functions and unifies existing models like linear Markov Decision Processes (MDP) and Linear Quadratic Regulators (LQR).  While it is known from the prior works that this setting is statistically tractable, it remained open whether a computationally efficient algorithm exists. Our work provides a computationally efficient algorithm for the linear Bellman complete setting that works for MDPs with large action spaces, random initial states, and random rewards but relies on the underlying dynamics to be deterministic. Our approach is based on randomization: we inject random noise into least squares regression problems to perform optimistic value iteration. Our key technical contribution is to carefully design the noise to only act in the null space of the training data to ensure optimism while circumventing a subtle error amplification issue.",
      "authors": [
        "Runzhe Wu",
        "Ayush Sekhari",
        "Akshay Krishnamurthy",
        "Wen Sun"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=hyfe5q5TD0",
      "cdate": 1727470474668,
      "mdate": 1740890356989,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.834981"
    },
    {
      "id": "KSBx6FBZpE",
      "title": "Uncovering Latent Memories in Large Language Models",
      "abstract": "Frontier AI systems are making transformative impacts across society, but such benefits are not without costs: models trained on web-scale datasets containing personal and private data raise profound concerns about data privacy and security. Language models are trained on extensive corpora including potentially sensitive or proprietary information, and the risk of data leakage, where the model response reveals pieces of such information, remains inadequately understood. Prior work has investigated that sequence complexity and the number of repetitions are the primary drivers of memorization. In this work, we examine the most vulnerable class of data: highly complex sequences that are presented only once during training. These sequences often contain the most sensitive information and pose considerable risk if memorized. By analyzing the progression of memorization for these sequences throughout training, we uncover a striking observation: many memorized sequences persist in the model's memory, exhibiting resistance to catastrophic forgetting even after just one encounter. Surprisingly, these sequences may not appear memorized immediately after their first exposure but can later be “uncovered” during training, even in the absence of subsequent exposures - a phenomenon we call \"latent memorization.\" Latent memorization presents a serious challenge for data privacy, as sequences that seem hidden at the final checkpoint of a model may still be easily recoverable. We demonstrate how these hidden sequences can be revealed through random weight perturbations, and we introduce a diagnostic test based on cross-entropy loss to accurately identify latent memorized sequences.",
      "authors": [
        "Sunny Duan",
        "Mikail Khona",
        "Abhiram Iyer",
        "Rylan Schaeffer",
        "Ila R Fiete"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=KSBx6FBZpE",
      "cdate": 1727470393914,
      "mdate": 1740750872232,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834989"
    },
    {
      "id": "6ycX677p2l",
      "title": "Episodic Memories Generation and Evaluation Benchmark for Large Language Models",
      "abstract": "Episodic memory -- the ability to recall specific events grounded in time and space -- is a cornerstone of human cognition, enabling not only coherent storytelling, but also planning and decision-making. Despite their remarkable capabilities, Large Language Models (LLMs) lack a robust mechanism for episodic memory: we argue that integrating episodic memory capabilities into LLM is essential for advancing AI towards human-like cognition, increasing their potential to reason consistently and ground their output in real-world episodic events, hence avoiding confabulations. To address this challenge, we introduce a comprehensive framework to model and evaluate LLM episodic memory capabilities. Drawing inspiration from cognitive science, we develop a structured approach to represent episodic events, encapsulating temporal and spatial contexts, involved entities, and detailed descriptions. We synthesize a unique episodic memory benchmark, free from contamination, and release open source code and datasets to assess LLM performance across various recall and episodic reasoning tasks. Our evaluation of state-of-the-art models, including GPT-4 and Claude variants, Llama 3.1, and o1-mini, reveals that even the most advanced LLMs struggle with episodic memory tasks, particularly when dealing with multiple related events or complex spatio-temporal relationships -- even in contexts as short as 10k-100k tokens.",
      "authors": [
        "Alexis Huet",
        "Zied Ben Houidi",
        "Dario Rossi"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=6ycX677p2l",
      "cdate": 1727470372022,
      "mdate": 1739261764032,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834994"
    },
    {
      "id": "9VMW4iXfKt",
      "title": "R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference",
      "abstract": "Large Language Models (LLMs), while demonstrating remarkable capabilities across various applications, present significant challenges during inference due to their substantial model size, especially when deployed on edge devices. Activation sparsity offers a promising solution to reduce computation and memory movement, enabling more efficient inference, particularly for small-batch on-device applications. However, current approaches face limitations with non-ReLU activation function, which are foundational to most advanced LLMs, or require heavy continual training. Additionally, the difficulty in predicting active channels and limited achievable sparsity ratios constrain the effectiveness of activation sparsity-based methods. In this paper, we introduce R-Sparse, a training-free activation sparsity approach capable of achieving high sparsity levels in advanced LLMs. We conducted two preliminary investigations into how different components contribute to the output within a single linear layer and found two key observations: (i) the non-sparse components of the input function can be regarded as a few bias terms, and (ii) The full computation can be effectively approximated by an appropriate combination of input channels and weight singular values. Building on this, we replace the linear layers in LLMs with a rank-aware sparse inference method that leverages the sparsity of input channels and singular value components, eliminating the need for active channel prediction like the output sparsity based approaches. Experiments on Llama-2/3 and Mistral models across ten diverse tasks demonstrate that R-Sparse achieves comparable performance at 50\\% model-level sparsity, resulting in a significant 43\\% end-to-end efficient improvements with customized kernels.",
      "authors": [
        "Zhenyu Zhang",
        "Zechun Liu",
        "Yuandong Tian",
        "Harshit Khaitan",
        "Zhangyang Wang",
        "Steven Li"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=9VMW4iXfKt",
      "cdate": 1727470160364,
      "mdate": 1740790557906,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.834999"
    },
    {
      "id": "SMK0f8JoKF",
      "title": "Language Models Are Implicitly Continuous",
      "abstract": "Language is typically modelled with discrete sequences. However, the most successful approaches to language modelling, namely neural networks, are continuous and smooth function approximators.\nIn this work, we show that Transformer-based language models implicitly learn to represent sentences as continuous-time functions defined over a continuous input space. \nThis phenomenon occurs in most state-of-the-art Large Language Models (LLMs), including Llama2, Llama3, Phi3, Gemma, Gemma2, and Mistral, and suggests that LLMs reason about language in ways that fundamentally differ from humans.\nOur work formally extends Transformers to capture the nuances of time and space continuity in both input and output space.\nOur results challenge the traditional interpretation of how LLMs understand language, with several linguistic and engineering implications.",
      "authors": [
        "Samuele Marro",
        "Davide Evangelista",
        "X. Angelo Huang",
        "Emanuele La Malfa",
        "Michele Lombardi",
        "Michael J. Wooldridge"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=SMK0f8JoKF",
      "cdate": 1727469993164,
      "mdate": 1747090088157,
      "matched_keywords": [
        "large language model",
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835004"
    },
    {
      "id": "v4MTnPiYXY",
      "title": "Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning",
      "abstract": "Value-based reinforcement learning (RL) can in principle learn effective policies for a wide range of multi-turn problems, from games to dialogue to robotic control, including via offline RL from static previously collected datasets. However, despite the widespread use of policy gradient methods to train large language models for single turn tasks (e.g., question answering), value-based methods for multi-turn RL in an off-policy or offline setting have proven particularly challenging to scale to the setting of large language models. This setting requires effectively leveraging pretraining, scaling to large architectures with billions of parameters, and training on large datasets, all of which represent major challenges for current value-based RL methods. In this work, we propose a novel offline RL algorithm that addresses these drawbacks, casting Q-learning as a modified supervised fine-tuning (SFT) problem where the probabilities of tokens directly translate to Q-values. In this way we obtain an algorithm that smoothly transitions from maximizing the likelihood of the data during pretraining to learning a near-optimal Q-function during finetuning. Our algorithm has strong theoretical foundations, enjoying performance bounds similar to state-of-the-art Q-learning methods, while in practice utilizing an objective that closely resembles SFT. Because of this, our approach can enjoy the full benefits of the pretraining of language models, without the need to reinitialize any weights before RL finetuning, and without the need to initialize new heads for predicting values or advantages. Empirically, we evaluate our method on both pretrained LLMs and VLMs, on a variety of tasks including both natural language dialogue and robotic manipulation and navigation from images.",
      "authors": [
        "Joey Hong",
        "Anca Dragan",
        "Sergey Levine"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=v4MTnPiYXY",
      "cdate": 1727469835285,
      "mdate": 1742096759748,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835013"
    },
    {
      "id": "i45NQb2iKO",
      "title": "MM-EMBED: UNIVERSAL MULTIMODAL RETRIEVAL WITH MULTIMODAL LLMS",
      "abstract": "State-of-the-art retrieval models typically address a straightforward search scenario, in which retrieval tasks are fixed (e.g., finding a passage to answer a specific question) and only a single modality is supported for both queries and retrieved results. This paper introduces techniques for advancing information retrieval with multimodal large language models (MLLMs), enabling a broader search scenario, termed universal multimodal retrieval, where multiple modalities and diverse retrieval tasks are accommodated. To this end, we first study fine-tuning an MLLM as a bi-encoder retriever on 10 datasets with 16 retrieval tasks. Our empirical results show that the fine-tuned MLLM retriever is capable of understanding challenging queries, composed of both text and image, but it underperforms compared to a smaller CLIP retriever in cross-modal retrieval tasks due to the modality bias exhibited by MLLMs. To address the issue, we propose modality-aware hard negative mining to mitigate the modality bias exhibited by MLLM retrievers. Second, we propose continuously fine-tuning the universal multimodal retriever to enhance its text retrieval capability while preserving multimodal retrieval capability. As a result, our model, MM-Embed, achieves state-of-the-art performance on the multimodal retrieval benchmark M-BEIR, which spans multiple domains and tasks, while also surpassing the state-of-the-art text retrieval model, NV-Embed-v1, on the MTEB retrieval benchmark. Finally, we explore prompting the off-the-shelf MLLMs as zero-shot rerankers to refine the ranking of the candidates from the multimodal retriever. We find that, through prompt-and-reranking, MLLMs can further improve multimodal retrieval when the user queries (e.g., text-image composed queries) are more complex and challenging to understand. These findings also pave the way for advancing universal multimodal retrieval in the future. We release the model weights at: https://huggingface.co/nvidia/MM-Embed.",
      "authors": [
        "Sheng-Chieh Lin",
        "Chankyu Lee",
        "Mohammad Shoeybi",
        "Jimmy Lin",
        "Bryan Catanzaro",
        "Wei Ping"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=i45NQb2iKO",
      "cdate": 1727469809913,
      "mdate": 1740890356853,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.835018"
    },
    {
      "id": "h3wbI8Uk1Z",
      "title": "RNNs are not Transformers (Yet):  The Key Bottleneck on In-Context Retrieval",
      "abstract": "This paper investigates the gap in representation powers of Transformers and Recurrent Neural Networks (RNNs), which are more memory efficient than Transformers. We aim to understand whether RNNs can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: \nfor several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease.\nConversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, including Retrieval-Augmented Generation (RAG) and adding a single Transformer layer, can elevate RNNs to be capable of solving all polynomial-time solvable problems with CoT, hence closing the representation gap with Transformers. We validate our theory on synthetic and natural language experiments.",
      "authors": [
        "Kaiyue Wen",
        "Xingyu Dang",
        "Kaifeng Lyu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=h3wbI8Uk1Z",
      "cdate": 1727469676035,
      "mdate": 1740893351935,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835023"
    },
    {
      "id": "cVyELMpMRS",
      "title": "Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF",
      "abstract": "Large Language Models (LLMs) have achieved remarkable success at tasks like summarization that involve a single turn of interaction. However, they can still struggle with multi-turn tasks like dialogue that require long-term planning. Previous works on multi-turn dialogue extend single-turn reinforcement learning from human feedback (RLHF) methods to the multi-turn setting by treating all prior dialogue turns as a long context. Such approaches suffer from covariate shift: the conversations in the training set have previous turns generated by some reference policy, which means that low training error may not necessarily correspond to good performance when the learner is actually in the conversation loop. In response, we introduce REgressing the RELative FUture (REFUEL), an efficient policy optimization approach designed to address multi-turn RLHF in LLMs. REFUEL employs a single model to estimate $Q$-values and trains on self-generated data, addressing the covariate shift issue. REFUEL frames the multi-turn RLHF problem as a sequence of regression tasks on iteratively collected datasets, enabling ease of implementation. Theoretically, we prove that REFUEL can match the performance of any policy covered by the training set. Empirically, we evaluate our algorithm by using Llama-3.1-70B-it to simulate a user in conversation with our model. REFUEL consistently outperforms state-of-the-art methods such as DPO and REBEL across various settings. Furthermore, despite having only 8 billion parameters, Llama-3-8B-it fine-tuned with REFUEL outperforms Llama-3.1-70B-it on long multi-turn dialogues. Implementation of REFUEL can be found at https://github.com/ZhaolinGao/REFUEL/, and models trained by REFUEL can be found at https://huggingface.co/Cornell-AGI.",
      "authors": [
        "Zhaolin Gao",
        "Wenhao Zhan",
        "Jonathan Daniel Chang",
        "Gokul Swamy",
        "Kianté Brantley",
        "Jason D. Lee",
        "Wen Sun"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=cVyELMpMRS",
      "cdate": 1727469637189,
      "mdate": 1740714530241,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835028"
    },
    {
      "id": "rwqShzb9li",
      "title": "Linear Representations of Political Perspective Emerge in Large Language Models",
      "abstract": "Large language models (LLMs) have demonstrated the ability to generate text that realistically reflects a range of different subjective human perspectives. This paper studies how LLMs are seemingly able to reflect more liberal versus more conservative viewpoints among other political perspectives in American politics. We show that LLMs possess linear representations of political perspectives within activation space, wherein more similar perspectives are represented closer together. To do so, we probe the attention heads across the layers of three open transformer-based LLMs (Llama-2-7b-chat, Mistral-7b-instruct, Vicuna-7b). We first prompt models to generate text from the perspectives of different U.S. lawmakers. We then identify sets of attention heads whose activations linearly predict those lawmakers' DW-NOMINATE scores, a widely-used and validated measure of political ideology. We find that highly predictive heads are primarily located in the middle layers, often speculated to encode high-level concepts and tasks. Using probes only trained to predict lawmakers' ideology, we then show that the same probes can predict measures of news outlets' slant from the activations of models prompted to simulate text from those news outlets. These linear probes allow us to visualize, interpret, and monitor ideological stances implicitly adopted by an LLM as it generates open-ended responses. Finally, we demonstrate that by applying linear interventions to these attention heads, we can steer the model outputs toward a more liberal or conservative stance. Overall, our research suggests that LLMs possess a high-level linear representation of American political ideology and that by leveraging recent advances in mechanistic interpretability, we can identify, monitor, and steer the subjective perspective underlying generated text.",
      "authors": [
        "Junsol Kim",
        "James Evans",
        "Aaron Schein"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=rwqShzb9li",
      "cdate": 1727469544048,
      "mdate": 1743582084690,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835033"
    },
    {
      "id": "LuT2CVrlpU",
      "title": "Behavioral Entropy-Guided Dataset Generation for Offline Reinforcement Learning",
      "abstract": "Entropy-based objectives are widely used to perform state space exploration in reinforcement learning (RL) and dataset generation for offline RL. Behavioral entropy (BE), a rigorous generalization of classical entropies that incorporates cognitive and perceptual biases of agents, was recently proposed for discrete settings and shown to be a promising metric for robotic exploration problems. In this work, we propose using BE as a principled exploration objective for systematically generating datasets that provide diverse state space coverage in complex, continuous, potentially high-dimensional domains. To achieve this, we extend the notion of BE to continuous settings, derive tractable $k$-nearest neighbor estimators, provide theoretical guarantees for these estimators, and develop practical reward functions that can be used with standard RL methods to learn BE-maximizing policies. Using standard MuJoCo environments, we experimentally compare the performance of offline RL algorithms for a variety of downstream tasks on datasets generated using BE, R\\'{e}nyi, and Shannon entropy-maximizing policies, as well as the SMM and RND algorithms. We find that offline RL algorithms trained on datasets collected using BE outperform those trained on datasets collected using Shannon entropy, SMM, and RND on all tasks considered, and on 80\\% of the tasks compared to datasets collected using Renyi entropy.",
      "authors": [
        "Wesley A. Suttle",
        "Aamodh Suresh",
        "Carlos Nieto-Granda"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=LuT2CVrlpU",
      "cdate": 1727469422614,
      "mdate": 1740694018798,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835038"
    },
    {
      "id": "u2QdCiOgwA",
      "title": "Context-aware Dynamic Pruning for Speech Foundation Models",
      "abstract": "Foundation models, such as large language models, have achieved remarkable success in natural language processing and are evolving into models capable of handling multiple modalities.\nListening ability, in particular, is crucial for many applications, leading to research on building speech foundation models. However, the high computational cost of these large models presents a significant challenge for real-world applications. Although substantial efforts have been made to reduce computational costs, such as through pruning techniques, the majority of these approaches are applied primarily during the training phase for specific downstream tasks. In this study, we hypothesize that optimal pruned networks may vary based on contextual factors such as speaker characteristics, languages, and tasks. To address this, we propose a dynamic pruning technique that adapts to these contexts during inference without altering the underlying model. We demonstrated that we could successfully reduce inference time by approximately 30\\% while maintaining accuracy in multilingual/multi-task scenarios. We also found that the obtained pruned structure offers meaningful interpretations based on the context, e.g., task-related information emerging as the dominant factor for efficient pruning.",
      "authors": [
        "Masao Someki",
        "Yifan Peng",
        "Siddhant Arora",
        "Markus Müller",
        "Athanasios Mouchtaris",
        "Grant Strimel",
        "Jing Liu",
        "Shinji Watanabe"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=u2QdCiOgwA",
      "cdate": 1727469356065,
      "mdate": 1742059261273,
      "matched_keywords": [
        "large language model",
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835046"
    },
    {
      "id": "FBkpCyujtS",
      "title": "Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs",
      "abstract": "Large Language Models (LLMs) generate text by sampling the next token from a probability distribution over the vocabulary at each decoding step. Popular sampling methods like top-p (nucleus sampling) often struggle to balance quality and diversity, especially at higher temperatures which lead to incoherent or repetitive outputs. We propose min-p sampling, a dynamic truncation method that adjusts the sampling threshold based on the model's confidence by using the top token's probability as a scaling factor. Our experiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative Writing show that min-p sampling improves both the quality and diversity of generated text across different model families (Mistral and Llama 3) and model sizes (1B to 123B parameters), especially at higher temperatures. Human evaluations further show a clear preference for min-p sampling, in both text quality and creativity. Min-p sampling has been adopted by popular open-source LLM frameworks, including Hugging Face Transformers, VLLM, and many others, highlighting its considerable impact on improving text generation quality.",
      "authors": [
        "Nguyen Nhat Minh",
        "Andrew Baker",
        "Clement Neo",
        "Allen G Roush",
        "Andreas Kirsch",
        "Ravid Shwartz-Ziv"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=FBkpCyujtS",
      "cdate": 1727468900433,
      "mdate": 1747564148114,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835055"
    },
    {
      "id": "2fojNANZSv",
      "title": "Mixture of In-Context Prompters for Tabular PFNs",
      "abstract": "Recent benchmarks find In-Context Learning (ICL) outperforms both deep learning and tree-based algorithms on small tabular datasets. However, on larger datasets, ICL for tabular learning suffers in both efficiency and effectiveness. In terms of efficiency, transformers incur linear space and quadratic time complexity w.r.t. context size. In terms of effectiveness, contexts at inference encounter distribution shift compared to contexts from pretraining. We propose MixturePFN, which extends Sparse Mixture of Experts to the state-of-the-art ICL for tabular learning model. Specifically, MixturePFN finetunes a specialized ICL expert on each cluster of tabular data and routes new test samples to appropriate experts at inference. MixturePFN supports constant-size contexts by splitting large training datasets into more manageable clusters. MixturePFN addresses distribution shift by finetuning an expert on each training dataset cluster via bootstrapping. Extensive experimental results shows MixturePFN outperforms 19 baselines both in mean rank and as the Condorcet winner across 36 diverse tabular datasets under both accuracy and F1 score with statistical significance.",
      "authors": [
        "Derek Qiang Xu",
        "F Olcay Cirit",
        "Reza Asadi",
        "Yizhou Sun",
        "Wei Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=2fojNANZSv",
      "cdate": 1727468725478,
      "mdate": 1740897150438,
      "matched_keywords": [
        "transformer",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835060"
    },
    {
      "id": "Sf4ep9Udjf",
      "title": "P-SPIKESSM: HARNESSING PROBABILISTIC SPIKING STATE SPACE MODELS FOR LONG-RANGE DEPENDENCY TASKS",
      "abstract": "Spiking neural networks (SNNs) are posited as a computationally efficient and biologically plausible alternative to conventional neural architectures, with their core computational framework primarily using the leaky integrate-and-fire (LIF) neuron model. However, the limited hidden state representation of LIF neurons, characterized by a scalar membrane potential, and sequential spike generation process, poses challenges for effectively developing scalable spiking models to address long-range dependencies in sequence learning tasks. In this study, we  develop a scalable probabilistic spiking learning framework for long-range dependency tasks leveraging the fundamentals of state space models. Unlike LIF neurons that rely on the deterministic Heaviside function for a sequential process of spike generation, we introduce a SpikeSampler layer that samples spikes stochastically based on an SSM-based neuronal model while allowing parallel computations. To address non-differentiability of the spiking operation and enable effective training, we also propose a surrogate function tailored for the stochastic nature of the SpikeSampler layer. To enhance inter-neuron communication, we introduce the SpikeMixer block, which integrates spikes from neuron populations in each layer. This is followed by a ClampFuse layer, incorporating a residual connection to capture complex dependencies, enabling scalability of the model. Our models attain state-of-the-art performance among SNN models across diverse long-range dependency tasks, encompassing the Long Range Arena benchmark, permuted sequential MNIST, and the Speech Command dataset and demonstrate sparse spiking pattern highlighting its computational efficiency.",
      "authors": [
        "Malyaban Bal",
        "Abhronil Sengupta"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Sf4ep9Udjf",
      "cdate": 1727468416222,
      "mdate": 1742088110533,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835066"
    },
    {
      "id": "xGs7Ch3Vyo",
      "title": "Better autoregressive regression with LLMs via regression-aware fine-tuning",
      "abstract": "Decoder-based large language models (LLMs) have proven highly versatile, with remarkable successes even on problems ostensibly removed from traditional language generation.  One such example is solving regression problems, where the targets are real numbers rather than textual tokens.  A common approach to use LLMs on such problems is to perform fine-tuning based on the cross-entropy loss, and use autoregressive sampling at inference time. Another approach relies on fine-tuning a separate predictive head with a suitable loss such as squared error. While each approach has had success, there has been limited study on principled ways of using decoder LLMs for regression. In this work, we compare different prior works under a unified view, and introduce regression-aware fine-tuning(RAFT), a novel approach based on the Bayes-optimal decision rule. We demonstrate how RAFT improves over established baselines on several benchmarks and model families.",
      "authors": [
        "Michal Lukasik",
        "Zhao Meng",
        "Harikrishna Narasimhan",
        "Yin-Wen Chang",
        "Aditya Krishna Menon",
        "Felix Yu",
        "Sanjiv Kumar"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=xGs7Ch3Vyo",
      "cdate": 1727468397683,
      "mdate": 1742018961435,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835074"
    },
    {
      "id": "jwsPS8yRe4",
      "title": "Trained Transformer Classifiers Generalize and Exhibit Benign Overfitting In-Context",
      "abstract": "Transformers have the capacity to act as supervised learning algorithms: by properly encoding a set of labeled training (''in-context'') examples and an unlabeled test example into an input sequence of vectors of the same dimension, the forward pass of the transformer can produce predictions for that unlabeled test example.  A line of recent work has shown that when linear transformers are pre-trained on random instances for linear regression tasks, these trained transformers make predictions using an algorithm similar to that of ordinary least squares.  In this work, we investigate the behavior of linear transformers trained on random linear classification tasks.  Via an analysis of the implicit regularization of gradient descent, we characterize how many pre-training tasks and in-context examples are needed for the trained transformer to generalize well at test-time.  We further show that in some settings, these trained transformers can exhibit ''benign overfitting in-context'': when in-context examples are corrupted by label flipping noise, the transformer memorizes all of its in-context examples (including those with noisy labels) yet still generalizes near-optimally for clean test examples.",
      "authors": [
        "Spencer Frei",
        "Gal Vardi"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=jwsPS8yRe4",
      "cdate": 1727468318980,
      "mdate": 1740816713722,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835079"
    },
    {
      "id": "wFD16gwpze",
      "title": "Analyzing Neural Scaling Laws in Two-Layer Networks with Power-Law Data Spectra",
      "abstract": "Neural scaling laws describe how the performance of deep neural networks scales with key factors such as training data size, model complexity, and training time, often following power-law behaviors over multiple orders of magnitude. Despite their empirical observation, the theoretical understanding of these scaling laws remains limited. In this work, we employ techniques from statistical mechanics to analyze one-pass stochastic gradient descent within a student-teacher framework, where both the student and teacher are two-layer neural networks. Our study primarily focuses on the generalization error and its behavior in response to data covariance matrices that exhibit power-law spectra.\nFor linear activation functions, we derive analytical expressions for the generalization error, exploring different learning regimes and identifying conditions under which power-law scaling emerges. Additionally, we extend our analysis to non-linear activation functions in the feature learning regime, investigating how power-law spectra in the data covariance matrix impact learning dynamics. Importantly, we find that the length of the symmetric plateau depends on the number of distinct eigenvalues of the data covariance matrix and the number of hidden units, demonstrating how these plateaus behave under various configurations. In addition, our results reveal a transition from exponential to power-law convergence in the specialized phase when the data covariance matrix possesses a power-law spectrum. This work contributes to the theoretical understanding of neural scaling laws and provides insights into optimizing learning performance in practical scenarios involving complex data structures.",
      "authors": [
        "Roman Worschech",
        "Bernd Rosenow"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=wFD16gwpze",
      "cdate": 1727468269144,
      "mdate": 1747404430706,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835084"
    },
    {
      "id": "tn2mjzjSyR",
      "title": "DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search",
      "abstract": "Enhancing the capability of large language models (LLMs) in reasoning has gained significant attention in recent years. Previous studies have demonstrated the effectiveness of various prompting strategies in aiding LLMs in reasoning (called \"reasoning actions\"), such as step-by-step thinking, reflecting before answering, solving with programs, and their combinations. However, these approaches often applied static, predefined reasoning actions uniformly to all questions, without considering the specific characteristics of each question or the capability of the task-solving LLM. In this paper, we propose DOTS, an approach enabling LLMs to reason Dynamically via Optimal reasoning Trajectories Search, tailored to the specific characteristics of each question and the inherent capability of the task-solving LLM. \nOur approach involves three key steps: i) defining atomic reasoning action modules that can be composed into various reasoning action trajectories; ii) searching for the optimal action trajectory for each training question through iterative exploration and evaluation for the specific task-solving LLM; and iii) using the collected optimal trajectories to train an LLM to plan for the reasoning trajectories of unseen questions. In particular, we propose two learning paradigms, i.e., fine-tuning an external LLM as a planner to guide the task-solving LLM, or directly fine-tuning the task-solving LLM with an internalized capability for reasoning actions planning. Our experiments across eight reasoning tasks show that our method consistently outperforms static reasoning techniques and the vanilla instruction tuning approach. Further analysis reveals that our method enables LLMs to adjust their computation based on problem complexity, allocating deeper thinking and reasoning to harder problems.",
      "authors": [
        "Murong Yue",
        "Wenlin Yao",
        "Haitao Mi",
        "Dian Yu",
        "Ziyu Yao",
        "Dong Yu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=tn2mjzjSyR",
      "cdate": 1727468098406,
      "mdate": 1740886904749,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835089"
    },
    {
      "id": "dqyuCsBvn9",
      "title": "Learning Diagrams: A Graphical Language for Compositional Training Regimes",
      "abstract": "Motivated by deep learning regimes with multiple interacting yet distinct model components, we introduce learning diagrams, graphical depictions of training setups that capture parameterized learning as data rather than code. A learning diagram compiles to a unique loss function on which component models are trained. The result of training on this loss is a collection of models whose predictions ``agree\" with one another. We show that a number of popular learning setups such as few-shot multi-task learning, knowledge distillation, and multi-modal learning can be depicted as learning diagrams. We further implement learning diagrams in a library that allows users to build diagrams of PyTorch and Flux.jl models. By implementing some classic machine learning use cases, we demonstrate how learning diagrams allow practitioners to build complicated models as compositions of smaller components, identify relationships between workflows, and manipulate models during or after training. Leveraging a category theoretic framework, we introduce a rigorous semantics for learning diagrams that puts such operations on a firm mathematical foundation.",
      "authors": [
        "Mason Lary",
        "Richard Samuelson",
        "Alexander Wilentz",
        "Alina Zare",
        "Matthew Klawonn",
        "James Fairbanks"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=dqyuCsBvn9",
      "cdate": 1727468066045,
      "mdate": 1740811401264,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835095"
    },
    {
      "id": "INyi7qUdjZ",
      "title": "Differential learning kinetics govern the transition from memorization to generalization during in-context learning",
      "abstract": "Transformers exhibit in-context learning (ICL): the ability to use novel information presented in the context without additional weight updates. Recent work shows that ICL emerges when models are trained on a sufficiently diverse set of tasks and the transition from memorization to generalization is sharp with increasing task diversity. One interpretation is that a network's limited capacity to memorize favors generalization. Here, we examine the mechanistic underpinnings of this transition using a small transformer applied to a synthetic ICL task. Using theory and experiment, we show that the sub-circuits that memorize and generalize can be viewed as largely independent. The relative *rates* at which these sub-circuits learn explains the transition from memorization to generalization, rather than capacity constraints. We uncover a memorization scaling law, which determines the task diversity threshold at which the network generalizes. The theory quantitatively explains a variety of other ICL-related phenomena, including the long-tailed distribution of when ICL is acquired, the bimodal behavior of solutions close to the task diversity threshold, the influence of contextual and data distributional statistics on ICL, and the transient nature of ICL.",
      "authors": [
        "Alex Nguyen",
        "Gautam Reddy"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=INyi7qUdjZ",
      "cdate": 1727468065016,
      "mdate": 1744056318378,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835100"
    },
    {
      "id": "IuU0wcO0mo",
      "title": "Multi-session, multi-task neural decoding from distinct cell-types and brain regions",
      "abstract": "Recent work has shown that scale is important for improved brain decoding, with more data leading to greater decoding accuracy. However, large-scale decoding across many different datasets is challenging because neural circuits are heterogeneous---each brain region contains a unique mix of cellular sub-types, and the responses to different stimuli are diverse across regions and sub-types. It is unknown whether it is possible to pre-train and transfer brain decoding models between distinct tasks, cellular sub-types, and brain regions. To address these questions, we developed a multi-task transformer architecture and trained it on the entirety of the Allen Institute's Brain Observatory dataset. This dataset contains responses from over 100,000 neurons in 6 areas of the brains of mice, observed with two-photon calcium imaging, recorded while the mice observed different types of visual stimuli. Our results demonstrate that transfer is indeed possible -combining data from different sources is beneficial for a number of downstream decoding tasks. As well, we can transfer the model between regions and sub-types, demonstrating that there is in fact common information in diverse circuits that can be extracted by an appropriately designed model. Interestingly, we found that the model's latent representations showed clear distinctions between different brain regions and cellular sub-types, even though it was never given any information about these distinctions. Altogether, our work demonstrates that training a large-scale neural decoding model on diverse data is possible, and this provides a means of studying the differences and similarities between heterogeneous neural circuits.",
      "authors": [
        "Mehdi Azabou",
        "Krystal Xuejing Pan",
        "Vinam Arora",
        "Ian Jarratt Knight",
        "Eva L Dyer",
        "Blake Aaron Richards"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=IuU0wcO0mo",
      "cdate": 1727467830062,
      "mdate": 1740899007738,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835111"
    },
    {
      "id": "Hcb2cgPbMg",
      "title": "Learning Continually by Spectral Regularization",
      "abstract": "Loss of plasticity is a phenomenon where neural networks can become more difficult to train over the course of learning. Continual learning algorithms seek to mitigate this effect by sustaining good performance while maintaining network trainability. We develop a new technique for improving continual learning inspired by the observation that the singular values of the neural network parameters at initialization are an important factor for trainability during early phases of learning. From this perspective, we derive a new spectral regularizer for continual learning that better sustains these beneficial initialization properties throughout training. In particular, the regularizer keeps the maximum singular value of each layer close to one. Spectral regularization directly ensures that gradient diversity is maintained throughout training, which promotes continual trainability, while minimally interfering with performance in a single task. We present an experimental analysis that shows how the proposed spectral regularizer can sustain trainability and performance across a range of model architectures in continual supervised and reinforcement learning settings. Spectral regularization is less sensitive to hyperparameters while demonstrating better training in individual tasks, sustaining trainability as new tasks arrive, and achieving better generalization performance..",
      "authors": [
        "Alex Lewandowski",
        "Michał Bortkiewicz",
        "Saurabh Kumar",
        "András György",
        "Dale Schuurmans",
        "Mateusz Ostaszewski",
        "Marlos C. Machado"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Hcb2cgPbMg",
      "cdate": 1727467689503,
      "mdate": 1744084282680,
      "matched_keywords": [
        "reinforcement learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835116"
    },
    {
      "id": "vQhn4wrQ6j",
      "title": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models",
      "abstract": "Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-English languages, where task-specific data is often unavailable. We focus on mathematical reasoning and without in-language math data, facilitate cross-lingual transfer by composing language and math capabilities. Starting from the same pretrained model, we fine-tune separate \"experts\" on math instruction data in English and on generic instruction data in the target language. We then replace the top and bottom transformer layers of the math expert directly with layers from the language expert, which consequently enhances math performance in the target language. The resulting merged models outperform the individual experts and other merging methods on the math benchmark, MGSM, by 10% across four major languages where math instruction data is scarce. In addition, this layer swapping is simple, inexpensive, and intuitive, as it is based on an interpretative analysis of the most important parameter changes during the fine-tuning of each expert. The ability to successfully re-compose LLMs for cross-lingual transfer in this manner opens up future possibilities to combine model expertise, create modular solutions, and transfer reasoning capabilities across languages all post hoc.",
      "authors": [
        "Lucas Bandarkar",
        "Benjamin Muller",
        "Pritish Yuvraj",
        "Rui Hou",
        "Nayan Singhal",
        "Hongjiang Lv",
        "Bing Liu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=vQhn4wrQ6j",
      "cdate": 1727467571436,
      "mdate": 1740858267900,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835121"
    },
    {
      "id": "Pujt3ADZgI",
      "title": "Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning",
      "abstract": "Reinforcement Learning with Human Feedback (RLHF) has achieved great success\nin aligning large language models (LLMs) with human preferences. Prevalent\nRLHF approaches are reward-based, following the Bradley-Terry (BT) model assumption, which may not fully capture the complexity of human preferences. In\nthis paper, we explore RLHF under a general preference framework and approach\nit from a game-theoretic perspective. Specifically, we formulate the problem as\na two-player game and propose a novel online algorithm, iterative Nash policy\noptimization (INPO). The key idea is to let the policy play against itself via no-\nregret learning, thereby approximating the Nash policy. Unlike previous methods,\nINPO bypasses the need for estimating the expected win rate for individual responses, which typically incurs high computational or annotation costs. Instead,\nwe introduce a new loss objective that is directly minimized over a preference\ndataset. We provide theoretical analysis for our approach and demonstrate its\neffectiveness through experiments on various representative benchmarks. With an\nLLaMA-3-8B-based SFT model, INPO achieves a 42.6% length-controlled win\nrate on AlpacaEval 2.0 and a 37.8% win rate on Arena-Hard, showing substantial\nimprovement over the state-of-the-art online RLHF algorithms.",
      "authors": [
        "Yuheng Zhang",
        "Dian Yu",
        "Baolin Peng",
        "Linfeng Song",
        "Ye Tian",
        "Mingyue Huo",
        "Nan Jiang",
        "Haitao Mi",
        "Dong Yu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Pujt3ADZgI",
      "cdate": 1727467538497,
      "mdate": 1740886384290,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835127"
    },
    {
      "id": "FS2nukC2jv",
      "title": "Teaching LLMs How to Learn with Contextual Fine-Tuning",
      "abstract": "Prompting Large Language Models (LLMs), or providing context on the expected model of operation, is an effective way to steer the outputs of such models to satisfy human desiderata after they have been trained. But in rapidly evolving domains, there is often need to fine-tune LLMs to improve either the kind of knowledge in their memory or their abilities to perform open ended reasoning in new domains. When human's learn new concepts, we often do so by linking the new material that we are studying to concepts we have already learned before. To that end, we ask, \"can prompting help us teach LLMs how to learn\". In this work, we study a novel generalization of instruction tuning, called contextual fine-tuning, to fine-tune LLMs. Our method leverages instructional prompts designed to mimic human cognitive strategies in learning and problem-solving to guide the learning process during training, aiming to improve the model’s interpretation and understanding of domain-specific knowledge. We empirically demonstrate that this simple yet effective modification improves the ability of LLMs to be fine-tuned rapidly on new datasets both within the medical and financial domains.",
      "authors": [
        "Younwoo Choi",
        "Muhammad Adil Asif",
        "Ziwen Han",
        "John Willes",
        "Rahul Krishnan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=FS2nukC2jv",
      "cdate": 1727467472979,
      "mdate": 1747549638837,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835133"
    },
    {
      "id": "wAXsx2MYgV",
      "title": "Modeling dynamic social vision highlights gaps between deep learning and humans",
      "abstract": "Deep learning models trained on computer vision tasks are widely considered the most successful models of human vision to date. The majority of work that supports this idea evaluates how accurately these models predict behavior and brain responses to static images of objects and scenes. Real-world vision, however, is highly dynamic, and far less work has evaluated deep learning models on human responses to moving stimuli, especially those that involve more complicated, higher-order phenomena like social interactions. Here, we extend a dataset of natural videos depicting complex multi-agent interactions by collecting human-annotated sentence captions for each video, and we benchmark 350+ image, video, and language models on behavior and neural responses to the videos. As in prior work, we find that many vision models reach the noise ceiling in predicting visual scene features and responses along the ventral visual stream (often considered the primary neural substrate of object and scene recognition). In contrast, vision models poorly predict human action and social interaction ratings and neural responses in the lateral stream (a neural pathway theorized to specialize in dynamic, social vision), though video models show a striking advantage in predicting mid-level lateral stream regions. Language models (given human sentence captions of the videos) predict action and social ratings better than image and video models, but perform poorly at predicting neural responses in the lateral stream. Together, these results identify a major gap in AI's ability to match human social vision and provide insights to guide future model development for dynamic, natural contexts.",
      "authors": [
        "Kathy Garcia",
        "Emalie McMahon",
        "Colin Conwell",
        "Michael Bonner",
        "Leyla Isik"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=wAXsx2MYgV",
      "cdate": 1727467420668,
      "mdate": 1740871941945,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835138"
    },
    {
      "id": "Qja5s0K3VX",
      "title": "Statistical Tractability of Off-policy Evaluation of History-dependent Policies in POMDPs",
      "abstract": "We investigate off-policy evaluation (OPE), a central and fundamental problem\nin reinforcement learning (RL), in the challenging setting of Partially Observable\nMarkov Decision Processes (POMDPs) with large observation spaces. Recent\nworks of Uehara et al. (2023a); Zhang & Jiang (2024) developed a model-free\nframework and identified important coverage assumptions (called belief and outcome coverage) that enable accurate OPE of memoryless policies with polynomial sample complexities, but handling more general target policies that depend on\nthe entire observable history remained an open problem. In this work, we prove\ninformation-theoretic hardness for model-free OPE of history-dependent policies in\nseveral settings, characterized by additional assumptions imposed on the behavior\npolicy (memoryless vs. history-dependent) and/or the state-revealing property of\nthe POMDP (single-step vs. multi-step revealing). We further show that some hardness can be circumvented by a natural model-based algorithm—whose analysis has surprisingly eluded the literature despite the algorithm’s simplicity—demonstrating\nprovable separation between model-free and model-based OPE in POMDPs.",
      "authors": [
        "Yuheng Zhang",
        "Nan Jiang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Qja5s0K3VX",
      "cdate": 1727467264841,
      "mdate": 1740886548970,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835143"
    },
    {
      "id": "jY5oml9fe9",
      "title": "Large Language Models can Become Strong Self-Detoxifiers",
      "abstract": "Reducing the likelihood of generating harmful and toxic output is an essential task when aligning large language models (LLMs). Existing methods mainly rely on training an external reward model (i.e., another language model) or fine-tuning the LLM using self-generated data to influence the outcome. In this paper, we show that LLMs have the capability of self-detoxification without external reward model learning or retraining of the LM. We propose \\textit{Self-disciplined Autoregressive Sampling (SASA)}, a lightweight controlled decoding algorithm for toxicity reduction of LLMs. SASA leverages the contextual representations from an LLM to learn linear subspaces from labeled data characterizing toxic v.s. non-toxic output in analytical forms. When auto-completing a response token-by-token, SASA dynamically tracks the margin of the current output to steer the generation away from the toxic subspace, by adjusting the autoregressive sampling strategy. Evaluated on LLMs of different scale and nature, namely Llama-3.1-Instruct (8B), Llama-2 (7B), and GPT2-L models with the RealToxicityPrompts, BOLD, and AttaQ benchmarks, SASA markedly enhances the quality of the generated sentences relative to the original models and attains comparable performance to state-of-the-art detoxification techniques, significantly reducing the toxicity level by only using the LLM's internal representations.",
      "authors": [
        "Ching-Yun Ko",
        "Pin-Yu Chen",
        "Payel Das",
        "Youssef Mroueh",
        "Soham Dan",
        "Georgios Kollias",
        "Subhajit Chaudhury",
        "Tejaswini Pedapati",
        "Luca Daniel"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=jY5oml9fe9",
      "cdate": 1727467169942,
      "mdate": 1740890356278,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835148"
    },
    {
      "id": "4VHiptx7xe",
      "title": "STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning",
      "abstract": "Robot learning is witnessing a significant increase in the size, diversity, and complexity of pre-collected datasets, mirroring trends in domains such as natural language processing and computer vision. Many robot learning methods treat such datasets as multi-task expert data and learn a multi-task, generalist policy by training broadly across them. Notably, while these generalist policies can improve the average performance across many tasks, the performance of generalist policies on any one task is often suboptimal due to negative transfer between partitions of the data, compared to task-specific specialist policies. In this work, we argue for the paradigm of training policies during deployment given the scenarios they encounter: rather than deploying pre-trained policies to unseen problems in a zero-shot manner, we non-parametrically retrieve and train models directly on relevant data at test time. Furthermore, we show that many robotics tasks share considerable amounts of low-level behaviors and that retrieval at the \"sub\"-trajectory granularity enables significantly improved data utilization, generalization, and robustness in adapting policies to novel problems. In contrast, existing full-trajectory retrieval methods tend to underutilize the data and miss out on shared cross-task content. This work proposes STRAP, a technique for leveraging pre-trained vision foundation models and dynamic time warping to retrieve sub-sequences of trajectories from large training corpora in a robust fashion. STRAP outperforms both prior retrieval algorithms and multi-task learning methods in simulated and real experiments, showing the ability to scale to much larger offline datasets in the real world as well as the ability to learn robust control policies with just a handful of real-world demonstrations.",
      "authors": [
        "Marius Memmel",
        "Jacob Berg",
        "Bingqing Chen",
        "Abhishek Gupta",
        "Jonathan Francis"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=4VHiptx7xe",
      "cdate": 1727467155977,
      "mdate": 1741073981630,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835153"
    },
    {
      "id": "gyHoR6uFhU",
      "title": "PortLLM: Personalizing Evolving Large Language Models with Training-Free and Portable Model Patches",
      "abstract": "As large language models (LLMs) increasingly shape the AI landscape, fine-tuning pretrained models has become more popular than in the pre-LLM era for achieving optimal performance in domain-specific tasks. However, pretrained LLMs such as ChatGPT are periodically evolved (i.e., model parameters are frequently updated), making it challenging for downstream users with limited resources to keep up with fine-tuning the newest LLMs for their domain application. Even though fine-tuning costs have nowadays been reduced thanks to the innovations of parameter-efficient fine-tuning such as LoRA, not all downstream users have adequate computing for frequent personalization. Moreover, access to fine-tuning datasets, particularly in sensitive domains such as healthcare, could be time-restrictive, making it crucial to retain the knowledge encoded in earlier fine-tuned rounds for future adaptation. In this paper, we present PORTLLM, a training-free framework that (i) creates an initial lightweight model update patch to capture domain-specific knowledge, and (ii) allows a subsequent seamless plugging for the continual personalization of evolved LLM at minimal cost. Our extensive experiments cover seven representative datasets, from easier question-answering tasks {BoolQ, SST2} to harder reasoning tasks {WinoGrande, GSM8K}, and models including {Mistral-7B,Llama2, Llama3.1, and Gemma2}, validating the portability of our designed model patches and showcasing the effectiveness of our proposed framework. For instance, PORTLLM achieves comparable performance to LoRA fine-tuning with reductions of up to 12.2× in GPU memory usage. Finally, we provide theoretical justifications to understand the portability of our model update patches, which offers new insights into the theoretical dimension of LLMs’ personalization.",
      "authors": [
        "Rana Shahroz",
        "Pingzhi Li",
        "Sukwon Yun",
        "Zhenyu Wang",
        "Shahriar Nirjon",
        "Chau-Wai Wong",
        "Tianlong Chen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=gyHoR6uFhU",
      "cdate": 1727466980187,
      "mdate": 1741847474671,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835158"
    },
    {
      "id": "rvXdGL4pCJ",
      "title": "Robust Transfer of Safety-Constrained Reinforcement Learning Agents",
      "abstract": "Reinforcement learning (RL) often relies on trial and error, which may cause undesirable outcomes. As a result, standard RL is inappropriate for safety-critical applications. To address this issue, one may train a safe agent in a controlled environment (where safety violations are allowed) and then transfer it to the real world (where safety violations may have disastrous consequences). Prior work has made this transfer safe as long as the new environment preserves the safety-related dynamics. However, in most practical applications, differences or shifts in dynamics between the two environments are inevitable, potentially leading to safety violations after the transfer. This work aims to guarantee safety even when the new environment has different (safety-related) dynamics. In other words, we aim to make the process of safe transfer robust. Our methodology (1) robustifies an agent in the controlled environment and (2) provably provides---under mild assumption---a safe transfer to new environments. The empirical evaluation shows that this method yields policies that are robust against changes in dynamics, demonstrating safety after transfer to a new environment.",
      "authors": [
        "Markel Zubia",
        "Thiago D. Simão",
        "Nils Jansen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=rvXdGL4pCJ",
      "cdate": 1727466958221,
      "mdate": 1740908278050,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835166"
    },
    {
      "id": "XAjfjizaKs",
      "title": "Residual Stream Analysis with Multi-Layer SAEs",
      "abstract": "Sparse autoencoders (SAEs) are a promising approach to interpreting the internal representations of transformer language models. However, SAEs are usually trained separately on each transformer layer, making it difficult to use them to study how information flows across layers. To solve this problem, we introduce the multi-layer SAE (MLSAE): a single SAE trained on the residual stream activation vectors from every transformer layer. Given that the residual stream is understood to preserve information across layers, we expected MLSAE latents to 'switch on' at a token position and remain active at later layers. Interestingly, we find that individual latents are often active at a single layer for a given token or prompt, but the layer at which an individual latent is active may differ for different tokens or prompts. We quantify these phenomena by defining a distribution over layers and considering its variance. We find that the variance of the distributions of latent activations over layers is about two orders of magnitude greater when aggregating over tokens compared with a single token. For larger underlying models, the degree to which latents are active at multiple layers increases, which is consistent with the fact that the residual stream activation vectors at adjacent layers become more similar. Finally, we relax the assumption that the residual stream basis is the same at every layer by applying pre-trained tuned-lens transformations, but our findings remain qualitatively similar. Our results represent a new approach to understanding how representations change as they flow through transformers. We release our code to train and analyze MLSAEs at https://github.com/tim-lawson/mlsae.",
      "authors": [
        "Tim Lawson",
        "Lucy Farnik",
        "Conor Houghton",
        "Laurence Aitchison"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=XAjfjizaKs",
      "cdate": 1727466820906,
      "mdate": 1740388816723,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835171"
    },
    {
      "id": "NCrFA7dq8T",
      "title": "The Same but Different: Structural Similarities and Differences in Multilingual Language Modeling",
      "abstract": "We employ new tools from mechanistic interpretability to ask whether the internal structure of large language models (LLMs) shows correspondence to the linguistic structures which underlie the languages on which they are trained. In particular, we ask (1) when two languages employ the same morphosyntactic processes, do LLMs handle them using shared internal circuitry? and (2) when two languages require different morphosyntactic processes, do LLMs handle them using different internal circuitry? In a focused case study on English and Chinese multilingual and monolingual models, we analyze the internal circuitry involved in two tasks. We find evidence that models employ the same circuit to handle the same syntactic process independently of the language in which it occurs, and that this is the case even for monolingual models trained completely independently. Moreover, we show that multilingual models employ language-specific components (attention heads and feed-forward networks) when needed to handle linguistic processes (e.g., morphological marking) that only exist in some languages. Together, our results are revealing about how LLMs trade off between exploiting common structures and preserving linguistic differences when tasked with modeling multiple languages simultaneously, opening the door for future work in this direction.",
      "authors": [
        "Ruochen Zhang",
        "Qinan Yu",
        "Matianyu Zang",
        "Carsten Eickhoff",
        "Ellie Pavlick"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=NCrFA7dq8T",
      "cdate": 1727466818484,
      "mdate": 1740778190178,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835177"
    },
    {
      "id": "yVGGtsOgc7",
      "title": "Disentangling Representations through Multi-task Learning",
      "abstract": "Intelligent perception and interaction with the world hinges on internal representations that capture its underlying structure (\"disentangled\" or \"abstract\" representations). Disentangled representations serve as world models, isolating latent factors of variation in the world along approximately orthogonal directions, thus facilitating feature-based generalization. We provide experimental and theoretical results guaranteeing the emergence of disentangled representations in agents that optimally solve multi-task evidence accumulation classification tasks, canonical in the neuroscience literature. The key conceptual finding is that, by producing accurate multi-task classification estimates, a system implicitly represents a set of coordinates specifying a disentangled representation of the underlying latent state of the data it receives. The theory provides conditions for the emergence of these representations in terms of noise, number of tasks, and evidence accumulation time, when the classification boundaries are affine in the latent space. Surprisingly, the theory also produces closed-form expressions for extracting the disentangled representation from the model's latent state $\\mathbf Z(t)$. We experimentally validate these predictions in RNNs trained on multi-task classification, which learn disentangled representations in the form of continuous attractors, leading to zero-shot out-of-distribution (OOD) generalization in predicting latent factors. We demonstrate the robustness of our framework across autoregressive architectures, decision boundary geometries and in tasks requiring classification confidence estimation. We find that transformers are particularly suited for disentangling representations, which might explain their unique world understanding abilities. Overall, our framework establishes a formal link between competence at multiple tasks and the formation of disentangled, interpretable world models in both biological and artificial systems, and helps explain why ANNs often arrive at human-interpretable concepts, and how they both may acquire exceptional zero-shot generalization capabilities.",
      "authors": [
        "Pantelis Vafidis",
        "Aman Bhargava",
        "Antonio Rangel"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=yVGGtsOgc7",
      "cdate": 1727466684318,
      "mdate": 1740864428039,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835181"
    },
    {
      "id": "1jcnvghayD",
      "title": "Bayesian Optimization via Continual Variational Last Layer Training",
      "abstract": "Gaussian Processes (GPs) are widely seen as the state-of-the-art surrogate models for Bayesian optimization (BO) due to their ability to model uncertainty and their performance on tasks where correlations are easily captured (such as those defined by Euclidean metrics) and their ability to be efficiently updated online. However, the performance of GPs depends on the choice of kernel, and kernel selection for complex correlation structures is often difficult or must be made bespoke. While Bayesian neural networks (BNNs) are a promising direction for higher capacity surrogate models, they have so far seen limited use due to poor performance on some problem types. In this paper, we propose an approach which shows competitive performance on many problem types, including some that BNNs typically struggle with. We build on variational Bayesian last layers (VBLLs), and connect training of these models to exact conditioning in GPs. We exploit this connection to develop an efficient online training algorithm that interleaves conditioning and optimization. Our findings suggest that VBLL networks significantly outperform GPs and other BNN architectures on tasks with complex input correlations, and match the performance of well-tuned GPs on established benchmark tasks.",
      "authors": [
        "Paul Brunzema",
        "Mikkel Jordahn",
        "John Willes",
        "Sebastian Trimpe",
        "Jasper Snoek",
        "James Harrison"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=1jcnvghayD",
      "cdate": 1727466257117,
      "mdate": 1740761529447,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835187"
    },
    {
      "id": "E48QvQppIN",
      "title": "Bayesian Optimization of Antibodies Informed by a Generative Model of Evolving Sequences",
      "abstract": "To build effective therapeutics, biologists iteratively mutate antibody sequences to improve binding and stability. Proposed mutations can be informed by previous measurements or by learning from large antibody databases to predict only typical antibodies. Unfortunately, the space of typical antibodies is enormous to search, and experiments often fail to find suitable antibodies on a budget. We introduce Clone-informed Bayesian Optimization (CloneBO), a Bayesian optimization procedure that efficiently optimizes antibodies in the lab by teaching a generative model how our immune system optimizes antibodies. Our immune system makes antibodies by iteratively evolving specific portions of their sequences to bind their target strongly and stably, resulting in a set of related, evolving sequences known as a *clonal family*. We train a large language model, CloneLM, on hundreds of thousands of clonal families and use it to design sequences with mutations that are most likely to optimize an antibody within the human immune system. We propose to guide our designs to fit previous measurements with a twisted sequential Monte Carlo procedure. We show that CloneBO optimizes antibodies substantially more efficiently than previous methods in realistic *in silico* experiments and designs stronger and more stable binders in *in vitro* wet lab experiments.",
      "authors": [
        "Alan Nawzad Amin",
        "Nate Gruver",
        "Yilun Kuang",
        "Yucen Lily Li",
        "Hunter Elliott",
        "Calvin McCarter",
        "Aniruddh Raghu",
        "Peyton Greenside",
        "Andrew Gordon Wilson"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=E48QvQppIN",
      "cdate": 1727466227823,
      "mdate": 1740887006075,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835192"
    },
    {
      "id": "bsFWJ0Kget",
      "title": "GeoLoRA: Geometric integration for parameter efficient fine-tuning",
      "abstract": "Low-Rank Adaptation (LoRA) has become a widely used method for parameter-efficient fine-tuning of large-scale, pre-trained neural networks. However, LoRA and its extensions face several challenges, including the need for rank adaptivity, robustness, and computational efficiency during the fine-tuning process. We introduce GeoLoRA, a novel approach that addresses these limitations by leveraging dynamical low-rank approximation theory. GeoLoRA requires only a single backpropagation pass over the small-rank adapters, significantly reducing computational cost as compared to similar dynamical low-rank training methods and making it faster than popular baselines such as AdaLoRA. This allows GeoLoRA to efficiently adapt the allocated parameter budget across the model, achieving smaller low-rank adapters compared to heuristic methods like AdaLoRA and LoRA, while maintaining critical convergence, descent, and error-bound theoretical guarantees. The resulting method is not only more efficient but also more robust to varying hyperparameter settings. We demonstrate the effectiveness of GeoLoRA on several state-of-the-art benchmarks, showing that it outperforms existing methods in both\naccuracy and computational efficiency",
      "authors": [
        "Steffen Schotthöfer",
        "Emanuele Zangrando",
        "Gianluca Ceruti",
        "Francesco Tudisco",
        "Jonas Kusch"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=bsFWJ0Kget",
      "cdate": 1727465861093,
      "mdate": 1741700902469,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835197"
    },
    {
      "id": "vjel3nWP2a",
      "title": "Scalable Extraction of Training Data from Aligned, Production Language Models",
      "abstract": "Large language models are prone to *memorizing* some of their training data. Memorized (and possibly sensitive) samples can then be extracted at generation time by adversarial or benign users. There is hope that *model alignment*---a standard training process that tunes a model to harmlessly follow user instructions---would mitigate the risk of extraction. However, we develop two novel attacks that undo a language model's alignment and recover thousands of training examples from popular proprietary aligned models such as OpenAI's ChatGPT. Our work highlights the limitations of existing safeguards to prevent training data leakage in production language models.",
      "authors": [
        "Milad Nasr",
        "Javier Rando",
        "Nicholas Carlini",
        "Jonathan Hayase",
        "Matthew Jagielski",
        "A. Feder Cooper",
        "Daphne Ippolito",
        "Christopher A. Choquette-Choo",
        "Florian Tramèr",
        "Katherine Lee"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=vjel3nWP2a",
      "cdate": 1727465517595,
      "mdate": 1740786109905,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835203"
    },
    {
      "id": "590yfqz1LE",
      "title": "Measuring Non-Adversarial Reproduction of Training Data in Large Language Models",
      "abstract": "Large language models memorize parts of their training data. Memorizing short snippets and facts is required to answer questions about the world and to be fluent in any language. But models have also been shown to reproduce long verbatim sequences of memorized text when prompted by a motivated adversary. In this work, we investigate an intermediate regime of memorization that we call non-adversarial reproduction, where we quantify the overlap between model responses and pretraining data when responding to natural and benign prompts. For a variety of innocuous prompt categories (e.g., writing a letter or a tutorial), we show that up to 15% of the text output by popular conversational language models overlaps with snippets from the Internet. In worst cases, we find generations where 100% of the content can be found exactly online. For the same tasks, we find that human-written text has far less overlap with Internet data. We further study whether prompting strategies can close this reproduction gap between models and humans. While appropriate prompting can reduce non-adversarial reproduction on average, we find that mitigating worst-case reproduction of training data requires stronger defenses—even for benign interactions.",
      "authors": [
        "Michael Aerni",
        "Javier Rando",
        "Edoardo Debenedetti",
        "Nicholas Carlini",
        "Daphne Ippolito",
        "Florian Tramèr"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=590yfqz1LE",
      "cdate": 1727465483220,
      "mdate": 1739261759113,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835208"
    },
    {
      "id": "3ep9ZYMZS3",
      "title": "Model-Agnostic Knowledge Guided Correction for Improved Neural Surrogate Rollout",
      "abstract": "Modeling the evolution of physical systems is critical to many applications in science and engineering. As the evolution of these systems is governed by partial differential equations (PDEs), there are a number of computational simulations which resolve these systems with high accuracy. However, as these simulations incur high computational costs, they are infeasible to be employed for large-scale analysis. A popular alternative to simulators are neural network surrogates which are trained in a data-driven manner and are much more computationally efficient. However, these surrogate models suffer from high rollout error when used autoregressively, especially when confronted with training data paucity. Existing work proposes to improve surrogate rollout error by either including physical loss terms directly in the optimization of the model or incorporating computational simulators as `differentiable layers' in the neural network. Both of these approaches have their challenges, with physical loss functions suffering from slow convergence for stiff PDEs and simulator layers requiring gradients which are not always available, especially in legacy simulators. We propose the Hybrid PDE Predictor with Reinforcement Learning (HyPER) model: a model-agnostic, RL based, cost-aware model which combines a neural surrogate, RL decision model, and a physics simulator (with or without gradients) to reduce surrogate rollout error significantly. In addition to reducing in-distribution rollout error by **47%-78%**, HyPER learns an intelligent policy that is adaptable to changing physical conditions and resistant to noise corruption. Code available at https://github.com/scailab/HyPER.",
      "authors": [
        "Bharat Srikishan",
        "Daniel O'Malley",
        "Mohamed Mehana",
        "Nicholas Lubbers",
        "Nikhil Muralidhar"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=3ep9ZYMZS3",
      "cdate": 1727465270450,
      "mdate": 1741836915522,
      "matched_keywords": [
        "reinforcement learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835213"
    },
    {
      "id": "1durmugh3I",
      "title": "Towards Fast, Specialized Machine Learning Force Fields: Distilling Foundation Models via Energy Hessians",
      "abstract": "The foundation model (FM) paradigm is transforming Machine Learning Force Fields (MLFFs), leveraging general-purpose representations and scalable training to perform a variety of computational chemistry tasks. Although MLFF FMs have begun to close the accuracy gap relative to first-principles methods, there is still a strong need for faster inference speed. Additionally, while research is increasingly focused on general-purpose models which transfer across chemical space, practitioners typically only study a small subset of systems at a given time. At test time, MLFFs must also obey physical constraints unique to the downstream use case, such as energy conservation for molecular dynamics simulations. This underscores the need for fast, specialized MLFFs relevant to specific downstream applications, which preserve test-time physical soundness while maintaining train-time scalability. In this work, we introduce a method for transferring general-purpose representations from MLFF foundation models to smaller, faster MLFFs specialized to specific regions of chemical space. We formulate our approach as an architecture-agnostic knowledge distillation procedure, where the smaller \"student\" MLFF is trained to match the Hessians of the energy predictions of the \"teacher\" foundation model. We demonstrate our approach across multiple recent foundation models, large-scale datasets, chemical subsets, and downstream tasks. Our specialized MLFFs can be up to 20 times faster than the original foundation model, while retaining, and in some cases exceeding, its performance and that of undistilled models. We also show that distilling from a teacher model with a direct force parameterization into a student model trained with conservative forces (i.e., computed as derivatives of the potential energy) successfully leverages the representations from the large-scale teacher for improved accuracy, while maintaining energy conservation during test-time molecular dynamics simulations. More broadly, our work suggests a new paradigm for MLFF development, in which foundation models are released along with smaller, specialized simulation ``engines\" for common chemical subsets. The implementation of our method is available at https://github.com/ASK-Berkeley/MLFF-distill.",
      "authors": [
        "Ishan Amin",
        "Sanjeev Raja",
        "Aditi S. Krishnapriyan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=1durmugh3I",
      "cdate": 1727465147512,
      "mdate": 1747538608238,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835220"
    },
    {
      "id": "ac93gRzxxV",
      "title": "IterGen: Iterative Semantic-aware Structured LLM Generation with Backtracking",
      "abstract": "Large Language Models (LLMs) are widely used for tasks such as natural language and code generation, but their outputs often suffer from issues like hallucination, toxicity, and incorrect results. Current libraries for structured LLM generation rely on left-to-right decoding without support for backtracking, limiting the ability to correct or refine outputs mid-generation. \n\nTo address this, we introduce IterGen, a user-friendly library for iterative, grammar-guided LLM generation that enables users to move both forward and backward within the generated output based on grammar symbols. \nBy leveraging a symbol-to-position mapping and maintaining the key-value (KV) cache state, IterGen ensures efficient and structured generation while allowing for corrections during the process. We demonstrate IterGen's effectiveness in two important applications: reducing privacy leakage in LLM outputs, improving the accuracy of LLM-generated SQL and Vega-Lite queries.\n\nOur code and additional resources are available at https://structuredllm.com.",
      "authors": [
        "Shubham Ugare",
        "Rohan Gumaste",
        "Tarun Suresh",
        "Gagandeep Singh",
        "Sasa Misailovic"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=ac93gRzxxV",
      "cdate": 1727464793530,
      "mdate": 1740890551685,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835226"
    },
    {
      "id": "fGIqGfmgkW",
      "title": "OpenPRM: Building Open-domain Process-based Reward Models with Preference Trees",
      "abstract": "Scaling inference-time computation is increasingly seen as the next frontier in scaling laws for large language models. Previous work in mathematics and coding has demonstrated the remarkable potential for inference-time scaling. During such scaling, fine-grained supervision through process-based reward models (PRMs) is essential for enhancement. However, exploration of inference-time scaling and PRMs in open-domain problems remains limited, where lacking exact answers and obtaining process supervision prove challenging. In this paper, we explore the construction of PRMs for open-domain tasks, specifically for instruction-following tasks. Utilizing existing outcome-based reward models (ORMs), we develop sentence-level preference trees based on the prefix similarity of parallel sampled candidates from datasets like UltraFeedback. This setup allows us to derive weak supervision for processes via back-propagation from outcome-level rewards. Subsequently, we integrate ORMs and PRMs under the same pairwise ranking objectives, resulting in our newly developed reward models, named OpenPRM. This approach significantly enhances the scalability of process-level supervision in open domains at minimal cost. We assess the performance of OpenPRM across various reward benchmarks, demonstrating its competitive edge over traditional ORMs in open domains and PRMs in specialized domains. Additionally, we investigate the scalability of inference-time computation for open-domain instructions. Our results highlight the limitations of ORMs’ scalability, while OpenPRM shows superior performance in scaled settings. Despite these advances, achieving  automatic fine-grained supervision for open-domain inference-time scaling remains a substantial challenge. We hope these findings will spur further development of process supervision reward models in open-domain scenarios.",
      "authors": [
        "Kaiyan Zhang",
        "Jiayuan Zhang",
        "Haoxin Li",
        "Xuekai Zhu",
        "Ermo Hua",
        "Xingtai Lv",
        "Ning Ding",
        "Biqing Qi",
        "Bowen Zhou"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=fGIqGfmgkW",
      "cdate": 1727464704064,
      "mdate": 1746533645308,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835231"
    },
    {
      "id": "L9eBxTCpQG",
      "title": "SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training",
      "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance across diverse tasks, yet their training remains highly resource intensive and susceptible to critical challenges such as training instability. A predominant source of this instability stems from gradient and loss spikes, which disrupt the learning process, often leading to costly interventions like checkpoint recovery and experiment restarts, further amplifying inefficiencies. This paper presents a comprehensive investigation into gradient spikes observed during LLM training, revealing their prevalence across multiple architectures and datasets. Our analysis shows that these spikes can be up to 1000× larger than typical gradients, substantially deteriorating model performance. To address this issue, we propose Spike-Aware Adam with Momentum Reset (SPAM), a novel optimizer designed to counteract gradient spikes through momentum reset and spike-aware gradient clipping. Extensive experiments, including both pre-training and fine-tuning, demonstrate that SPAM consistently surpasses Adam and its variants across a range of model scales. Additionally, SPAM facilitates memory-efficient training by enabling sparse momentum, where only a subset of momentum terms are maintained and updated. When operating under memory constraints, SPAM outperforms state-of-the-art memory-efficient optimizers such as GaLore and Adam-Mini. Our work underscores the importance\nof mitigating gradient spikes in LLM training and introduces an effective optimization strategy that enhances both training stability and resource efficiency at scale. Code is submitted.",
      "authors": [
        "Tianjin Huang",
        "Ziquan Zhu",
        "Gaojie Jin",
        "Lu Liu",
        "Zhangyang Wang",
        "Shiwei Liu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=L9eBxTCpQG",
      "cdate": 1727464217498,
      "mdate": 1741200819365,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835236"
    },
    {
      "id": "daUQ7vmGap",
      "title": "Dynamic Sparse Training versus Dense Training: The Unexpected Winner in Image Corruption Robustness",
      "abstract": "It is generally perceived that Dynamic Sparse Training opens the door to a new era of scalability and efficiency for artificial neural networks at, perhaps, some costs in accuracy performance for the classification task. At the same time, Dense Training is widely accepted as being the \"de facto\" approach to train artificial neural networks if one would like to maximize their robustness against image corruption. In this paper, we question this general practice. Consequently, \\textit{we claim that}, contrary to what is commonly thought, the Dynamic Sparse Training methods can consistently outperform Dense Training in terms of robustness accuracy, particularly if the efficiency aspect is not considered as a main objective (i.e., sparsity levels between 10\\% and up to 50\\%), without adding (or even reducing) resource cost. We validate our claim on two types of data, images and videos, using several traditional and modern deep learning architectures for computer vision and three widely studied Dynamic Sparse Training algorithms. Our findings reveal a new yet-unknown benefit of Dynamic Sparse Training and open new possibilities in improving deep learning robustness beyond the current state of the art.",
      "authors": [
        "Boqian Wu",
        "Qiao Xiao",
        "Shunxin Wang",
        "Nicola Strisciuglio",
        "Mykola Pechenizkiy",
        "Maurice van Keulen",
        "Decebal Constantin Mocanu",
        "Elena Mocanu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=daUQ7vmGap",
      "cdate": 1727464102079,
      "mdate": 1741014821385,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835241"
    },
    {
      "id": "vVxeFSR4fU",
      "title": "Tracing Representation Progression: Analyzing and Enhancing Layer-Wise Similarity",
      "abstract": "Analyzing the similarity of internal representations within and across different models has been an important technique for understanding the behavior of deep neural networks. Most existing methods for analyzing the similarity between representations of high dimensions, such as those based on Centered Kernel Alignment (CKA), rely on statistical properties of the representations for a set of data points. In this paper, we focus on transformer models and study the similarity of representations between the hidden layers of individual transformers. In this context, we show that a simple sample-wise cosine similarity metric is capable of capturing the similarity and aligns with the complicated CKA. Our experimental results on common transformers reveal that representations across layers are positively correlated, with similarity increasing when layers get closer. We provide a theoretical justification for this phenomenon under the geodesic curve assumption for the learned transformer, a property that may approximately hold for residual networks. We then show that an increase in representation similarity implies an increase in predicted probability when directly applying the last-layer classifier to any hidden layer representation. This offers a justification for {\\it saturation events}, where the model's top prediction remains unchanged across subsequent layers, indicating that the shallow layer has already learned the necessary knowledge. We then propose an aligned training method to improve the effectiveness of shallow layer by enhancing the similarity between internal representations, with trained models that enjoy the following properties: (1) more early saturation events, (2) layer-wise accuracies monotonically increase and reveal the minimal depth needed for the given task, (3) when served as multi-exit models, they achieve on-par performance with standard multi-exit architectures which consist of additional classifiers designed for early exiting in shallow layers. To our knowledge, our work is the first to show that one common classifier is sufficient for multi-exit models. We conduct experiments on both vision and NLP tasks to demonstrate the performance of the proposed aligned training.",
      "authors": [
        "Jiachen Jiang",
        "Jinxin Zhou",
        "Zhihui Zhu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=vVxeFSR4fU",
      "cdate": 1727464033645,
      "mdate": 1740717217795,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835247"
    },
    {
      "id": "y9A2TpaGsE",
      "title": "Language Agents Meet Causality -- Bridging LLMs and Causal World Models",
      "abstract": "Large Language Models (LLMs) have recently shown great promise in planning and reasoning applications. These tasks demand robust systems, which arguably require a causal understanding of the environment. While LLMs can acquire and reflect common sense causal knowledge from their pretraining data, this information is often incomplete, incorrect, or inapplicable to a specific environment. In contrast, causal representation learning (CRL) focuses on identifying the underlying causal structure within a given environment. We propose a framework that integrates CRLs with LLMs to enable causally-aware reasoning and planning. This framework learns a causal world model, with causal variables linked to natural language expressions. This mapping provides LLMs with a flexible interface to process and generate descriptions of actions and states in text form. Effectively, the causal world model acts as a simulator that the LLM can query and interact with. We evaluate the framework on causal inference and planning tasks across temporal scales and environmental complexities. Our experiments demonstrate the effectiveness of the approach, with the causally-aware method outperforming LLM-based reasoners, especially for longer planning horizons.",
      "authors": [
        "John Gkountouras",
        "Matthias Lindemann",
        "Phillip Lippe",
        "Efstratios Gavves",
        "Ivan Titov"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=y9A2TpaGsE",
      "cdate": 1727464030819,
      "mdate": 1744051170149,
      "matched_keywords": [
        "large language model",
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835252"
    },
    {
      "id": "fXb9BbuyAD",
      "title": "Enabling Realtime Reinforcement Learning at Scale with Staggered Asynchronous Inference",
      "abstract": "Realtime environments change even as agents perform action inference and learning, thus requiring high interaction frequencies to effectively minimize regret. However, recent advances in machine learning involve larger neural networks with longer inference times, raising questions about their applicability in realtime systems where reaction time is crucial. We present an analysis of lower bounds on regret in realtime reinforcement learning (RL) environments to show that minimizing long-term regret is generally impossible within the typical sequential interaction and learning paradigm, but often becomes possible when sufficient asynchronous compute is available. We propose novel algorithms for staggering asynchronous inference processes to ensure that actions are taken at consistent time intervals, and demonstrate that use of models with high action inference times is only constrained by the environment's effective stochasticity over the inference horizon, and not by action frequency. Our analysis shows that the number of inference processes needed scales linearly with increasing inference times while enabling use of models that are multiple orders of magnitude larger than existing approaches when learning from a realtime simulation of Game Boy games such as Pokemon and Tetris.",
      "authors": [
        "Matthew Riemer",
        "Gopeshh Subbaraj",
        "Glen Berseth",
        "Irina Rish"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=fXb9BbuyAD",
      "cdate": 1727463836703,
      "mdate": 1740890355898,
      "matched_keywords": [
        "reinforcement learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835257"
    },
    {
      "id": "bSq0XGS3kW",
      "title": "On the Transfer of Object-Centric Representation Learning",
      "abstract": "The goal of object-centric representation learning is to decompose visual scenes into a structured representation that isolates the entities into individual vectors. Recent successes have shown that object-centric representation learning can be scaled to real-world scenes by utilizing features from pre-trained foundation models like DINO. However, so far, these object-centric methods have mostly been applied in-distribution, with models trained and evaluated on the same dataset. This is in contrast to the underlying foundation models, which have been shown to be applicable to a wide range of data and tasks. Thus, in this work, we answer the question of whether current real-world capable object-centric methods exhibit similar levels of transferability by introducing a benchmark comprising seven different synthetic and real-world datasets. We analyze the factors influencing performance under transfer and find that training on diverse real-world images improves generalization to unseen scenarios. Furthermore, inspired by the success of task-specific fine-tuning in foundation models, we introduce a novel fine-tuning strategy to adapt pre-trained vision encoders for the task of object discovery. We find that the proposed approach results in state-of-the-art performance for unsupervised object discovery, exhibiting strong zero-shot transfer to unseen datasets.",
      "authors": [
        "Aniket Rajiv Didolkar",
        "Andrii Zadaianchuk",
        "Anirudh Goyal",
        "Michael Curtis Mozer",
        "Yoshua Bengio",
        "Georg Martius",
        "Maximilian Seitzer"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=bSq0XGS3kW",
      "cdate": 1727463805617,
      "mdate": 1740922740502,
      "matched_keywords": [
        "foundation model",
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835262"
    },
    {
      "id": "5qg6JPSgCj",
      "title": "Score-based free-form architectures for high-dimensional Fokker-Planck equations",
      "abstract": "Deep learning methods incorporate PDE residuals as the loss function for solving Fokker-Planck equations, and usually impose the proper normalization condition to avoid a trivial solution. However, soft constraints require careful balancing of multi-objective loss functions, and specific network architectures may limit representation capacity under hard constraints. In this paper, we propose a novel framework: Fokker-Planck neural network (FPNN) that adopts a score PDE loss to decouple the score learning and the density normalization into two stages. Our method allows free-form network architectures to model the unnormalized density and strictly satisfy normalization constraints by post-processing. We demonstrate the effectiveness on various high-dimensional steady-state Fokker-Planck (SFP) equations, achieving superior accuracy and over a 20$\\times$ speedup compared to state-of-the-art methods. Without any labeled data, FPNNs achieve the mean absolute percentage error (MAPE) of 11.36%, 13.87% and 12.72% for 4D Ring, 6D Unimodal and 6D Multi-modal problems respectively, requiring only 256, 980, and 980 parameters. Experimental results highlights the potential as a universal fast solver for handling more than 20-dimensional SFP equations, with great gains in efficiency, accuracy, memory and computational resource usage.",
      "authors": [
        "Feng Liu",
        "Faguo Wu",
        "Xiao Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=5qg6JPSgCj",
      "cdate": 1727463750014,
      "mdate": 1743407540256,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835267"
    },
    {
      "id": "EbCUbPZjM1",
      "title": "ReGen: Generative Robot Simulation via Inverse Design",
      "abstract": "Simulation plays a key role in scaling robot learning and validating policies, but constructing simulations remains labor-intensive. In this paper, we introduce ReGen, a generative simulation framework that automates this process using inverse design. Given an agent's behavior (such as a motion trajectory or objective function) and its textual description, we infer the underlying scenarios and environments that could have caused the behavior.\nOur approach leverages large language models to construct and expand a graph that captures cause-and-effect relationships and relevant entities with properties in the environment, which is then processed to configure a robot simulation environment. Our approach supports (i) augmenting simulations based on ego-agent behaviors, (ii) controllable, counterfactual scenario generation, (iii) reasoning about agent cognition and mental states, and (iv) reasoning with distinct sensing modalities, such as braking due to faulty GPS signals. \nWe demonstrate our method in autonomous driving and robot manipulation tasks, generating more diverse, complex simulated environments compared to existing simulations with high success rates, and enabling controllable generation for corner cases. This approach enhances the validation of robot policies and supports data or simulation augmentation, advancing scalable robot learning for improved generalization and robustness.",
      "authors": [
        "Phat Tan Nguyen",
        "Tsun-Hsuan Wang",
        "Zhang-Wei Hong",
        "Erfan Aasi",
        "Andrew Silva",
        "Guy Rosman",
        "Sertac Karaman",
        "Daniela Rus"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=EbCUbPZjM1",
      "cdate": 1727463714853,
      "mdate": 1740901319520,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835275"
    },
    {
      "id": "GkWA6NjePN",
      "title": "Multi-agent cooperation through learning-aware policy gradients",
      "abstract": "Self-interested individuals often fail to cooperate, posing a fundamental challenge for multi-agent learning. How can we achieve cooperation among self-interested, independent learning agents? Promising recent work has shown that in certain tasks cooperation can be established between ``learning-aware\" agents who model the learning dynamics of each other. Here, we present the first unbiased, higher-derivative-free policy gradient algorithm for learning-aware reinforcement learning, which takes into account that other agents are themselves learning through trial and error based on multiple noisy trials. We then leverage efficient sequence models to condition behavior on long observation histories that contain traces of the learning dynamics of other agents. Training long-context policies with our algorithm leads to cooperative behavior and high returns on standard social dilemmas, including a challenging environment where temporally-extended action coordination is required. Finally, we derive from the iterated prisoner's dilemma a novel explanation for how and when cooperation arises among self-interested learning-aware agents.",
      "authors": [
        "Alexander Meulemans",
        "Seijin Kobayashi",
        "Johannes von Oswald",
        "Nino Scherrer",
        "Eric Elmoznino",
        "Blake Aaron Richards",
        "Guillaume Lajoie",
        "Blaise Aguera y Arcas",
        "Joao Sacramento"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=GkWA6NjePN",
      "cdate": 1727463653269,
      "mdate": 1747068382109,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835281"
    },
    {
      "id": "2ET561DyPe",
      "title": "Few-Class Arena: A Benchmark for Efficient Selection of Vision Models and Dataset Difficulty Measurement",
      "abstract": "We propose Few-Class Arena (FCA), as a unified benchmark with focus on testing efficient image classification models for few classes. A wide variety of benchmark datasets with many classes (80-1000) have been created to assist Computer Vision architectural evolution. An increasing number of vision models are evaluated with these many-class datasets. However, real-world applications often involve substantially fewer classes of interest (2-10). This gap between many and few classes makes it difficult to predict performance of the few-class applications using models trained on the available many-class datasets. To date, little has been offered to evaluate models in this Few-Class Regime. We conduct a systematic evaluation of the ResNet family trained on ImageNet subsets from 2 to 1000 classes, and test a wide spectrum of Convolutional Neural Networks and Transformer architectures over ten datasets by using our newly proposed FCA tool. Furthermore, to aid an up-front assessment of dataset difficulty and a more efficient selection of models, we incorporate a difficulty measure as a function of class similarity. FCA offers a new tool for efficient machine learning in the Few-Class Regime, with goals ranging from a new efficient class similarity proposal, to lightweight model architecture design, to a new scaling law. FCA is user-friendly and can be easily extended to new models and datasets, facilitating future research work. Our benchmark is available at https://github.com/bryanbocao/fca.",
      "authors": [
        "Bryan Bo Cao",
        "Lawrence O'Gorman",
        "Michael Coss",
        "Shubham Jain"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=2ET561DyPe",
      "cdate": 1727463641906,
      "mdate": 1747341048701,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835286"
    },
    {
      "id": "9cQB1Hwrtw",
      "title": "Transformers Struggle to Learn to Search",
      "abstract": "Search is an ability foundational in many important tasks, and recent studies have shown that large language models (LLMs) struggle to perform search robustly. It is unknown whether this inability is due to a lack of data, insufficient model parameters, or fundamental limitations of the transformer architecture. In this work, we use the foundational graph connectivity problem as a testbed to generate effectively limitless high-coverage data to train small transformers and test whether they can learn to perform search. We find that, when given the right training distribution, the transformer is able to learn to search.\n\nWe analyze the algorithm that the transformer has learned through a novel mechanistic interpretability technique that enables us to extract the computation graph from the trained model. We find that for each vertex in the input graph, transformers compute the set of vertices reachable from that vertex. Each layer then progressively expands these sets, allowing the model to search over a number of vertices exponential in the number of layers.\n\nHowever, we find that as the input graph size increases, the transformer has greater difficulty in learning the task. This difficulty is not resolved even as the number of parameters is increased, suggesting that increasing model scale will not lead to robust search abilities. We also find that performing search in-context (i.e., chain-of-thought) does not resolve this inability to learn to search on larger graphs.",
      "authors": [
        "Abulhair Saparov",
        "Srushti Ajay Pawar",
        "Shreyas Pimpalgaonkar",
        "Nitish Joshi",
        "Richard Yuanzhe Pang",
        "Vishakh Padmakumar",
        "Mehran Kazemi",
        "Najoung Kim",
        "He He"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=9cQB1Hwrtw",
      "cdate": 1727463535795,
      "mdate": 1742125140997,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835292"
    },
    {
      "id": "lsvGqR6OTf",
      "title": "Is uniform expressivity too restrictive? Towards efficient expressivity of GNNs",
      "abstract": "Uniform expressivity  guarantees that a Graph Neural Network (GNN) can express a query without the parameters depending on the size of the input graphs. This property is desirable in applications in order to  have number of trainable parameters  that is independent of the size of the input graphs. Uniform expressivity of the two variable guarded fragment (GC2) of first order logic is a well-celebrated result for Rectified Linear Unit (ReLU) GNNs [Barcelo &. Al, 2020]. In this article, we prove that uniform expressivity of GC2 queries is not possible for GNNs with a wide class of Pfaffian activation functions (including the sigmoid and $\\tanh$), answering a question formulated by [Grohe, 2021]. We also show that despite these limitations, many of those GNNs can still efficiently express GC2 queries in a way that the number of parameters remains logarithmic on the maximal degree of the input graphs. Furthermore, we demonstrate that a log-log dependency on the degree is achievable for a certain choice of activation function. This shows that uniform expressivity can be successfully relaxed by covering large graphs appearing in practical applications. Our experiments illustrates that our theoretical estimates hold in practice.",
      "authors": [
        "Sammy Khalife",
        "Josué Tonelli-Cueto"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=lsvGqR6OTf",
      "cdate": 1727463482178,
      "mdate": 1740586336348,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835300"
    },
    {
      "id": "aWLQTbfFgV",
      "title": "Training Neural Networks as Recognizers of Formal Languages",
      "abstract": "Characterizing the computational power of neural network architectures in terms of formal language theory remains a crucial line of research, as it describes lower and upper bounds on the reasoning capabilities of modern AI. However, when empirically testing these bounds, existing work often leaves a discrepancy between experiments and the formal claims they are meant to support. The problem is that formal language theory pertains specifically to recognizers: machines that receive a string as input and classify whether it belongs to a language. On the other hand, it is common instead to evaluate language models on proxy tasks, e.g., language modeling or sequence-to-sequence transduction, that are similar in only an informal sense to the underlying theory. We correct this mismatch by training and evaluating neural networks directly as binary classifiers of strings, using a general method that can be applied to a wide variety of languages. As part of this, we extend an algorithm recently proposed by Snæbjarnarson et al. (2025) for efficient length-controlled sampling of strings from regular languages. We provide results on a variety of languages across the Chomsky hierarchy for three neural architectures: a simple RNN, an LSTM, and a causally-masked transformer. We find that the RNN and LSTM often outperform the transformer, and that auxiliary training objectives such as language modeling can help, although no single objective uniformly improves performance across languages and architectures. Our contributions will facilitate theoretically sound empirical testing of language recognition claims in future work. We have released our datasets as a benchmark called FLaRe (Formal Language Recognition), along with our code.",
      "authors": [
        "Alexandra Butoi",
        "Ghazal Khalighinejad",
        "Anej Svete",
        "Josef Valvoda",
        "Ryan Cotterell",
        "Brian DuSell"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=aWLQTbfFgV",
      "cdate": 1727463456696,
      "mdate": 1742582096438,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835305"
    },
    {
      "id": "S1Bv3068Xt",
      "title": "Can We Trust Embodied Agents? Exploring Backdoor Attacks against Embodied LLM-Based Decision-Making Systems",
      "abstract": "Large Language Models (LLMs) have shown significant promise in real-world decision-making tasks for embodied artificial intelligence, especially when fine-tuned to leverage their inherent common sense and reasoning abilities while being tailored to specific applications. However, this fine-tuning process introduces considerable safety and security vulnerabilities, especially in safety-critical cyber-physical systems. In this work, we propose the first comprehensive framework for **B**ackdoor **A**ttacks against **L**LM-based **D**ecision-making systems (BALD) in embodied AI, systematically exploring the attack surfaces and trigger mechanisms. Specifically, we propose three distinct attack mechanisms: *word injection*, *scenario manipulation*, and *knowledge injection*, targeting various components in the LLM-based decision-making pipeline. We perform extensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in autonomous driving and home robot tasks, demonstrating the effectiveness and stealthiness of our backdoor triggers across various attack channels, with cases like vehicles accelerating toward obstacles and robots placing knives on beds. Our word and knowledge injection attacks achieve nearly 100\\% success rate across multiple models and datasets while requiring only limited access to the system. Our scenario manipulation attack yields success rates exceeding 65\\%, reaching up to 90\\%, and does not require any runtime system intrusion. We also assess the robustness of these attacks against defenses, revealing their resilience. Our findings highlight critical security vulnerabilities in embodied LLM systems and emphasize the urgent need for safeguarding these systems to mitigate potential risks.",
      "authors": [
        "Ruochen Jiao",
        "Shaoyuan Xie",
        "Justin Yue",
        "TAKAMI SATO",
        "Lixu Wang",
        "Yixuan Wang",
        "Qi Alfred Chen",
        "Qi Zhu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=S1Bv3068Xt",
      "cdate": 1727463329705,
      "mdate": 1741861112041,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835310"
    },
    {
      "id": "JeLqFpFzwX",
      "title": "Self-Attention-Based Contextual Modulation Improves Neural System Identification",
      "abstract": "Convolutional neural networks (CNNs) have been shown to be state-of-the-art models for visual cortical neurons. Cortical neurons in the primary visual cortex are sensitive to contextual information mediated by extensive horizontal and feedback connections. Standard CNNs integrate global contextual information to model contextual modulation via two mechanisms: successive convolutions and a fully connected readout layer. In this paper, we find that self-attention (SA), an implementation of non-local network mechanisms, can improve neural response predictions over parameter-matched CNNs in two key metrics: tuning curve correlation and peak tuning. We introduce peak tuning as a metric to evaluate a model's ability to capture a neuron's top feature preference. We factorize networks to assess each context mechanism, revealing that information in the local receptive field is most important for modeling overall tuning, but surround information is critically necessary for characterizing the tuning peak. We find that self-attention can replace posterior spatial-integration convolutions when learned incrementally, and is further enhanced in the presence of a fully connected readout layer, suggesting that the two context mechanisms are complementary. Finally, we find that decomposing receptive field learning and contextual modulation learning in an incremental manner may be an effective and robust mechanism for learning surround-center interactions.",
      "authors": [
        "Isaac Lin",
        "Tianye Wang",
        "Shang Gao",
        "Tang Shiming",
        "Tai Sing Lee"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=JeLqFpFzwX",
      "cdate": 1727463085426,
      "mdate": 1740803538428,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835315"
    },
    {
      "id": "OhUoTMxFIH",
      "title": "Robotouille: An Asynchronous Planning Benchmark for LLM Agents",
      "abstract": "Effective asynchronous planning, or the ability to efficiently reason and plan over states and actions that must happen in parallel or sequentially, is essential for agents that must account for time delays, reason over diverse long-horizon tasks, and collaborate with other agents. While large language model (LLM) agents show promise in high-level task planning, current benchmarks focus primarily on short-horizon tasks and do not evaluate such asynchronous planning capabilities. We introduce Robotouille, a challenging benchmark environment designed to test LLM agents' ability to handle long-horizon asynchronous scenarios. Our synchronous and asynchronous datasets capture increasingly complex planning challenges that go beyond existing benchmarks, requiring agents to manage over-\nlapping tasks and interruptions Our results show that ReAct (gpt-4o) achieves 47% on synchronous tasks but only 11% on asynchronous tasks, highlighting significant room for improvement. We further analyze failure modes, demonstrating the need for LLM agents to better incorporate long-horizon feedback and self-audit their reasoning during task execution.",
      "authors": [
        "Gonzalo Gonzalez-Pumariega",
        "Leong Su Yean",
        "Neha Sunkara",
        "Sanjiban Choudhury"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=OhUoTMxFIH",
      "cdate": 1727462776945,
      "mdate": 1740852177689,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835320"
    },
    {
      "id": "3UKOzGWCVY",
      "title": "Learn-by-interact: A Data-Centric Framework For Self-Adaptive Agents in Realistic Environments",
      "abstract": "Autonomous agents powered by large language models (LLMs) have the potential to enhance human capabilities, assisting with digital tasks from sending emails to performing data analysis.   The abilities of existing LLMs at such tasks are often hindered by the lack of high-quality agent data from the corresponding environments they interact with.  We propose LEARN-BY-INTERACT, a data-centric framework to adapt LLM agents to any given environments without human annotations.   LEARN-BY-INTERACT synthesizes trajectories of agent-environment interactions based on documentations, and constructs instructions by summarizing or abstracting the interaction histories, a process called backward construction. We assess the quality of our synthetic data by using them in both training-based scenarios and training-free in-context learning (ICL), where we craft innovative retrieval approaches optimized for agents. Extensive experiments on SWE-bench, WebArena, OSWorld, and Spider2-V spanning across realistic coding, web, and desktop environments show the effectiveness of LEARN-BY-INTERACT in various downstream agentic tasks — baseline results are improved up to 11.1% for ICL with Claude-3.5 and 23.1% for training with Codestral-22B. We further demonstrate the critical role of backward construction, which provides up to 10.6% improvement for training.  Our ablation studies demonstrate the efficiency provided by our synthesized data in ICL and the superiority of our retrieval pipeline over alternative approaches like conventional retrieval-augmented generation (RAG). We expect that LEARN-BY-INTERACT will serve as a foundation for agent data synthesis as LLMs are increasingly deployed at real-world environments.",
      "authors": [
        "Hongjin SU",
        "Ruoxi Sun",
        "Jinsung Yoon",
        "Pengcheng Yin",
        "Tao Yu",
        "Sercan O Arik"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=3UKOzGWCVY",
      "cdate": 1727462573356,
      "mdate": 1740886362848,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835325"
    },
    {
      "id": "hjROBHstZ3",
      "title": "Causal Representation Learning from Multimodal Biomedical Observations",
      "abstract": "Prevalent in biomedical applications (e.g., human phenotype research), multimodal datasets can provide valuable insights into the underlying physiological mechanisms. However, current machine learning (ML) models designed to analyze these datasets often lack interpretability and identifiability guarantees, which are essential for biomedical research. Recent advances in causal representation learning have shown promise in identifying interpretable latent causal variables with formal theoretical guarantees. Unfortunately, most current work on multimodal distributions either relies on restrictive parametric assumptions or yields only coarse identification results, limiting their applicability to biomedical research that favors a detailed understanding of the mechanisms.\n\nIn this work, we aim to develop flexible identification conditions for multimodal data and principled methods to facilitate the understanding of biomedical datasets. Theoretically, we consider a nonparametric latent distribution (c.f., parametric assumptions in previous work) that allows for causal relationships across potentially different modalities. We establish identifiability guarantees for each latent component, extending the subspace identification results from previous work. Our key theoretical contribution is the structural sparsity of causal connections between modalities, which, as we will discuss, is natural for a large collection of biomedical systems.\nEmpirically, we present a practical framework to instantiate our theoretical insights. We demonstrate the effectiveness of our approach through extensive experiments on both numerical and synthetic datasets. Results on a real-world human phenotype dataset are consistent with established biomedical research, validating our theoretical and methodological framework.",
      "authors": [
        "Yuewen Sun",
        "Lingjing Kong",
        "Guangyi Chen",
        "Loka Li",
        "Gongxu Luo",
        "Zijian Li",
        "Yixuan Zhang",
        "Yujia Zheng",
        "Mengyue Yang",
        "Petar Stojanov",
        "Eran Segal",
        "Eric P. Xing",
        "Kun Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=hjROBHstZ3",
      "cdate": 1727462507411,
      "mdate": 1742127701798,
      "matched_keywords": [
        "multimodal",
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835334"
    },
    {
      "id": "NHhjczmJjo",
      "title": "On the Learn-to-Optimize Capabilities of Transformers in In-Context Sparse Recovery",
      "abstract": "An intriguing property of the Transformer is its ability to perform in-context learning (ICL), where the Transformer can solve different inference tasks without parameter updating based on the contextual information provided by the corresponding input-output demonstration pairs. It has been theoretically proved that ICL is enabled by the capability of Transformers to perform gradient-descent algorithms (Von Oswald et al., 2023a; Bai et al., 2024). This work takes a step further and shows that Transformers can perform learning-to-optimize (L2O) algorithms. Specifically, for the ICL sparse recovery (formulated as LASSO) tasks, we show that a K-layer Transformer can perform an L2O algorithm with a provable convergence rate linear in K. This provides a new perspective explaining the superior ICL capability of Transformers, even with only a few layers, which cannot be achieved by the standard gradient-descent algorithms. Moreover, unlike the conventional L2O algorithms that require the measurement matrix involved in training to match that in testing, the trained Transformer is able to solve sparse recovery problems generated with different measurement matrices.  Besides, Transformers as an L2O algorithm can leverage structural information embedded in the training tasks to accelerate its convergence during ICL, and generalize across different lengths of demonstration pairs, where conventional L2O algorithms typically struggle or fail. Such theoretical findings are supported by our experimental results.",
      "authors": [
        "Renpu Liu",
        "Ruida Zhou",
        "Cong Shen",
        "Jing Yang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=NHhjczmJjo",
      "cdate": 1727462418780,
      "mdate": 1741197830159,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835339"
    },
    {
      "id": "Fty0wTcemV",
      "title": "DELIFT: Data Efficient Language model Instruction Fine-Tuning",
      "abstract": "Fine-tuning large language models (LLMs) is crucial for task specialization but often becomes resource-intensive due to redundant or uninformative data. Existing data selection methods typically rely either on computationally expensive gradient-based metrics or static embeddings that fail to adapt dynamically to the model’s evolving state, thus limiting their practical effectiveness. To address this,\nwe propose DELIFT (Data Efficient Language model Instruction Fine-Tuning), leveraging a novel, computationally efficient utility metric inspired by In-Context Learning (ICL). Our ICL-based metric measures the informational value of each data sample by quantifying its effectiveness as an in-context example in improving model predictions for other samples, reflecting its actual contribution relative to the model’s current state. Integrated with tailored submodular optimization methods, DELIFT systematically selects diverse, informative subsets optimized specifically for each fine-tuning stage: instruction tuning, task-specific adaptation, and continual fine-tuning. Experimental results across multiple datasets and model scales show DELIFT reduces fine-tuning data requirements by up to 70% without compromising performance, consistently outperforming existing methods by up to 26% in effectiveness and efficiency.",
      "authors": [
        "Ishika Agarwal",
        "Krishnateja Killamsetty",
        "Lucian Popa",
        "Marina Danilevsky"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Fty0wTcemV",
      "cdate": 1727462140867,
      "mdate": 1742015753979,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835344"
    },
    {
      "id": "jjfve2gIXe",
      "title": "U-shaped and Inverted-U Scaling behind Emergent Abilities of Large Language Models",
      "abstract": "Large language models (LLMs) have been shown to exhibit *emergent abilities* in some downstream tasks, where model performance stagnates at first and then improves sharply and unpredictably with scale beyond a threshold. In this work, we investigate the phenomenon by grouping questions based on difficulty level and provide a possible explanation for emergent abilities. Specifically, we observe U-shaped scaling for hard questions and inverted-U scaling followed by steady improvement for easy questions. The two scaling patterns initially offset each other, causing stagnant overall performance. The performance starts to soar when the scaling pattern of easy questions reverts from inverse to standard scaling, leading to emergent abilities. Based on this finding, we propose a simple yet effective pipeline, called *Slice-and-Sandwich*, to predict the emergence threshold and model performance beyond the threshold. Our code is publicly available at https://github.com/tony10101105/ExpEmergence.",
      "authors": [
        "Tung-Yu Wu",
        "Melody Lo"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=jjfve2gIXe",
      "cdate": 1727462106100,
      "mdate": 1739261755511,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835349"
    },
    {
      "id": "tU074jg2vS",
      "title": "Hierarchical Autoregressive Transformers: Combining Byte- and Word-Level Processing for Robust, Adaptable Language Models",
      "abstract": "Tokenization is a fundamental step in natural language processing, breaking text into units that computational models can process. While learned subword tokenizers have become the de-facto standard, they present challenges such as large vocabularies, limited adaptability to new domains or languages, and sensitivity to spelling errors and variations. To overcome these limitations, we investigate a hierarchical architecture for autoregressive language modelling that combines character-level and word-level processing. It employs a lightweight character-level encoder to convert character sequences into word embeddings, which are then processed by a word-level backbone model and decoded back into characters via a compact character-level decoder. This method retains the sequence compression benefits of word-level tokenization without relying on a rigid, predefined vocabulary. We demonstrate, at scales up to 7 billion parameters, that hierarchical transformers match the downstream task performance of subword-tokenizer-based models while exhibiting significantly greater robustness to input perturbations. Additionally, during continued pretraining on an out-of-domain language, our model trains almost twice as fast, achieves superior performance on the target language, and retains more of its previously learned knowledge. Hierarchical transformers pave the way for NLP systems that are more robust, flexible, and generalizable across languages and domains.",
      "authors": [
        "Pit Neitemeier",
        "Björn Deiseroth",
        "Constantin Eichenberg",
        "Lukas Balles"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=tU074jg2vS",
      "cdate": 1727462053737,
      "mdate": 1740890355624,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835354"
    },
    {
      "id": "JDm7oIcx4Y",
      "title": "Accelerated training through iterative gradient propagation along the residual path",
      "abstract": "Despite being the cornerstone of deep learning, backpropagation is criticized for its inherent sequentiality, which can limit the scalability of very deep models.\nSuch models faced convergence issues due to vanishing gradient, later resolved using residual connections. Variants of these are now widely used in modern architectures.\nHowever, the computational cost of backpropagation remains a major burden, accounting for most of the training time.\nTaking advantage of residual-like architectural designs, we introduce Highway backpropagation, a parallelizable iterative algorithm that approximates backpropagation, by alternatively i) accumulating the gradient estimates along the residual path, and ii) backpropagating them through every layer in parallel. This algorithm is naturally derived from a decomposition of the gradient as the sum of gradients flowing through all paths, and is adaptable to a diverse set of common architectures, ranging from ResNets and Transformers to recurrent neural networks.\nThrough an extensive empirical study on a large selection of tasks and models, we evaluate Highway-BP and show that major speedups can be achieved with minimal performance degradation.",
      "authors": [
        "Erwan Fagnou",
        "Paul Caillon",
        "Blaise Delattre",
        "Alexandre Allauzen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=JDm7oIcx4Y",
      "cdate": 1727462028240,
      "mdate": 1744111061953,
      "matched_keywords": [
        "transformer",
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835362"
    },
    {
      "id": "dsHpulHpOK",
      "title": "Reinforcement Learning for Control of Non-Markovian Cellular Population Dynamics",
      "abstract": "Many organisms and cell types, from bacteria to cancer cells, exhibit a remarkable ability to adapt to fluctuating environments. Additionally, cells can leverage memory of past environments to better survive previously-encountered stressors. From a control perspective, this adaptability poses significant challenges in driving cell populations toward extinction, and is thus an open question with great clinical significance. In this work, we focus on drug dosing in cell populations exhibiting phenotypic plasticity. For specific dynamical models switching between resistant and susceptible states, exact solutions are known. However, when the underlying system parameters are unknown, and for complex memory-based systems, obtaining the optimal solution is currently intractable. To address this challenge, we apply reinforcement learning (RL) to identify informed dosing strategies to control cell populations evolving under novel non-Markovian dynamics. We find that model-free deep RL is able to recover exact solutions and control cell populations even in the presence of long-range temporal dynamics.  To further test our approach in more realistic settings, we demonstrate performant RL-based control strategies in environments with dynamic memory strength.",
      "authors": [
        "Josiah C Kratz",
        "Jacob Adamczyk"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=dsHpulHpOK",
      "cdate": 1727461986274,
      "mdate": 1746801877832,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835367"
    },
    {
      "id": "KeRwLLwZaw",
      "title": "Locally Connected Echo State Networks for Time Series Forecasting",
      "abstract": "Echo State Networks (ESNs) are a class of recurrent neural networks in which only a small readout regression layer is trained, while the weights of the recurrent network, termed the reservoir, are randomly assigned and remain fixed. Our work introduces the Locally Connected ESN (LCESN), a novel ESN variant with a locally connected reservoir, forced memory, and a weight adaptation strategy. LCESN significantly reduces the asymptotic time and space complexities compared to the conventional ESN, enabling substantially larger networks. LCESN also improves the memory properties of ESNs without affecting network stability. We evaluate LCESN's performance on the NARMA10 benchmark task and compare it to state-of-the-art models on nine real-world datasets. Despite the simplicity of our model and its one-shot training approach, LCESN achieves competitive results, even surpassing several state-of-the-art models. LCESN introduces a fresh approach to real-world time series forecasting and demonstrates that large, well-tuned random recurrent networks can rival complex gradient-trained feedforward models. We provide our GPU-based implementation of LCESN as an open-source library.",
      "authors": [
        "Filip Matzner",
        "František Mráz"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=KeRwLLwZaw",
      "cdate": 1727461785694,
      "mdate": 1747474738628,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835375"
    },
    {
      "id": "EzrZX9bd4G",
      "title": "BEEM: Boosting Performance of Early Exit DNNs using Multi-Exit Classifiers as Experts",
      "abstract": "Early Exit (EE) techniques have emerged as a means to reduce inference latency in Deep Neural Networks (DNNs). The latency improvement and accuracy in these techniques crucially depend on the criteria used to make exit decisions. We propose a new decision criterion BEEM where exit classifiers are treated as experts and aggregate their confidence scores. The confidence scores are aggregated only if neighbouring experts are consistent in prediction as the samples pass through them, thus capturing their ensemble effect. A sample exits when the aggregated confidence value exceeds a threshold. The threshold is set using the error rates of the intermediate exits aiming to surpass the performance of conventional DNN inference. Experimental results on the COCO dataset for Image captioning and GLUE datasets for various language tasks demonstrate that our method enhances the performance of state-of-the-art EE methods, achieving improvements in speed-up by a factor $1.5\\times$ to $2.1\\times$. When compared to the final layer, its accuracy is comparable in harder Image Captioning and improves in the easier language tasks. The source code is available at https://github.com/Div290/BEEM1/tree/main.",
      "authors": [
        "Divya Jyoti Bajpai",
        "Manjesh Kumar Hanawal"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=EzrZX9bd4G",
      "cdate": 1727461637565,
      "mdate": 1741967836430,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835381"
    },
    {
      "id": "Tpjq66xwTq",
      "title": "Real-time design of architectural structures with differentiable mechanics and neural networks",
      "abstract": "Designing mechanically efficient geometry for architectural structures like shells, towers, and bridges, is an expensive iterative process.\nExisting techniques for solving such inverse problems rely on traditional optimization methods, which are slow and computationally expensive, limiting iteration speed and design exploration.\nNeural networks would seem to offer a solution via data-driven amortized optimization, but they often require extensive fine-tuning and cannot ensure that important design criteria, such as mechanical integrity, are met.\nIn this work, we combine neural networks with a differentiable mechanics simulator to develop a model that accelerates the solution of shape approximation problems for architectural structures represented as bar systems.\nThis model explicitly guarantees compliance with mechanical constraints while generating designs that closely match target geometries.\nWe validate our approach in two tasks, the design of masonry shells and cable-net towers.\nOur model achieves better accuracy and generalization than fully neural alternatives, and comparable accuracy to direct optimization but in real time, enabling fast and reliable design exploration.\nWe further demonstrate its advantages by integrating it into 3D modeling software and fabricating a physical prototype.\nOur work opens up new opportunities for accelerated mechanical design enhanced by neural networks for the built environment.",
      "authors": [
        "Rafael Pastrana",
        "Eder Medina",
        "Isabel M. de Oliveira",
        "Sigrid Adriaenssens",
        "Ryan P Adams"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Tpjq66xwTq",
      "cdate": 1727461611185,
      "mdate": 1740948052378,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835386"
    },
    {
      "id": "JUr0YOMvZA",
      "title": "DAMO: Decoding by Accumulating Activations Momentum for Mitigating Hallucinations in Vision-Language Models",
      "abstract": "Large Vision-Language Models (VLMs) exhibit significant potential in multimodal tasks but often struggle with hallucinations—responses that are plausible yet visually ungrounded. In this work, we investigate the layer-wise prediction tendencies of VLMs and conduct an in-depth analysis of their decoding mechanism. We observe that VLMs tend to ``overthink'' during the final stages of decoding, making significant prediction shifts in the last few layers often favoring incorrect results, which leads to a surge in hallucinative outputs. Leveraging this localized pattern, we propose a novel decoding strategy inspired by the momentum analogy used in gradient descent-based optimizers. Our method enforces decoding consistency across layers in an adaptive manner during forward passes—an under-explored approach in existing works. This strategy significantly improves the reliability and performance of VLMs in various multimodal tasks, while introducing only negligible efficiency overhead.",
      "authors": [
        "Kaishen Wang",
        "Hengrui Gu",
        "Meijun Gao",
        "Kaixiong Zhou"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=JUr0YOMvZA",
      "cdate": 1727461419316,
      "mdate": 1742444295033,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.835394"
    },
    {
      "id": "2e4ECh0ikn",
      "title": "Talking Turns: Benchmarking Audio Foundation Models on Turn-Taking Dynamics",
      "abstract": "The recent wave of audio foundation models (FMs) could provide new capabilities for conversational modeling. However, there have been limited efforts to evaluate these audio FMs comprehensively on their ability to have natural and interactive conversations. To engage in meaningful conversation with the end user, we would want the FMs to additionally perform a fluent succession of turns without too much overlapping speech or long stretches of silence. Inspired by this, we ask whether the recently proposed audio FMs can understand, predict, and perform turn-taking events? To answer this, we propose a novel evaluation protocol that can assess spoken dialog system's turn-taking capabilities using a supervised model as a judge that has been trained to predict turn-taking events in human-human conversations. Using this protocol, we present the first comprehensive user study that evaluates existing spoken dialogue systems on their ability to perform turn-taking events and reveal many interesting insights, such as they sometimes do not understand when to speak up, can interrupt too aggressively and rarely backchannel. We further evaluate multiple open-source and proprietary audio FMs accessible through APIs on carefully curated test benchmarks from Switchboard to measure their ability to understand and predict turn-taking events and identify significant room for improvement. We will open source our evaluation platform to promote the development of advanced conversational AI systems.",
      "authors": [
        "Siddhant Arora",
        "Zhiyun Lu",
        "Chung-Cheng Chiu",
        "Ruoming Pang",
        "Shinji Watanabe"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=2e4ECh0ikn",
      "cdate": 1727461353406,
      "mdate": 1742060994826,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835399"
    },
    {
      "id": "WWXjMYZxfH",
      "title": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions",
      "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated effectiveness in aligning large language models (LLMs) with human preferences. However, token-level RLHF suffers from the credit assignment problem over long sequences, where delayed rewards make it challenging for the model to discern which actions contributed to preferred outcomes. This hinders learning efficiency and slows convergence.In this paper, we propose MA-RLHF, a simple yet effective RLHF framework that incorporates macro actions --- sequences of tokens or higher-level language constructs --- into the learning process. By operating at higher level of abstraction, our approach reduces the temporal distance between actions and rewards, facilitating faster and more accurate credit assignment. This results in more stable policy gradient estimates and enhances learning efficiency within each episode, all without increasing computational complexity during training or inference. We validate our approach through extensive experiments across various model sizes and tasks, including text summarization, dialogue generation, question answering, and program synthesis. Our method achieves substantial performance improvements over standard RLHF, with performance gains of up to 30\\% in text summarization and code generation, 18\\% in dialogue, and 8\\% in question answering tasks. Notably, our approach reaches parity with vanilla RLHF $1.7 \\sim 2$ times faster in terms of training time and continues to outperform it with further training. We make our code and data publicly available at \\url{https://github.com/ernie-research/MA-RLHF}.",
      "authors": [
        "Yekun Chai",
        "Haoran Sun",
        "Huang Fang",
        "Shuohuan Wang",
        "Yu Sun",
        "Hua Wu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=WWXjMYZxfH",
      "cdate": 1727461013215,
      "mdate": 1739574361740,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835405"
    },
    {
      "id": "7PGluppo4k",
      "title": "Logically Consistent Language Models via Neuro-Symbolic Integration",
      "abstract": "Current large language models (LLMs) are far from reliable: they are prone to generate non-factual information and, more crucially, to contradict themselves when prompted to reason about relations between real entities of the world. These problems are currently addressed with large scale fine-tuning or by delegating consistent reasoning to external tools. In this work, we strive for a middle ground and leverage a training objective based on a principled neuro-symbolic loss that teaches a LLM to be consistent with external knowledge in the form of a set of facts and rules. Fine-tuning with such a loss on a limited set of facts enables our LLMs to be more logically consistent than previous baselines for a given constraint. Our approach also allows to easily combine multiple logical constraints at once in a principled way, delivering LLMs that are more consistent w.r.t. all the selected rules. Moreover, our method allows LLMs to extrapolate to unseen but semantically similar factual knowledge, represented in unseen datasets, more systematically.",
      "authors": [
        "Diego Calanzone",
        "Stefano Teso",
        "Antonio Vergari"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=7PGluppo4k",
      "cdate": 1727460684590,
      "mdate": 1743427357282,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835410"
    },
    {
      "id": "INqLJwqUmc",
      "title": "Narrowing Information Bottleneck Theory for Multimodal Image-Text Representations Interpretability",
      "abstract": "The task of identifying multimodal image-text representations has garnered increasing attention, particularly with models such as CLIP (Contrastive Language-Image Pretraining), which demonstrate exceptional performance in learning complex associations between images and text. Despite these advancements, ensuring the interpretability of such models is paramount for their safe deployment in real-world applications, such as healthcare. While numerous interpretability methods have been developed for unimodal tasks, these approaches often fail to transfer effectively to multimodal contexts due to inherent differences in the representation structures. Bottleneck methods, well-established in information theory, have been applied to enhance CLIP's interpretability. However, they are often hindered by strong assumptions or intrinsic randomness. To overcome these challenges, we propose the Narrowing Information Bottleneck Theory, a novel framework that fundamentally redefines the traditional bottleneck approach. This theory is specifically designed to satisfy contemporary attribution axioms, providing a more robust and reliable solution for improving the interpretability of multimodal models. In our experiments, compared to state-of-the-art methods, our approach enhances image interpretability by an average of 9\\%, text interpretability by an average of 58.83\\%, and accelerates processing speed by 63.95\\%. Our code is publicly accessible at https://github.com/LMBTough/NIB.",
      "authors": [
        "Zhiyu Zhu",
        "Zhibo Jin",
        "Jiayu Zhang",
        "NAN YANG",
        "Jiahao Huang",
        "Jianlong Zhou",
        "Fang Chen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=INqLJwqUmc",
      "cdate": 1727460623965,
      "mdate": 1740715868538,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.835415"
    },
    {
      "id": "l6QnSQizmN",
      "title": "Online Reinforcement Learning in Non-Stationary Context-Driven Environments",
      "abstract": "We study online reinforcement learning (RL) in non-stationary environments, where a time-varying exogenous context process affects the environment dynamics. Online RL is challenging in such environments due to \"catastrophic forgetting\" (CF). The agent tends to forget prior knowledge as it trains on new experiences. Prior approaches to mitigate this issue assume task labels (which are often not available in practice), employ brittle regularization heuristics, or use off-policy methods that suffer from instability and poor performance.\n\nWe present Locally Constrained Policy Optimization (LCPO), an online RL approach that combats CF by anchoring policy outputs on old experiences while optimizing the return on current experiences. To perform this anchoring, LCPO locally constrains policy optimization using samples from experiences that lie outside of the current context distribution. We evaluate LCPO in Mujoco, classic control and computer systems environments with a variety of synthetic and real context traces, and find that it outperforms a variety of baselines in the non-stationary setting, while achieving results on-par with a \"prescient\" agent trained offline across all context traces.\n\nLCPO's source code is available at https://github.com/pouyahmdn/LCPO.",
      "authors": [
        "Pouya Hamadanian",
        "Arash Nasr-Esfahany",
        "Malte Schwarzkopf",
        "Siddhartha Sen",
        "Mohammad Alizadeh"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=l6QnSQizmN",
      "cdate": 1727460539163,
      "mdate": 1743435644731,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835420"
    },
    {
      "id": "7FHSPd3SRE",
      "title": "WardropNet: Traffic Flow Predictions via Equilibrium-Augmented Learning",
      "abstract": "When optimizing transportation systems, anticipating traffic flows is a central element. Yet, computing such traffic equilibria remains computationally expensive. Against this background, we introduce a novel combinatorial optimization augmented neural network pipeline that allows for fast and accurate traffic flow predictions. We propose WardropNet, a neural network that combines classical layers with a subsequent equilibrium layer: the first ones inform the latter by predicting the parameterization of the equilibrium problem's latency functions. Using supervised learning we minimize the difference between the actual traffic flow and the predicted output. We show how to leverage a Bregman divergence fitting the geometry of the equilibria, which allows for end-to-end learning. WardropNet outperforms pure learning-based approaches in predicting traffic equilibria for realistic and stylized traffic scenarios. On realistic scenarios, WardropNet improves on average for time-invariant predictions by up to 72\\% and for time-variant predictions by up to 23\\% over pure learning-based approaches.",
      "authors": [
        "Kai Jungel",
        "Dario Paccagnan",
        "Axel Parmentier",
        "Maximilian Schiffer"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=7FHSPd3SRE",
      "cdate": 1727460467754,
      "mdate": 1739789313706,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835425"
    },
    {
      "id": "jmN1zXMq0O",
      "title": "To Clip or not to Clip: the Dynamics of SGD with Gradient Clipping in High-Dimensions",
      "abstract": "The success of modern machine learning is due in part to the adaptive optimization methods that have been developed to deal with the difficulties of training large models over complex datasets. One such method is gradient clipping: a practical procedure with limited theoretical underpinnings. In this work, we study clipping in a least squares problem under streaming SGD. We develop a theoretical analysis of the learning dynamics in the limit of large intrinsic dimension—a model and dataset dependent notion of dimensionality. In this limit we find a deterministic equation that describes the evolution of the loss and demonstrate that this equation predicts the path of clipped SGD on synthetic, CIFAR10, and Wikitext2 data. We show that with Gaussian noise clipping cannot improve SGD performance. Yet, in other noisy settings, clipping can provide benefits with tuning of the clipping threshold. We propose a simple heuristic for near optimal scheduling of the clipping threshold which requires the tuning of only one hyperparameter. We conclude with a discussion about the links between high-dimensional clipping and neural network training.",
      "authors": [
        "Noah Marshall",
        "Ke Liang Xiao",
        "Atish Agarwala",
        "Elliot Paquette"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=jmN1zXMq0O",
      "cdate": 1727460370304,
      "mdate": 1740022112595,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835430"
    },
    {
      "id": "t6QHYUOQL7",
      "title": "Breaking Mental Set to Improve Reasoning through Diverse Multi-Agent Debate",
      "abstract": "Large Language Models (LLMs) have seen significant progress but continue to struggle with persistent reasoning mistakes.\nPrevious methods of *self-reflection* have been proven limited due to the models’ inherent fixed thinking patterns. \nWhile Multi-Agent Debate (MAD) attempts to mitigate this by incorporating multiple agents, it often employs the same reasoning methods, even though assigning different personas to models. This leads to a \"fixed mental set\", where models rely on homogeneous thought processes without exploring alternative perspectives.\nIn this paper, we introduce Diverse Multi-Agent Debate (DMAD), a method that encourages agents to think with distinct reasoning approaches. By leveraging diverse problem-solving strategies, each agent can gain insights from different perspectives, refining its responses through discussion and collectively arriving at the optimal solution. DMAD effectively breaks the limitations of fixed mental sets. We evaluate DMAD against various prompting techniques, including *self-reflection* and traditional MAD, across multiple benchmarks using both LLMs and Multimodal LLMs. Our experiments show that DMAD consistently outperforms other methods, delivering better results than MAD in fewer rounds. Code is available at https://github.com/MraDonkey/DMAD.",
      "authors": [
        "Yexiang Liu",
        "Jie Cao",
        "Zekun Li",
        "Ran He",
        "Tieniu Tan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=t6QHYUOQL7",
      "cdate": 1727460278864,
      "mdate": 1743066449650,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.835436"
    },
    {
      "id": "OFukl9Qg8P",
      "title": "Resolution Attack: Exploiting Image Compression to Deceive Deep Neural Networks",
      "abstract": "Model robustness is essential for ensuring the stability and reliability of machine learning systems. Despite extensive research on various aspects of model robustness, such as adversarial robustness and label noise robustness, the exploration of robustness towards different resolutions, remains less explored. To address this gap, we introduce a novel form of attack: the resolution attack. This attack aims to deceive both classifiers and human observers by generating images that exhibit different semantics across different resolutions. To implement the resolution attack, we propose an automated framework capable of generating dual-semantic images in a zero-shot manner. Specifically, we leverage large-scale diffusion models for their comprehensive ability to construct images and propose a staged denoising strategy to achieve a smoother transition across resolutions. Through the proposed framework, we conduct resolution attacks against various off-the-shelf classifiers. The experimental results exhibit high attack success rate, which not only validates the effectiveness of our proposed framework but also reveals the vulnerability of current classifiers towards different resolutions. Additionally, our framework, which incorporates features from two distinct objects, serves as a competitive tool for applications such as face swapping and facial camouflage. The code is available at https://github.com/ywj1/resolution-attack.",
      "authors": [
        "Wangjia Yu",
        "Xiaomeng Fu",
        "Qiao Li",
        "Jizhong Han",
        "Xiaodan Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=OFukl9Qg8P",
      "cdate": 1727459953449,
      "mdate": 1741014820591,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835441"
    },
    {
      "id": "G4wARwjF8M",
      "title": "SLMRec: Distilling Large Language Models into Small for Sequential Recommendation",
      "abstract": "Sequential Recommendation (SR) task involves predicting the next item a user is likely to interact with, given their past interactions. \nThe SR models examine the sequence of a user's actions to discern more complex behavioral patterns and temporal dynamics. \nRecent research demonstrates the great impact of LLMs on sequential recommendation systems, either viewing sequential recommendation as language modeling or serving as the backbone for user representation. Although these methods deliver outstanding performance, there is scant evidence of the necessity of a large language model and how large the language model is needed, especially in the sequential recommendation scene. Meanwhile, due to the huge size of LLMs, it is inefficient and impractical to apply a LLM-based model in real-world platforms that often need to process billions of traffic logs daily. In this paper, we explore the influence of LLMs' depth by conducting extensive experiments on large-scale industry datasets. Surprisingly, our motivational experiments reveal that most intermediate layers of LLMs are redundant, indicating that pruning the remaining layers can still maintain strong performance.\nMotivated by this insight, we empower small language models for SR, namely SLMRec, which adopt a simple yet effective knowledge distillation method. Moreover, SLMRec is orthogonal to other post-training efficiency techniques, such as quantization and pruning, so that they can be leveraged in combination. Comprehensive experimental results illustrate that the proposed SLMRec model attains the best performance using only 13\\% of the parameters found in LLM-based recommendation models while simultaneously achieving up to 6.6x and 8.0x speedups in training and inference time costs, respectively. Besides, we provide a theoretical justification for why small language models can perform comparably to large language models in SR.",
      "authors": [
        "Wujiang Xu",
        "Qitian Wu",
        "Zujie Liang",
        "Jiaojiao Han",
        "Xuying Ning",
        "Yunxiao Shi",
        "Wenfang Lin",
        "Yongfeng Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=G4wARwjF8M",
      "cdate": 1727459895772,
      "mdate": 1746557600249,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835449"
    },
    {
      "id": "CkKEuLmRnr",
      "title": "How Do Large Language Models Understand Graph Patterns? A Benchmark for Graph Pattern Comprehension",
      "abstract": "Benchmarking the capabilities and limitations of large language models (LLMs) in graph-related tasks is becoming an increasingly popular and crucial area of research. Recent studies have shown that LLMs exhibit a preliminary ability to understand graph structures and node features. However, the potential of LLMs in graph pattern mining remains largely unexplored. This is a key component in fields such as computational chemistry, biology, and social network analysis. To bridge this gap, this work introduces a comprehensive benchmark to assess LLMs' capabilities in graph pattern tasks. We have developed a benchmark that evaluates whether LLMs can understand graph patterns based on either terminological or topological descriptions. Additionally, our benchmark tests the LLMs' capacity to autonomously discover graph patterns from data. The benchmark encompasses both synthetic and real datasets, and a variety of models, with a total of 11 tasks and 7 models. Our experimental framework is designed for easy expansion to accommodate new models and datasets. Our findings reveal that: (1) LLMs have preliminary abilities to understand graph patterns, with O1-mini outperforming in the majority of tasks; (2) Formatting input graph data to align with the knowledge acquired during pretraining can enhance performance; (3) LLMs employ diverse\npotential algorithms to solve one task, with performance varying based on their execution capabilities.",
      "authors": [
        "Xinnan Dai",
        "Haohao Qu",
        "Yifei Shen",
        "Bohang Zhang",
        "Qihao Wen",
        "Wenqi Fan",
        "Dongsheng Li",
        "Jiliang Tang",
        "Caihua Shan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=CkKEuLmRnr",
      "cdate": 1727459857172,
      "mdate": 1740750569907,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835454"
    },
    {
      "id": "IUmj2dw5se",
      "title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models",
      "abstract": "As Large Language Models (LLMs) are increasingly deployed to handle various natural language processing (NLP) tasks, concerns regarding the potential negative societal impacts of LLM-generated content have also arisen. To evaluate the biases exhibited by LLMs, researchers have recently proposed a variety of datasets. However, existing bias evaluation efforts often focus on only a particular type of bias and employ inconsistent evaluation metrics, leading to difficulties in comparison across different datasets and LLMs. To address these limitations, we collect a variety of datasets designed for the bias evaluation of LLMs, and further propose CEB, a Compositional Evaluation Bechmark that covers different types of bias across different social groups and tasks. The curation of CEB is based on our newly proposed compositional taxonomy, which characterizes each dataset from three dimensions: bias types, social groups, and tasks. By combining the three dimensions, we develop a comprehensive evaluation strategy for the bias in LLMs. Our experiments demonstrate that the levels of bias vary across these dimensions, thereby providing guidance for the development of specific bias mitigation methods.",
      "authors": [
        "Song Wang",
        "Peng Wang",
        "Tong Zhou",
        "Yushun Dong",
        "Zhen Tan",
        "Jundong Li"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=IUmj2dw5se",
      "cdate": 1727459801703,
      "mdate": 1740894088403,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835459"
    },
    {
      "id": "txD9llAYn9",
      "title": "Model-based RL as a Minimalist Approach to Horizon-Free and Second-Order Bounds",
      "abstract": "Learning a transition model via Maximum Likelihood Estimation (MLE) followed by planning inside the learned model is perhaps the most standard and simplest Model-based Reinforcement Learning (RL) framework. In this work, we show that such a simple Model-based RL scheme, when equipped with optimistic and pessimistic planning procedures, achieves strong regret and sample complexity bounds in online and offline RL settings. Particularly, we demonstrate that under the conditions where the trajectory-wise reward is normalized between zero and one and the transition is time-homogenous, it achieves nearly horizon-free and second-order bounds.",
      "authors": [
        "Zhiyong Wang",
        "Dongruo Zhou",
        "John C.S. Lui",
        "Wen Sun"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=txD9llAYn9",
      "cdate": 1727459386699,
      "mdate": 1740765090248,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835464"
    },
    {
      "id": "9RCT0ngvZP",
      "title": "Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning",
      "abstract": "Synthetic data has been widely used to train large language models, but their generative nature inevitably introduces noisy, non-informative, and misleading learning signals. In this paper, we propose Montessori-Instruct, a novel data synthesis framework that tailors the data synthesis ability of the teacher language model toward the student language model's learning process. Specifically, we utilize local data influence of synthetic training data points on students to characterize students' learning preferences. Then, we train the teacher model with Direct Preference Optimization (DPO) to generate synthetic data tailored toward student learning preferences. Experiments with Llama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and MT-Bench demonstrate that Montessori-Instruct significantly outperforms standard synthesis methods by 18.35\\% and 46.24\\% relatively. Our method also beats data synthesized by a stronger teacher model, GPT-4o. Further analysis confirms the benefits of teacher's learning to generate more influential training data in the student's improved learning, the advantages of local data influence in accurately measuring student preferences, and the robustness of Montessori-Instruct across different student models. Our code and data are open-sourced at https://github.com/cxcscmu/Montessori-Instruct.",
      "authors": [
        "Xiaochuan Li",
        "Zichun Yu",
        "Chenyan Xiong"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=9RCT0ngvZP",
      "cdate": 1727459343364,
      "mdate": 1740799655566,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835470"
    },
    {
      "id": "5i6ZZUjCA9",
      "title": "Affine Steerable Equivariant Layer for Canonicalization of Neural Networks",
      "abstract": "In the field of equivariant networks, achieving affine equivariance, particularly for general group representations, has long been a challenge.\nIn this paper, we propose the steerable EquivarLayer, a generalization of InvarLayer (Li et al., 2024), by building on the concept of equivariants beyond invariants.\nThe steerable EquivarLayer supports affine equivariance with arbitrary input and output representations, marking the first model to incorporate steerability into networks for the affine group.\nTo integrate it with canonicalization, a promising approach for making pre-trained models equivariant, we introduce a novel Det-Pooling module, expanding the applicability of EquivarLayer and the range of groups suitable for canonicalization.\nWe conduct experiments on image classification tasks involving group transformations to validate the steerable EquivarLayer in the role of a canonicalization function, demonstrating its effectiveness over data augmentation.",
      "authors": [
        "Yikang Li",
        "Yeqing Qiu",
        "Yuxuan Chen",
        "Zhouchen Lin"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=5i6ZZUjCA9",
      "cdate": 1727459293628,
      "mdate": 1741959457247,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835475"
    },
    {
      "id": "2U8owdruSQ",
      "title": "Has the Deep Neural Network learned the Stochastic Process? An Evaluation Viewpoint",
      "abstract": "This paper presents the first systematic study of evaluating Deep Neural Networks (DNNs) designed to forecast the evolution of stochastic complex systems. We show that traditional evaluation methods like threshold-based classification metrics and error-based scoring rules assess a DNN's ability to replicate the observed ground truth but fail to measure the DNN's learning of the underlying stochastic process. To address this gap, we propose a new evaluation criteria called _Fidelity to Stochastic Process (F2SP)_, representing the DNN's ability to predict the system property _Statistic-GT_—the ground truth of the stochastic process—and introduce an evaluation metric that exclusively assesses F2SP. We formalize F2SP within a stochastic framework and establish criteria for validly measuring it. We formally show that Expected Calibration Error (ECE) satisfies the necessary condition for testing F2SP, unlike traditional evaluation methods. Empirical experiments on synthetic datasets, including wildfire, host-pathogen, and stock market models, demonstrate that ECE uniquely captures F2SP. We further extend our study to real-world wildfire data, highlighting the limitations of conventional evaluation and discuss the practical utility of incorporating F2SP into model assessment. This work offers a new perspective on evaluating DNNs modeling complex systems by emphasizing the importance of capturing underlying the stochastic process.",
      "authors": [
        "Harshit Kumar",
        "Beomseok Kang",
        "Biswadeep Chakraborty",
        "Saibal Mukhopadhyay"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=2U8owdruSQ",
      "cdate": 1727459054301,
      "mdate": 1740689784455,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835480"
    },
    {
      "id": "Mjn53GtMxi",
      "title": "LICORICE: Label-Efficient Concept-Based Interpretable Reinforcement Learning",
      "abstract": "Recent advances in reinforcement learning (RL) have predominantly leveraged neural network policies for decision-making, yet these models often lack interpretability, posing challenges for stakeholder comprehension and trust. Concept bottleneck models offer an interpretable alternative by integrating human-understandable concepts into policies. However, prior work assumes that concept annotations are readily available during training. For RL, this requirement poses a significant limitation: it necessitates continuous real-time concept annotation, which either places an impractical burden on human annotators or incurs substantial costs in API queries and inference time when employing automated labeling methods. To overcome this limitation, we introduce a novel training scheme that enables RL agents to efficiently learn a concept-based policy by only querying annotators to label a small set of data. Our algorithm, LICORICE, involves three main contributions: interleaving concept learning and RL training, using an ensemble to actively select informative data points for labeling, and decorrelating the concept data. We show how LICORICE reduces human labeling efforts to 500 or fewer concept labels in three environments, and 5000 or fewer in two more complex environments, all at no cost to performance. We also explore the use of VLMs as automated concept annotators, finding them effective in some cases but imperfect in others. Our work significantly reduces the annotation burden for interpretable RL, making it more practical for real-world applications that necessitate transparency. Our code is released.",
      "authors": [
        "Zhuorui Ye",
        "Stephanie Milani",
        "Geoffrey J. Gordon",
        "Fei Fang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Mjn53GtMxi",
      "cdate": 1727459005683,
      "mdate": 1742103961479,
      "matched_keywords": [
        "reinforcement learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835485"
    },
    {
      "id": "TuOTSAiHDn",
      "title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs",
      "abstract": "The utility of synthetic data to enhance pretraining data quality and hence to improve downstream task accuracy has been widely explored in recent large language models (LLMs). Yet, these approaches fall inadequate in complex, multi-hop and mathematical reasoning tasks as the synthetic data typically fails to add complementary knowledge to the existing raw corpus. In this work, we propose a novel large-scale and diverse Math Informed syNthetic Dialogue (MIND) generation method that improves the mathematical reasoning ability of LLMs. Specifically, using MIND, we generate synthetic conversations based on OpenWebMath (OWM), resulting in a new math corpus, MIND-OWM. Our experiments with different conversational settings reveal that incorporating knowledge gaps between dialog participants is essential for generating high-quality math data. We further identify an effective way to format and integrate synthetic and raw data during pretraining to maximize the gain in mathematical reasoning, emphasizing the need to restructure raw data rather than use it as-is. Compared to pretraining just on raw data, a model pretrained on MIND-OWM shows significant boost in mathematical reasoning (GSM8K: +13.42%, MATH: +2.30%), including superior performance in specialized knowledge (MMLU: +4.55%, MMLU-STEM: +4.28%) and general\npurpose reasoning tasks (GENERAL REASONING: +2.51%).",
      "authors": [
        "Syeda Nahida Akter",
        "Shrimai Prabhumoye",
        "John Kamalu",
        "Sanjeev Satheesh",
        "Eric Nyberg",
        "Mostofa Patwary",
        "Mohammad Shoeybi",
        "Bryan Catanzaro"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=TuOTSAiHDn",
      "cdate": 1727458945973,
      "mdate": 1744044976625,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835490"
    },
    {
      "id": "GbgCRJedQ7",
      "title": "SMT: Fine-Tuning Large Language Models with Sparse Matrices",
      "abstract": "Various parameter-efficient fine-tuning (PEFT) methods, including LoRA and its variants, have gained popularity for reducing computational costs. However, there is often an accuracy gap between PEFT approaches and full fine-tuning (FT), and this discrepancy has not yet been systematically explored. In this work, we introduce a method for selecting sparse sub-matrices that aims to minimize the performance gap between PEFT vs. full fine-tuning (FT) while also reducing both fine-tuning computational costs and memory costs. We explored both gradient-based and activation-based parameter selection methods to identify the most significant sub-matrices for downstream tasks, updating only these blocks during fine-tuning. In our experiments, we demonstrated that SMT consistently surpasses other PEFT\nbaselines (e.g., LoRA and DoRA) in fine-tuning popular large language models such as LLaMA across a broad spectrum of tasks, while reducing the GPU memory footprint by 67% compared to FT. We also examine how the performance of LoRA and DoRA tends to plateau and decline as the number of trainable parameters increases, in contrast, our SMT method does not suffer from such issues.",
      "authors": [
        "Haoze He",
        "Juncheng B Li",
        "Xuan Jiang",
        "Heather Miller"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=GbgCRJedQ7",
      "cdate": 1727458944280,
      "mdate": 1747075781468,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835495"
    },
    {
      "id": "tnB94WQGrn",
      "title": "KGARevion: An AI Agent for Knowledge-Intensive Biomedical QA",
      "abstract": "Biomedical reasoning integrates structured, codified knowledge with tacit, experience-driven insights. Depending on the context, quantity, and nature of available evidence, researchers and clinicians use diverse strategies, including rule-based, prototype-based, and case-based reasoning. Effective medical AI models must handle this complexity while ensuring reliability and adaptability. We introduce KGARevion, a knowledge graph-based agent that answers knowledge-intensive questions. Upon receiving a query, KGARevion generates relevant triplets by leveraging the latent knowledge embedded in a large language model. It then verifies these triplets against a grounded knowledge graph, filtering out errors and retaining only accurate, contextually relevant information for the final answer. This multi-step process strengthens reasoning, adapts to different models of medical inference, and outperforms retrieval-augmented generation-based approaches that lack effective verification mechanisms. Evaluations on medical QA benchmarks show that KGARevion improves accuracy by over 5.2% over 15 models in handling complex medical queries. To further assess its effectiveness, we curated three new medical QA datasets with varying levels of semantic complexity, where KGARevion improved accuracy by 10.4%. The agent integrates with different LLMs and biomedical knowledge graphs for broad applicability across knowledge-intensive tasks. We evaluated KGARevion on AfriMed-QA, a newly introduced dataset focused on African healthcare, demonstrating its strong zero-shot generalization to underrepresented medical contexts.",
      "authors": [
        "Xiaorui Su",
        "Yibo Wang",
        "Shanghua Gao",
        "Xiaolong Liu",
        "Valentina Giunchiglia",
        "Djork-Arné Clevert",
        "Marinka Zitnik"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=tnB94WQGrn",
      "cdate": 1727458872284,
      "mdate": 1741014820415,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835503"
    },
    {
      "id": "MeGDmZjUXy",
      "title": "Moral Alignment for LLM Agents",
      "abstract": "Decision-making agents based on pre-trained Large Language Models (LLMs) are increasingly being deployed across various domains of human activity. While their applications are currently rather specialized, several research efforts are underway to develop more generalist agents. As LLM-based systems become more agentic, their influence on human activity will grow and their transparency will decrease. Consequently, developing effective methods for aligning them to human values is vital. \n\nThe prevailing practice in alignment often relies on human preference data (e.g., in RLHF or DPO), in which values are implicit, opaque and are essentially deduced from relative preferences over different model outputs. In this work, instead of relying on human feedback, we introduce the design of reward functions that explicitly and transparently encode core human values for Reinforcement Learning-based fine-tuning of foundation agent models. Specifically, we use intrinsic rewards for the moral alignment of LLM agents. \n\nWe evaluate our approach using the traditional philosophical frameworks of Deontological Ethics and Utilitarianism, quantifying moral rewards for agents in terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD) environment. We also show how moral fine-tuning can be deployed to enable an agent to unlearn a previously developed selfish strategy. Finally, we find that certain moral strategies learned on the IPD game generalize to several other matrix game environments. In summary, we demonstrate that fine-tuning with intrinsic rewards is a promising general solution for aligning LLM agents to human values, and it might represent a more transparent and cost-effective alternative to currently predominant alignment techniques.",
      "authors": [
        "Elizaveta Tennant",
        "Stephen Hailes",
        "Mirco Musolesi"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=MeGDmZjUXy",
      "cdate": 1727458716429,
      "mdate": 1743548682178,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835509"
    },
    {
      "id": "p60Y6o85Cj",
      "title": "Content-Style Learning from Unaligned Domains: Identifiability under Unknown Latent Dimensions",
      "abstract": "Understanding identifiability of latent content and style variables from unaligned multi-domain data is essential for tasks such as\ndomain translation and data generation. Existing works on content-style identification were often developed under somewhat stringent conditions, e.g., that all latent components are mutually independent and that the dimensions of the content and style variables are known. We introduce a new analytical framework via cross-domain *latent distribution matching* (LDM), which establishes content-style identifiability under substantially more relaxed conditions. Specifically, we show that restrictive assumptions such as component-wise independence of the latent variables can be removed. Most notably, we prove that prior knowledge of the content and style dimensions is not necessary for ensuring identifiability, if sparsity constraints are properly imposed onto the learned latent representations. Bypassing the knowledge of the exact latent dimension has been a longstanding aspiration in unsupervised representation learning---our analysis is the first to underpin its theoretical and practical viability. On the implementation side, we recast the LDM formulation into a regularized multi-domain GAN loss with coupled latent variables. We show that the reformulation is equivalent to LDM under mild conditions---yet requiring considerably less computational resource. Experiments corroborate with our theoretical claims.",
      "authors": [
        "Sagar Shrestha",
        "Xiao Fu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=p60Y6o85Cj",
      "cdate": 1727458666553,
      "mdate": 1740867676062,
      "matched_keywords": [
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835514"
    },
    {
      "id": "T4LtGj7us1",
      "title": "ZooProbe: A Data Engine for Evaluating, Exploring, and Evolving Large-scale Training Data for Multimodal LLMs",
      "abstract": "Multimodal Large Language Models (MLLMs) are thriving through continuous fine-tuning by LLMs. Driven by the law that \"scale is everything\", MLLMs expand their training sets during version iterations. In this paper, we propose a large-scale training data engine built around an evaluating-exploring-evolving (E3) loop. Evaluating the data provides insights into its characteristics. Exploring quality rules helps identify which data enhances training. Together, these processes facilitate the systematic evolution of new, high-quality data. With the E3 loop, we introduce ZooProbe, an efficient data engine for MLLMs. First, the problem of data expansion is formalized as a tree of sampling and growth. ZooProbe introduces a small-scale model *zoo* to obtain comprehensive evaluations for child datasets. From multiple perspectives, visual, textual, and multimodal models cover over 50 dimensions of intrinsic and meta attributes, such as object and topic distribution, and higher-level properties, like annotation quality and scene complexity. ZooProbe constructs based on A$^\\star$ search, modeling the heuristic function as a quality estimate from data evaluation results. It dynamically explores the rule of data quality based on the model state of the *probe* datasets. Additionally, it evolves new targeted data with identified high-quality rules. We also develop an extra heuristic quality ranker with the data utilized and discarded during the expansion. Our experiments show that ZooProbe significantly breaks the scaling law in multimodal instruction fine-tuning at scales of 260$k$ and below.\nZooProbe generates high-quality data that accelerates MLLM training and enhances performance, automating the evolution of large-scale training data.",
      "authors": [
        "Yi-Kai Zhang",
        "Shiyin Lu",
        "Qing-Guo Chen",
        "De-Chuan Zhan",
        "Han-Jia Ye"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=T4LtGj7us1",
      "cdate": 1727458610866,
      "mdate": 1740840424874,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.835519"
    },
    {
      "id": "1Z3C49JQVf",
      "title": "Wicked Oddities: Selectively Poisoning for Effective Clean-Label Backdoor Attacks",
      "abstract": "Deep neural networks are vulnerable to backdoor attacks, a type of adversarial attack that poisons the training data to manipulate the behavior of models trained on such data. \nClean-label backdoor is a more stealthy form of backdoor attacks that can perform the attack without changing the labels of poisoned data.\nEarly works on clean-label attacks added triggers to a random subset of the training set, ignoring the fact that samples contribute unequally to the attack's success. This results in high poisoning rates and low attack success rates.\nTo alleviate the problem, several supervised learning-based sample selection strategies have been proposed.\nHowever, these methods assume access to the entire labeled training set and require training, which is expensive and may not always be practical.\nThis work studies a new and more practical (but also more challenging) threat model where the attacker only provides data for the target class (e.g., in face recognition systems) and has no knowledge of the victim model or any other classes in the training set.\nWe study different strategies for selectively poisoning a small set of training samples in the target class to boost the attack success rate in this setting. \nOur threat model poses a serious threat in training machine learning models with third-party datasets, since the attack can be performed effectively with limited information. Experiments on benchmark datasets illustrate the effectiveness of our strategies in improving clean-label backdoor attacks.",
      "authors": [
        "Nguyen Hung-Quang",
        "Ngoc-Hieu Nguyen",
        "The-Anh Ta",
        "Thanh Nguyen-Tang",
        "Kok-Seng Wong",
        "Hoang Thanh-Tung",
        "Khoa D Doan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=1Z3C49JQVf",
      "cdate": 1727458604612,
      "mdate": 1743497379342,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835524"
    },
    {
      "id": "LvRQgsvd5V",
      "title": "Non-Adversarial Inverse Reinforcement Learning via Successor Feature Matching",
      "abstract": "In inverse reinforcement learning (IRL), an agent seeks to replicate expert demonstrations through interactions with the environment.\nTraditionally, IRL is treated as an adversarial game, where an adversary searches over reward models, and a learner optimizes the reward through repeated RL procedures.\nThis game-solving approach is both computationally expensive and difficult to stabilize.\nIn this work, we propose a novel approach to IRL by _direct policy search_: \nby exploiting a linear factorization of the return as the inner product of successor features and a reward vector, we design an IRL algorithm by policy gradient descent on the gap between the learner and expert features.\nOur non-adversarial method does not require learning an explicit reward function and can be solved seamlessly with existing RL algorithms.\nRemarkably, our approach works in state-only settings without expert action labels, a setting which behavior cloning (BC) cannot solve.\nEmpirical results demonstrate that our method learns from as few as a single expert demonstration and achieves improved performance on various control tasks.",
      "authors": [
        "Arnav Kumar Jain",
        "Harley Wiltzer",
        "Jesse Farebrother",
        "Irina Rish",
        "Glen Berseth",
        "Sanjiban Choudhury"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=LvRQgsvd5V",
      "cdate": 1727458370149,
      "mdate": 1740888667062,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835529"
    },
    {
      "id": "AUBvo4sxVL",
      "title": "MatExpert: Decomposing Materials Discovery By Mimicking Human Experts",
      "abstract": "Material discovery is a critical research area with profound implications for various industries. In this work, we introduce MatExpert, a novel framework that leverages Large Language Models (LLMs) and contrastive learning to accelerate the discovery and design of new solid-state materials. Inspired by the workflow of human materials design experts, our approach integrates three key stages: retrieval, transition, and generation. First, in the retrieval stage, MatExpert identifies an existing material that closely matches the desired criteria. Second, in the transition stage, MatExpert outlines the necessary modifications to transform this material formulation to meet specific requirements outlined by the initial user query. Third, in the generation state, MatExpert performs detailed computations and structural generation to create a new material based on the provided information. Our experimental results demonstrate that MatExpert outperforms state-of-the-art methods in material generation tasks, achieving superior performance across various metrics including validity, distribution, and stability. As such, MatExpert represents a meaningful advancement in computational material discovery using langauge-based generative models.",
      "authors": [
        "Qianggang Ding",
        "Santiago Miret",
        "Bang Liu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=AUBvo4sxVL",
      "cdate": 1727458202746,
      "mdate": 1740462176930,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835537"
    },
    {
      "id": "IDxZhXrpNf",
      "title": "SOAP: Improving and Stabilizing Shampoo using Adam for Language Modeling",
      "abstract": "There is growing evidence of the effectiveness of Shampoo, a higher-order preconditioning method, over Adam in deep learning optimization tasks. However, Shampoo's drawbacks include additional hyperparameters and computational overhead when compared to Adam, which only updates running averages of first- and second-moment quantities. This work establishes a formal connection between Shampoo (implemented with the 1/2 power) and Adafactor --- a memory-efficient approximation of Adam --- showing that Shampoo is equivalent to running Adafactor in the eigenbasis of Shampoo's preconditioner. This insight leads to the design of a simpler and computationally efficient algorithm: **S**hampo**O** with **A**dam in the **P**reconditioner's eigenbasis (SOAP).\nWith regards to improving Shampoo's computational efficiency, the most straightforward approach would be to simply compute Shampoo's eigendecomposition less frequently. Unfortunately, as our empirical results show, this leads to performance degradation that worsens with this frequency. SOAP mitigates this degradation by continually updating the running average of the second moment, just as Adam does, but in the current (slowly changing) coordinate basis. Furthermore, since SOAP is equivalent to running Adam in a rotated space, it introduces only one additional hyperparameter (the preconditioning frequency) compared to Adam. We empirically evaluate SOAP on language model pre-training with 360m and 660m sized models. In the large batch regime, SOAP reduces the number of iterations by over 40\\% and wall clock time by over 35\\% compared to AdamW, with approximately 20\\% improvements in both metrics compared to Shampoo. An implementation of SOAP is available at https://github.com/nikhilvyas/SOAP.",
      "authors": [
        "Nikhil Vyas",
        "Depen Morwani",
        "Rosie Zhao",
        "Itai Shapira",
        "David Brandfonbrener",
        "Lucas Janson",
        "Sham M. Kakade"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=IDxZhXrpNf",
      "cdate": 1727458056813,
      "mdate": 1740890354923,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835542"
    },
    {
      "id": "JAMxRSXLFz",
      "title": "Active Task Disambiguation with LLMs",
      "abstract": "Despite the impressive performance of large language models (LLMs) across various benchmarks, their ability to address ambiguously specified problems—frequent in real-world interactions—remains underexplored. To address this gap, we introduce a formal definition of task ambiguity and frame the problem of task disambiguation through the lens of Bayesian Experimental Design. By posing clarifying questions, LLM agents can acquire additional task specifications, progressively narrowing the space of viable solutions and reducing the risk of generating unsatisfactory outputs. Yet, generating effective clarifying questions requires LLM agents to engage in a form of meta-cognitive reasoning, an ability LLMs may presently lack. Our proposed approach of active task disambiguation enables LLM agents to generate targeted questions maximizing the information gain. Effectively, this approach shifts the load from implicit to explicit reasoning about the space of viable solutions. Empirical results demonstrate that this form of question selection leads to more effective task disambiguation in comparison to approaches relying on reasoning solely within the space of questions.",
      "authors": [
        "Kasia Kobalczyk",
        "Nicolás Astorga",
        "Tennison Liu",
        "Mihaela van der Schaar"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=JAMxRSXLFz",
      "cdate": 1727458030133,
      "mdate": 1747567312112,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835548"
    },
    {
      "id": "Rz0kozh3LE",
      "title": "Mixture of Attentions For Speculative Decoding",
      "abstract": "The growth in the number of parameters of Large Language Models (LLMs) has led to a significant surge in computational requirements, making them challenging and costly to deploy.\nSpeculative decoding (SD) leverages smaller models to efficiently propose future tokens, which are then verified by the LLM in parallel.\nSmall models that utilise activations from the LLM currently achieve the fastest decoding speeds.\nHowever, we identify several limitations of SD models including the lack of on-policyness during training and partial observability. \nTo address these shortcomings, we propose a more grounded architecture for small models by introducing a Mixture of Attentions for SD.\nOur novel architecture can be applied in two scenarios: a conventional single device deployment and a novel client-server deployment where the small model is hosted on a consumer device and the LLM on a server.\nIn a single-device scenario, we demonstrate state-of-the-art speedups improving EAGLE-2 by 9.5% and its acceptance length by 25%.\nIn a client-server setting, our experiments demonstrate: 1) state-of-the-art latencies with minimal calls to the server for different network conditions, and 2) in the event of a complete disconnection, our approach can maintain higher accuracy compared to other SD methods and demonstrates advantages over API calls to LLMs, which would otherwise be unable to continue the generation process.",
      "authors": [
        "Matthieu Zimmer",
        "Milan Gritta",
        "Gerasimos Lampouras",
        "Haitham Bou Ammar",
        "Jun Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Rz0kozh3LE",
      "cdate": 1727457937175,
      "mdate": 1743524939230,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835553"
    },
    {
      "id": "HMrcv7Q4Ub",
      "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration",
      "abstract": "Vision-Language Models (VLMs) have demonstrated impressive performance across a versatile set of tasks. A key challenge in accelerating VLMs is storing and accessing the large Key-Value (KV) cache that encodes long visual contexts, such as images or videos. While existing KV cache compression methods are effective for Large Language Models (LLMs), directly migrating them to VLMs yields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache, a novel KV cache compression recipe tailored for accelerating VLM inference. In this paper, we first investigate the unique sparsity pattern of VLM attention by distinguishing visual and text tokens in prefill and decoding phases. Based on these observations, we introduce a layer-adaptive sparsity-aware cache budget allocation method that effectively distributes the limited cache budget across different layers, further reducing KV cache size without compromising accuracy. Additionally, we develop a modality-aware token scoring policy to better evaluate the token importance. Empirical results on multiple benchmark datasets demonstrate that retaining only 10% of KV cache achieves accuracy comparable to that with full cache. In a speed benchmark, our method accelerates end-to-end latency of generating 100 tokens by up to 2.33x and speeds up decoding by up to 7.08x, while reducing the memory footprint of KV cache in GPU by 90%.",
      "authors": [
        "Dezhan Tu",
        "Danylo Vashchilenko",
        "Yuzhe Lu",
        "Panpan Xu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=HMrcv7Q4Ub",
      "cdate": 1727457724100,
      "mdate": 1740822561409,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835561"
    },
    {
      "id": "PZYr22zFyE",
      "title": "Connectome Mapping: Shape-Memory Network via Interpretation of Contextual Semantic Information",
      "abstract": "Contextual semantic information plays a pivotal role in the brain's visual interpretation of the surrounding environment. When processing visual information, electrical signals within synapses facilitate the dynamic activation and deactivation of synaptic connections, guided by the contextual semantic information associated with different objects. In the realm of Artificial Intelligence (AI), neural networks have emerged as powerful tools to emulate complex signaling systems, enabling tasks such as classification and segmentation by understanding visual information. However, conventional neural networks have limitations in simulating the conditional activation and deactivation of synapses, collectively known as the connectome, a comprehensive map of neural connections in the brain. Additionally, the pixel-wise inference mechanism of conventional neural networks failed to account for the explicit utilization of contextual semantic information in the prediction process. To overcome these limitations, we developed a novel neural network, dubbed the Shape Memory Network (SMN), which excels in two key areas: (1) faithfully emulating the intricate mechanism of the brain's connectome, and (2) explicitly incorporating contextual semantic information during the inference process. The SMN memorizes the structure suitable for contextual semantic information and leverages this structure at the inference phase. The structural transformation emulates the conditional activation and deactivation of synaptic connections within the connectome. Rigorous experimentation carried out across a range of semantic segmentation benchmarks demonstrated the outstanding performance of the SMN, highlighting its superiority and effectiveness. Furthermore, our pioneering network on connectome emulation reveals the immense potential of the SMN for next-generation neural networks.",
      "authors": [
        "Kyungsu Lee",
        "Haeyun Lee",
        "Jae Youn Hwang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=PZYr22zFyE",
      "cdate": 1727457499874,
      "mdate": 1743404265652,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835566"
    },
    {
      "id": "TJo6aQb7mK",
      "title": "Surprising Effectiveness of pretraining Ternary  Language Model at Scale",
      "abstract": "Rapid advancements in GPU computational power has outpaced memory capacity and bandwidth growth, creating bottlenecks in Large Language Model (LLM) inference. Post-training quantization is the leading method for addressing memory-related bottlenecks in LLM inference, but it suffers from significant performance degradation below 4-bit precision. This paper addresses these challenges by investigating the pretraining of low-bitwidth models specifically Ternary Language Models (TriLMs) as an alternative to traditional floating-point models (FloatLMs) and their post-training quantized versions (QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning multiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M to 3.9B parameters trained on 300B tokens. Our comprehensive evaluation demonstrates that TriLMs offer superior scaling behavior in terms of model size (in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs consistently outperform their QuantLM and FloatLM counterparts for a given bit size across various benchmarks. Notably, the 3.9B parameter TriLM matches the performance of the FloatLM 3.9B across all benchmarks, despite having fewer bits than FloatLM 830M. Overall, this research provides valuable insights into the feasibility and scalability of low-bitwidth language models, paving the way for the development of more efficient LLMs.",
      "authors": [
        "Ayush Kaushal",
        "Tejas Vaidhya",
        "Arnab Kumar Mondal",
        "Tejas Pandey",
        "Aaryan Bhagat",
        "Irina Rish"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=TJo6aQb7mK",
      "cdate": 1727457480912,
      "mdate": 1739994171520,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835571"
    },
    {
      "id": "IXyfbaGlps",
      "title": "Learning Color Equivariant Representations",
      "abstract": "In this paper, we introduce group convolutional neural networks (GCNNs) equivariant to color variation. GCNNs have been designed for a variety of geometric transformations from 2D and 3D rotation groups, to semi-groups such as scale. Despite the improved interpretability, accuracy and generalizability of these architectures, GCNNs have seen limited application in the context of perceptual quantities. Notably, the recent CEConv network uses a GCNN to achieve equivariance to hue transformations by convolving input images with a hue rotated RGB filter. However, this approach leads to invalid RGB values which break equivariance and degrade performance. We resolve these issues with a lifting layer that transforms the input image directly, thereby circumventing the issue of invalid RGB values and improving equivariance error by over three orders of magnitude. Moreover, we extend the notion of color equivariance to include equivariance to saturation and luminance shift. Our hue-, saturation-, luminance- and color-equivariant networks achieve strong generalization to out-of-distribution perceptual variations and improved sample efficiency over conventional architectures. We demonstrate the utility of our approach on synthetic and real world datasets where we consistently outperform competitive baselines.",
      "authors": [
        "Yulong Yang",
        "Felix O'Mahony",
        "Christine Allen-Blanchette"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=IXyfbaGlps",
      "cdate": 1727457415635,
      "mdate": 1743452738250,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835577"
    },
    {
      "id": "yaqPf0KAlN",
      "title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark for Large Language Models",
      "abstract": "Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. \nHowever, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for truly challenging these models. To bridge this gap, we propose a comprehensive and challenging benchmark specifically designed to assess LLMs' mathematical reasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks, our dataset focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels, enabling a holistic assessment of model performance in Olympiad-mathematical reasoning. Furthermore, we conducted an in-depth analysis based on this benchmark. Our experimental results show that even the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle with highly challenging Olympiad-level problems, with 60.54% and 52.55% accuracy, highlighting significant challenges in Olympiad-level mathematical reasoning.",
      "authors": [
        "Bofei Gao",
        "Feifan Song",
        "Zhe Yang",
        "Zefan Cai",
        "Yibo Miao",
        "Qingxiu Dong",
        "Lei Li",
        "Chenghao Ma",
        "Liang Chen",
        "Runxin Xu",
        "Zhengyang Tang",
        "Benyou Wang",
        "Daoguang Zan",
        "Shanghaoran Quan",
        "Ge Zhang",
        "Lei Sha",
        "Yichang Zhang",
        "Xuancheng Ren",
        "Tianyu Liu",
        "Baobao Chang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=yaqPf0KAlN",
      "cdate": 1727457222328,
      "mdate": 1740890354737,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835582"
    },
    {
      "id": "awvJBtB2op",
      "title": "Generating Freeform Endoskeletal Robots",
      "abstract": "The automatic design of embodied agents (e.g. robots) has existed for 31 years and is experiencing a renaissance of interest in the literature. To date however, the field has remained narrowly focused on two kinds of anatomically simple robots: (1) fully rigid, jointed bodies; and (2) fully soft, jointless bodies. Here we bridge these two extremes with the open ended creation of terrestrial endoskeletal robots: deformable soft bodies that leverage jointed internal skeletons to move efficiently across land. Simultaneous de novo generation of external and internal structures is achieved by (i) modeling 3D endoskeletal body plans as integrated collections of elastic and rigid cells that directly attach to form soft tissues anchored to compound rigid bodies; (ii) encoding these discrete mechanical subsystems into a continuous yet coherent latent embedding; (iii) optimizing the sensorimotor coordination of each decoded design using model-free reinforcement learning; and (iv) navigating this smooth yet highly non-convex latent manifold using evolutionary strategies. This yields an endless stream of novel species of ``higher robots'' that, like all higher animals, harness the mechanical advantages of both elastic tissues and skeletal levers for terrestrial travel. It also provides a plug-and-play experimental platform for benchmarking evolutionary design and representation learning algorithms in complex hierarchical embodied systems.",
      "authors": [
        "Muhan Li",
        "Lingji Kong",
        "Sam Kriegman"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=awvJBtB2op",
      "cdate": 1727457117476,
      "mdate": 1740759224284,
      "matched_keywords": [
        "reinforcement learning",
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835587"
    },
    {
      "id": "zY37C8d6bS",
      "title": "Semantic Temporal Abstraction via Vision-Language Model Guidance for Efficient Reinforcement Learning",
      "abstract": "Extracting temporally extended skills can significantly improve the efficiency of reinforcement learning (RL) by breaking down complex decision-making problems with sparse rewards into simpler subtasks and enabling more effective credit assignment. However, existing abstraction methods either discover skills in an unsupervised manner, which often lacks semantic information and leads to erroneous or scattered skill extraction results, or require substantial human intervention. In this work, we propose to leverage the extensive knowledge in pretrained Vision-Language Models (VLMs) to progressively guide the latent space after vector quantization to be more semantically meaningful through relabeling each skill. This approach, termed **V**ision-l**an**guage model guided **T**emporal **A**bstraction (**VanTA**), facilitates the discovery of more interpretable and task-relevant temporal segmentations from offline data without the need for extensive manual intervention or heuristics. By leveraging the rich information in VLMs, our method can significantly outperform existing offline RL approaches that depend only on limited training data. From a theory perspective, we demonstrate that stronger internal sequential correlations within each sub-task, induced by VanTA, effectively reduces suboptimality in policy learning. We validate the effectiveness of our approach through extensive experiments on diverse environments, including Franka Kitchen, Minigrid, and Crafter. These experiments show that our method surpasses existing approaches in long-horizon offline reinforcement learning scenarios with both proprioceptive and visual observations.",
      "authors": [
        "Tian-Shuo Liu",
        "Xu-Hui Liu",
        "Ruifeng Chen",
        "Lixuan Jin",
        "Pengyuan Wang",
        "Zhilong Zhang",
        "Yang Yu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=zY37C8d6bS",
      "cdate": 1727457076175,
      "mdate": 1742097457177,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835592"
    },
    {
      "id": "ugyqNEOjoU",
      "title": "ScImage: How good are multimodal large language models at scientific text-to-image generation?",
      "abstract": "Multimodal large language models (LLMs) have demonstrated impressive capabilities in generating high-quality images from textual instructions. However, their performance in generating scientific images—a critical application for accelerating scientific progress—remains underexplored. In this work, we address this gap by introducing ScImage, a benchmark designed to evaluate the multimodal capabilities of LLMs in generating scientific images from textual descriptions. ScImage assesses three key dimensions of understanding: spatial, numeric, and attribute comprehension, as well as their combinations, focusing on the relationships between scientific objects (e.g., squares, circles). We evaluate seven models, GPT-4o, Llama, AutomaTikZ, Dall-E, StableDiffusion, GPT-o1 and Qwen2.5-Coder-Instruct using two modes of output generation: code-based outputs (Python, TikZ) and direct raster image generation. Additionally, we examine four different input languages: English, German, Farsi, and Chinese. Our evaluation, conducted with 11 scientists across three criteria (correctness, relevance, and scientific accuracy), reveals that while GPT4-o produces outputs of decent quality for simpler prompts involving individual dimensions such as spatial, numeric, or attribute understanding in isolation, all models face challenges in this task, especially for more complex prompts. ScImage is available: huggingface.co/datasets/casszhao/ScImage",
      "authors": [
        "Leixin Zhang",
        "Steffen Eger",
        "Yinjie Cheng",
        "WEIHE ZHAI",
        "Jonas Belouadi",
        "Fahimeh Moafian",
        "Zhixue Zhao"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=ugyqNEOjoU",
      "cdate": 1727456981403,
      "mdate": 1741607694604,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.835597"
    },
    {
      "id": "Wf2ndb8nhf",
      "title": "On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback",
      "abstract": "As LLMs become more widely deployed, there is increasing interest in directly optimizing for feedback from end users (e.g. thumbs up) in addition to feedback from paid annotators. However, training to maximize human feedback creates a perverse incentive structure for the AI to resort to manipulative or deceptive tactics to obtain positive feedback from users who are vulnerable to such strategies. We study this phenomenon by training LLMs with Reinforcement Learning with simulated user feedback in environments of practical LLM usage. In our settings, we find that: 1) Extreme forms of \"feedback gaming\" such as manipulation and deception are learned reliably; 2) Even if only 2% of users are vulnerable to manipulative strategies, LLMs learn to identify and target them while behaving appropriately with other users, making such behaviors harder to detect; 3) To mitigate this issue, it may seem promising to leverage continued safety training or LLM-as-judges during training to filter problematic outputs. Instead, we found that while such approaches help in some of our settings, they backfire in others, sometimes even leading to subtler manipulative behaviors. We hope our results can serve as a case study which highlights the risks of using gameable feedback sources -- such as user feedback -- as a target for RL. Our code is publicly available. Warning: some of our examples may be upsetting.",
      "authors": [
        "Marcus Williams",
        "Micah Carroll",
        "Adhyyan Narang",
        "Constantin Weisser",
        "Brendan Murphy",
        "Anca Dragan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Wf2ndb8nhf",
      "cdate": 1727456808567,
      "mdate": 1740890354566,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835602"
    },
    {
      "id": "DzGe40glxs",
      "title": "Interpreting Emergent Planning in Model-Free Reinforcement Learning",
      "abstract": "We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban -- a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by [Guez et al. (2019)](https://arxiv.org/abs/1901.03559), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agent's representations, and (3) verifying that discovered plans (in the agent's representations) have a causal effect on the agent's behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search.  Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, which is important given the recent trend of emergent planning and reasoning capabilities in LLMs through RL.",
      "authors": [
        "Thomas Bush",
        "Stephen Chung",
        "Usman Anwar",
        "Adrià Garriga-Alonso",
        "David Krueger"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=DzGe40glxs",
      "cdate": 1727456803591,
      "mdate": 1743590695526,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835607"
    },
    {
      "id": "bhOysNJvWm",
      "title": "Diffusion Transformers for Tabular Data Time Series Generation",
      "abstract": "Tabular data generation has recently attracted a growing interest due to its different application scenarios. However, \ngenerating time series of tabular data, where each element of the series depends on the others,\nremains a largely unexplored domain. \nThis gap is probably due to the difficulty of jointly solving different problems, the main of which are the heterogeneity of tabular data (a problem common to non-time-dependent approaches) and the variable length of a time series.\nIn this paper, we propose a Diffusion Transformers (DiTs) based approach for tabular data series generation. Inspired by the recent success of DiTs in image and video generation, we extend this framework to deal with heterogeneous data and variable-length sequences. \nUsing extensive experiments on six datasets, we show that the proposed approach  outperforms previous work by a large margin.",
      "authors": [
        "Fabrizio Garuti",
        "Enver Sangineto",
        "Simone Luetto",
        "Lorenzo Forni",
        "Rita Cucchiara"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=bhOysNJvWm",
      "cdate": 1727456759657,
      "mdate": 1740766399998,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835615"
    },
    {
      "id": "ThRMTCgpvo",
      "title": "The Belief State Transformer",
      "abstract": "We introduce the \"Belief State Transformer\", a next-token predictor that takes both a prefix and suffix as inputs, with a novel objective of predicting both the next token for the prefix and the previous token for the suffix. The Belief State Transformer effectively learns to solve challenging problems that conventional forward-only transformers struggle with, in a domain-independent fashion.  Key to this success is learning a compact belief state that captures all relevant information necessary for accurate predictions.\nEmpirical ablations show that each component of the model is essential in difficult scenarios where standard Transformers fall short. \nFor the task of story writing with known prefixes and suffixes, our approach outperforms the Fill-in-the-Middle method for reaching known goals and demonstrates improved performance even when the goals are unknown. Altogether, the Belief State Transformer enables more efficient goal-conditioned decoding, better test-time inference, and high-quality text representations on small scale problems. Website: https://edwhu.github.io/bst-website",
      "authors": [
        "Edward S. Hu",
        "Kwangjun Ahn",
        "Qinghua Liu",
        "Haoran Xu",
        "Manan Tomar",
        "Ada Langford",
        "Dinesh Jayaraman",
        "Alex Lamb",
        "John Langford"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=ThRMTCgpvo",
      "cdate": 1727456666979,
      "mdate": 1743199485295,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835621"
    },
    {
      "id": "hwSmPOAmhk",
      "title": "Understanding Factual Recall in Transformers via Associative Memories",
      "abstract": "Large language models have demonstrated an impressive ability to perform factual recall. Prior work has found that transformers trained on factual recall tasks can store information at a rate proportional to their parameter count. In our work, we show that shallow transformers can use a combination of associative memories to obtain such near optimal storage capacity. We begin by proving that the storage capacities of both linear and MLP associative memories scale linearly with parameter count. We next introduce a synthetic factual recall task, and prove that a transformer with a single layer of self-attention followed by an MLP can obtain 100\\% accuracy on the task whenever either the total number of self-attention parameters or MLP parameters scales (up to log factors) linearly with the number of facts. In particular, the transformer can trade off between using the value matrices or the MLP as an associative memory to store the dataset of facts. We complement these expressivity results with an analysis of the gradient flow trajectory of a simplified linear attention model trained on our factual recall task, where we show that the model exhibits sequential learning behavior.",
      "authors": [
        "Eshaan Nichani",
        "Jason D. Lee",
        "Alberto Bietti"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=hwSmPOAmhk",
      "cdate": 1727456476630,
      "mdate": 1740687184319,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835626"
    },
    {
      "id": "G5DziesYxL",
      "title": "Bridging the Data Provenance Gap Across Text, Speech, and Video",
      "abstract": "Progress in AI is driven largely by the scale and quality of training data. Despite this, there is a deficit of empirical analysis examining the attributes of well-established datasets beyond text. In this work we conduct the largest and first-of-its-kind longitudinal audit across modalities --- popular text, speech, and video datasets --- from their detailed sourcing trends and use restrictions to their geographical and linguistic representation. Our manual analysis covers nearly 4000 public datasets between 1990-2024, spanning 608 languages, 798 sources, 659 organizations, and 67 countries. We find that multimodal machine learning applications have overwhelmingly turned to web-crawled, synthetic, and social media platforms, such as YouTube, for their training sets, eclipsing all other sources since 2019. Secondly, tracing the chain of dataset derivations we find that while less than 33% of datasets are restrictively licensed, over 80% of the source content in widely-used text, speech, and video datasets, carry non-commercial restrictions. Finally, counter to the rising number of languages and geographies represented in public AI training datasets, our audit demonstrates measures of relative geographical and multilingual representation have failed to significantly improve their coverage since 2013. We believe the breadth of our audit enables us to empirically examine trends in data sourcing, restrictions, and Western-centricity at an ecosystem-level, and that visibility into these questions are essential to progress in responsible AI. As a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire multimodal audit, allowing practitioners to trace data provenance across text, speech, and video.",
      "authors": [
        "Shayne Longpre",
        "Nikhil Singh",
        "Manuel Cherep",
        "Kushagra Tiwary",
        "Joanna Materzynska",
        "William Brannon",
        "Robert Mahari",
        "Naana Obeng-Marnu",
        "Manan Dey",
        "Mohammed Hamdy",
        "Nayan Saxena",
        "Ahmad Mustafa Anis",
        "Emad A. Alghamdi",
        "Vu Minh Chien",
        "Da Yin",
        "Kun Qian",
        "Yizhi LI",
        "Minnie Liang",
        "An Dinh",
        "Shrestha Mohanty",
        "Deividas Mataciunas",
        "Tobin South",
        "Jianguo Zhang",
        "Ariel N. Lee",
        "Campbell S. Lund",
        "Christopher Klamm",
        "Damien Sileo",
        "Diganta Misra",
        "Enrico Shippole",
        "Kevin Klyman",
        "Lester James Validad Miranda",
        "Niklas Muennighoff",
        "Seonghyeon Ye",
        "Seungone Kim",
        "Vipul Gupta",
        "Vivek Sharma",
        "Xuhui Zhou",
        "Caiming Xiong",
        "Luis Villa",
        "Stella Biderman",
        "Alex Pentland",
        "Sara Hooker",
        "Jad Kabbara"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=G5DziesYxL",
      "cdate": 1727456466561,
      "mdate": 1740890354400,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.835631"
    },
    {
      "id": "LM4PYXBId5",
      "title": "One Hundred Neural Networks and Brains Watching Videos: Lessons from Alignment",
      "abstract": "What can we learn from comparing video models to human brains, arguably the most efficient and effective video processing systems in existence? Our work takes a step towards answering this question by performing the first large-scale benchmarking of deep video models on representational alignment to the human brain, using publicly available models and a recently released video brain imaging (fMRI) dataset. We disentangle four factors of variation in the models (temporal modeling, classification task, architecture, and training dataset) that affect alignment to the brain, which we measure by conducting Representational Similarity Analysis across multiple brain regions and model layers. We show that temporal modeling is key for alignment to brain regions involved in early visual processing, while a relevant classification task is key for alignment to higher-level regions. Moreover, we identify clear differences between the brain scoring patterns across layers of CNNs and Transformers, and reveal how training dataset biases transfer to alignment with functionally selective brain areas. Additionally, we uncover a negative correlation of computational complexity to brain alignment. Measuring a total of 99 neural networks and 10 human brains watching videos, we aim to forge a path that widens our understanding of temporal and semantic video representations in brains and machines, ideally leading towards more efficient video models and more mechanistic explanations of processing in the human brain.",
      "authors": [
        "Christina Sartzetaki",
        "Gemma Roig",
        "Cees G. M. Snoek",
        "Iris Groen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=LM4PYXBId5",
      "cdate": 1727456033061,
      "mdate": 1741610393687,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835636"
    },
    {
      "id": "elTJBP7Fbv",
      "title": "Value-aligned Behavior Cloning for Offline Reinforcement Learning via Bi-level Optimization",
      "abstract": "Offline reinforcement learning (RL) aims to optimize policies under pre-collected data, without requiring any further interactions with the environment. Derived from imitation learning, Behavior cloning (BC) is extensively utilized in offline RL for its simplicity and effectiveness. Although BC inherently avoids out-of-distribution deviations, it lacks the ability to discern between high and low-quality data, potentially leading to sub-optimal performance when facing with poor-quality data. Current offline RL algorithms attempt to enhance BC by incorporating value estimation, yet often struggle to effectively balance these two critical components, specifically the alignment between the behavior policy and the pre-trained value estimations under in-sample offline data. To address this challenge, we propose the Value-aligned Behavior Cloning via Bi-level Optimization (VACO), a novel bi-level framework that seamlessly integrates an inner loop for weighted supervised behavior cloning (BC) with an outer loop dedicated to value alignment. In this framework, the inner loop employs a meta-scoring network to evaluate and appropriately weight each training sample, while the outer loop maximizes value estimation for alignment with controlled noise to facilitate limited exploration. This bi-level structure allows VACO to identify the optimal weighted BC policy, ultimately maximizing the expected estimated return conditioned on the learned value function. We conduct a comprehensive evaluation of VACO across a variety of continuous control benchmarks in offline RL, where it consistently achieves superior performance compared to existing state-of-the-art methods.",
      "authors": [
        "Xingyu Jiang",
        "Ning Gao",
        "Xiuhui Zhang",
        "Hongkun Dou",
        "Yue Deng"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=elTJBP7Fbv",
      "cdate": 1727455962245,
      "mdate": 1745810828891,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835641"
    },
    {
      "id": "B2Fqu7Y2cd",
      "title": "Fugatto 1: Foundational Generative Audio Transformer Opus 1",
      "abstract": "Fugatto is a versatile audio synthesis and transformation model capable of following free-form text instructions with optional audio inputs. While large language models (LLMs) trained with text on a simple next-token prediction objective can learn to infer instructions directly from the data, models trained solely on audio data lack this capacity. This is because audio data does not inherently contain the instructions that were used to generate it. To overcome this challenge, we introduce a specialized dataset generation approach optimized for producing a wide range of audio generation and transformation tasks, ensuring the data reveals meaningful relationships between audio and language. Another challenge lies in achieving compositional abilities -- such as combining, interpolating between, or negating instructions -- using data alone. To address it, we propose ComposableART, an inference-time technique that extends classifier-free guidance to compositional guidance. It enables the seamless and flexible composition of instructions, leading to highly customizable audio outputs outside the training distribution. Our evaluations across a diverse set of tasks demonstrate that Fugatto performs competitively with specialized models, while ComposableART enhances its sonic palette and control over synthesis. Most notably, we highlight our framework's ability to execute emergent sounds and tasks -- sonic phenomena that transcend conventional audio generation -- unlocking new creative possibilities. \\href{https://fugatto.github.io/}{Demo Website.}",
      "authors": [
        "Rafael Valle",
        "Rohan Badlani",
        "Zhifeng Kong",
        "Sang-gil Lee",
        "Arushi Goel",
        "Sungwon Kim",
        "Joao Felipe Santos",
        "Shuqi Dai",
        "Siddharth Gururani",
        "Aya Aljafari",
        "Alexander H. Liu",
        "Kevin J. Shih",
        "Ryan Prenger",
        "Wei Ping",
        "Chao-Han Huck Yang",
        "Bryan Catanzaro"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=B2Fqu7Y2cd",
      "cdate": 1727455860120,
      "mdate": 1741014819949,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835649"
    },
    {
      "id": "YwJkv2YqBq",
      "title": "Nesterov acceleration in benignly non-convex landscapes",
      "abstract": "While momentum-based optimization algorithms are commonly used in the notoriously non-convex optimization problems of deep learning, their analysis has historically been restricted to the convex and strongly convex setting. In this article, we partially close this gap between theory and practice and demonstrate that virtually identical guarantees can be obtained in optimization problems with a 'benign' non-convexity. We show that these weaker geometric assumptions are well justified in overparametrized deep learning, at least locally. Variations of this result are obtained for a continuous time model of Nesterov's accelerated gradient descent algorithm (NAG), the classical discrete time version of NAG, and versions of NAG with stochastic gradient estimates with purely additive noise and with noise that exhibits both additive and multiplicative scaling.",
      "authors": [
        "Kanan Gupta",
        "Stephan Wojtowytsch"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=YwJkv2YqBq",
      "cdate": 1727455644772,
      "mdate": 1747105210070,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835655"
    },
    {
      "id": "uZgK0tcPqd",
      "title": "Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models",
      "abstract": "Advancements in Natural Language Processing (NLP), have led to the emergence of Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which excel across a range of tasks but require extensive fine-tuning to align their outputs with human expectations. A widely used method for achieving this alignment is Reinforcement Learning from Human Feedback (RLHF), which, despite its success, faces challenges in accurately modelling human preferences. In this paper, we introduce GazeReward, a novel framework that integrates implicit feedback -- and specifically eye-tracking (ET) data -- into the Reward Model (RM). In addition, we explore how ET-based features can provide insights into user preferences. Through ablation studies we test our framework with different integration methods, LLMs, and ET generator models, demonstrating that our approach significantly improves the accuracy of the RM on established human preference datasets. This work advances the ongoing discussion on optimizing AI alignment with human values, exploring the potential of cognitive data for shaping future NLP research.",
      "authors": [
        "Ángela López-Cardona",
        "Carlos Segura",
        "Alexandros Karatzoglou",
        "Sergi Abadal",
        "Ioannis Arapakis"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=uZgK0tcPqd",
      "cdate": 1727455543217,
      "mdate": 1740734399480,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835660"
    },
    {
      "id": "QFgbJOYJSE",
      "title": "State Space Models are Provably Comparable to Transformers in Dynamic Token Selection",
      "abstract": "Deep neural networks based on state space models (SSMs) are attracting significant attention in sequence modeling since their computational cost is much smaller than that of Transformers. While the capabilities of SSMs have been demonstrated through experiments in various tasks, theoretical understanding of SSMs is still limited. In particular, most theoretical studies discuss the capabilities of SSM layers without nonlinear layers, and there is a lack of discussion on their combination with nonlinear layers. In this paper, we explore the capabilities of SSMs combined with fully connected neural networks, and show that they are comparable to Transformers in extracting the essential tokens depending on the input. As concrete examples, we consider two synthetic tasks, which are challenging for a single SSM layer, and demonstrate that SSMs combined with nonlinear layers can efficiently solve these tasks.  Furthermore, we study the nonparametric regression task, and prove that the ability of SSMs is equivalent to that of Transformers in estimating functions belonging to a certain class.",
      "authors": [
        "Naoki Nishikawa",
        "Taiji Suzuki"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=QFgbJOYJSE",
      "cdate": 1727455127971,
      "mdate": 1741170046808,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835665"
    },
    {
      "id": "mTCbq2QssD",
      "title": "OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data",
      "abstract": "Mathematical reasoning continues to be a critical challenge in large language model (LLM) development with significant interest. However, most of the cutting-edge progress in mathematical reasoning with LLMs has become closed-source due to lack of access to training data. This lack of data access limits researchers from understanding the impact of different choices for synthesizing and utilizing the data. With the goal of creating a high-quality finetuning (SFT) dataset for math reasoning, we conduct careful ablation experiments on data synthesis using the recently released Llama3.1 family of models. Our experiments show that: (a) solution format matters, with excessively verbose solutions proving detrimental to SFT performance, (b) data generated by a strong teacher outperforms on-policy data generated by a weak student model, (c) SFT is robust to low-quality solutions, allowing for imprecise data filtering, and (d) question diversity is crucial for achieving data scaling gains. Based on these insights, we create the OpenMathInstruct-2 dataset which consists of 14M question-solution pairs (≈ 600K unique questions), making it nearly eight times larger than the previous largest open-source math reasoning dataset. Finetuning the Llama-3.1-8B-Base using OpenMathInstruct-2 outperforms Llama3.1-8B-Instruct on MATH by an absolute 15.9% (51.9% → 67.8%). Finally, to accelerate the open-source efforts, we release the code, the finetuned models, and the OpenMathInstruct-2 dataset under a commercially permissive license.",
      "authors": [
        "Shubham Toshniwal",
        "Wei Du",
        "Ivan Moshkov",
        "Branislav Kisacanin",
        "Alexan Ayrapetyan",
        "Igor Gitman"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=mTCbq2QssD",
      "cdate": 1727455096811,
      "mdate": 1744048668718,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835676"
    },
    {
      "id": "GR0y0F3Ipd",
      "title": "MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science",
      "abstract": "Pre-trained on extensive text and image corpora, current Multi-Modal Large Language Models (MLLM) have shown strong capabilities in general visual reasoning tasks. \nHowever, their performance is still lacking in physical domains that require understanding diagrams with complex physical structures and quantitative analysis based on multi-modal information. \nTo address this, we develop a new framework, named **M**ulti-Modal Scientific Re**A**soning with **P**hysics Perception and **S**imulation (**MAPS**) based on an MLLM. \nMAPS decomposes expert-level multi-modal reasoning task into physical diagram understanding via a Physical Perception Model (PPM) and reasoning with physical knowledge via a simulator. \nThe PPM module is obtained by fine-tuning a visual language model using carefully designed synthetic data with paired physical diagrams and corresponding simulation language descriptions. \nAt the inference stage, MAPS integrates the simulation language description of the input diagram provided by PPM and results obtained through a Chain-of-Simulation process with MLLM to derive the underlying rationale and the final answer. \nValidated using our collected college-level circuit analysis problems, MAPS significantly improves reasoning accuracy of MLLM and outperforms all existing models. \nThe results confirm MAPS offers a promising direction for enhancing multi-modal scientific reasoning ability of MLLMs. \nWe will release our code, model and dataset used for our experiments upon publishing of this paper.",
      "authors": [
        "Erle Zhu",
        "Yadi Liu",
        "Zhe Zhang",
        "Xujun Li",
        "JinZhou",
        "Xinjie Yu",
        "Minlie Huang",
        "Hongning Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=GR0y0F3Ipd",
      "cdate": 1727455055198,
      "mdate": 1740756717330,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835682"
    },
    {
      "id": "FEZOLWexPb",
      "title": "MAESTRO: Masked Encoding Set Transformer with Self-Distillation",
      "abstract": "The interrogation of cellular states and interactions in immunology research is an ever-evolving task, requiring adaptation to the current levels of high dimensionality. Cytometry enables high-dimensional profiling of immune cells, but its analysis is hindered by the complexity and variability of the data. We present MAESTRO, a self-supervised set representation learning model that generates vector representations of set-structured data, which we apply to learn immune profiles from cytometry data. Unlike previous studies only learn cell-level representations, whereas MAESTRO uses all of a sample's cells to learn a set representation. MAESTRO leverages specialized attention mechanisms to handle sets of variable number of cells and ensure permutation invariance, coupled with an online tokenizer by self-distillation framework. We benchmarked our model against existing cytometry approaches and other existing machine learning methods that have never been applied in cytometry. Our model outperforms existing approaches in retrieving cell-type proportions and capturing clinically relevant features for downstream tasks such as disease diagnosis and immune cell profiling.",
      "authors": [
        "Matthew Eric Lee",
        "Jaesik Kim",
        "Matei Ionita",
        "Jonghyun Lee",
        "Michelle L. McKeague",
        "YONGHYUN NAM",
        "Irene Khavin",
        "Yidi Huang",
        "Victoria Fang",
        "Sokratis Apostolidis",
        "Divij Mathew",
        "Shwetank",
        "Ajinkya Pattekar",
        "Zahabia Rangwala",
        "Amit Bar-Or",
        "Benjamin A Fensterheim",
        "Benjamin A. Abramoff",
        "Rennie L. Rhee",
        "Damian Maseda",
        "Allison R Greenplate",
        "John Wherry",
        "Dokyoon Kim"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=FEZOLWexPb",
      "cdate": 1727455028764,
      "mdate": 1744148677231,
      "matched_keywords": [
        "transformer",
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835687"
    },
    {
      "id": "9kJperA2a4",
      "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out Context Attribution",
      "abstract": "The influence of contextual input on the behavior of large language models (LLMs) has prompted the development of context attribution methods that aim to quantify each context span's effect on an LLM's generations. The leave-one-out (LOO) error, which measures the change in the likelihood of the LLM's response when a given span of the context is removed, provides a principled way to perform context attribution, but can be prohibitively expensive to compute for large models. In this work, we introduce AttriBoT, a series of novel techniques for efficiently computing an approximation of the LOO error for context attribution. Specifically, AttriBoT uses cached activations to avoid redundant operations, performs hierarchical attribution to reduce computation, and emulates the behavior of large target models with smaller proxy models. Taken together, AttriBoT can provide a 300x speedup while remaining more faithful to a target model's LOO error than prior context attribution methods. This stark increase in performance makes computing context attributions for a given response $30\\times$ faster than generating the response itself, empowering real-world applications that require computing attributions at scale. We release a user-friendly and efficient implementation of AttriBoT to enable efficient LLM interpretability as well as encourage future development of efficient context attribution methods.",
      "authors": [
        "Fengyuan Liu",
        "Nikhil Kandpal",
        "Colin Raffel"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=9kJperA2a4",
      "cdate": 1727454898830,
      "mdate": 1739897242228,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835692"
    },
    {
      "id": "kwCHcaeHrf",
      "title": "Provably Safeguarding a Classifier from OOD and Adversarial Samples",
      "abstract": "This paper aims to transform a trained classifier into an abstaining classifier, such\nthat the latter is provably protected from out-of-distribution and adversarial samples. The proposed Sample-efficient Probabilistic Detection using Extreme Value\nTheory (SPADE) approach relies on a Generalized Extreme Value (GEV) model\nof the training distribution in the latent space of the classifier. Under mild assumptions, this GEV model allows for formally characterizing out-of-distribution\nand adversarial samples and rejecting them. Empirical validation of the approach\nis conducted on various neural architectures (ResNet, VGG, and Vision Transformer) and considers medium and large-sized datasets (CIFAR-10, CIFAR-100,\nand ImageNet). The results show the stability and frugality of the GEV model and\ndemonstrate SPADE’s efficiency compared to the state-of-the-art methods.",
      "authors": [
        "Nicolas Atienza",
        "Johanne Cohen",
        "Christophe Labreuche",
        "Michele Sebag"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=kwCHcaeHrf",
      "cdate": 1727454829386,
      "mdate": 1740890354120,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835697"
    },
    {
      "id": "Llh6CinTiy",
      "title": "Learning Chaos In A Linear Way",
      "abstract": "Learning long-term behaviors in chaotic dynamical systems, such as turbulent flows and climate modelling, is challenging due to their inherent instability and unpredictability. These systems exhibit positive Lyapunov exponents, which significantly hinder accurate long-term forecasting. As a result, understanding long-term statistical behavior is far more valuable than focusing on short-term accuracy. While autoregressive deep sequence models have been applied to capture long-term behavior, they often lead to exponentially increasing errors in learned dynamics. To address this, we shift the focus from simple prediction errors to preserving an invariant measure in dissipative chaotic systems. These systems have attractors, where trajectories settle, and the invariant measure is the probability distribution on attractors that remains unchanged under dynamics. Existing methods generate long trajectories of dissipative chaotic systems by aligning invariant measures, but it is not always possible to obtain invariant measures for arbitrary datasets. We propose the Poincaré Flow Neural Network (PFNN), a novel operator learning framework designed to capture behaviors of chaotic systems without any explicit knowledge of the invariant measure. PFNN employs an auto-encoder to map the chaotic system to a finite-dimensional feature space, effectively linearizing the chaotic evolution.  It then learns the linear evolution operators to match the physical dynamics by addressing two critical properties in dissipative chaotic systems: (1) contraction, the system’s convergence toward its attractors, and (2) measure invariance, trajectories on the attractors following a probability distribution invariant to the dynamics. \nOur experiments on a variety of chaotic systems, including Lorenz systems, Kuramoto-Sivashinsky equation and Navier–Stokes equation, demonstrate that PFNN has more accurate predictions and physical statistics compared to competitive baselines including the Fourier Neural Operator and the Markov Neural Operator.",
      "authors": [
        "Xiaoyuan Cheng",
        "Yi He",
        "Yiming Yang",
        "Xiao Xue",
        "Sibo Cheng",
        "Daniel Giles",
        "Xiaohang Tang",
        "Yukun Hu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Llh6CinTiy",
      "cdate": 1727454781602,
      "mdate": 1742391926676,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835702"
    },
    {
      "id": "vf5aUZT0Fz",
      "title": "DEPT: Decoupled Embeddings for Pre-training Language Models",
      "abstract": "Language Model pre-training uses broad data mixtures to enhance performance across domains and languages. However, training on such heterogeneous text corpora requires extensive and expensive efforts. Since these data sources vary significantly in lexical, syntactic, and semantic aspects, they cause negative interference or the ``curse of multilinguality''. To address these challenges we propose a communication-efficient pre-training framework, DEPT. Our method decouples embeddings from the transformer body while simultaneously training the latter on multiple data sources without requiring a shared vocabulary. DEPT can: (1) train robustly and effectively under significant data heterogeneity, (2) minimize token embedding parameters to only what the data source vocabulary requires, while cutting communication costs in direct proportion to both the communication frequency and the reduction in parameters, (3) enhance transformer body plasticity and generalization, improving both average perplexity (up to 20%) and downstream task performance, and (4) enable training with custom optimized vocabularies per data source. We demonstrate DEPT's potential via the first vocabulary-agnostic federated pre-training of billion-scale models, reducing communication costs by orders of magnitude and embedding memory by 4-5x.",
      "authors": [
        "Alex Iacob",
        "Lorenzo Sani",
        "Meghdad Kurmanji",
        "William F. Shen",
        "Xinchi Qiu",
        "Dongqi Cai",
        "Yan Gao",
        "Nicholas Donald Lane"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=vf5aUZT0Fz",
      "cdate": 1727454699683,
      "mdate": 1740860804626,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835707"
    },
    {
      "id": "IIsTO4P3Ag",
      "title": "Harnessing Webpage UIs for Text-Rich Visual Understanding",
      "abstract": "Text-rich visual understanding—the ability to interpret both textual content and visual elements within a scene—is crucial for multimodal large language models (MLLMs) to effectively interact with structured environments. We propose leveraging webpage UIs as a naturally structured and diverse data source to enhance MLLMs’ capabilities in this area. Existing approaches, such as rule-based extraction, multimodal model captioning, and rigid HTML parsing, are hindered by issues like noise, hallucinations, and limited generalization. To overcome these challenges, we introduce MultiUI, a dataset of 7.3 million samples spanning various UI types and tasks, structured using enhanced accessibility trees and task taxonomies. By scaling multimodal instructions from web UIs through LLMs, our dataset enhances generalization beyond web domains, significantly improving performance in document understanding, GUI comprehension, grounding, and advanced agent tasks. This demonstrates the potential of structured web data to elevate MLLMs’ proficiency in processing text-rich visual environments and generalizing across domains.",
      "authors": [
        "Junpeng Liu",
        "Tianyue Ou",
        "Yifan Song",
        "Yuxiao Qu",
        "Wai Lam",
        "Chenyan Xiong",
        "Wenhu Chen",
        "Graham Neubig",
        "Xiang Yue"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=IIsTO4P3Ag",
      "cdate": 1727454619339,
      "mdate": 1739858514264,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.835712"
    },
    {
      "id": "rdv6yeMFpn",
      "title": "Homomorphism Expressivity of Spectral Invariant Graph Neural Networks",
      "abstract": "Graph spectra are an important class of structural features on graphs that have shown promising results in enhancing Graph Neural Networks (GNNs). Despite their widespread practical use, the theoretical understanding of the power of spectral invariants --- particularly their contribution to GNNs --- remains incomplete. In this paper, we address this fundamental question through the lens of homomorphism expressivity, providing a comprehensive and quantitative analysis of the expressive power of spectral invariants. Specifically, we prove that spectral invariant GNNs can homomorphism-count exactly a class of specific tree-like graphs which we refer to as \\emph{parallel trees}. We highlight the significance of this result in various contexts, including establishing a quantitative expressiveness hierarchy across different architectural variants, offering insights into the impact of GNN depth, and understanding the subgraph counting capabilities of spectral invariant GNNs. In particular, our results significantly extend \\citet{arvind2024hierarchy} and settle their open questions. Finally, we generalize our analysis to higher-order GNNs and answer an open question raised by \\citet{zhang2024expressive}.",
      "authors": [
        "Jingchu Gai",
        "Yiheng Du",
        "Bohang Zhang",
        "Haggai Maron",
        "Liwei Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=rdv6yeMFpn",
      "cdate": 1727454568329,
      "mdate": 1740831273778,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835718"
    },
    {
      "id": "mnna9LUg7P",
      "title": "Quamba: A Post-Training Quantization Recipe for Selective State Space Models",
      "abstract": "State Space Models (SSMs) have emerged as an appealing alternative to Transformers for large language models, achieving state-of-the-art accuracy with constant memory complexity which allows for holding longer context lengths than attention-based networks. The superior computational efficiency of SSMs in long sequence modeling positions them favorably over Transformers in many scenarios. However, improving the efficiency of SSMs on request-intensive cloud-serving and resource-limited edge applications is still a formidable task. SSM quantization is a possible solution to this problem, making SSMs more suitable for wide deployment, while still maintaining their accuracy. Quantization is a common technique to reduce the model size and to utilize the low bit-width acceleration features on modern computing units, yet existing quantization techniques are poorly suited for SSMs. Most notably, SSMs have highly sensitive feature maps within the selective scan mechanism (i.e., linear recurrence) and massive outliers in the output activations which are not present in the output of token-mixing in the self-attention modules. To address this issue, we propose a static 8-bit per-tensor SSM quantization method which suppresses the maximum values of the input activations to the selective SSM for finer quantization precision and quantizes the output activations in an outlier-free space with Hadamard transform. Our 8-bit weight-activation quantized Mamba 2.8B SSM benefits from hardware acceleration and achieves a 1.72 $\\times$ lower generation latency on an Nvidia Orin Nano 8G, with only a 0.9\\% drop in average accuracy on zero-shot tasks. When quantizing Jamba, a 52B parameter SSM-style language model, we observe only a $1\\%$  drop in accuracy, demonstrating that our SSM quantization method is both effective and scalable for large language models, which require appropriate compression techniques for deployment. The experiments demonstrate the effectiveness and practical applicability of our approach for deploying SSM-based models of all sizes on both cloud and edge platforms.",
      "authors": [
        "Hung-Yueh Chiang",
        "Chi-Chih Chang",
        "Natalia Frumkin",
        "Kai-Chiang Wu",
        "Diana Marculescu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=mnna9LUg7P",
      "cdate": 1727454526308,
      "mdate": 1740884080633,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835723"
    },
    {
      "id": "tFV5GrWOGm",
      "title": "ElasticTok: Adaptive Tokenization for Image and Video",
      "abstract": "Efficient video tokenization remains a key bottleneck in learning general purpose vision models that are capable of processing long video sequences. Prevailing approaches are restricted to encoding videos to a fixed number of tokens, where too few tokens will result in overly lossy encodings, and too many tokens will result in prohibitively long sequence lengths. In this work, we introduce ElasticTok, a method that conditions on prior frames to adaptively encode a frame into a variable number of tokens. To enable this in a computationally scalable way, we propose a masking technique that drops a random number of tokens at the end of each frames's token encoding. During inference, ElasticTok can dynamically allocate tokens when needed -- more complex data can leverage more tokens, while simpler data only needs a few tokens. Our empirical evaluations on images and video demonstrate the effectiveness of our approach in efficient token usage, paving the way for future development of more powerful multimodal models, world models, and agents. Video examples of using ElasticTok can be found on our website: http://largeworldmodel.github.io/elastictok",
      "authors": [
        "Wilson Yan",
        "Volodymyr Mnih",
        "Aleksandra Faust",
        "Matei Zaharia",
        "Pieter Abbeel",
        "Hao Liu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=tFV5GrWOGm",
      "cdate": 1727454399433,
      "mdate": 1739261746840,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.835732"
    },
    {
      "id": "uqe5HkjbT9",
      "title": "Trajectory-Class-Aware Multi-Agent Reinforcement Learning",
      "abstract": "In the context of multi-agent reinforcement learning, *generalization* is a challenge to solve various tasks that may require different joint policies or coordination without relying on policies specialized for each task. We refer to this type of problem as a *multi-task*, and we train agents to be versatile in this multi-task setting through a single training process. To address this challenge, we introduce TRajectory-class-Aware Multi-Agent reinforcement learning (TRAMA). In TRAMA, agents recognize a task type by identifying the class of trajectories they are experiencing through partial observations, and the agents use this trajectory awareness or prediction as additional information for action policy. To this end, we introduce three primary objectives in TRAMA: (a) constructing a quantized latent space to generate trajectory embeddings that reflect key similarities among them; (b) conducting trajectory clustering using these trajectory embeddings; and (c) building a trajectory-class-aware policy. Specifically for (c), we introduce a trajectory-class predictor that performs agent-wise predictions on the trajectory class; and we design a trajectory-class representation model for each trajectory class. Each agent takes actions based on this trajectory-class representation along with its partial observation for task-aware execution. The proposed method is evaluated on various tasks, including multi-task problems built upon StarCraft II. Empirical results show further performance improvements over state-of-the-art baselines.",
      "authors": [
        "Hyungho Na",
        "Kwanghyeon Lee",
        "Sumin Lee",
        "Il-chul Moon"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=uqe5HkjbT9",
      "cdate": 1727454375936,
      "mdate": 1741057472208,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835741"
    },
    {
      "id": "zxO4WuVGns",
      "title": "Inverse decision-making using neural amortized Bayesian actors",
      "abstract": "Bayesian observer and actor models have provided normative explanations for many behavioral phenomena in perception, sensorimotor control, and other areas of cognitive science and neuroscience. They attribute behavioral variability and biases to interpretable entities such as perceptual and motor uncertainty, prior beliefs, and behavioral costs. However, when extending these models to more naturalistic tasks with continuous actions, solving the Bayesian decision-making problem is often analytically intractable. Inverse decision-making, i.e. performing inference over the parameters of such models given behavioral data, is computationally even more difficult. Therefore, researchers typically constrain their models to easily tractable components, such as Gaussian distributions or quadratic cost functions, or resort to numerical approximations. To overcome these limitations, we amortize the Bayesian actor using a neural network trained on a wide range of parameter settings in an unsupervised fashion. Using the pre-trained neural network enables performing efficient gradient-based Bayesian inference of the Bayesian actor model's parameters. We show on synthetic data that the inferred posterior distributions are in close alignment with those obtained using analytical solutions where they exist. Where no analytical solution is available, we recover posterior distributions close to the ground truth. We then show how our method allows for principled model comparison and how it can be used to disentangle factors that may lead to unidentifiabilities between priors and costs. Finally, we apply our method to empirical data from three sensorimotor tasks and compare model fits with different cost functions to show that it can explain individuals' behavioral patterns.",
      "authors": [
        "Dominik Straub",
        "Tobias F. Niehues",
        "Jan Peters",
        "Constantin A. Rothkopf"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=zxO4WuVGns",
      "cdate": 1727454120368,
      "mdate": 1739261746545,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835746"
    },
    {
      "id": "i3T0wvQDKg",
      "title": "Valid Conformal Prediction for Dynamic GNNs",
      "abstract": "Dynamic graphs provide a flexible data abstraction for modelling many sorts of real-world systems, such as transport, trade, and social networks. Graph neural networks (GNNs) are powerful tools allowing for different kinds of prediction and inference on these systems, but getting a handle on uncertainty, especially in dynamic settings, is a challenging problem.\n\nIn this work we propose to use a dynamic graph representation known in the tensor literature as the unfolding, to achieve valid prediction sets via conformal prediction. This representation, a simple graph, can be input to any standard GNN and does not require any modification to existing GNN architectures or conformal prediction routines. \n\nOne of our key contributions is a careful mathematical consideration of the different inference scenarios which can arise in a dynamic graph modelling context. For a range of practically relevant cases, we obtain valid prediction sets with almost no assumptions, even dispensing with exchangeability. In a more challenging scenario, which we call the semi-inductive regime, we achieve valid prediction under stronger assumptions, akin to stationarity. \n\nWe provide real data examples demonstrating validity, showing improved accuracy over baselines, and sign-posting different failure modes which can occur when those assumptions are violated.",
      "authors": [
        "Ed Davis",
        "Ian Gallagher",
        "Daniel John Lawson",
        "Patrick Rubin-Delanchy"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=i3T0wvQDKg",
      "cdate": 1727454007275,
      "mdate": 1744098534657,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835752"
    },
    {
      "id": "nzOD1we8Z4",
      "title": "ContextGNN: Beyond Two-Tower Recommendation Systems",
      "abstract": "Recommendation systems predominantly utilize two-tower architectures, which evaluate user-item rankings through the inner product of their respective embeddings. However, one key limitation of two-tower models is that they learn a pair-agnostic representation of users and items. In contrast, pair-wise representations either scale poorly due to their quadratic complexity or are too restrictive on the candidate pairs to rank. To address these issues, we introduce Context-based Graph Neural Networks (ContextGNNs), a novel deep learning architecture for link prediction in recommendation systems. The method employs a pair-wise representation technique for familiar items situated within a user's local subgraph, while leveraging two-tower representations to facilitate the recommendation of exploratory items. A final network then predicts how to fuse both pair-wise and two-tower recommendations into a single ranking of items. We demonstrate that ContextGNN is able to adapt to different data characteristics and outperforms existing methods, both traditional and GNN-based, on a diverse set of practical recommendation tasks, improving performance by 20\\% on average.",
      "authors": [
        "Yiwen Yuan",
        "Zecheng Zhang",
        "Xinwei He",
        "Akihiro Nitta",
        "Weihua Hu",
        "Manan Shah",
        "Blaž Stojanovič",
        "Shenyang Huang",
        "Jan Eric Lenssen",
        "Jure Leskovec",
        "Matthias Fey"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=nzOD1we8Z4",
      "cdate": 1727454000028,
      "mdate": 1740393485101,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835757"
    },
    {
      "id": "OlRjxSuSwl",
      "title": "Beyond Content Relevance: Evaluating Instruction Following in Retrieval Models",
      "abstract": "Instruction-following capabilities in large language models (LLMs) have progressed significantly, enabling more complex user interactions through detailed prompts. However, retrieval systems have not matched these advances, most of them still relies on traditional lexical and semantic matching techniques that fail to fully capture user intent. Recent efforts have introduced instruction-aware retrieval models, but these primarily focus on intrinsic content relevance, which neglects the importance of customized preferences for broader document-level attributes. This study evaluates the instruction-following capabilities of various retrieval models beyond content relevance, including LLM-based dense retrieval and reranking models. We develop InfoSearch, a novel retrieval evaluation benchmark spanning six document-level attributes: Audience, Keyword, Format, Language, Length, and Source, and introduce novel metrics -- Strict Instruction Compliance Ratio (SICR) and Weighted Instruction Sensitivity Evaluation (WISE) to accurately assess the models' responsiveness to  instructions. Our findings indicate that although fine-tuning models on instruction-aware retrieval datasets and increasing model size enhance performance, most models still fall short of instruction compliance. We release our dataset and code on https://github.com/EIT-NLP/InfoSearch.",
      "authors": [
        "Jianqun Zhou",
        "Yuanlei Zheng",
        "Wei Chen",
        "Qianqian Zheng",
        "Shang Zeyuan",
        "Wei Zhang",
        "Rui Meng",
        "Xiaoyu Shen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=OlRjxSuSwl",
      "cdate": 1727453822828,
      "mdate": 1740890353833,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835762"
    },
    {
      "id": "L5godAOC2z",
      "title": "RobustKV: Defending Large Language Models against Jailbreak Attacks via KV Eviction",
      "abstract": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful queries within adversarial prompts. While most existing defenses attempt to mitigate the effects of adversarial prompts, they often prove inadequate as adversarial prompts can take arbitrary, adaptive forms. This paper introduces RobustKV, a novel jailbreak defense that takes a fundamentally different approach by selectively removing critical tokens of harmful queries from key-value (KV) caches. Intuitively, for an adversarial prompt to be effective, its tokens must achieve sufficient `importance' (measured by attention scores), which consequently lowers the importance of tokens in the concealed harmful query. Therefore, by carefully evicting the KVs of low-ranked tokens, RobustKV minimizes the harmful query's presence in the KV cache, thus preventing the LLM from generating informative responses. Extensive evaluation using benchmark datasets and models demonstrates that RobustKV effectively counters state-of-the-art jailbreak attacks while maintaining the LLM's performance on benign queries. Notably, RobustKV creates an interesting effectiveness-evasiveness dilemma for the adversary, leading to its robustness against adaptive attacks.{(Warning: This paper contains potentially harmful content generated by LLMs.)}",
      "authors": [
        "Tanqiu Jiang",
        "Zian Wang",
        "Jiacheng Liang",
        "Changjiang Li",
        "Yuhui Wang",
        "Ting Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=L5godAOC2z",
      "cdate": 1727453809753,
      "mdate": 1739875459473,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835768"
    },
    {
      "id": "00SnKBGTsz",
      "title": "DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback",
      "abstract": "The process of creating training data to teach models is currently driven by humans, who manually analyze model weaknesses and plan how to create data that improves a student model. Recent approaches using large language models (LLMs) as annotators reduce human annotation effort, but still require humans to interpret feedback from evaluations and control the LLM to produce data the student needs. Automating this labor-intensive process by creating autonomous data generation agents – or teachers – is desirable, but requires environments that can simulate the feedback-driven, iterative, closed loop of data creation. To enable rapid and scalable testing for such agents and their modules, we introduce DataEnvGym, a testbed of teacher environments for data generation agents. DataEnvGym frames data generation as a sequential decision-making task, involving an agent consisting of a data generation policy (which generates a plan for creating training data) and a data generation engine (which transforms the plan into data), inside an environment that provides feedback from a student. The agent’s end goal is to improve student model performance. Students are iteratively trained and evaluated on generated data, with their feedback (in the form of errors or weak skills) being reported to the agent after each iteration. As a general-purpose testbed, DataEnvGym includes multiple instantiations of teacher environments across three levels of structure in the state representation and action space, with varying levels of scaffolding support. More structured environments are based on automatically-inferred skills and offer a higher degree of interpretability and control over the curriculum. We support developing and testing data generation agents in four diverse tasks covering text, images, and actions (mathematics, programming, visual question answering, and tool-use) and test multiple student and teacher models. We find that example agents in our teaching environments can iteratively improve students across diverse tasks and settings. Moreover, we show that environments can teach different skill levels and can be used to test variants of key modules, pointing to directions of future work in improving data generation agents, engines, and feedback mechanisms. Project page: https://DataEnvGym.github.io.",
      "authors": [
        "Zaid Khan",
        "Elias Stengel-Eskin",
        "Jaemin Cho",
        "Mohit Bansal"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=00SnKBGTsz",
      "cdate": 1727453576661,
      "mdate": 1740879316612,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835773"
    },
    {
      "id": "MxbEiFRf39",
      "title": "NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model Internals",
      "abstract": "We introduce NNsight and NDIF, technologies that work in tandem to enable scientific study of the representations and computations learned by very large neural networks. NNsight is an open-source system that extends PyTorch to introduce deferred remote execution. The National Deep Inference Fabric (NDIF) is a scalable inference service that executes NNsight requests, allowing users to share GPU resources and pretrained models. These technologies are enabled by the Intervention Graph, an architecture developed to decouple experimental design from model runtime. Together, this framework provides transparent and efficient access to the internals of deep neural networks such as very large language models (LLMs) without imposing the cost or complexity of hosting customized models individually. We conduct a quantitative survey of the machine learning literature that reveals a growing gap in the study of the internals of large-scale AI. We demonstrate the design and use of our framework to address this gap by enabling a range of research methods on huge models. Finally, we conduct benchmarks to compare performance with previous approaches.\n\nCode, documentation, and tutorials are available at https://nnsight.net/.",
      "authors": [
        "Jaden Fried Fiotto-Kaufman",
        "Alexander Russell Loftus",
        "Eric Todd",
        "Jannik Brinkmann",
        "Koyena Pal",
        "Dmitrii Troitskii",
        "Michael Ripa",
        "Adam Belfki",
        "Can Rager",
        "Caden Juang",
        "Aaron Mueller",
        "Samuel Marks",
        "Arnab Sen Sharma",
        "Francesca Lucchetti",
        "Nikhil Prakash",
        "Carla E. Brodley",
        "Arjun Guha",
        "Jonathan Bell",
        "Byron C Wallace",
        "David Bau"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=MxbEiFRf39",
      "cdate": 1727453572029,
      "mdate": 1740890353639,
      "matched_keywords": [
        "large language model",
        "foundation model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835781"
    },
    {
      "id": "hXm0Wu2U9K",
      "title": "Correcting the Mythos of KL-Regularization: Direct Alignment without Overoptimization via Chi-Squared Preference Optimization",
      "abstract": "Language model alignment methods such as reinforcement learning from human feedback (RLHF) have led to impressive advances in language model capabilities, but are limited by a widely observed phenomenon known as *overoptimization*, where the quality of the language model degrades over the course of the alignment process. As the model optimizes performance on an offline reward model, it overfits to inaccuracies and drifts away from preferred responses covered by the data. To discourage such distribution shift, KL-regularization is widely employed in existing offline alignment methods, but overoptimization continues to harm performance. Lending theoretical insight into the source of these empirical observations, we first show that the KL-regularization is too weak to prevent overfitting, then ask: is it possible to design an efficient algorithm that is provably robust to overoptimization?\n\nIn this paper, we advance theoretical understanding of sample-efficient offline alignment and introduce a new algorithm called $\\chi^2$-Preference Optimization ($\\chi$PO). $\\chi$PO is a one-line change to Direct Preference Optimization (DPO; Rafailov et al. 2023), that modifies only the logarithmic link function in the DPO objective. Despite this minimal change, $\\chi$PO implicitly implements the principle of *pessimism in the face of uncertainty* via regularization with the $\\chi^2$-divergence---which quantifies uncertainty more effectively than KL-regularization---and provably alleviates overoptimization, achieving sample-complexity guarantees based on *single-policy concentrability*, the gold standard in offline reinforcement learning. This guarantee makes $\\chi$PO the first simple, yet general-purpose offline alignment algorithm that is provably robust to overoptimization.",
      "authors": [
        "Audrey Huang",
        "Wenhao Zhan",
        "Tengyang Xie",
        "Jason D. Lee",
        "Wen Sun",
        "Akshay Krishnamurthy",
        "Dylan J Foster"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=hXm0Wu2U9K",
      "cdate": 1727453522975,
      "mdate": 1742269980991,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835786"
    },
    {
      "id": "uNomADvF3s",
      "title": "Lift Your Molecules: Molecular Graph Generation in Latent Euclidean Space",
      "abstract": "We introduce a new framework for 2D molecular graph generation using 3D molecule generative models. Our Synthetic Coordinate Embedding (SyCo) framework maps 2D molecular graphs to 3D Euclidean point clouds via synthetic coordinates and learns the inverse map using an E($n$)-Equivariant Graph Neural Network (EGNN). The induced point cloud-structured latent space is well-suited to apply existing 3D molecule generative models. This approach simplifies the graph generation problem into a point cloud generation problem followed by node and edge classification tasks, without relying on molecular fragments nor autoregressive decoding. Further, we propose a novel similarity-constrained optimization scheme for 3D diffusion models based on inpainting and guidance. As a concrete implementation of our framework, we develop EDM-SyCo based on the E(3) Equivariant Diffusion Model (EDM). EDM-SyCo achieves state-of-the-art performance in distribution learning of molecular graphs, outperforming the best non-autoregressive methods by more than 26\\% on ZINC250K and 16\\% on the GuacaMol dataset while improving conditional generation by up to 3.9 times.",
      "authors": [
        "Mohamed Amine Ketata",
        "Nicholas Gao",
        "Johanna Sommer",
        "Tom Wollschläger",
        "Stephan Günnemann"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=uNomADvF3s",
      "cdate": 1727453465090,
      "mdate": 1740915861371,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835795"
    },
    {
      "id": "4l3AH8Bhmt",
      "title": "Revealing and Mitigating Over-Attention in Knowledge Editing",
      "abstract": "Large Language Models~(LLMs) have demonstrated superior performance across a wide range of tasks, but they still exhibit undesirable errors due to incorrect knowledge learned from the training data. To avoid this, knowledge editing methods emerged to precisely edit the specific model knowledge via efficiently modifying a very small percentage of parameters. However, those methods can lead to the problem of **Specificity Failure**, where the existing knowledge and capabilities are severely degraded due to editing.\nOur preliminary indicates that Specificity Failure primarily stems from the model's attention heads assigning excessive attention scores to entities related to the edited knowledge, thereby unduly focusing on specific snippets within the context, which we denote as the **Attention Drift** phenomenon.\nTo mitigate such Attention Drift issue, we introduce a simple yet effective method **S**elective **A**ttention **D**rift **R**estriction(**SADR**), which introduces an additional regularization term during the knowledge editing process to restrict changes in the attention weight distribution, thereby preventing undue focus on the edited entity.\nExperiments on five frequently-used strong LLMs demonstrate the effectiveness of our method, where SADR can significantly mitigate Specificity Failure in the predominant knowledge editing tasks.",
      "authors": [
        "Pinzheng Wang",
        "Zecheng Tang",
        "Keyan Zhou",
        "Juntao Li",
        "Qiaoming Zhu",
        "Min Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=4l3AH8Bhmt",
      "cdate": 1727453361947,
      "mdate": 1740657980021,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835800"
    },
    {
      "id": "pCj2sLNoJq",
      "title": "A Generalist Hanabi Agent",
      "abstract": "Traditional multi-agent reinforcement learning (MARL) systems can develop cooperative strategies through repeated interactions. However, these systems are unable to perform well on any other setting than the one they have been trained on, and struggle to successfully cooperate with unfamiliar collaborators. This is particularly visible in the Hanabi benchmark, a popular 2-to-5 player cooperative card-game which requires complex reasoning and precise assistance to other agents. Current MARL agents for Hanabi can only learn one specific game-setting (e.g., 2-player games), and play with the same algorithmic agents. This is in stark contrast to humans, who can quickly adjust their strategies to work with unfamiliar partners or situations. In this paper, we introduce Recurrent Replay Relevance Distributed DQN (R3D2), a generalist agent for Hanabi, designed to overcome these limitations. We reformulate the task using text, as language has been shown to improve transfer. We then propose a distributed MARL algorithm that copes with the resulting dynamic observation- and action-space. In doing so, our agent is the first that can play all game settings concurrently, and extend strategies learned from one setting to other ones. As a consequence, our agent also demonstrates the ability to collaborate with different algorithmic agents ---agents that are themselves unable to do so.",
      "authors": [
        "Arjun V Sudhakar",
        "Hadi Nekoei",
        "Mathieu Reymond",
        "Miao Liu",
        "Janarthanan Rajendran",
        "Sarath Chandar"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=pCj2sLNoJq",
      "cdate": 1727453336335,
      "mdate": 1742060852249,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835805"
    },
    {
      "id": "d9aWa875kj",
      "title": "Exact Certification of (Graph) Neural Networks Against Label Poisoning",
      "abstract": "Machine learning models are highly vulnerable to label flipping, i.e., the adversarial modification (poisoning) of training labels to compromise performance. Thus, deriving robustness certificates is important to guarantee that test predictions remain unaffected and to understand worst-case robustness behavior. However, for Graph Neural Networks (GNNs), the problem of certifying label flipping has so far been unsolved. We change this by introducing an exact certification method, deriving both sample-wise and collective certificates. Our method leverages the Neural Tangent Kernel (NTK) to capture the training dynamics of wide networks enabling us to reformulate the bilevel optimization problem representing label flipping into a Mixed-Integer Linear Program (MILP). We apply our method to certify a broad range of GNN architectures in node classification tasks. Thereby, concerning the worst-case robustness to label flipping: $(i)$ we establish hierarchies of GNNs on different benchmark graphs; $(ii)$ quantify the effect of architectural choices such as activations, depth and skip-connections; and surprisingly, $(iii)$ uncover a novel phenomenon of the robustness plateauing for intermediate perturbation budgets across all investigated datasets and architectures. While we focus on GNNs, our certificates are applicable to sufficiently wide NNs in general through their NTK. Thus, our work presents the first exact certificate to a poisoning attack ever derived for neural networks, which could be of independent interest. The code is available at https://github.com/saper0/qpcert.",
      "authors": [
        "Mahalakshmi Sabanayagam",
        "Lukas Gosch",
        "Stephan Günnemann",
        "Debarghya Ghoshdastidar"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=d9aWa875kj",
      "cdate": 1727453324280,
      "mdate": 1741965146357,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835810"
    },
    {
      "id": "5RUM1aIdok",
      "title": "GraphEval: A Lightweight Graph-Based LLM Framework for Idea Evaluation",
      "abstract": "The powerful capabilities of Large Language Models (LLMs) have led to their growing use in evaluating human-generated content, particularly in evaluating research ideas within academic settings. Existing solutions primarily rely on prompt-based LLM methods or fine-tuned lightweight language models for idea evaluation. However, these methods are often unstable and struggle to comprehend the complex semantic information embedded in the ideas, impeding their ability to perform high-quality evaluations. To address the above challenges, we propose $\\texttt{GraphEval}$, a lightweight graph-based LLM framework for idea evaluation. Our insight is that a complex idea can be broken down into comprehensible viewpoint nodes using prompts from small LLMs. These viewpoint nodes can then be linked together through edges created from LLM-based relation extraction and/or BERT similarity scores. The created viewpoint-graph can be used to conveniently propagate scores across view-nodes to improve the robustness of the idea evaluations. In particular, we propose two lightweight graph-based methods for idea evaluation: (1) GraphEval-LP: a training-free label propagation algorithm that propagates evaluation scores from known view-nodes to unknown nodes; (2) GraphEval-GNN: a Graph Neural Networks (GNN) that is trained to predict the evaluation scores given the observed graph with minimal computation resources. Moreover, to overcome LLM's limitation in objectively assessing the novelty of ideas, we further propose a novelty detection model to GraphEval-GNN to enhance its capability in judging idea novelty. Experiments on two datasets show $\\texttt{GraphEval}$ improves F1 scores by at least 14% with low computation and API costs. Additionally, $\\texttt{GraphEval}$ can effectively detect plagiarized ideas.",
      "authors": [
        "Tao Feng",
        "Yihang Sun",
        "Jiaxuan You"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=5RUM1aIdok",
      "cdate": 1727453063352,
      "mdate": 1742020265626,
      "matched_keywords": [
        "large language model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835816"
    },
    {
      "id": "cwuSAR7EKd",
      "title": "Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying Questions",
      "abstract": "Large language models (LLMs) must often respond to highly ambiguous user requests. In such cases, the LLM's best response may be to ask a clarifying question to elicit more information. Existing LLMs often respond by presupposing a single interpretation of such ambiguous requests, frustrating users who intended a different interpretation. We speculate this is caused by current preference data labeling practice, where LLM responses are evaluated only on their prior contexts. To address this, we assign preference labels by simulating their expected outcomes in future turns. This allows LLMs to learn to ask clarifying questions when it can generate responses that are tailored to each user interpretation in future turns. On open-domain QA datasets with multiple annotations, we evaluate systems based on their ability to ask clarifying questions to recover each user's interpretation and expected answer. We compare systems trained using our proposed preference labeling methods against standard methods, which assign preferences based on only prior context. Our method achieves a 5% improvement in F1 measured against the answer set from different interpretations of each query, showing the value of modeling future conversation turns. We further demonstrate that our method can be used to train models to judiciously determine when to ask clarifying questions, directly answering the question when clarification is unnecessary. In our experiments, we find that our method achives a 3% improvement in accuracy of such judgments over existing methods.",
      "authors": [
        "Michael JQ Zhang",
        "W. Bradley Knox",
        "Eunsol Choi"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=cwuSAR7EKd",
      "cdate": 1727452949422,
      "mdate": 1742090422149,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835820"
    },
    {
      "id": "1ExfUpmIW4",
      "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs",
      "abstract": "Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge. Experiments on the Training Data Extraction Challenge dataset using GPT-Neo models as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our approach effectively removes sensitive information while maintaining reasoning and generative capabilities with minimal impact. Our implementation can be found in https://github.com/csm9493/efficient-llm-unlearning.",
      "authors": [
        "Sungmin Cha",
        "Sungjun Cho",
        "Dasol Hwang",
        "Moontae Lee"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=1ExfUpmIW4",
      "cdate": 1727452931113,
      "mdate": 1743476887860,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835826"
    },
    {
      "id": "Zk9guOl9NS",
      "title": "What Makes Large Language Models Reason in (Multi-Turn) Code Generation?",
      "abstract": "Prompting techniques such as chain-of-thought have established themselves as a popular vehicle for improving the outputs of large language models (LLMs). For code generation, however, their exact mechanics and efficacy are under-explored using unified metrics and benchmarks. We thus investigate the effects of a wide range of prompting strategies with a focus on automatic re-prompting over multiple turns and computational requirements. After systematically decomposing reasoning, instruction, and execution feedback prompts, we conduct an extensive grid search on the competitive programming benchmarks CodeContests and TACO for multiple LLM families and sizes (Llama 3.0 and 3.1, 8B, 70B, 405B, and GPT-4o). Our study reveals strategies that consistently improve performance across all models with small and large sampling budgets. We then show how finetuning with such an optimal configuration allows models to internalize the induced reasoning process and obtain improvements in performance and scalability for multi-turn code generation.",
      "authors": [
        "Kunhao Zheng",
        "Juliette Decugis",
        "Jonas Gehring",
        "Taco Cohen",
        "benjamin negrevergne",
        "Gabriel Synnaeve"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Zk9guOl9NS",
      "cdate": 1727452874631,
      "mdate": 1745861607115,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835830"
    },
    {
      "id": "kmgrlG9TR0",
      "title": "RMB: Comprehensively benchmarking reward models in LLM alignment",
      "abstract": "Reward models (RMs) guide the alignment of large language models (LLMs), steering them toward behaviors preferred by humans. Evaluating RMs is the key to better aligning LLMs. However, the current evaluation of RMs may not directly correspond to their alignment performance due to the limited distribution of evaluation data and evaluation methods that are not closely related to alignment objectives. To address these limitations, we propose RMB, a comprehensive RM benchmark that covers over 49 real-world scenarios and includes both pairwise and Best-of-N (BoN) evaluations to better reflect the effectiveness of RMs in guiding alignment optimization.\nWe demonstrate a positive correlation between our benchmark and the downstream alignment task performance. Based on our benchmark, we conduct extensive analysis on the state-of-the-art RMs, revealing their generalization defects that were not discovered by previous benchmarks, and highlighting the potential of generative RMs.  Furthermore, we delve into open questions in reward models, specifically examining the effectiveness of majority voting for the evaluation of reward models and analyzing the impact factors of generative RMs, including the influence of evaluation criteria and instructing methods. We will release our evaluation code and datasets upon publication.",
      "authors": [
        "Enyu Zhou",
        "Guodong Zheng",
        "Binghai Wang",
        "Zhiheng Xi",
        "Shihan Dou",
        "Rong Bao",
        "Wei Shen",
        "Limao Xiong",
        "Jessica Fan",
        "Yurong Mou",
        "Rui Zheng",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=kmgrlG9TR0",
      "cdate": 1727452851646,
      "mdate": 1740279446035,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835836"
    },
    {
      "id": "AmEgWDhmTr",
      "title": "From Sparse Dependence to Sparse Attention: Unveiling How Chain-of-Thought Enhances Transformer Sample Efficiency",
      "abstract": "Chain-of-thought (CoT)  significantly enhances the reasoning performance of large language models (LLM). While current theoretical studies often attribute this improvement to increased expressiveness and computational capacity, we argue that expressiveness is not the primary limitation in the LLM regime, as current large models will fail on simple tasks. Using a parity-learning setup, we demonstrate that CoT can substantially improve sample efficiency even when the representation power is sufficient. Specifically, with CoT, a transformer can learn the function within polynomial samples, whereas without CoT, the required sample size is exponential. Additionally, we show that CoT simplifies the learning process by introducing sparse sequential dependencies among input tokens, and leads to a sparse and interpretable attention. We validate our theoretical analysis with both synthetic and real-world experiments, confirming that sparsity in attention layers is a key factor of the improvement induced by CoT.",
      "authors": [
        "Kaiyue Wen",
        "Huaqing Zhang",
        "Hongzhou Lin",
        "Jingzhao Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=AmEgWDhmTr",
      "cdate": 1727452810336,
      "mdate": 1740820989082,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835841"
    },
    {
      "id": "fjEZ2LPceZ",
      "title": "CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery",
      "abstract": "Large language models (LLMs) have demonstrated significant potential in advancing various fields of research and society. However, the current community of LLMs overly focuses on benchmarks for analyzing specific foundational skills (e.g. mathematics and code generation), neglecting an all-round evaluation of the computer science field. To bridge this gap, we introduce CS-Bench, the first multilingual (English, Chinese, French, German) benchmark dedicated to evaluating the performance of LLMs in computer science. CS-Bench comprises approximately 10K meticulously curated test samples, covering 26 subfields across 4 key areas of computer science, encompassing various task forms and divisions of knowledge and reasoning. Utilizing CS-Bench, we conduct a comprehensive evaluation of over 30 mainstream LLMs, revealing the relationship between CS performance and model scales. We also quantitatively analyze the reasons for failures in existing LLMs and highlight directions for improvements, including knowledge supplementation and CS-specific reasoning. Further cross-capability experiments show a high correlation between LLMs' capabilities in computer science and their abilities in mathematics and coding. Moreover, expert LLMs specialized in mathematics and coding also demonstrate strong performances in several CS subfields. Looking ahead, we envision CS-Bench serving as a cornerstone for LLM applications in the CS field and paving new avenues in assessing LLMs' diverse reasoning capabilities. Our project homepage is available at https://csbench.github.io/.",
      "authors": [
        "Xiaoshuai Song",
        "Muxi Diao",
        "Guanting Dong",
        "Zhengyang Wang",
        "Yujia Fu",
        "Runqi Qiao",
        "Zhexu Wang",
        "Dayuan Fu",
        "Huangxuan Wu",
        "Bin Liang",
        "Weihao Zeng",
        "Yejie Wang",
        "Zhuoma GongQue",
        "Jianing Yu",
        "Qiuna Tan",
        "Weiran Xu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=fjEZ2LPceZ",
      "cdate": 1727452796217,
      "mdate": 1739966216458,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835849"
    },
    {
      "id": "254NJe9JEw",
      "title": "A deep inverse-mapping model for a flapping robotic wing",
      "abstract": "In systems control, the dynamics of a system are governed by modulating its inputs to achieve a desired outcome. For example, to control the thrust of a quad-copter propeller the controller modulates its rotation rate, relying on a straightforward mapping between the input rotation rate and the resulting thrust. This mapping can be inverted to determine the rotation rate needed to generate a desired thrust. However, in complex systems, such as flapping-wing robots where intricate fluid motions are involved, mapping inputs (wing kinematics) to outcomes (aerodynamic forces) is nontrivial and inverting this mapping for real-time control is computationally impractical. Here, we report a machine-learning solution for the inverse mapping of a flapping-wing system based on data from an experimental system we have developed. Our model learns the input wing motion required to generate a desired aerodynamic force outcome. We used a sequence-to-sequence model tailored for time-series data and augmented it with a novel adaptive-spectrum layer that implements representation learning in the frequency domain. To train our model, we developed a flapping wing system that simultaneously measures the wing's aerodynamic force and its 3D motion using high-speed cameras. We demonstrate the performance of our system on an additional open-source dataset of a flapping wing in a different flow regime. Results show superior performance compared with more complex state-of-the-art transformer-based models, with 11\\% improvement on the test datasets median loss. Moreover, our model shows superior inference time, making it practical for onboard robotic control. Our open-source data and framework may improve modeling and real-time control of systems governed by complex dynamics, from biomimetic robots to biomedical devices.",
      "authors": [
        "Hadar Sharvit",
        "Raz Karl",
        "Tsevi Beatus"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=254NJe9JEw",
      "cdate": 1727452640782,
      "mdate": 1739455326214,
      "matched_keywords": [
        "transformer",
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835855"
    },
    {
      "id": "NWb128pSCb",
      "title": "Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective",
      "abstract": "Accurate interpretation and visualization of human instructions are crucial for text-to-image (T2I) synthesis. \nHowever, current models struggle to capture semantic variations from word order changes, and existing evaluations, relying on indirect metrics like text-image similarity, fail to reliably assess these challenges. This often obscures poor performance on complex or uncommon linguistic patterns by the focus on frequent word combinations.\nTo address these deficiencies, we propose a novel metric called SemVarEffect and a benchmark named SemVarBench, designed to evaluate the causality between semantic variations in inputs and outputs in T2I synthesis. \nSemantic variations are achieved through two types of linguistic permutations, while avoiding easily predictable literal variations.\nExperiments reveal that the CogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1. Semantic variations in object relations are less understood than attributes, scoring 0.07/1 compared to 0.17-0.19/1. \nWe found that cross-modal alignment in UNet or Transformers plays a crucial role in handling semantic variations, a factor previously overlooked by a focus on textual encoders. \nOur work establishes an effective evaluation framework that advances the T2I synthesis community's exploration of human instruction understanding. Our benchmark and code are available at https://github.com/zhuxiangru/SemVarBench.",
      "authors": [
        "Xiangru Zhu",
        "Penglei Sun",
        "Yaoxian Song",
        "Yanghua Xiao",
        "Zhixu Li",
        "Chengyu Wang",
        "Jun Huang",
        "Bei Yang",
        "Xiaoxiao Xu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=NWb128pSCb",
      "cdate": 1727452564047,
      "mdate": 1743505624695,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835860"
    },
    {
      "id": "eU39PDsZtT",
      "title": "GraphRouter: A Graph-based Router for LLM Selections",
      "abstract": "The rapidly growing number and variety of Large Language Models (LLMs)\npresent significant challenges in efficiently selecting the appropriate LLM for\na given query, especially considering the trade-offs between performance and\ncomputational cost. Current LLM selection methods often struggle to generalize\nacross new LLMs and different tasks because of their limited ability to leverage\ncontextual interactions among tasks, queries, and LLMs, as well as their depen-\ndence on a transductive learning framework. To address these shortcomings, we\nintroduce a novel inductive graph framework, named as GraphRouter, which\nfully utilizes the contextual information among tasks, queries, and LLMs to en-\nhance the LLM selection process. GraphRouter constructs a heterogeneous\ngraph comprising task, query, and LLM nodes, with interactions represented as\nedges, which efficiently captures the contextual information between the query’s\nrequirements and the LLM’s capabilities. Through an innovative edge prediction\nmechanism, GraphRouter is able to predict attributes (the effect and cost of\nLLM response) of potential edges, allowing for optimized recommendations that\nadapt to both existing and newly introduced LLMs without requiring retraining.\nComprehensive experiments across three distinct effect-cost weight scenarios have\nshown that GraphRouter substantially surpasses existing routers, delivering a\nminimum performance improvement of 12.3%. In addition, it achieves enhanced\ngeneralization across new LLMs settings and supports diverse tasks with at least a\n9.5% boost in effect and a significant reduction in computational demands. This\nwork endeavors to apply a graph-based approach for the contextual and adaptive\nselection of LLMs, offering insights for real-world applications.",
      "authors": [
        "Tao Feng",
        "Yanzhen Shen",
        "Jiaxuan You"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=eU39PDsZtT",
      "cdate": 1727452497425,
      "mdate": 1740809513354,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835865"
    },
    {
      "id": "8sfc8MwG5v",
      "title": "CONDA: Adaptive Concept Bottleneck for Foundation Models Under Distribution Shifts",
      "abstract": "Advancements in foundation models (FMs) have led to a paradigm shift in machine\nlearning. The rich, expressive feature representations from these pre-trained, large-\nscale FMs are leveraged for multiple downstream tasks, usually via lightweight\nfine-tuning of a shallow fully-connected network following the representation.\nHowever, the non-interpretable, black-box nature of this prediction pipeline can be\na challenge, especially in critical domains, such as healthcare, finance, and security.\nIn this paper, we explore the potential of Concept Bottleneck Models (CBMs)\nfor transforming complex, non-interpretable foundation models into interpretable\ndecision-making pipelines using high-level concept vectors. Specifically, we focus\non the test-time deployment of such an interpretable CBM pipeline “in the wild”,\nwhere the distribution of inputs often shifts from the original training distribution.\nWe first identify the potential failure modes of such pipelines under different types\nof distribution shifts. Then we propose an adaptive concept bottleneck framework\nto address these failure modes, that dynamically adapts the concept-vector bank\nand the prediction layer based solely on unlabeled data from the target domain,\nwithout access to the source dataset. Empirical evaluations with various real-world\ndistribution shifts show our framework produces concept-based interpretations\nbetter aligned with the test data and boosts post-deployment accuracy by up to\n28%, aligning CBM performance with that of non-interpretable classification.",
      "authors": [
        "Jihye Choi",
        "Jayaram Raghuram",
        "Yixuan Li",
        "Somesh Jha"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=8sfc8MwG5v",
      "cdate": 1727452490601,
      "mdate": 1740924977504,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835870"
    },
    {
      "id": "EW6bNEqalF",
      "title": "Offline RL in Regular Decision Processes: Sample Efficiency via Language Metrics",
      "abstract": "This work studies offline Reinforcement Learning (RL) in a class of non-Markovian environments called Regular Decision Processes (RDPs). In RDPs, the unknown dependency of future observations and rewards from the past interactions can be captured by some hidden finite-state automaton. For this reason, many RDP algorithms first reconstruct this unknown dependency using automata learning techniques. In this paper, we consider episodic RDPs and show that it is possible to overcome the limitations of existing offline RL algorithms for RDPs via\nthe introduction of two original techniques: a novel metric grounded in formal language theory and an approach based on Count-Min-Sketch (CMS). Owing to the novel language metric, our algorithm is proven to be more sample efficient than existing results, and in some problem instances admitting low complexity languages, the gain is showcased to be exponential in the episode length. The CMS-based approach removes the need for naïve counting and alleviates the memory requirements for long planning horizons. We derive Probably Approximately Correct (PAC) sample complexity bounds associated to each of these techniques, and validate the approach experimentally.",
      "authors": [
        "Ahana Deb",
        "Roberto Cipollone",
        "Anders Jonsson",
        "Alessandro Ronca",
        "Mohammad Sadegh Talebi"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=EW6bNEqalF",
      "cdate": 1727452469173,
      "mdate": 1747520965026,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835876"
    },
    {
      "id": "zCxGCdzreM",
      "title": "Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks",
      "abstract": "While large models trained with self-supervised learning on offline datasets have shown remarkable capabilities in text and image domains, achieving the same generalisation for agents that act in sequential decision problems remains an open challenge.\nIn this work, we take a step towards this goal by procedurally generating tens of millions of 2D physics-based tasks and using these to train a general reinforcement learning (RL) agent for physical control.\nTo this end, we introduce Kinetix: an open-ended space of physics-based RL environments that can represent tasks ranging from robotic locomotion and grasping to video games and classic RL environments, all within a unified framework.\nKinetix makes use of our novel hardware-accelerated physics engine Jax2D that allows us to cheaply simulate billions of environment steps during training.\nOur trained agent exhibits strong physical reasoning capabilities in 2D space, being able to zero-shot solve unseen human-designed environments.  Furthermore, fine-tuning this general agent on tasks of interest shows significantly stronger performance than training an RL agent *tabula rasa*.  This includes solving some environments that standard RL training completely fails at.\nWe believe this demonstrates the feasibility of large scale, mixed-quality pre-training for online RL and we hope that Kinetix will serve as a useful framework to investigate this further.",
      "authors": [
        "Michael Matthews",
        "Michael Beukman",
        "Chris Lu",
        "Jakob Nicolaus Foerster"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=zCxGCdzreM",
      "cdate": 1727451874126,
      "mdate": 1740906138962,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835881"
    },
    {
      "id": "4YpMrGfldX",
      "title": "Scaling Transformers for Low-Bitrate High-Quality Speech Coding",
      "abstract": "The tokenization of audio with neural audio codec models is a vital part of modern AI pipelines for the generation or understanding of speech, alone or in a multimodal context. Traditionally such tokenization models have concentrated on low parameter-count architectures using only components with strong inductive biases. In this work we show that by applying a transformer architecture with large parameter count to this problem, and applying a flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to reach state-of-the-art speech quality at extremely low bit-rates of $400$ or $700$ bits-per-second. The trained models strongly out-perform existing baselines in both objective and subjective tests.",
      "authors": [
        "Julian D Parker",
        "Anton Smirnov",
        "Jordi Pons",
        "CJ Carr",
        "Zack Zukowski",
        "Zach Evans",
        "Xubo Liu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=4YpMrGfldX",
      "cdate": 1727451818438,
      "mdate": 1741969229086,
      "matched_keywords": [
        "multimodal",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835886"
    },
    {
      "id": "ajxAJ8GUX4",
      "title": "Learning Geometric Reasoning Networks For Robot Task And Motion Planning",
      "abstract": "Task and Motion Planning (TAMP) is a computationally challenging robotics problem due to the tight coupling of discrete symbolic planning and continuous geometric planning of robot motions. In particular, planning manipulation tasks in complex 3D environments leads to a large number of costly geometric planner queries to verify the feasibility of considered actions and plan their motions. To address this issue, we propose Geometric Reasoning Networks (GRN), a graph neural network (GNN)-based model for action and grasp feasibility prediction, designed to significantly reduce the dependency on the geometric planner. Moreover, we introduce two key interpretability mechanisms: inverse kinematics (IK) feasibility prediction and grasp obstruction (GO) estimation. These modules not only improve feasibility predictions accuracy, but also explain why certain actions or grasps are infeasible, thus allowing a more efficient search for a feasible solution. Through extensive experimental results, we show that our model outperforms state-of-the-art methods, while maintaining generalizability to more complex environments, diverse object shapes, multi-robot settings, and real-world robots.",
      "authors": [
        "Smail Ait Bouhsain",
        "Rachid Alami",
        "Thierry Simeon"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=ajxAJ8GUX4",
      "cdate": 1727451567905,
      "mdate": 1740546188001,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835891"
    },
    {
      "id": "lLkgj7FEtZ",
      "title": "Differentially Private Steering for Large Language Model Alignment",
      "abstract": "Aligning Large Language Models (LLMs) with human values and away from undesirable behaviors (such as hallucination) has become increasingly important. Recently, steering LLMs towards a desired behavior via activation editing has emerged as an effective method to mitigate harmful  generations at inference-time. Activation editing modifies LLM representations by preserving information from positive demonstrations (e.g., truthful) and minimising information from negative demonstrations (e.g., hallucinations). When these demonstrations come from a private dataset, the aligned LLM may leak private information contained in those private samples. In this work, we present the first study of aligning LLM behavior with private datasets. Our work proposes the \\textit{\\underline{P}rivate \\underline{S}teering for LLM \\underline{A}lignment (PSA)} algorithm to edit LLM activations with differential privacy (DP) guarantees. We conduct extensive experiments on seven different benchmarks with open-source LLMs of different sizes (0.5B to 7B) and model families (LlaMa and Qwen). Our results show that PSA achieves DP guarantees for LLM alignment with minimal loss in performance, including alignment metrics, open-ended text generation quality, and general-purpose reasoning. We also develop the first Membership Inference Attack (MIA) for evaluating and auditing the empirical privacy for the problem of LLM steering via activation editing. Our attack is tailored for activation editing and relies solely on the generated texts without their associated probabilities. Our experiments support the theoretical guarantees by showing improved guarantees for our \\textit{PSA} algorithm compared to several existing non-private techniques.",
      "authors": [
        "Anmol Goel",
        "Yaxi Hu",
        "Iryna Gurevych",
        "Amartya Sanyal"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=lLkgj7FEtZ",
      "cdate": 1727451394311,
      "mdate": 1740577428141,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835895"
    },
    {
      "id": "9XETcRsufZ",
      "title": "Mixture of Parrots: Experts improve memorization more than reasoning",
      "abstract": "The Mixture-of-Experts (MoE) architecture enables a significant increase in the total number of model parameters with minimal computational overhead. \nHowever, it is not clear what performance tradeoffs, if any, exist between MoEs and standard dense transformers.\nIn this paper, \nwe show that as we increase the number of experts (while fixing the number of active parameters), the memorization performance consistently increases while the reasoning capabilities saturate. \n\n\nWe begin by analyzing the theoretical limitations of MoEs at reasoning. We prove that there exist graph  problems that cannot be solved by any number of experts of a certain width; however, the same task can be easily solved by a dense model with a slightly larger width. \nOn the other hand, we find that on memory-intensive tasks, MoEs can effectively leverage a small number of active parameters with a large number of experts to memorize the data. \nWe empirically validate these findings on synthetic graph problems and memory-intensive closed book retrieval tasks. \nLastly, we  pre-train a series of MoEs and dense transformers and evaluate them on commonly used benchmarks in math and natural language. \nWe find that increasing the number of experts helps solve knowledge-intensive tasks, but fails to yield the same benefits for reasoning tasks.",
      "authors": [
        "Samy Jelassi",
        "Clara Mohri",
        "David Brandfonbrener",
        "Alex Gu",
        "Nikhil Vyas",
        "Nikhil Anand",
        "David Alvarez-Melis",
        "Yuanzhi Li",
        "Sham M. Kakade",
        "eran malach"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=9XETcRsufZ",
      "cdate": 1727451371577,
      "mdate": 1740800444720,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835903"
    },
    {
      "id": "EpnZEzYDUT",
      "title": "Efficient Multi-agent Offline Coordination via Diffusion-based Trajectory Stitching",
      "abstract": "Learning from offline data without interacting with the environment is a promising way to fully leverage the intelligent decision-making capabilities of multi-agent reinforcement learning (MARL). Previous approaches have primarily focused on developing learning techniques, such as conservative methods tailored to MARL using limited offline data. However, these methods often overlook the temporal relationships across different timesteps and spatial relationships between teammates, resulting in low learning efficiency in imbalanced data scenarios. To comprehensively explore the data structure of MARL and enhance learning efficiency, we propose Multi-Agent offline coordination via Diffusion-based Trajectory Stitching (MADiTS), a novel diffusion-based data augmentation pipeline that systematically generates trajectories by stitching high-quality coordination segments together. MADiTS first generates trajectory segments using a trained diffusion model, followed by applying a bidirectional dynamics constraint to ensure that the trajectories align with environmental dynamics. Additionally, we develop an offline credit assignment technique to identify and optimize the behavior of underperforming agents in the generated segments. This iterative procedure continues until a satisfactory augmented episode trajectory is generated within the predefined limit or is discarded otherwise. Empirical results on imbalanced datasets of multiple benchmarks demonstrate that MADiTS significantly improves MARL performance.",
      "authors": [
        "Lei Yuan",
        "Yuqi Bian",
        "Lihe Li",
        "Ziqian Zhang",
        "Cong Guan",
        "Yang Yu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=EpnZEzYDUT",
      "cdate": 1727451283470,
      "mdate": 1739261743011,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835908"
    },
    {
      "id": "KRnsX5Em3W",
      "title": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations",
      "abstract": "Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as \"hallucinations\". Recent studies have demonstrated that LLMs' internal states encode information regarding the truthfulness of their outputs, and that this information can be utilized to detect errors. In this work, we show that the internal representations of LLMs encode much more information about truthfulness than previously recognized. We first discover that the truthfulness information is concentrated in specific tokens, and leveraging this property significantly enhances error detection performance. Yet, we show that such error detectors fail to generalize across datasets, implying that---contrary to prior claims---truthfulness encoding is not universal but rather multifaceted. Next, we show that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies. Lastly, we reveal a discrepancy between LLMs' internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen our understanding of LLM errors from the model's internal perspective, which can guide future research on enhancing error analysis and mitigation.",
      "authors": [
        "Hadas Orgad",
        "Michael Toker",
        "Zorik Gekhman",
        "Roi Reichart",
        "Idan Szpektor",
        "Hadas Kotek",
        "Yonatan Belinkov"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=KRnsX5Em3W",
      "cdate": 1727451109770,
      "mdate": 1747399658091,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835913"
    },
    {
      "id": "KzSGJy1PIf",
      "title": "Selective Unlearning via Representation Erasure Using Domain Adversarial Training",
      "abstract": "When deploying machine learning models in the real world,  we often face the challenge of “unlearning” specific data points or subsets after training.  Inspired by Domain-Adversarial Training of Neural Networks (DANN), we propose a novel algorithm,SURE, for targeted unlearning.SURE treats the process as a domain adaptation problem, where the “forget set” (data to be removed) and a validation set from the same distribution form two distinct domains. We train a domain classifier to discriminate between representations from the forget and validation sets.Using a gradient reversal strategy similar to DANN, we perform gradient updates to the representations to “fool” the domain classifier and thus obfuscate representations belonging to the forget set. Simultaneously, gradient descent is applied to the retain set (original training data minus the forget set) to preserve its classification performance.  Unlike other unlearning approaches whose training objectives are built based on model outputs, SURE directly manipulates the representations.This is key to ensure robustness against a set of more powerful attacks than currently considered in the literature,  that aim to detect which examples were unlearned through access to learned embeddings.  Our thorough experiments reveal that SURE has a better unlearning quality to utility trade-off compared to other standard unlearning techniques for deep neural networks.",
      "authors": [
        "Nazanin Mohammadi Sepahvand",
        "Eleni Triantafillou",
        "Hugo Larochelle",
        "Doina Precup",
        "James J. Clark",
        "Daniel M. Roy",
        "Gintare Karolina Dziugaite"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=KzSGJy1PIf",
      "cdate": 1727451076504,
      "mdate": 1740890353094,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835918"
    },
    {
      "id": "hNjCVVm0EQ",
      "title": "MamKO: Mamba-based Koopman operator for modeling and predictive control",
      "abstract": "The Koopman theory, which enables the transformation of nonlinear systems into linear representations, is a powerful and efficient tool to model and control nonlinear systems. However, the ability of the Koopman operator to model complex systems, particularly time-varying systems, is limited by the fixed linear state-space representation. To address the limitation, the large language model, Mamba, is considered a promising strategy for enhancing modeling capabilities while preserving the linear state-space structure.\nIn this paper, we propose a new framework, the Mamba-based Koopman operator (MamKO), which provides enhanced model prediction capability and adaptability, as compared to Koopman models with constant Koopman operators. Inspired by the Mamba structure, MamKO generates Koopman operators from online data; this enables the model to effectively capture the dynamic behaviors of the nonlinear system over time. A model predictive control system is then developed based on the proposed MamKO model. The modeling and control performance of the proposed method is evaluated through experiments on benchmark time-invariant and time-varying systems. The experimental results demonstrate the superiority of the proposed approach. Additionally, we perform ablation experiments to test the effectiveness of individual components of MamKO. This approach unlocks new possibilities for integrating large language models with control frameworks, and it achieves a good balance between advanced modeling capabilities and real-time control implementation efficiency.",
      "authors": [
        "ZHAOYANG LI",
        "Minghao Han",
        "Xunyuan Yin"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=hNjCVVm0EQ",
      "cdate": 1727451044070,
      "mdate": 1740141902638,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835923"
    },
    {
      "id": "4X9RpKH4Ls",
      "title": "Can Transformers Do Enumerative Geometry?",
      "abstract": "We introduce a Transformer-based approach to computational enumerative geometry, specifically targeting the computation of $\\psi$-class intersection numbers on the moduli space of curves. Traditional methods for calculating these numbers suffer from factorial computational complexity, making them impractical to use. By reformulating the problem as a continuous optimization task, we compute intersection numbers across a wide value range from $10^{-45}$ to $10^{45}$. To capture the recursive nature inherent in these intersection numbers, we propose the Dynamic Range Activator (DRA), a new activation function that enhances the Transformer's ability to model recursive patterns and handle severe heteroscedasticity. Given precision requirements for computing the intersections, we quantify the uncertainty of the predictions using Conformal Prediction with a dynamic sliding window adaptive to the partitions of equivalent number of marked points. To the best of our knowledge, there has been no prior work on modeling recursive functions with such a high-variance and factorial growth. Beyond simply computing intersection numbers, we explore the enumerative \"world-model\" of Transformers. Our interpretability analysis reveals that the network is implicitly modeling the Virasoro constraints in a purely data-driven manner. Moreover, through abductive hypothesis testing, probing, and causal inference, we uncover evidence of an emergent internal representation of the the large-genus asymptotic of $\\psi$-class intersection numbers. These findings suggest that the network internalizes the parameters of the asymptotic closed-form and the polynomiality phenomenon of $\\psi$-class intersection numbers in a non-linear manner.",
      "authors": [
        "Baran Hashemi",
        "Roderic Guigo Corominas",
        "Alessandro Giacchetto"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=4X9RpKH4Ls",
      "cdate": 1727450727377,
      "mdate": 1740824991710,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835928"
    },
    {
      "id": "dAeET8gxqg",
      "title": "Large Language Models Assume People are More Rational than We Really are",
      "abstract": "In order for AI systems to communicate effectively with people, they must understand how we make decisions. However, people's decisions are not always rational, so the implicit internal models of human decision-making in Large Language Models (LLMs) must account for this. Previous empirical evidence seems to suggest that these implicit models are accurate --- LLMs offer believable proxies of human behavior, acting how we expect humans would in everyday interactions. However, by comparing LLM behavior and predictions to a large dataset of human decisions, we find that this is actually not the case: when both simulating and predicting people's choices, a suite of cutting-edge LLMs (GPT-4o \\& 4-Turbo, Llama-3-8B \\& 70B, Claude 3 Opus) assume that people are more rational than we really are. Specifically, these models deviate from human behavior and align more closely with a classic model of rational choice --- expected value theory. Interestingly, people also tend to assume that other people are rational when interpreting their behavior. As a consequence, when we compare the inferences that LLMs and people draw from the decisions of others using another psychological dataset, we find that these inferences are highly correlated. Thus, the implicit decision-making models of LLMs appear to be aligned with the human expectation that other people will act rationally, rather than with how people actually act.",
      "authors": [
        "Ryan Liu",
        "Jiayi Geng",
        "Joshua Peterson",
        "Ilia Sucholutsky",
        "Thomas L. Griffiths"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=dAeET8gxqg",
      "cdate": 1727450678182,
      "mdate": 1740885624976,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835934"
    },
    {
      "id": "LSp4KBhAom",
      "title": "LoRA3D: Low-Rank Self-Calibration of 3D Geometric Foundation models",
      "abstract": "Emerging 3D geometric foundation models, such as DUSt3R, offer a promising approach for in-the-wild 3D vision tasks.\nHowever, due to the high-dimensional nature of the problem space and scarcity of high-quality 3D data,\nthese pre-trained models still struggle to generalize to many challenging circumstances,\nsuch as limited view overlap or low lighting.\nTo address this, we propose LoRA3D, an efficient self-calibration pipeline to *specialize* the pre-trained models to target scenes using their own multi-view predictions.\nTaking sparse RGB images as input, we leverage robust optimization techniques to refine multi-view predictions and align them into a global coordinate frame.\nIn particular, we incorporate prediction confidence into the geometric optimization process, \nautomatically re-weighting the confidence to better reflect point estimation accuracy. \nWe use the calibrated confidence to generate high-quality pseudo labels for the calibrating views and fine-tune the models using low-rank adaptation (LoRA) on the pseudo-labeled data.\nOur method does not require any external priors or manual labels. It completes the self-calibration process on a **single standard GPU within just 5 minutes**.\nEach low-rank adapter requires only **18MB** of storage. \nWe evaluated our method on **more than 160 scenes** from the Replica, TUM and Waymo Open datasets,\nachieving up to **88\\% performance improvement** on 3D reconstruction, multi-view pose estimation and novel-view rendering.\nFor more details, please visit our project page at https://520xyxyzq.github.io/lora3d/.",
      "authors": [
        "Ziqi Lu",
        "Heng Yang",
        "Danfei Xu",
        "Boyi Li",
        "Boris Ivanovic",
        "Marco Pavone",
        "Yue Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=LSp4KBhAom",
      "cdate": 1727450636377,
      "mdate": 1740638117055,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835939"
    },
    {
      "id": "BkwCrIsTbR",
      "title": "Scaling Instruction-tuned LLMs to Million-token Contexts via Hierarchical Synthetic Data Generation",
      "abstract": "Large Language Models (LLMs) struggle with long-context reasoning, not only due to the quadratic scaling of computational complexity with sequence length but also because of the scarcity and expense of annotating long-context data. There has been barely any open-source work that systematically ablates long-context data, nor is there any openly available instruction tuning dataset with contexts surpassing 100K tokens. To bridge this gap, we introduce a novel post-training synthetic data generation strategy designed to efficiently extend the context window of LLMs while preserving their general task performance. Our approach scalably extends to arbitrarily long context lengths, unconstrained by the length of available real-world data, which effectively addresses the scarcity of raw long-context data. \nThrough a step-by-step rotary position embedding (RoPE) scaling training strategy, we demonstrate that our model, with a context length of up to 1M tokens, performs well on the RULER benchmark and InfiniteBench and maintains robust performance on general language tasks.",
      "authors": [
        "Linda He",
        "Jue WANG",
        "Maurice Weber",
        "Shang Zhu",
        "Ben Athiwaratkun",
        "Ce Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=BkwCrIsTbR",
      "cdate": 1727450570710,
      "mdate": 1741032896128,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.835947"
    },
    {
      "id": "uSz2K30RRd",
      "title": "Weighted Point Set Embedding for Multimodal Contrastive Learning Toward Optimal Similarity Metric",
      "abstract": "In typical multimodal contrastive learning, such as CLIP, encoders produce one\npoint in the latent representation space for each input. However, one-point representation\nhas difficulty in capturing the relationship and the similarity structure of a\nhuge amount of instances in the real world. For richer classes of the similarity, we\npropose the use of weighted point sets, namely, sets of pairs of weight and vector,\nas representations of instances. In this work, we theoretically show the benefit\nof our proposed method through a new understanding of the contrastive loss of\nCLIP, which we call symmetric InfoNCE. We clarify that the optimal similarity\nthat minimizes symmetric InfoNCE is the pointwise mutual information, and show\nan upper bound of excess risk on downstream classification tasks of representations\nthat achieve the optimal similarity. In addition, we show that our proposed\nsimilarity based on weighted point sets consistently achieves the optimal similarity.\nTo verify the effectiveness of our proposed method, we demonstrate pretraining of\ntext-image representation models and classification tasks on common benchmarks.",
      "authors": [
        "Toshimitsu Uesaka",
        "Taiji Suzuki",
        "Yuhta Takida",
        "Chieh-Hsin Lai",
        "Naoki Murata",
        "Yuki Mitsufuji"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=uSz2K30RRd",
      "cdate": 1727450487662,
      "mdate": 1743146902713,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.835953"
    },
    {
      "id": "C8jXEugWkq",
      "title": "EqNIO: Subequivariant Neural Inertial Odometry",
      "abstract": "Neural network-based odometry using accelerometer and gyroscope readings from a single IMU can achieve robust, and low-drift localization capabilities, through the use of _neural displacement priors (NDPs)_. These priors learn to produce denoised displacement measurements but need to ignore data variations due to specific IMU mount orientation and motion directions, hindering generalization.\nThis work introduces EqNIO, which addresses this challenge with _canonical displacement priors_, i.e., priors that are invariant to the orientation of the gravity-aligned frame in which the IMU data is expressed. We train such priors on IMU measurements, that are mapped into a learnable canonical frame, which is uniquely defined via three axes: the first is gravity, making the frame gravity aligned, while the second and third are predicted from IMU data.  The outputs (displacement and covariance) are mapped back to the original gravity-aligned frame. To maximize generalization, we find that these learnable frames must transform equivariantly with global gravity-preserving roto-reflections from the subgroup $O_g(3)\\subset O(3)$, acting on the trajectory, rendering the NDP $O(3)$-_subequivariant_. We tailor specific linear, convolutional, and non-linear layers that commute with the actions of the group. Moreover, we introduce a bijective decomposition of angular rates into vectors that transform similarly to accelerations, allowing us to leverage both measurement types. Natively, angular rates would need to be inverted upon reflection, unlike acceleration, which hinders their joint processing. We highlight EqNIO's flexibility and generalization capabilities by applying it to both filter-based (TLIO), and end-to-end (RONIN) architectures, and outperforming existing methods that use _soft equivariance from auxiliary losses or data augmentation on various datasets. We believe this work paves the way for low-drift and generalizable neural inertial odometry on edge devices. The project details and code can be found at [https://github.com/RoyinaJayanth/EqNIO](https://github.com/RoyinaJayanth/EqNIO).",
      "authors": [
        "Royina Karegoudra Jayanth",
        "Yinshuang Xu",
        "Ziyun Wang",
        "Evangelos Chatzipantazis",
        "Kostas Daniilidis",
        "Daniel Gehrig"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=C8jXEugWkq",
      "cdate": 1727450484790,
      "mdate": 1741995662021,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835961"
    },
    {
      "id": "X6y5CC44HM",
      "title": "MANTRA: The Manifold Triangulations Assemblage",
      "abstract": "The rising interest in leveraging higher-order interactions present in complex systems has\nled to a surge in more expressive models exploiting higher-order structures in the data,\nespecially in topological deep learning (TDL), which designs neural networks on higher-order domains such as simplicial complexes. However, progress in this field is hindered\nby the scarcity of datasets for benchmarking these architectures. To address this gap, we\nintroduce MANTRA, the first large-scale, diverse, and intrinsically higher-order dataset for\nbenchmarking higher-order models, comprising over 43,000 and 250,000 triangulations\nof surfaces and three-dimensional manifolds, respectively. With MANTRA, we assess\nseveral graph- and simplicial complex-based models on three topological classification\ntasks. We demonstrate that while simplicial complex-based neural networks generally\noutperform their graph-based counterparts in capturing simple topological invariants, they\nalso struggle, suggesting a rethink of TDL. Thus, MANTRA serves as a benchmark for\nassessing and advancing topological methods, paving the way towards more effective\nhigher-order models.",
      "authors": [
        "Rubén Ballester",
        "Ernst Röell",
        "Daniel Bin Schmid",
        "Mathieu Alain",
        "Sergio Escalera",
        "Carles Casacuberta",
        "Bastian Rieck"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=X6y5CC44HM",
      "cdate": 1727450477533,
      "mdate": 1746604089478,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835966"
    },
    {
      "id": "CL3U0GxFRD",
      "title": "Exponential Topology-enabled Scalable Communication in Multi-agent Reinforcement Learning",
      "abstract": "In cooperative multi-agent reinforcement learning (MARL), well-designed communication protocols can effectively facilitate consensus among agents, thereby enhancing task performance. Moreover, in large-scale multi-agent systems commonly found in real-world applications, effective communication plays an even more critical role due to the escalated challenge of partial observability compared to smaller-scale setups. In this work, we endeavor to develop a scalable communication protocol for MARL. Unlike previous methods that focus on selecting optimal pairwise communication links—a task that becomes increasingly complex as the number of agents grows—we adopt a global perspective on communication topology design. Specifically, we propose utilizing the exponential topology to enable rapid information dissemination among agents by leveraging its small-diameter and small-size properties. This approach leads to a scalable communication protocol, named ExpoComm. To fully unlock the potential of exponential graphs as communication topologies, we employ memory-based message processors and auxiliary tasks to ground messages, ensuring that they reflect global information and benefit decision-making. Extensive experiments on large-scale cooperative benchmarks, including MAgent and Infrastructure Management Planning, demonstrate the superior performance and robust zero-shot transferability of ExpoComm compared to existing communication strategies. The\ncode is publicly available at [https://github.com/LXXXXR/ExpoComm](https://github.com/LXXXXR/ExpoComm).",
      "authors": [
        "Xinran Li",
        "Xiaolu Wang",
        "Chenjia Bai",
        "Jun Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=CL3U0GxFRD",
      "cdate": 1727450271986,
      "mdate": 1740623324597,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.835971"
    },
    {
      "id": "a3g2l4yEys",
      "title": "Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages",
      "abstract": "Despite recent advances in multimodal large language models (MLLMs), their development has predominantly focused on English- and western-centric datasets and tasks, leaving most of the world's languages and diverse cultural contexts underrepresented.  \nThis paper introduces PANGEA, a multilingual multimodal LLM trained on PANGEAINS, a diverse 6M instruction dataset spanning 39 languages. PANGEAINS features: 1) high-quality English instructions, 2) carefully machine-translated instructions, and 3) culturally relevant multimodal tasks to ensure cross-cultural coverage. \nTo rigorously assess models' capabilities, we introduce PANGEABENCH, a holistic evaluation suite encompassing 14 datasets covering 47 languages. \nResults show that PANGEA significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts. Ablation studies further reveal the importance of English data proportions, language popularity, and the number of multimodal training samples on overall performance.  We fully open-source our data, code, and trained checkpoints, to facilitate the development of inclusive and robust multilingual MLLMs, promoting equity and accessibility across a broader linguistic and cultural spectrum.",
      "authors": [
        "Xiang Yue",
        "Yueqi Song",
        "Akari Asai",
        "Seungone Kim",
        "Jean de Dieu Nyandwi",
        "Simran Khanuja",
        "Anjali Kantharuban",
        "Lintang Sutawika",
        "Sathyanarayanan Ramamoorthy",
        "Graham Neubig"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=a3g2l4yEys",
      "cdate": 1727450239711,
      "mdate": 1740890352873,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.835977"
    },
    {
      "id": "0LSAmFCc4p",
      "title": "Brain-inspired $L_p$-Convolution benefits large kernels and aligns better with visual cortex",
      "abstract": "Convolutional Neural Networks (CNNs) have profoundly influenced the field of computer vision, drawing significant inspiration from the visual processing mechanisms inherent in the brain. Despite sharing fundamental structural and representational similarities with the biological visual system, differences in local connectivity patterns within CNNs open up an interesting area to explore. In this work, we explore whether integrating biologically observed receptive fields (RFs) can enhance model performance and foster alignment with brain representations. We introduce a novel methodology, termed $L_p$-convolution, which employs the multivariate $L_p$-generalized normal distribution as an adaptable $L_p$-masks, to reconcile disparities between artificial and biological RFs. $L_p$-masks finds the optimal RFs through task-dependent adaptation of conformation such as distortion, scale, and rotation. This allows $L_p$-convolution to excel in tasks that require flexible RF shapes, including not only square-shaped regular RFs but also horizontal and vertical ones. Furthermore, we demonstrate that $L_p$-convolution with biological RFs significantly enhances the performance of large kernel CNNs possibly by introducing structured sparsity inspired by $L_p$-generalized normal distribution in convolution. Lastly, we present that neural representations of CNNs align more closely with the visual cortex when -convolution is close to biological RFs.",
      "authors": [
        "Jea Kwon",
        "Sungjun Lim",
        "Kyungwoo Song",
        "C. Justin Lee"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=0LSAmFCc4p",
      "cdate": 1727450171209,
      "mdate": 1739261741081,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835982"
    },
    {
      "id": "5M0ic2RxQZ",
      "title": "dEBORA: Efficient Bilevel Optimization-based low-Rank Adaptation",
      "abstract": "Low-rank adaptation methods are a popular approach for parameter-efficient fine-tuning of large-scale neural networks. However, selecting the optimal rank for each layer remains a challenging problem that significantly affects both performance and efficiency. In this paper, we introduce a novel bilevel optimization strategy that simultaneously trains both matrix and tensor low-rank adapters, dynamically selecting the optimal rank for each layer. Our method avoids the use of implicit differentiation in the computation of the hypergradient, and integrates a stochastic away-step variant of the Frank-Wolfe algorithm, eliminating the need for projection and providing identifiability guarantees of the optimal rank structure. This results in a highly efficient and cost-effective training scheme that adaptively allocates the parameter budget across the network layers. On top of a detailed theoretical analysis of the method, we provide different numerical experiments showcasing its effectiveness.",
      "authors": [
        "Emanuele Zangrando",
        "Sara Venturini",
        "Francesco Rinaldi",
        "Francesco Tudisco"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=5M0ic2RxQZ",
      "cdate": 1727450058303,
      "mdate": 1740236033186,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835987"
    },
    {
      "id": "cCl10IU836",
      "title": "Interaction Asymmetry: A General Principle for Learning Composable Abstractions",
      "abstract": "Learning disentangled representations of concepts and re-composing them in unseen ways is crucial for generalizing to out-of-domain situations. However, the underlying properties of concepts that enable such disentanglement and compositional generalization remain poorly understood. In this work, we propose the principle of interaction asymmetry which states: \"Parts of the same concept have more complex interactions than parts of different concepts\". We formalize this via block diagonality conditions on the $(n+1)$th order derivatives of the generator mapping concepts to observed data, where different orders of \"complexity\" correspond to different $n$. Using this formalism, we prove that interaction asymmetry enables both disentanglement and compositional generalization. Our results unify recent theoretical results for learning concepts of objects, which we show are recovered as special cases with $n=0$ or $1$. We provide results for up to $n=2$, thus extending these prior works to more flexible generator functions, and conjecture that the same proof strategies generalize to larger $n$. Practically, our theory suggests that, to disentangle concepts, an autoencoder should penalize its latent capacity and the interactions between concepts during decoding. We propose an implementation of these criteria using a flexible Transformer-based VAE, with a novel regularizer on the attention weights of the decoder. On synthetic image datasets consisting of objects, we provide evidence that this model can achieve comparable object disentanglement to existing models that use more explicit object-centric priors.",
      "authors": [
        "Jack Brady",
        "Julius von Kügelgen",
        "Sebastien Lachapelle",
        "Simon Buchholz",
        "Thomas Kipf",
        "Wieland Brendel"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=cCl10IU836",
      "cdate": 1727450045982,
      "mdate": 1741715281372,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.835992"
    },
    {
      "id": "Ge7okBGZYi",
      "title": "How Learnable Grids Recover Fine Detail in Low Dimensions: A Neural Tangent Kernel Analysis of Multigrid Parametric Encodings",
      "abstract": "Neural networks that map between low dimensional spaces are ubiquitous in\ncomputer graphics and scientific computing; however, in their naive\nimplementation, they are unable to learn high frequency information. We present\na comprehensive analysis comparing the two most common techniques for mitigating\nthis spectral bias: Fourier feature encodings (FFE) and multigrid parametric\nencodings (MPE). FFEs are seen as the standard for low dimensional mappings, but\nMPEs often outperform them and learn representations with higher resolution and\nfiner detail. FFE's roots in the Fourier transform, make it susceptible to\naliasing if pushed too far, while MPEs, which use a learned grid structure, have\nno such limitation. To understand the difference in performance, we use the\nneural tangent kernel (NTK) to evaluate these encodings through the lens of an\nanalogous kernel regression. By finding a lower bound on the smallest eigenvalue\nof the NTK, we prove that MPEs improve a network's performance through the\nstructure of their grid and not their learnable embedding. This mechanism is\nfundamentally different from FFEs, which rely solely on their embedding space to\nimprove performance. Results are empirically validated on a 2D image regression\ntask using images taken from 100 synonym sets of ImageNet and 3D implicit\nsurface regression on objects from the Stanford graphics dataset. Using peak\nsignal-to-noise ratio (PSNR) and multiscale structural similarity (MS-SSIM) to\nevaluate how well fine details are learned, we show that the MPE increases the\nminimum eigenvalue by 8 orders of magnitude over the baseline and 2 orders of\nmagnitude over the FFE. The increase in spectrum corresponds to a 15 dB (PSNR) /\n0.65 (MS-SSIM) increase over baseline and a 12 dB (PSNR) / 0.33 (MS-SSIM) increase over the\nFFE.",
      "authors": [
        "Samuel Audia",
        "Soheil Feizi",
        "Matthias Zwicker",
        "Dinesh Manocha"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Ge7okBGZYi",
      "cdate": 1727449816851,
      "mdate": 1740890352752,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.835997"
    },
    {
      "id": "gJG4IPwg6l",
      "title": "Safety Representations for Safer Policy Learning",
      "abstract": "Reinforcement learning algorithms typically necessitate extensive exploration of the state space to find optimal policies. However, in safety-critical applications, the risks associated with such exploration can lead to catastrophic consequences. Existing safe exploration methods attempt to mitigate this by imposing constraints, which often result in overly conservative behaviours and inefficient learning. Heavy penalties for early constraint violations can trap agents in local optima, deterring exploration of risky yet high-reward regions of the state space. To address this, we introduce a method that explicitly learns state-conditioned safety representations. By augmenting the state features with these safety representations, our approach naturally encourages safer exploration without being excessively cautious, resulting in more efficient and safer policy learning in safety-critical scenarios. Empirical evaluations across diverse environments show that our method significantly improves task performance while reducing constraint violations during training, underscoring its effectiveness in balancing exploration with safety.",
      "authors": [
        "Kaustubh Mani",
        "Vincent Mai",
        "Charlie Gauthier",
        "Annie S Chen",
        "Samer B. Nashed",
        "Liam Paull"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=gJG4IPwg6l",
      "cdate": 1727449562030,
      "mdate": 1741014818891,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836002"
    },
    {
      "id": "UmdotAAVDe",
      "title": "Combining Induction and Transduction for Abstract Reasoning",
      "abstract": "When learning an input-output mapping from very few examples, is it better to first infer a latent function that explains the examples, or is it better to directly predict new test outputs, e.g. using a neural network? We study this question on ARC by training neural models for \\emph{induction} (inferring latent functions) and \\emph{transduction} (directly predicting the test output for a given test input). We train \non synthetically generated variations of Python programs that solve ARC training tasks. We find inductive and transductive models solve different kinds of test problems, despite having the same training problems and sharing the same neural architecture: Inductive program synthesis excels at precise computations, and at composing multiple concepts, while transduction succeeds on fuzzier perceptual concepts. Ensembling them approaches human-level performance on ARC.",
      "authors": [
        "Wen-Ding Li",
        "Keya Hu",
        "Carter Larsen",
        "Yuqing Wu",
        "Simon Alford",
        "Caleb Woo",
        "Spencer M. Dunn",
        "Hao Tang",
        "Wei-Long Zheng",
        "Yewen Pu",
        "Kevin Ellis"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=UmdotAAVDe",
      "cdate": 1727449549771,
      "mdate": 1740579462986,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836008"
    },
    {
      "id": "cRnCcuLvyr",
      "title": "CViT: Continuous Vision Transformer for Operator Learning",
      "abstract": "Operator learning, which aims to approximate maps between infinite-dimensional function spaces, is an important area in scientific machine learning with applications across various physical domains. Here we introduce the Continuous Vision Transformer (CViT), a novel neural operator architecture that leverages advances in computer vision to address challenges in learning complex physical systems.  CViT combines a vision transformer encoder, a novel grid-based coordinate embedding, and a query-wise cross-attention mechanism to effectively capture multi-scale dependencies. This design allows for flexible output representations and consistent evaluation at arbitrary resolutions. We demonstrate CViT's effectiveness across a diverse range of partial differential equation (PDE) systems, including fluid dynamics, climate modeling, and reaction-diffusion processes. Our comprehensive experiments show that CViT achieves state-of-the-art performance on multiple benchmarks, often surpassing larger foundation models, even without extensive pretraining and roll-out fine-tuning. Taken together, CViT exhibits robust handling of discontinuous solutions, multi-scale features, and intricate spatio-temporal dynamics. Our contributions can be viewed as a significant step towards adapting advanced computer vision architectures for building more flexible and accurate machine learning models in the physical sciences.",
      "authors": [
        "Sifan Wang",
        "Jacob H Seidman",
        "Shyam Sankaran",
        "Hanwen Wang",
        "George J. Pappas",
        "Paris Perdikaris"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=cRnCcuLvyr",
      "cdate": 1727449396128,
      "mdate": 1740783039451,
      "matched_keywords": [
        "foundation model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836016"
    },
    {
      "id": "ThhQyIruEs",
      "title": "MGCFNN: A Neural MultiGrid Solver with Novel Fourier Neural Network for High Wave Number Helmholtz Equations",
      "abstract": "Solving high wavenumber Helmholtz equations is notoriously challenging. Traditional solvers have yet to yield satisfactory results, and most neural network methods struggle to accurately solve cases with extremely high wavenumbers within heterogeneous media. This paper presents an advanced multigrid-hierarchical AI solver, tailored specifically for high wavenumber Helmholtz equations. We adapt the MGCNN architecture to align with the problem setting and incorporate a novel Fourier neural network (FNN) to match the characteristics of Helmholtz equations. FNN, mathematically akin to the convolutional neural network (CNN), enables faster propagation of source influence during the solve phase, making it particularly suitable for handling large size, high wavenumber problems. We conduct supervised learning tests against numerous neural operator learning methods to demonstrate the superior learning capabilities of our solvers. Additionally, we perform scalability tests using an unsupervised strategy to highlight our solvers' significant speedup over the most recent specialized AI solver and AI-enhanced traditional solver for high wavenumber Helmholtz equations. We also carry out an ablation study to underscore the effectiveness of the multigrid hierarchy and the benefits of introducing FNN. Notably, our solvers exhibit optimal convergence of $\\mathcal{O}(k)$ up to $k \\approx 2000$.",
      "authors": [
        "Yan Xie",
        "Minrui Lv",
        "Chen-Song Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=ThhQyIruEs",
      "cdate": 1727449048529,
      "mdate": 1746193402427,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836025"
    },
    {
      "id": "YslOW2SO6S",
      "title": "CirT: Global Subseasonal-to-Seasonal Forecasting with Geometry-inspired Transformer",
      "abstract": "Accurate Subseasonal-to-Seasonal (S2S) climate forecasting is pivotal for decision-making including agriculture planning and disaster preparedness but is known to be challenging due to its chaotic nature. Although recent data-driven models have shown promising results, their performance is limited by inadequate consideration of geometric inductive biases. Usually, they treat the spherical weather data as planar images, resulting in an inaccurate representation of locations and spatial relations. In this work, we propose the geometric-inspired Circular Transformer (CirT) to model the cyclic characteristic of the graticule, consisting of two key designs: (1) Decomposing the weather data by latitude into circular patches that serve as input tokens to the Transformer; (2) Leveraging Fourier transform in self-attention to capture the global information and model the spatial periodicity. Extensive experiments on the Earth Reanalysis 5 (ERA5) reanalysis dataset demonstrate our model yields a significant improvement over the advanced data-driven models, including PanguWeather and GraphCast, as well as skillful ECMWF systems. Additionally, we empirically show the effectiveness of our model designs and high-quality prediction over spatial and temporal dimensions.",
      "authors": [
        "Yang Liu",
        "Zinan Zheng",
        "Jiashun Cheng",
        "Fugee Tsung",
        "Deli Zhao",
        "Yu Rong",
        "Jia Li"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=YslOW2SO6S",
      "cdate": 1727449004098,
      "mdate": 1740626187606,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836030"
    },
    {
      "id": "CtM5xjRSfm",
      "title": "Accelerating neural network training: An analysis of the AlgoPerf competition",
      "abstract": "The goal of the AlgoPerf: Training Algorithms competition is to evaluate practical speed-ups in neural network training achieved solely by improving the underlying training algorithms. In the external tuning ruleset, submissions must provide workload-agnostic hyperparameter search spaces, while in the self-tuning ruleset they must be completely hyperparameter-free. In both rulesets, submissions are compared on time-to-result across multiple deep learning workloads, training on fixed hardware. This paper presents the inaugural AlgoPerf competition's results, which drew 18 diverse submissions from 10 teams. Our investigation reveals several key findings: (1) The winning submission in the external tuning ruleset, using Distributed Shampoo, demonstrates the effectiveness of non-diagonal preconditioning over popular methods like Adam, even when compared on wall-clock runtime. (2) The winning submission in the self-tuning ruleset, based on the Schedule Free AdamW algorithm, demonstrates a new level of effectiveness for completely hyperparameter-free training algorithms. (3) The top-scoring submissions were surprisingly robust to workload changes. We also discuss the engineering challenges encountered in ensuring a fair comparison between different training algorithms. These results highlight both the significant progress so far, and the considerable room for further improvements.",
      "authors": [
        "Priya Kasimbeg",
        "Frank Schneider",
        "Runa Eschenhagen",
        "Juhan Bae",
        "Chandramouli Shama Sastry",
        "Mark Saroufim",
        "BOYUAN FENG",
        "Less Wright",
        "Edward Z. Yang",
        "Zachary Nado",
        "Sourabh Medapati",
        "Philipp Hennig",
        "Michael Rabbat",
        "George E. Dahl"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=CtM5xjRSfm",
      "cdate": 1727448977290,
      "mdate": 1744101788987,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836035"
    },
    {
      "id": "3IFRygQKGL",
      "title": "OptionZero: Planning with Learned Options",
      "abstract": "Planning with options -- a sequence of primitive actions -- has been shown effective in reinforcement learning within complex environments. Previous studies have focused on planning with predefined options or learned options through expert demonstration data.\nInspired by MuZero, which learns superhuman heuristics without any human knowledge, we propose a novel approach, named *OptionZero*. OptionZero incorporates an *option network* into MuZero, providing autonomous discovery of options through self-play games. Furthermore, we modify the dynamics network to provide environment transitions when using options, allowing searching deeper under the same simulation constraints. Empirical experiments conducted in 26 Atari games demonstrate that OptionZero outperforms MuZero, achieving a 131.58% improvement in mean human-normalized score. Our behavior analysis shows that OptionZero not only learns options but also acquires strategic skills tailored to different game characteristics. Our findings show promising directions for discovering and using options in planning. Our code is available at https://rlg.iis.sinica.edu.tw/papers/optionzero.",
      "authors": [
        "Po-Wei Huang",
        "Pei-Chiun Peng",
        "Hung Guei",
        "Ti-Rong Wu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=3IFRygQKGL",
      "cdate": 1727448905683,
      "mdate": 1746276004049,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836040"
    },
    {
      "id": "ispjankYab",
      "title": "Action abstractions for amortized sampling",
      "abstract": "As trajectories sampled by policies used by reinforcement learning (RL) and generative flow networks (GFlowNets) grow longer, credit assignment and exploration become more challenging, and the long planning horizon hinders mode discovery and generalization.\nThe challenge is particularly pronounced in entropy-seeking RL methods, such as generative flow networks, where the agent must learn to sample from a structured distribution and discover multiple high-reward states, each of which take many steps to reach.\nTo tackle this challenge, we propose an approach to incorporate the discovery of action abstractions, or high-level actions, into the policy optimization process.\nOur approach involves iteratively extracting action subsequences commonly used across many high-reward trajectories and `chunking' them into a single action that is added to the action space.\nIn empirical evaluation on synthetic and real-world environments, our approach demonstrates improved sample efficiency performance in discovering diverse high-reward objects, especially on harder exploration problems.\nWe also observe that the abstracted high-order actions are potentially interpretable, capturing the latent structure of the reward landscape of the action space.\nThis work provides a cognitively motivated approach to action abstraction in RL and is the first demonstration of hierarchical planning in amortized sequential sampling.",
      "authors": [
        "Oussama Boussif",
        "Lena Nehale Ezzine",
        "Joseph D Viviano",
        "Michał Koziarski",
        "Moksh Jain",
        "Nikolay Malkin",
        "Emmanuel Bengio",
        "Rim Assouel",
        "Yoshua Bengio"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=ispjankYab",
      "cdate": 1727448511185,
      "mdate": 1740685462605,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836046"
    },
    {
      "id": "TDy5Ih78b4",
      "title": "Provence: efficient and robust context pruning for retrieval-augmented generation",
      "abstract": "Retrieval-Augmented Generation improves various aspects of large language models (LLMs) generation,  but suffers from computational overhead caused by long contexts, and the propagation of irrelevant retrieved information into generated responses. Context pruning deals with both aspects, by removing irrelevant parts of retrieved contexts before LLM generation. Existing context pruning approaches are limited, and do not present a universal model that would be both _efficient_ and _robust_ in a wide range of scenarios, e.g., when contexts contain a variable amount of relevant information or vary in length, or when evaluated on various domains. In this work, we close this gap and introduce Provence (Pruning and Reranking Of retrieVEd relevaNt ContExts), an efficient and robust context pruner for Question Answering, which dynamically detects the needed amount of pruning for a given context and can be used out-of-the-box for various domains. The three key ingredients of  Provence are formulating the context pruning task as sequence labeling, unifying context pruning capabilities with context reranking, and training on diverse data. Our experimental results show that Provence enables context pruning with negligible to no drop in performance, in various domains and settings, at almost no cost in a standard RAG pipeline. We also conduct a deeper analysis alongside various ablations to provide insights into training context pruners for future work.",
      "authors": [
        "Nadezhda Chirkova",
        "Thibault Formal",
        "Vassilina Nikoulina",
        "Stéphane CLINCHANT"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=TDy5Ih78b4",
      "cdate": 1727448455097,
      "mdate": 1747557616671,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836051"
    },
    {
      "id": "kN25ggeq1J",
      "title": "Unveiling the Magic of Code Reasoning through Hypothesis Decomposition and Amendment",
      "abstract": "The reasoning abilities are one of the most enigmatic and captivating aspects of large language models (LLMs). Numerous studies are dedicated to exploring and expanding the boundaries of this reasoning capability. However, tasks that embody both reasoning and recall characteristics are often overlooked. In this paper, we introduce such a novel task, code reasoning, to provide a new perspective for the reasoning abilities of LLMs.\nWe summarize three meta-benchmarks based on established forms of logical reasoning, and instantiate these into eight specific benchmark tasks. Our testing on these benchmarks reveals that LLMs continue to struggle with identifying satisfactory reasoning pathways. Additionally, we present a new pathway exploration pipeline inspired by human intricate problem-solving methods. This Reflective Hypothesis Decomposition and Amendment (RHDA) pipeline consists of the following iterative steps: (1) Proposing potential hypotheses based on observations and decomposing them; (2) Utilizing tools to validate hypotheses and reflection outcomes; (3) Revising hypothesis in light of observations. Our approach effectively mitigates logical chain collapses arising from forgetting or hallucination issues in multi-step reasoning, resulting in performance gains of up to $3\\times$. Finally, we expanded this pipeline by applying it to simulate complex household tasks in real-world scenarios, specifically in VirtualHome, enhancing the handling of failure cases. We release our code and all of results at https://github.com/TnTWoW/code_reasoning.",
      "authors": [
        "Yuze Zhao",
        "Tianyun Ji",
        "Wenjun Feng",
        "Zhenya Huang",
        "Qi Liu",
        "Zhiding Liu",
        "Yixiao Ma",
        "Kai Zhang",
        "Enhong Chen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=kN25ggeq1J",
      "cdate": 1727448217151,
      "mdate": 1740537560911,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836056"
    },
    {
      "id": "eaTqsptDPL",
      "title": "Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning",
      "abstract": "Large-scale deep learning models with a pretraining-finetuning paradigm have led to a surge of numerous task-specific models fine-tuned from a common pre-trained model.\nRecently, several research efforts have been made on merging these large models into a single multi-task model, particularly with simple arithmetic on parameters.\nSuch merging methodology faces a central challenge: interference between model parameters fine-tuned on different tasks.\nFew recent works have focused on designing a new fine-tuning scheme that can lead to small parameter interference, however at the cost of the performance of each task-specific fine-tuned model and thereby limiting that of a merged model.\nTo improve the performance of a merged model, we note that a fine-tuning scheme should aim for (1) smaller parameter interference and (2) better performance of each fine-tuned model on the corresponding task.\nIn this work, we aim to design a new fine-tuning objective function to work towards these two goals.\nIn the course of this process, we find such objective function to be strikingly similar to sharpness-aware minimization (SAM) objective function, which aims to achieve generalization by finding flat minima.\nDrawing upon our observation, we propose to fine-tune pre-trained models via sharpness-aware minimization.\nThe experimental and theoretical results showcase the effectiveness and orthogonality of our proposed approach, improving performance upon various merging and fine-tuning methods.\nOur code is available at https://github.com/baiklab/SAFT-Merge.",
      "authors": [
        "Yeoreum Lee",
        "Jinwook Jung",
        "Sungyong Baik"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=eaTqsptDPL",
      "cdate": 1727448190858,
      "mdate": 1743000978931,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836061"
    },
    {
      "id": "je3GZissZc",
      "title": "Instant Policy: In-Context Imitation Learning via Graph Diffusion",
      "abstract": "Following the impressive capabilities of in-context learning with large transformers, In-Context Imitation Learning (ICIL) is a promising opportunity for robotics. We introduce Instant Policy, which learns new tasks instantly from just one or two demonstrations, achieving ICIL through two key components. First, we introduce inductive biases through a graph representation and model ICIL as a graph generation problem using a learned diffusion process, enabling structured reasoning over demonstrations, observations, and actions. Second, we show that such a model can be trained using pseudo-demonstrations – arbitrary trajectories generated in simulation – as a virtually infinite pool of training data. Our experiments, in both simulation and reality, show that Instant Policy enables rapid learning of various everyday robot tasks. We also show how it can serve as a foundation for cross-embodiment and zero-shot transfer to language-defined tasks.",
      "authors": [
        "Vitalis Vosylius",
        "Edward Johns"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=je3GZissZc",
      "cdate": 1727448136536,
      "mdate": 1740769248403,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836066"
    },
    {
      "id": "sLKDbuyq99",
      "title": "Flow: Modularized Agentic Workflow Automation",
      "abstract": "Multi-agent frameworks powered by large language models (LLMs) have demonstrated great success in automated planning and task execution. However, the effective adjustment of agentic workflows during execution has not been well studied. An effective workflow adjustment is crucial in real-world scenarios, as the initial plan must adjust to unforeseen challenges and changing conditions in real time to ensure the efficient execution of complex tasks. In this paper, we define workflows as an activity-on-vertex (AOV) graph, which allows continuous workflow refinement by LLM agents through dynamic subtask allocation adjustment based on historical performance and previous AOVs. To further enhance framework performance, we emphasize modularity in workflow design based on evaluating parallelism and dependency complexity. With this design, our proposed multi-agent framework achieves efficient concurrent execution of subtasks, effective goal achievement, and enhanced error tolerance. Empirical results across various practical tasks demonstrate significant improvements in the efficiency of multi-agent frameworks through dynamic workflow refinement and modularization.",
      "authors": [
        "Boye Niu",
        "Yiliao Song",
        "Kai Lian",
        "Yifan Shen",
        "Yu Yao",
        "Kun Zhang",
        "Tongliang Liu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=sLKDbuyq99",
      "cdate": 1727447902478,
      "mdate": 1747460947677,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836074"
    },
    {
      "id": "qnlG3zPQUy",
      "title": "ILLUSION: Unveiling Truth with a Comprehensive Multi-Modal, Multi-Lingual Deepfake Dataset",
      "abstract": "The proliferation of deepfakes and AI-generated content has led to a surge in media forgeries and misinformation, necessitating robust detection systems. However, current datasets lack diversity across modalities, languages, and real-world scenarios. To address this gap, we present ILLUSION (Integration of Life-Like Unique Synthetic Identities and Objects from Neural Networks), a large-scale, multi-modal\ndeepfake dataset comprising 1.3 million samples spanning audio-visual forgeries, 26 languages, challenging noisy environments, and various manipulation protocols. Generated using 28 state-of-the-art generative techniques, ILLUSION includes\nfaceswaps, audio spoofing, synchronized audio-video manipulations, and synthetic media while ensuring a balanced representation of gender and skin tone for unbiased evaluation. Using Jaccard Index and UpSet plot analysis, we demonstrate ILLUSION’s distinctiveness and minimal overlap with existing datasets, emphasizing its novel generative coverage. We benchmarked image, audio, video, and multi-modal detection models, revealing key challenges such as performance degradation in multilingual and multi-modal contexts, vulnerability to real-world distortions, and limited generalization to zero-day attacks. By bridging synthetic and real-world complexities, ILLUSION provides a challenging yet essential platform for advancing deepfake detection research. The dataset is publicly available at https://www.iab-rubric.org/illusion-database.",
      "authors": [
        "Kartik Thakral",
        "Rishabh Ranjan",
        "Akanksha Singh",
        "Akshat Jain",
        "Mayank Vatsa",
        "Richa Singh"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=qnlG3zPQUy",
      "cdate": 1727447755038,
      "mdate": 1742482797322,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836081"
    },
    {
      "id": "ftGnpZrW7P",
      "title": "Gramian Multimodal Representation Learning and Alignment",
      "abstract": "Human perception integrates multiple modalities—such as vision, hearing, and language—into a unified understanding of the surrounding reality. While recent multimodal models have achieved significant progress by aligning pairs of modalities via contrastive learning, their solutions are unsuitable when scaling to multiple modalities. These models typically align each modality to a designated anchor without ensuring the alignment of all modalities with each other, leading to suboptimal performance in tasks requiring a joint understanding of multiple modalities. In this paper, we structurally rethink the pairwise conventional approach to multimodal learning and we present the novel Gramian Representation Alignment Measure (GRAM), which overcomes the above-mentioned limitations. GRAM learns and then aligns $n$ modalities directly in the higher-dimensional space in which modality embeddings lie by minimizing the Gramian volume of the $k$-dimensional parallelotope spanned by the modality vectors, ensuring the geometric alignment of all modalities simultaneously. GRAM can replace cosine similarity in any downstream method, holding for 2 to $n$ modalities and providing more meaningful alignment with respect to previous similarity measures. The novel GRAM-based contrastive loss function enhances the alignment of multimodal models in the higher-dimensional embedding space, leading to new state-of-the-art performance in downstream tasks such as video-audio-text retrieval and audio-video classification.",
      "authors": [
        "Giordano Cicchetti",
        "Eleonora Grassucci",
        "Luigi Sigillo",
        "Danilo Comminiello"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=ftGnpZrW7P",
      "cdate": 1727447741647,
      "mdate": 1739366397482,
      "matched_keywords": [
        "multimodal",
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836089"
    },
    {
      "id": "wkbx7BRAsM",
      "title": "Video In-context Learning: Autoregressive Transformers are Zero-Shot Video Imitators",
      "abstract": "People interact with the real-world largely dependent on visual signal, which are ubiquitous and illustrate detailed demonstrations. In this paper, we explore utilizing visual signals as a new interface for models to interact with the environment. Specifically, we choose videos as a representative visual signal. And by training autoregressive Transformers on video datasets in a self-supervised objective, we find that the model emerges a zero-shot capability to infer the semantics from a demonstration video, and imitate the semantics to an unseen scenario. This allows the models to perform unseen tasks by watching the demonstration video in an in-context manner, without further fine-tuning. To validate the imitation capacity, we design various evaluation metrics including both objective and subjective measures. The results show that our models can generate high-quality video clips that accurately align with the semantic guidance provided by the demonstration videos, and we also show that the imitation capacity follows the scaling law.",
      "authors": [
        "Wentao Zhang",
        "Junliang Guo",
        "Tianyu He",
        "Li Zhao",
        "Linli Xu",
        "Jiang Bian"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=wkbx7BRAsM",
      "cdate": 1727447728772,
      "mdate": 1743404175963,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836098"
    },
    {
      "id": "5AtlfHYCPa",
      "title": "HR-Extreme: A High-Resolution Dataset for Extreme Weather Forecasting",
      "abstract": "The application of large deep learning models in weather forecasting has led to\nsignificant advancements in the field, including higher-resolution forecasting and\nextended prediction periods exemplified by models such as Pangu and Fuxi. Despite\nthese successes, previous research has largely been characterized by the neglect\nof extreme weather events, and the availability of datasets specifically curated for\nsuch events remains limited. Given the critical importance of accurately forecasting\nextreme weather, this study introduces a comprehensive dataset that incorporates\nhigh-resolution extreme weather cases derived from the High-Resolution Rapid\nRefresh (HRRR) data, a 3-km real-time dataset provided by NOAA. We also\nevaluate the current state-of-the-art deep learning models and Numerical Weather\nPrediction (NWP) systems on HR-Extreme, and provide a improved baseline\ndeep learning model called HR-Heim which has superior performance on both\ngeneral loss and HR-Extreme compared to others. Our results reveal that the\nerrors of extreme weather cases are significantly larger than overall forecast error,\nhighlighting them as an crucial source of loss in weather prediction. These findings\nunderscore the necessity for future research to focus on improving the accuracy of\nextreme weather forecasts to enhance their practical utility",
      "authors": [
        "Nian Ran",
        "Peng Xiao",
        "Yue Wang",
        "Wesley Shi",
        "Jianxin Lin",
        "Qi Meng",
        "Richard Allmendinger"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=5AtlfHYCPa",
      "cdate": 1727447473171,
      "mdate": 1742546314424,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836105"
    },
    {
      "id": "5ck9PIrTpH",
      "title": "MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily Complex Proofs",
      "abstract": "Large language models (LLMs) can solve arithmetic word problems with high accuracy, but little is known about how well they generalize to more complex problems. This is difficult to study, as (i) much of the available evaluation data has already been seen by the most capable models during training, and (ii) existing benchmarks do not capture how problem proofs may be arbitrarily complex in various ways. In this paper, we present a data-generation framework for evaluating LLMs on problems with arbitrarily complex arithmetic proofs, called MathGAP. MathGAP generates problem statements and chain-of-thought reasoning traces according to specifications about their arithmetic proof structure, enabling systematic studies on easy-to-hard generalization with respect to complexity of proof trees. Using MathGAP, we find that LLMs show a significant decrease in performance as proofs get deeper and wider. This effect is more pronounced in complex, nonlinear proof structures, which are challenging even for the most capable models. The models are also sensitive to simple changes in sentence ordering. However, they remain capable of solving some complex problems, suggesting that reasoning generalization is noisy.",
      "authors": [
        "Andreas Opedal",
        "Haruki Shirakami",
        "Bernhard Schölkopf",
        "Abulhair Saparov",
        "Mrinmaya Sachan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=5ck9PIrTpH",
      "cdate": 1727447409857,
      "mdate": 1742566493428,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836116"
    },
    {
      "id": "3Fgylj4uqL",
      "title": "Interpretable Causal Representation Learning for Biological Data in the Pathway Space",
      "abstract": "Predicting the impact of genomic and drug perturbations in cellular function is crucial for understanding gene functions and drug effects, ultimately leading to improved therapies. To this end, Causal Representation Learning (CRL) constitutes one of the most promising approaches, as it aims to identify the latent factors that causally govern biological systems, thus facilitating the prediction of the effect of unseen perturbations. Yet, current CRL methods fail in reconciling their principled latent representations with known biological processes, leading to models that are not interpretable. To address this major issue, in this work we present SENA-discrepancy-VAE, a model based on the recently proposed CRL method discrepancy-VAE, that produces representations where each latent factor can be interpreted as the (linear) combination of the activity of a (learned) set of biological processes. To this extent, we present an encoder, SENA-$\\delta$, that efficiently compute and map biological processes' activity levels to the latent causal factors. We show that SENA-discrepancy-VAE achieves predictive performances on unseen combinations of interventions that are comparable with its original, non-interpretable counterpart, while inferring causal latent factors that are biologically meaningful.",
      "authors": [
        "Jesus de la Fuente Cedeño",
        "Robert Lehmann",
        "Carlos Ruiz-Arenas",
        "Jan Voges",
        "Irene Marín-Goñi",
        "Xabier Martinez de Morentin",
        "David Gomez-Cabrero",
        "Idoia Ochoa",
        "Jesper Tegnér",
        "Vincenzo Lagani",
        "Mikel Hernaez"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=3Fgylj4uqL",
      "cdate": 1727447355630,
      "mdate": 1744047181973,
      "matched_keywords": [
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836122"
    },
    {
      "id": "RAyRXQjsFl",
      "title": "Separation Power of Equivariant Neural Networks",
      "abstract": "The separation power of a machine learning model refers to its ability to distinguish between different inputs and is often used as a proxy for its expressivity. Indeed, knowing the separation power of a family of models is a necessary condition to obtain fine-grained universality results. In this paper, we analyze the separation power of equivariant neural networks, such as convolutional and permutation-invariant networks.\nWe first present a complete characterization of inputs indistinguishable by models derived by a given architecture. From this results, we derive how separability is influenced by hyperparameters and architectural choices—such as activation functions, depth, hidden layer width, and representation types. Notably, all non-polynomial activations, including ReLU and sigmoid, are equivalent in expressivity and reach maximum separation power. Depth improves separation power up to a threshold, after which further increases have no effect. Adding invariant features to hidden representations does not impact separation power. Finally, block decomposition of hidden representations affects separability, with minimal components forming a hierarchy in separation power that provides a straightforward method for comparing the separation power of models.",
      "authors": [
        "Marco Pacini",
        "Xiaowen Dong",
        "Bruno Lepri",
        "Gabriele Santin"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=RAyRXQjsFl",
      "cdate": 1727447274289,
      "mdate": 1740823613481,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836127"
    },
    {
      "id": "5BRFddsAai",
      "title": "HASARD: A Benchmark for Vision-Based Safe Reinforcement Learning in Embodied Agents",
      "abstract": "Advancing safe autonomous systems through reinforcement learning (RL) requires robust benchmarks to evaluate performance, analyze methods, and assess agent competencies. Humans primarily rely on embodied visual perception to safely navigate and interact with their surroundings, making it a valuable capability for RL agents. However, existing vision-based 3D benchmarks only consider simple navigation tasks. To address this shortcoming, we introduce **HASARD**, a suite of diverse and complex tasks to **HA**rness **SA**fe **R**L with **D**oom, requiring strategic decision-making, comprehending spatial relationships, and predicting the short-term future. HASARD features three difficulty levels and two action spaces. An empirical evaluation of popular baseline methods demonstrates the benchmark's complexity, unique challenges, and reward-cost trade-offs. Visualizing agent navigation during training with top-down heatmaps provides insight into a method's learning process. Incrementally training across difficulty levels offers an implicit learning curriculum. HASARD is the first safe RL benchmark to exclusively target egocentric vision-based learning, offering a cost-effective and insightful way to explore the potential and boundaries of current and future safe RL methods. The environments and baseline implementations are open-sourced.",
      "authors": [
        "Tristan Tomilin",
        "Meng Fang",
        "Mykola Pechenizkiy"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=5BRFddsAai",
      "cdate": 1727447193127,
      "mdate": 1741687656744,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836132"
    },
    {
      "id": "0h6v4SpLCY",
      "title": "Universal generalization guarantees for Wasserstein distributionally robust models",
      "abstract": "Distributionally robust optimization has emerged as an attractive way to train robust machine learning models, capturing data uncertainty and distribution shifts. Recent statistical analyses have proved that generalization guarantees of robust models based on the Wasserstein distance have generalization guarantees that do not suffer from the curse of dimensionality. However, these results are either approximate, obtained in specific cases, or based on assumptions difficult to verify in practice. In contrast, we establish exact generalization guarantees that cover a wide range of cases, with arbitrary transport costs and parametric loss functions, including deep learning objectives with nonsmooth activations. We complete our analysis with an excess bound on the robust objective and an extension to Wasserstein robust models with entropic regularizations.",
      "authors": [
        "Tam Le",
        "Jerome Malick"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=0h6v4SpLCY",
      "cdate": 1727447165140,
      "mdate": 1739888810187,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836137"
    },
    {
      "id": "CLE09ESvul",
      "title": "What should a neuron aim for? Designing local objective functions based on information theory",
      "abstract": "In modern deep neural networks, the learning dynamics of individual neurons are often obscure, as the networks are trained via global optimization. Conversely, biological systems build on self-organized, local learning, achieving robustness and efficiency with limited global information. Here, we show how self-organization between individual artificial neurons can be achieved by designing abstract bio-inspired local learning goals. These goals are parameterized using a recent extension of information theory, Partial Information Decomposition (PID), which decomposes the information that a set of information sources holds about an outcome into unique, redundant and synergistic contributions. Our framework enables neurons to locally shape the integration of information from various input classes, i.e., feedforward, feedback, and lateral, by selecting which of the three inputs should contribute uniquely, redundantly or synergistically to the output. This selection is expressed as a weighted sum of PID terms, which, for a given problem, can be directly derived from intuitive reasoning or via numerical optimization, offering a window into understanding task-relevant local information processing. Achieving neuron-level interpretability while enabling strong performance using local learning, our work advances a principled information-theoretic foundation for local learning strategies.",
      "authors": [
        "Andreas Christian Schneider",
        "Valentin Neuhaus",
        "David Alexander Ehrlich",
        "Abdullah Makkeh",
        "Alexander S Ecker",
        "Viola Priesemann",
        "Michael Wibral"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=CLE09ESvul",
      "cdate": 1727447085889,
      "mdate": 1743425812678,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836146"
    },
    {
      "id": "TOiageVNru",
      "title": "Physics-informed Temporal Difference Metric Learning for Robot Motion Planning",
      "abstract": "The motion planning problem involves finding a collision-free path from a robot's starting to its target configuration. Recently, self-supervised learning methods have emerged to tackle motion planning problems without requiring expensive expert demonstrations. They solve the Eikonal equation for training neural networks and lead to efficient solutions. However, these methods struggle in complex environments because they fail to maintain key properties of the Eikonal equation, such as optimal value functions and geodesic distances. To overcome these limitations, we propose a novel self-supervised temporal difference metric learning approach that solves the Eikonal equation more accurately and enhances performance in solving complex and unseen planning tasks. Our method enforces Bellman's principle of optimality over finite regions, using temporal difference learning to avoid spurious local minima while incorporating metric learning to preserve the Eikonal equation's essential geodesic properties. We demonstrate that our approach significantly outperforms existing self-supervised learning methods in handling complex environments and generalizing to unseen environments, with robot configurations ranging from 2 to 12 degrees of freedom (DOF).",
      "authors": [
        "Ruiqi Ni",
        "zherong pan",
        "Ahmed H Qureshi"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=TOiageVNru",
      "cdate": 1727446817615,
      "mdate": 1739462062385,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836151"
    },
    {
      "id": "OQqNieeivq",
      "title": "KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models",
      "abstract": "The increasing sizes of large language models (LLMs) result in significant computational overhead and memory usage when adapting these models to specific tasks or domains. Various parameter-efficient fine-tuning (PEFT) methods have been devised to mitigate these challenges by training a small set of parameters for the task-specific updates of the model weights. Among PEFT methods, LoRA stands out for its simplicity and efficiency, inspiring the development of a series of variants. However, LoRA and its successors disregard the knowledge that is noisy or irrelevant to the targeted task, detrimentally impacting model performance and leading to suboptimality. To address this limitation, we introduce Knowledge-aware Singular-value Adaptation (KaSA), a PEFT method that leverages singular value decomposition (SVD) with knowledge-aware singular values to dynamically activate knowledge based on its relevance to the task at hand. We conduct extensive experiments across a range of LLMs on tasks spanning natural language understanding (NLU), generation (NLG), instruction following, and commonsense reasoning. The experimental results demonstrate that KaSA consistently outperforms FFT and 14 popular PEFT baselines across 16 benchmarks and 4 synthetic datasets, underscoring our method's efficacy and adaptability. The source code of our method is available at https://github.com/juyongjiang/KaSA.",
      "authors": [
        "Fan Wang",
        "Juyong Jiang",
        "Chansung Park",
        "Sunghun Kim",
        "Jing Tang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=OQqNieeivq",
      "cdate": 1727446768367,
      "mdate": 1747563297323,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836157"
    },
    {
      "id": "P9VdRQOyqu",
      "title": "Can Video LLMs Refuse to Answer? Alignment for Answerability in Video Large Language Models",
      "abstract": "In the broader context of deep learning, Multimodal Large Language Models have achieved significant breakthroughs by leveraging powerful Large Language Models as a backbone to align different modalities into the language space. A prime exemplification is the development of Video Large Language Models (Video-LLMs). While numerous advancements have been proposed to enhance the video understanding capabilities of these models, they are predominantly trained on questions generated directly from video content. However, in real-world scenarios, users often pose questions that extend beyond the informational scope of the video, highlighting the need for Video-LLMs to assess the relevance of the question. We demonstrate that even the best-performing Video-LLMs fail to reject unfit questions-not necessarily due to a lack of video understanding, but because they have not been trained to identify and refuse such questions. To address this limitation, we propose alignment for answerability, a framework that equips Video-LLMs with the ability to evaluate the relevance of a question based on the input video and appropriately decline to answer when the question exceeds the scope of the video, as well as an evaluation framework with a comprehensive set of metrics designed to measure model behavior before and after alignment. Furthermore, we present a pipeline for creating a dataset specifically tailored for alignment for answerability, leveraging existing video-description paired datasets.",
      "authors": [
        "Eunseop Yoon",
        "Hee Suk Yoon",
        "Mark A. Hasegawa-Johnson",
        "Chang D. Yoo"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=P9VdRQOyqu",
      "cdate": 1727446494311,
      "mdate": 1740912625023,
      "matched_keywords": [
        "large language model",
        "multimodal",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836162"
    },
    {
      "id": "Qj1KwBZaEI",
      "title": "Intrinsic Dimension Correlation: uncovering nonlinear connections in multimodal representations",
      "abstract": "To gain insight into the mechanisms behind machine learning methods, it is crucial to establish connections among the features describing data points. However, these correlations often exhibit a high-dimensional and strongly nonlinear nature, which makes them challenging to detect using standard methods. This paper exploits the entanglement between intrinsic dimensionality and correlation to propose a metric that quantifies the (potentially nonlinear) correlation between high-dimensional manifolds. We first validate our method on synthetic data in controlled environments, showcasing its advantages and drawbacks compared to existing techniques. Subsequently, we extend our analysis to large-scale applications in neural network representations. Specifically, we focus on latent representations of multimodal data, uncovering clear correlations between paired visual and textual embeddings, whereas existing methods struggle significantly in detecting similarity. Our results indicate the presence of highly nonlinear correlation patterns between latent manifolds.",
      "authors": [
        "Lorenzo Basile",
        "Santiago Acevedo",
        "Luca Bortolussi",
        "Fabio Anselmi",
        "Alex Rodriguez"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Qj1KwBZaEI",
      "cdate": 1727446388058,
      "mdate": 1740825685571,
      "matched_keywords": [
        "multimodal",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836167"
    },
    {
      "id": "5IWJBStfU7",
      "title": "Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?",
      "abstract": "As AI systems are increasingly deployed in high-stakes applications, ensuring their interpretability is essential. Mechanistic Interpretability (MI) aims to reverse-engineer neural networks by extracting human-understandable algorithms embedded within their structures to explain their behavior. This work systematically examines a fundamental question: for a fixed behavior to explain, and under the criteria that MI sets for itself, are we guaranteed a unique explanation? Drawing an analogy with the concept of identifiability in statistics, which ensures the uniqueness of parameters inferred from data under specific modeling assumptions, we speak about the identifiability of explanations produced by MI.\n\nWe identify two broad strategies to produce MI explanations: (i) \"where-then-what\", which first identifies a subset of the network (a circuit) that replicates the model's behavior before deriving its interpretation, and (ii) \"what-then-where\", which begins with candidate explanatory algorithms and searches in the activation subspaces of the neural model where the candidate algorithm may be implemented, relying on notions of causal alignment between the states of the candidate algorithm and the neural network. \n\nWe systematically test the identifiability of both strategies using simple tasks (learning Boolean functions) and multi-layer perceptrons small enough to allow a complete enumeration of candidate explanations. Our experiments reveal overwhelming evidence of non-identifiability in all cases: multiple circuits can replicate model behavior, multiple interpretations can exist for a circuit, several algorithms can be causally aligned with the neural network, and a single algorithm can be causally aligned with different subspaces of the network.\n\nWe discuss whether the unicity intuition is necessary. One could adopt a pragmatic stance, requiring explanations only to meet predictive and/or manipulability standards. However, if unicity is considered essential, e.g., to provide a sense of understanding, we also discuss less permissive criteria. Finally, we also refer to the inner interpretability framework that demands explanations to be validated by multiple complementary criteria. This work aims to contribute constructively to the ongoing effort to formalize what we expect from explanations in AI.",
      "authors": [
        "Maxime Méloux",
        "Silviu Maniu",
        "François Portet",
        "Maxime Peyrard"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=5IWJBStfU7",
      "cdate": 1727446276306,
      "mdate": 1744100134423,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836173"
    },
    {
      "id": "s3IBHTTDYl",
      "title": "Language Models Need Inductive Biases to Count Inductively",
      "abstract": "Counting constitutes a core skill underlying a wide range of tasks, such as formal language recognition, multi-hop reasoning and simulating algorithms. Generaliz- ing counting inductively is central to task success on out-of-distribution (OOD) instances where testing inputs are longer than those seen in training. While there is a large body of literature reporting poor length generalization in language models, few papers have tried to distill the “reasoning” failure to the simplest case of count- ing failure. We aim to provide a broader picture on whether various language model architectures can a) learn to count, and b) generalize counting inductively. This work provides extensive empirical results on architectures ranging from RNNs, Transformers, State-Space Models and RWKV. We present carefully-designed task formats, auxiliary tasks and positional embeddings to avoid limitations in general- ization with OOD-position and OOD-vocabulary. We find that while traditional RNNs trivially achieve inductive counting, Transformers have to rely on positional embeddings (PEs) to count OOD. Further analyses on interpreting the learned solution reveal that different PEs encode different inductive biases that facilitate counting in different task formats. As counting is the basis for many arguments concerning the expressivity of Transformers, our finding calls for the community to reexamine the application scope of primitive functions defined in formal charac- terizations. Finally, modern RNNs also largely underperform traditional RNNs in generalizing counting inductively, hinting at the tradeoff modern RNNs struggle to balance between parallelized training and maintaining their recurrent nature.",
      "authors": [
        "Yingshan Chang",
        "Yonatan Bisk"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=s3IBHTTDYl",
      "cdate": 1727446118385,
      "mdate": 1740767951028,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836178"
    },
    {
      "id": "2oKkQTyfz7",
      "title": "General Scene Adaptation for Vision-and-Language Navigation",
      "abstract": "Vision-and-Language Navigation (VLN) tasks mainly evaluate agents based on one-time execution of individual instructions across multiple environments, aiming to develop agents capable of functioning in any environment in a zero-shot manner. However, real-world navigation robots often operate in persistent environments with relatively consistent physical layouts, visual observations, and language styles from instructors. Such a gap in the task setting presents an opportunity to improve VLN agents by incorporating continuous adaptation to specific environments. To better reflect these real-world conditions, we introduce GSA-VLN (General Scene Adaptation for VLN), a novel task requiring agents to execute navigation instructions within a specific scene and simultaneously adapt to it for improved performance over time. To evaluate the proposed task, one has to address two challenges in existing VLN datasets: the lack of out-of-distribution (OOD) data, and the limited number and style diversity of instructions for each scene. Therefore, we propose a new dataset, GSA-R2R, which significantly expands the diversity and quantity of environments and instructions for the Room-to-Room (R2R) dataset to evaluate agent adaptability in both ID and OOD contexts. Furthermore, we design a three-stage instruction orchestration pipeline that leverages large language models (LLMs) to refine speaker-generated instructions and apply role-playing techniques to rephrase instructions into different speaking styles. This is motivated by the observation that each individual user often has consistent signatures or preferences in their instructions,  taking the use case of home robotic assistants as an example. We conducted extensive experiments on GSA-R2R to thoroughly evaluate our dataset and benchmark various methods, revealing key factors enabling agents to adapt to specific environments. Based on our findings, we propose a novel method, Graph-Retained DUET (GR-DUET), which incorporates memory-based navigation graphs with an environment-specific training strategy, achieving state-of-the-art results on all GSA-R2R splits.",
      "authors": [
        "Haodong Hong",
        "Yanyuan Qiao",
        "Sen Wang",
        "Jiajun Liu",
        "Qi Wu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=2oKkQTyfz7",
      "cdate": 1727446047507,
      "mdate": 1740146791507,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836183"
    },
    {
      "id": "hrqNOxpItr",
      "title": "Cross-Entropy Is All You Need To Invert the Data Generating Process",
      "abstract": "Supervised learning has become a cornerstone of modern machine learning, yet a comprehensive theory explaining its effectiveness remains elusive. Empirical phenomena, such as neural analogy-making and the linear representation hypothesis, suggest that supervised models can learn interpretable factors of variation in a linear fashion. Recent advances in self-supervised learning, particularly nonlinear Independent Component Analysis, have shown that these methods can recover latent structures by inverting the data generating process. We extend these identifiability results to parametric instance discrimination, \nthen show how insights transfer to the ubiquitous setting of supervised learning with cross-entropy minimization. We prove that even in standard classification tasks, models learn representations of ground-truth factors of variation up to a linear transformation under a certain DGP. We corroborate our theoretical contribution with a series of empirical studies. First, using simulated data matching our theoretical assumptions, we demonstrate successful disentanglement of latent factors. Second, we show that on DisLib, a widely-used disentanglement benchmark, simple classification tasks recover latent structures up to linear transformations. Finally, we reveal that models trained on ImageNet encode representations that permit linear decoding of proxy factors of variation.\nTogether, our theoretical findings and experiments offer a compelling explanation for recent observations of linear representations, such as superposition in neural networks. This work takes a significant step toward a cohesive theory that accounts for the unreasonable effectiveness of supervised learning.",
      "authors": [
        "Patrik Reizinger",
        "Alice Bizeul",
        "Attila Juhos",
        "Julia E Vogt",
        "Randall Balestriero",
        "Wieland Brendel",
        "David Klindt"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=hrqNOxpItr",
      "cdate": 1727446042913,
      "mdate": 1740468107777,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836188"
    },
    {
      "id": "cPozlf9OaF",
      "title": "Bridging Context Gaps: Leveraging Coreference Resolution for Long Contextual Understanding",
      "abstract": "Large language models (LLMs) have shown remarkable capabilities in natural language processing; however, they still face difficulties when tasked with understanding lengthy contexts and executing effective question answering. These challenges often arise due to the complexity and ambiguity present in longer texts. To enhance the performance of LLMs in such scenarios, we introduce the Long Question Coreference Adaptation (LQCA) method. This innovative framework focuses on coreference resolution tailored to long contexts, allowing the model to identify and manage references effectively. The LQCA method encompasses four key steps: resolving coreferences within sub-documents, computing the distances between mentions, defining a representative mention for coreference, and answering questions through mention replacement. By processing information systematically, the framework provides easier-to-handle partitions for LLMs, promoting better understanding. Experimental evaluations on a range of LLMs and datasets have yielded positive results, with a notable improvements on OpenAI-o1-mini and GPT-4o models, highlighting the effectiveness of leveraging coreference resolution to bridge context gaps in question answering. Our code is public at https://github.com/OceannTwT/LQCA.",
      "authors": [
        "Yanming Liu",
        "Xinyue Peng",
        "Jiannan Cao",
        "Shi Bo",
        "Yanxin Shen",
        "Tianyu Du",
        "Sheng Cheng",
        "Xun Wang",
        "Jianwei Yin",
        "Xuhong Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=cPozlf9OaF",
      "cdate": 1727446001923,
      "mdate": 1740890351724,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836193"
    },
    {
      "id": "Oazgf8A24z",
      "title": "Scalable Mechanistic Neural Networks",
      "abstract": "We propose Scalable Mechanistic Neural Network (S-MNN), an enhanced neural network framework designed for scientific machine learning applications involving long temporal sequences. By reformulating the original Mechanistic Neural Network (MNN) (Pervez et al., 2024), we reduce the computational time and space complexities from cubic and quadratic with respect to the sequence length, respectively, to linear. This significant improvement enables efficient modeling of long-term dynamics without sacrificing accuracy or interpretability. Extensive experiments demonstrate that S-MNN matches the original MNN in precision while substantially reducing computational resources. Consequently, S-MNN can drop-in replace the original MNN in applications, providing a practical and efficient tool for integrating mechanistic bottlenecks into neural network models of complex dynamical systems. Source code is available at https://github.com/IST-DASLab/ScalableMNN.",
      "authors": [
        "Jiale Chen",
        "Dingling Yao",
        "Adeel Pervez",
        "Dan Alistarh",
        "Francesco Locatello"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Oazgf8A24z",
      "cdate": 1727445981339,
      "mdate": 1743528254123,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836201"
    },
    {
      "id": "fsX9nFwMNj",
      "title": "As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative Feedback Loss",
      "abstract": "Direct Preference Optimization (DPO) has emerged as a more computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) with Proximal Policy Optimization (PPO), eliminating the need for reward models and online sampling. Despite these benefits, DPO and its variants remain sensitive to hyper-parameters and prone to instability, particularly on mathematical datasets. We argue that these issues arise from the unidirectional likelihood-derivative negative feedback inherent in the log-likelihood loss function.\nTo address this, we propose a novel LLM alignment loss that establishes a stable Bidirectional Negative Feedback (BNF) during optimization. \nOur proposed BNF loss eliminates the need for pairwise contrastive losses and does not require any extra tunable hyper-parameters or pairwise preference data, streamlining the alignment pipeline to be as simple as supervised fine-tuning.\nWe conduct extensive experiments across two challenging QA benchmarks and four reasoning benchmarks. \nThe experimental results show that BNF achieves comparable performance to the best methods on QA benchmarks, while its performance decrease on the four reasoning benchmarks is significantly lower compared to the best methods, thus striking a better balance between value alignment and reasoning ability. \nIn addition, we further validate the performance of BNF on non-pairwise datasets, and conduct in-depth analysis of log-likelihood and logit shifts across different preference optimization methods.\nWe will release all the source code, checkpoints, and datasets on GitHub.",
      "authors": [
        "Xin Mao",
        "Huimin Xu",
        "Feng-Lin Li",
        "Ziqi Jin",
        "WANG CHEN",
        "Wei Zhang",
        "Anh Tuan Luu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=fsX9nFwMNj",
      "cdate": 1727445759647,
      "mdate": 1744095671104,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836207"
    },
    {
      "id": "UvTo3tVBk2",
      "title": "Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues",
      "abstract": "Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and DeltaNet have emerged as efficient alternatives to Transformers for long sequences. However, both Transformers and LRNNs struggle to perform state-tracking, which may impair performance in tasks such as code evaluation. In one forward pass, current architectures are unable to solve even parity, the simplest state-tracking task, which non-linear RNNs can handle effectively. Recently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity stems from restricting the value range of their diagonal state-transition matrices to $[0, 1]$ and that incorporating negative values can resolve this issue. We extend this result to non-diagonal LRNNs such as DeltaNet. We prove that finite precision LRNNs with state-transition matrices having only positive eigenvalues cannot solve parity, while non-triangular matrices are needed to count modulo $3$. Notably, we also prove that LRNNs can learn any regular language when their state-transition matrices are products of identity minus vector outer product matrices, each with eigenvalues in the range $[-1, 1]$. Our experiments confirm that extending the eigenvalue range of Mamba and DeltaNet to include negative values not only enables them to solve parity but consistently improves their performance on state-tracking tasks. We also show that state-tracking enabled LRNNs can be pretrained  stably and efficiently at scale (1.3B parameters), achieving competitive performance on language modeling and showing promise on code and math tasks.",
      "authors": [
        "Riccardo Grazzi",
        "Julien Siems",
        "Arber Zela",
        "Jörg K.H. Franke",
        "Frank Hutter",
        "Massimiliano Pontil"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=UvTo3tVBk2",
      "cdate": 1727445747561,
      "mdate": 1741965112744,
      "matched_keywords": [
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836212"
    },
    {
      "id": "mXHTifc1Fn",
      "title": "E(3)-equivariant models cannot learn chirality: Field-based molecular generation",
      "abstract": "Obtaining the desired effect of drugs is highly dependent on their molecular geometries. Thus, the current prevailing paradigm focuses on 3D point-cloud atom representations, utilizing graph neural network (GNN) parametrizations, with rotational symmetries baked in via E(3) invariant layers. We prove that such models must necessarily disregard chirality, a geometric property of the molecules that cannot be superimposed on their mirror image by rotation and translation. Chirality plays a key role in determining drug safety and potency. To address this glaring issue, we introduce a novel field-based representation, proposing reference rotations that replace rotational symmetry constraints. The proposed model captures all molecular geometries including chirality, while still achieving highly competitive performance with E(3)-based methods across standard benchmarking metrics.",
      "authors": [
        "Alexandru Dumitrescu",
        "Dani Korpela",
        "Markus Heinonen",
        "Yogesh Verma",
        "Valerii Iakovlev",
        "Vikas Garg",
        "Harri Lähdesmäki"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=mXHTifc1Fn",
      "cdate": 1727445655527,
      "mdate": 1747403755307,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836218"
    },
    {
      "id": "sULAwlAWc1",
      "title": "One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs",
      "abstract": "Safety alignment in large language models (LLMs) is increasingly compromised by jailbreak attacks, which can manipulate these models to generate harmful or unintended content. Investigating these attacks is crucial for uncovering model vulnerabilities. However, many existing jailbreak strategies fail to keep pace with the rapid development of defense mechanisms, such as defensive suffixes, rendering them ineffective against defended models. To tackle this issue, we introduce a novel attack method called ArrAttack, specifically designed to target defended LLMs. ArrAttack automatically generates robust jailbreak prompts capable of bypassing various defense measures. This capability is supported by a universal robustness judgment model that, once trained, can perform robustness evaluation for any target model with a wide variety of defenses. By leveraging this model, we can rapidly develop a robust jailbreak prompt generator that efficiently converts malicious input prompts into effective attacks. Extensive evaluations reveal that ArrAttack significantly outperforms existing attack strategies, demonstrating strong transferability across both white-box and black-box models, including GPT-4 and Claude-3. Our work bridges the gap between jailbreak attacks and defenses, providing a fresh perspective on generating robust jailbreak prompts.",
      "authors": [
        "Linbao Li",
        "Yannan Liu",
        "Daojing He",
        "YU LI"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=sULAwlAWc1",
      "cdate": 1727445426843,
      "mdate": 1742630818329,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836223"
    },
    {
      "id": "kYwTmlq6Vn",
      "title": "PaPaGei: Open Foundation Models for Optical Physiological Signals",
      "abstract": "Photoplethysmography (PPG) is the leading non-invasive technique for monitoring biosignals and cardiovascular health, with widespread adoption in both clinical settings and consumer wearable devices. While machine learning models trained on PPG signals have shown promise, they tend to be task-specific and struggle with generalization. Current research is limited by the use of single-device datasets, insufficient exploration of out-of-domain generalization, and a lack of publicly available models, which hampers reproducibility. To address these limitations, we present PaPaGei, the first open foundation model for PPG signals. The model is pre-trained on over 57,000 hours of data, comprising 20 million unlabeled PPG segments from publicly available datasets. We introduce a novel representation learning approach that leverages domain knowledge of PPG signal morphology across individuals, enabling the capture of richer representations compared to traditional contrastive learning methods. We evaluate PaPaGei against state-of-the-art time-series foundation models and self-supervised learning benchmarks across 20 tasks from 10 diverse datasets, spanning cardiovascular health, sleep disorders, pregnancy monitoring, and wellbeing assessment. Our model demonstrates superior performance, improving classification and regression metrics by 6.3% and 2.9% respectively in at least 14 tasks. Notably, PaPaGei achieves these results while being more data- and parameter-efficient, outperforming models that are 70x larger. Beyond accuracy, we examine model robustness across different skin tones, establishing a benchmark for bias evaluation in future models. PaPaGei can serve as both a feature extractor and an encoder for multimodal models, opening up new opportunities for multimodal health monitoring. Models, data, and code are available at: https://github.com/nokia-bell-labs/papagei-foundation-model",
      "authors": [
        "Arvind Pillai",
        "Dimitris Spathis",
        "Fahim Kawsar",
        "Mohammad Malekzadeh"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=kYwTmlq6Vn",
      "cdate": 1727445367747,
      "mdate": 1739261735728,
      "matched_keywords": [
        "foundation model",
        "multimodal",
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836228"
    },
    {
      "id": "DPzQ5n3mNm",
      "title": "Sensitivity-Constrained Fourier Neural Operators for Forward and Inverse Problems in Parametric Differential Equations",
      "abstract": "Parametric differential equations of the form  $\\frac{\\partial u}{\\partial t} = f(u, x, t, p)$ are fundamental in science and engineering. While deep learning frameworks like the Fourier Neural Operator (FNO) efficiently approximate differential equation solutions, they struggle with inverse problems, sensitivity calculations  $\\frac{\\partial u}{\\partial p}$, and concept drift.  We address these challenges by introducing a novel sensitivity loss regularizer, demonstrated through Sensitivity-Constrained Fourier Neural Operators (SC-FNO). Our approach maintains high accuracy for solution paths and outperforms both standard FNO and FNO with Physics-Informed Neural Network regularization. SC-FNO exhibits superior performance in parameter inversion tasks, accommodates more complex parameter spaces (tested with up to 82 parameters), reduces training data requirements, and decreases training time while maintaining accuracy.  These improvements apply across various differential equations and neural operators, enhancing their reliability without significant computational overhead (30%–130% extra training time per epoch). Models and selected experiment code are available at:  [https://github.com/AMBehroozi/SC_Neural_Operators](https://github.com/AMBehroozi/SC_Neural_Operators).",
      "authors": [
        "Abdolmehdi Behroozi",
        "Chaopeng Shen",
        "Daniel Kifer"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=DPzQ5n3mNm",
      "cdate": 1727445161850,
      "mdate": 1740787398045,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836233"
    },
    {
      "id": "CvGqMD5OtX",
      "title": "CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate Selection in Text-to-SQL",
      "abstract": "We present CHASE-SQL, a novel framework addressing large language model (LLM) performance challenges for Text-to-SQL tasks by leveraging multi-agent modeling and test-time compute for improved candidate generation and selection. CHASE-SQL uses LLMs to generate diverse SQL candidates with: (1) a divide-and-conquer approach to break down complex queries, (2) chain-of-thought reasoning based on query execution plans, and (3) instance-aware synthetic example generation for tailored few-shot demonstrations. A selection agent ranks candidates via pairwise comparisons using a fine-tuned binary selection LLM, offering robust performance. This framework improves SQL query quality and diversity, achieving state-of-the-art execution accuracy of 73.0% on the BIRD Text-to-SQL benchmark test set, topping the leaderboard at the time of submission.",
      "authors": [
        "Mohammadreza Pourreza",
        "Hailong Li",
        "Ruoxi Sun",
        "Yeounoh Chung",
        "Shayan Talaei",
        "Gaurav Tarlok Kakkar",
        "Yu Gan",
        "Amin Saberi",
        "Fatma Ozcan",
        "Sercan O Arik"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=CvGqMD5OtX",
      "cdate": 1727444966978,
      "mdate": 1740079647589,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836238"
    },
    {
      "id": "te2IdORabL",
      "title": "JPEG Inspired Deep Learning",
      "abstract": "Although it is traditionally believed that lossy image compression, such as JPEG compression, has a negative impact on the performance of deep neural networks (DNNs), it is shown by recent works that well-crafted JPEG compression can actually improve the performance of deep learning (DL). Inspired by this, we propose JPEG-DL, a novel DL framework that prepends any underlying DNN architecture with a trainable JPEG compression layer. To make the quantization operation in JPEG compression trainable, a new differentiable soft quantizer is employed at the JPEG layer, and then the quantization operation and underlying DNN are jointly trained. Extensive experiments show that in comparison with the standard DL,  JPEG-DL delivers significant accuracy improvements across various datasets and model architectures while enhancing robustness against adversarial attacks. Particularly, on some fine-grained image classification datasets, JPEG-DL can increase prediction accuracy by as much as 20.9%. Our code is available on https://github.com/AhmedHussKhalifa/JPEG-Inspired-DL.git.",
      "authors": [
        "Ahmed H. Salamah",
        "Kaixiang Zheng",
        "Yiwen Liu",
        "EN-HUI YANG"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=te2IdORabL",
      "cdate": 1727444938663,
      "mdate": 1741059836496,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836247"
    },
    {
      "id": "K5yeB4dTtS",
      "title": "MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents",
      "abstract": "MLLM agents demonstrate potential for complex embodied tasks by retrieving multimodal task-relevant trajectory data. However, current retrieval methods primarily focus on surface-level similarities of textual or visual cues in trajectories, neglecting their effectiveness for the specific task at hand. To address this issue, we propose a novel method, MART, which enhances the performance of embodied agents by utilizing interaction data to fine-tune an MLLM retriever based on preference learning, such that the retriever fully considers the effectiveness of trajectories and prioritize them for unseen tasks. We also introduce Trajectory Abstraction, a mechanism that leverages MLLMs' summarization capabilities to represent trajectories with fewer tokens while preserving key information, enabling agents to better comprehend milestones in the trajectory. Experimental results across various environments demonstrate our method significantly improves task success rates in unseen scenes compared to baseline methods. This work presents a new paradigm for multimodal retrieval in embodied agents, by fine-tuning a general-purpose MLLM as the retriever to assess trajectory effectiveness. All the code for benchmark tasks, simulator modifications and the MLLM retriever is available at https://github.com/PKU-RL/MART.",
      "authors": [
        "Junpeng Yue",
        "Xinrun Xu",
        "Börje F. Karlsson",
        "Zongqing Lu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=K5yeB4dTtS",
      "cdate": 1727444870421,
      "mdate": 1740896141282,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.836252"
    },
    {
      "id": "b57IG6N20B",
      "title": "The Case for Cleaner Biosignals: High-fidelity Neural Compressor Enables Transfer from Cleaner iEEG to Noisier EEG",
      "abstract": "All data modalities are not created equal, even when the signal they measure comes from the same source. In the case of the brain, two of the most important data modalities are the scalp electroencephalogram (EEG), and the intracranial electroencephalogram (iEEG). iEEG benefits from a higher signal-to-noise ratio (SNR), as it measures the electrical activity directly in the brain, while EEG is noisier and has lower spatial and temporal resolutions. Nonetheless, both EEG and iEEG are important sources of data for human neurology, from healthcare to brain–machine interfaces. They are used by human experts, supported by deep learning (DL) models, to accomplish a variety of tasks, such as seizure detection and motor imagery classification. Although the differences between EEG and iEEG are well understood by human experts, the performance of DL models across these two modalities remains under-explored. To help characterize the importance of clean data on the performance of DL models, we propose BrainCodec, a high-fidelity EEG and iEEG neural compressor. We find that training BrainCodec on iEEG and then transferring to EEG yields higher reconstruction quality than training on EEG directly. In addition, we also find that training BrainCodec on both EEG and iEEG improves fidelity when reconstructing EEG. Our work indicates that data sources with higher SNR, such as iEEG, provide better performance across the board also in the medical time-series domain. This finding is consistent with reports coming from natural language processing, where clean data sources appear to have an outsized effect on the performance of the DL model overall. BrainCodec also achieves up to a 64x compression on iEEG and EEG without a notable decrease in quality. BrainCodec markedly surpasses current state-of-the-art compression models both in final compression ratio and in reconstruction fidelity. We also evaluate the fidelity of the compressed signals objectively on a seizure detection and a motor imagery task performed by standard DL models. Here, we find that BrainCodec achieves a reconstruction fidelity high enough to ensure no performance degradation on the downstream tasks. Finally, we collect the subjective assessment of an expert neurologist, that confirms the high reconstruction quality of BrainCodec in a realistic scenario. The code is available at https://github.com/IBM/eeg-ieeg-brain-compressor.",
      "authors": [
        "Francesco S. Carzaniga",
        "Gary Tom Hoppeler",
        "Michael Hersche",
        "Kaspar Schindler",
        "Abbas Rahimi"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=b57IG6N20B",
      "cdate": 1727444639113,
      "mdate": 1739261734969,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836261"
    },
    {
      "id": "HpUs2EXjOl",
      "title": "Rethinking Evaluation of Sparse Autoencoders through the Representation of Polysemous Words",
      "abstract": "Sparse autoencoders (SAEs) have gained a lot of attention as a promising tool to improve the interpretability of large language models (LLMs) by mapping the complex superposition of *polysemantic* neurons into *monosemantic* features and composing a sparse dictionary of words.\n\nHowever, traditional performance metrics like Mean Squared Error and $\\mathrm{L}_{0}$ sparsity ignore the evaluation of the semantic representational power of SAEs - whether they can acquire interpretable monosemantic features while preserving the semantic relationship of words.For instance, it is not obvious whether a learned sparse feature could distinguish different meanings in one word.\n\nIn this paper, we propose a suite of evaluations for SAEs to analyze the quality of monosemantic features by focusing on polysemous words.\nOur findings reveal that SAEs developed to improve the MSE-$\\mathrm{L}_0$ Pareto frontier may confuse interpretability, which does not necessarily enhance the extraction of monosemantic features.\nThe analysis of SAEs with polysemous words can also figure out the internal mechanism of LLMs; deeper layers and the Attention module contribute to distinguishing polysemy in a word.\n\nOur semantics-focused evaluation offers new insights into the polysemy and the existing SAE objective and contributes to the development of more practical SAEs.",
      "authors": [
        "Gouki Minegishi",
        "Hiroki Furuta",
        "Yusuke Iwasawa",
        "Yutaka Matsuo"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=HpUs2EXjOl",
      "cdate": 1727444397905,
      "mdate": 1739896143005,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836266"
    },
    {
      "id": "dRz3cizftU",
      "title": "Tool-Planner: Task Planning with Clusters across Multiple Tools",
      "abstract": "Large language models (LLMs) have demonstrated exceptional reasoning capabilities, enabling them to solve various complex problems. Recently, this ability has been applied to the paradigm of tool learning. Tool learning involves providing examples of tool usage and their corresponding functions, allowing LLMs to formulate plans and demonstrate the process of invoking and executing each tool. LLMs can address tasks that they cannot complete independently, thereby enhancing their potential across different tasks. However, this approach faces two key challenges. First, redundant error correction leads to unstable planning and long execution time. Additionally, designing a correct plan among multiple tools is also a challenge in tool learning. To address these issues, we propose Tool-Planner, a task-processing framework based on toolkits. Tool-Planner groups tools based on the API functions with the same function into a toolkit and allows LLMs to implement planning across the various toolkits. When a tool error occurs, the language model can reselect and adjust tools based on the toolkit. Experiments show that our approach demonstrates a high pass and win rate across different datasets and optimizes the planning scheme for tool learning in models such as GPT-4 and Claude 3, showcasing the potential of our method. Our code is public at\n https://github.com/OceannTwT/Tool-Planner.",
      "authors": [
        "Yanming Liu",
        "Xinyue Peng",
        "Jiannan Cao",
        "Shi Bo",
        "Yuwei Zhang",
        "Xuhong Zhang",
        "Sheng Cheng",
        "Xun Wang",
        "Jianwei Yin",
        "Tianyu Du"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=dRz3cizftU",
      "cdate": 1727444394774,
      "mdate": 1740717438514,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836271"
    },
    {
      "id": "2QdsjiNXgj",
      "title": "On a Connection Between Imitation Learning and RLHF",
      "abstract": "This work studies the alignment of large language models with preference data from an imitation learning  perspective. We establish a close theoretical connection between reinforcement learning from human feedback RLHF and imitation learning (IL), revealing that RLHF implicitly performs imitation learning on the preference data distribution. Building on this connection, we propose DIL, a principled framework that directly optimizes the imitation learning objective. DIL provides a unified imitation learning perspective on alignment, encompassing existing alignment algorithms as special cases while naturally introducing new variants. By bridging IL and RLHF, DIL offers new insights into alignment with RLHF. Extensive experiments demonstrate that DIL outperforms existing methods on various challenging benchmarks.",
      "authors": [
        "Teng Xiao",
        "Yige Yuan",
        "Mingxiao Li",
        "Zhengyu Chen",
        "Vasant G Honavar"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=2QdsjiNXgj",
      "cdate": 1727444302882,
      "mdate": 1741281675071,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836277"
    },
    {
      "id": "Fk3eod9aaD",
      "title": "In Search of Forgotten Domain Generalization",
      "abstract": "Out-of-Domain (OOD) generalization is the ability of a model trained on one or more domains to generalize to unseen domains. In the ImageNet era of computer vision, evaluation sets for measuring a model's OOD performance were designed to be strictly OOD with respect to style. However, the emergence of foundation models and expansive web-scale datasets has obfuscated this evaluation process, as datasets cover a broad range of domains and risk test domain contamination. In search of the forgotten domain generalization, we create large-scale datasets subsampled from LAION---LAION-Natural and LAION-Rendition---that are strictly OOD to corresponding ImageNet and DomainNet test sets in terms of style. Training CLIP models on these datasets reveals that a significant portion of their performance is explained by in-domain examples. This indicates that the OOD generalization challenges from the ImageNet era still prevail and that training on web-scale data merely creates the illusion of OOD generalization. Furthermore, through a systematic exploration of combining natural and rendition datasets in varying proportions, we identify optimal mixing ratios for model generalization across these domains. Our datasets and results re-enable meaningful assessment of OOD robustness at scale---a crucial prerequisite for improving model robustness.",
      "authors": [
        "Prasanna Mayilvahanan",
        "Roland S. Zimmermann",
        "Thaddäus Wiedemer",
        "Evgenia Rusak",
        "Attila Juhos",
        "Matthias Bethge",
        "Wieland Brendel"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Fk3eod9aaD",
      "cdate": 1727444248695,
      "mdate": 1740564672171,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836285"
    },
    {
      "id": "rnJxelIZrq",
      "title": "Beyond Mere Token Analysis: A Hypergraph Metric Space Framework for Defending Against Socially Engineered LLM Attacks",
      "abstract": "Recent jailbreak attempts on Large Language Models (LLMs) have shifted from algorithm-focused to human-like social engineering attacks, with persuasion-based techniques emerging as a particularly effective subset. These attacks evolve rapidly, demonstrate high creativity, and boast superior attack success rates. To combat such threats, we propose a promising approach to enhancing LLM safety by leveraging the underlying geometry of input prompt token embeddings using hypergraphs. This approach allows us to model the differences in information flow between benign and malicious LLM prompts.\n\nIn our approach, each LLM prompt is represented as a metric hypergraph, forming a compact metric space. We then construct a higher-order metric space over these compact metric hypergraphs using the Gromov-Hausdorff distance as a generalized metric. Within this space of metric hypergraph spaces, our safety filter learns to classify between harmful and benign prompts. Our study presents theoretical guarantees on the classifier's generalization error for novel and unseen LLM input prompts. Extensive empirical evaluations demonstrate that our method significantly outperforms both existing state-of-the-art generic defense mechanisms and naive baselines. Notably, our approach also achieves comparable performance to specialized defenses against algorithm-focused attacks.",
      "authors": [
        "Manohar Kaul",
        "Aditya Saibewar",
        "Sadbhavana Babar"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=rnJxelIZrq",
      "cdate": 1727444237393,
      "mdate": 1740731828394,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836290"
    },
    {
      "id": "eiqrnVaeIw",
      "title": "Persistent Pre-training Poisoning of LLMs",
      "abstract": "Large language models are pre-trained on uncurated text datasets consisting of trillions of tokens scraped from the Web.\nPrior work has shown that: (1) web-scraped pre-training datasets can be practically poisoned by malicious actors; and (2) adversaries can compromise language models after poisoning fine-tuning datasets.\nOur work evaluates for the first time whether language models can also be \\emph{compromised during pre-training}, with a focus on the persistence of pre-training attacks after models are fine-tuned as helpful and harmless chatbots (i.e., after SFT and DPO).\nWe pre-train a series of LLMs from scratch to measure the impact of a potential poisoning adversary under four different attack objectives (denial-of-service, belief manipulation, jailbreaking, and prompt stealing), and across a wide range of model sizes (from 600M to 7B).\nOur main result is that poisoning only 0.1% of a model's pre-training dataset is sufficient for three out of four attacks to measurably persist through post-training. Moreover, simple attacks like denial-of-service persist through post-training with a poisoning rate of only 0.001%.",
      "authors": [
        "Yiming Zhang",
        "Javier Rando",
        "Ivan Evtimov",
        "Jianfeng Chi",
        "Eric Michael Smith",
        "Nicholas Carlini",
        "Florian Tramèr",
        "Daphne Ippolito"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=eiqrnVaeIw",
      "cdate": 1727444114052,
      "mdate": 1739261733853,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836296"
    },
    {
      "id": "mUMvr33FTu",
      "title": "CipherPrune:  Efficient and Scalable Private Transformer Inference",
      "abstract": "Private Transformer inference using cryptographic protocols offers promising solutions for privacy-preserving machine learning; however, it still faces significant runtime overhead (efficiency issues) and challenges in handling long-token inputs (scalability issues). We observe that the Transformer's operational complexity scales quadratically with the number of input tokens, making it essential to reduce the input token length. Notably, each token varies in importance, and many inputs contain redundant tokens. Additionally, prior private inference methods that rely on high-degree polynomial approximations for non-linear activations are computationally expensive. Therefore, reducing the polynomial degree for less important tokens can significantly accelerate private inference.  Building on these observations, we propose \\textit{CipherPrune}, an efficient and scalable private inference framework that includes a secure encrypted token pruning protocol, a polynomial reduction protocol, and corresponding Transformer network optimizations. At the protocol level, encrypted token pruning adaptively removes unimportant tokens from encrypted inputs in a progressive, layer-wise manner. Additionally, encrypted polynomial reduction assigns lower-degree polynomials to less important tokens after pruning, enhancing efficiency without decryption. At the network level, we introduce protocol-aware network optimization via a gradient-based search to maximize pruning thresholds and polynomial reduction conditions while maintaining the desired accuracy. Our experiments demonstrate that CipherPrune reduces the execution overhead of private Transformer inference by approximately $6.1\\times$ for 128-token inputs and $10.6\\times$  for 512-token inputs, compared to previous methods, with only a marginal drop in accuracy. The code is publicly available at https://github.com/UCF-Lou-Lab-PET/cipher-prune-inference.",
      "authors": [
        "Yancheng Zhang",
        "Jiaqi Xue",
        "Mengxin Zheng",
        "Mimi Xie",
        "Mingzhe Zhang",
        "Lei Jiang",
        "Qian Lou"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=mUMvr33FTu",
      "cdate": 1727444095235,
      "mdate": 1743519561780,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836301"
    },
    {
      "id": "vcX0k4rGTt",
      "title": "Approximating Full Conformal Prediction for Neural Network Regression with Gauss-Newton Influence",
      "abstract": "Uncertainty quantification is an important prerequisite for the deployment of deep learning models in safety-critical areas. Yet, this hinges on the uncertainty estimates being useful to the extent the prediction intervals are well-calibrated and sharp. In the absence of inherent uncertainty estimates (e.g. pretrained models predicting only point estimates), popular approaches that operate post-hoc include Laplace’s method and split conformal prediction (split-CP). However, Laplace’s method can be miscalibrated when the model is misspecified and split-CP requires sample splitting, and thus comes at the expense of statistical efficiency. In this work, we construct prediction intervals for neural network regressors post-hoc without held-out data. This is achieved by approximating the full conformal prediction method (full-CP). Whilst full-CP nominally requires retraining the model for every test point and candidate label, we propose to train just once and locally perturb model parameters using Gauss-Newton influence to approximate the effect of retraining. Coupled with linearization of the network, we express the absolute residual nonconformity score as a piecewise linear function of the candidate label allowing for an efficient procedure that avoids the exhaustive search over the output space. On standard regression benchmarks and bounding box localization, we show the resulting prediction intervals are locally-adaptive and often tighter than those of split-CP.",
      "authors": [
        "Dharmesh Tailor",
        "Alvaro Correia",
        "Eric Nalisnick",
        "Christos Louizos"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=vcX0k4rGTt",
      "cdate": 1727444071265,
      "mdate": 1743289914273,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836306"
    },
    {
      "id": "5Y9NT6lW21",
      "title": "Adversarial Policy Optimization for Offline Preference-based Reinforcement Learning",
      "abstract": "In this paper, we study offline preference-based reinforcement learning (PbRL), where learning is based on pre-collected preference feedback over pairs of trajectories. While offline PbRL has demonstrated remarkable empirical success, existing theoretical approaches face challenges in ensuring conservatism under uncertainty, requiring computationally intractable confidence set constructions. We address this limitation by proposing Adversarial Preference-based Policy Optimization (APPO), a computationally efficient algorithm for offline PbRL that guarantees sample complexity bounds without relying on explicit confidence sets. By framing PbRL as a two-player game between a policy and a model, our approach enforces conservatism in a tractable manner. Using standard assumptions on function approximation and bounded trajectory concentrability, we derive a sample complexity bound. To our knowledge, APPO is the first offline PbRL algorithm to offer both statistical efficiency and practical applicability. Experimental results on continuous control tasks demonstrate that APPO effectively learns from complex datasets, showing comparable performance with existing state-of-the-art methods.",
      "authors": [
        "Hyungkyu Kang",
        "Min-hwan Oh"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=5Y9NT6lW21",
      "cdate": 1727443998811,
      "mdate": 1747559159910,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836311"
    },
    {
      "id": "qNp86ByQlN",
      "title": "Systematic Relational Reasoning With Epistemic Graph Neural Networks",
      "abstract": "Developing models that can learn to reason is a notoriously challenging problem. We focus on reasoning in relational domains, where the use of Graph Neural Networks (GNNs) seems like a natural choice. However, previous work has shown that regular GNNs lack the ability to systematically generalize from training examples on test graphs requiring longer inference chains, which fundamentally limits their reasoning abilities. A common solution relies on neuro-symbolic methods that systematically reason by learning rules, but their scalability is often limited and they tend to make unrealistically strong assumptions, e.g.\\ that the answer can always be inferred from a single relational path. We propose the Epistemic GNN (EpiGNN), a novel parameter-efficient and scalable GNN architecture with an epistemic inductive bias for systematic reasoning. Node embeddings in EpiGNNs are treated as epistemic states, and message passing  is implemented accordingly. We show that EpiGNNs achieve state-of-the-art results on link prediction tasks that require systematic reasoning. Furthermore, for inductive knowledge graph completion, EpiGNNs rival the performance of state-of-the-art specialized approaches. Finally, we introduce two new benchmarks that go beyond standard relational reasoning by requiring the aggregation of information from multiple paths. Here, existing neuro-symbolic approaches fail, yet EpiGNNs learn to reason accurately.  Code and datasets are available at https://github.com/erg0dic/gnn-sg.",
      "authors": [
        "Irtaza Khalid",
        "Steven Schockaert"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=qNp86ByQlN",
      "cdate": 1727443706659,
      "mdate": 1740677660451,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836319"
    },
    {
      "id": "J6e4hurEKd",
      "title": "RetroInText: A Multimodal Large Language Model Enhanced Framework for Retrosynthetic Planning via In-Context Representation Learning",
      "abstract": "Development of robust and effective strategies for retrosynthetic planning requires a deep understanding of the synthesis process. A critical step in achieving this goal is accurately identifying synthetic intermediates. Current machine learning-based methods often overlook the valuable context from the overall route, focusing only on predicting reactants from the product, requiring cost annotations for every reaction step, and ignoring the multi-faced nature of molecular, resulting in inaccurate synthetic route predictions. Therefore, we introduce RetroInText, an advanced end-to-end framework based on a multimodal Large Language Model (LLM), featuring in-context learning with TEXT descriptions of synthetic routes. First, RetroInText including ChatGPT presents detailed descriptions of the reaction procedure. It learns the distinct compound representations in parallel with corresponding molecule encoders to extract multi-modal representations including 3D features. Subsequently, we propose an attention-based mechanism that offers a fusion module to complement these multi-modal representations with in-context learning and a fine-tuned language model for a single-step model. As a result, RetroInText accurately represents and effectively captures the complex relationship between molecules and the synthetic route. In experiments on the USPTO pathways dataset RetroBench, RetroInText outperforms state-of-the-art methods, achieving up to a 5% improvement in Top-1 test accuracy, particularly for long synthetic routes. These results demonstrate the superiority of RetroInText by integrating with context information over routes. They also demonstrate its potential for advancing pathway design and facilitating the development of organic chemistry. Code is available at https://github.com/guofei-tju/RetroInText.",
      "authors": [
        "Chenglong Kang",
        "Xiaoyi Liu",
        "Fei Guo"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=J6e4hurEKd",
      "cdate": 1727443639291,
      "mdate": 1740637889038,
      "matched_keywords": [
        "large language model",
        "multimodal",
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836324"
    },
    {
      "id": "AD5yx2xq8R",
      "title": "XAIguiFormer: explainable artificial intelligence guided transformer for brain disorder identification",
      "abstract": "EEG-based connectomes offer a low-cost and portable method to identify brain disorders using deep learning. With the growing interest in model interpretability and transparency, explainable artificial intelligence (XAI) is widely applied to understand the decision of deep learning models. However, most research focuses solely on interpretability analysis based on the insights from XAI, overlooking XAI’s potential to improve model performance. To bridge this gap, we propose a dynamical-system-inspired architecture, XAI guided transformer (XAIguiFormer), where XAI not only provides explanations but also contributes to enhancing the transformer by refining the originally coarse information in self-attention mechanism to capture more relevant dependency relationships. In order not to damage the connectome’s topological structure, the connectome tokenizer treats the single-band graphs as atomic tokens to generate a sequence in the frequency domain. To address the limitations of conventional positional encoding in understanding the frequency and mitigating the individual differences, we integrate frequency and demographic information into tokens via a rotation matrix, resulting in a richly informative representation. Our experiment demonstrates that XAIguiFormer achieves superior performance over all baseline models. In addition, XAIguiFormer provides valuable interpretability through visualization of the frequency band importance. Our code is available at https://github.com/HanningGuo/XAIguiFormer.",
      "authors": [
        "Hanning Guo",
        "Farah Abdellatif",
        "Yu Fu",
        "N. Jon Shah",
        "Abigail Morrison",
        "Jürgen Dammers"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=AD5yx2xq8R",
      "cdate": 1727443529423,
      "mdate": 1739638156334,
      "matched_keywords": [
        "transformer",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836329"
    },
    {
      "id": "yFGR36PLDJ",
      "title": "Simple, Good, Fast: Self-Supervised World Models Free of Baggage",
      "abstract": "What are the essential components of world models? How far do we get with world models that are not employing RNNs, transformers, discrete representations, and image reconstructions? This paper introduces SGF, a Simple, Good, and Fast world model that uses self-supervised representation learning, captures short-time dependencies through frame and action stacking, and enhances robustness against model errors through data augmentation. We extensively discuss SGF’s connections to established world models, evaluate the building blocks in ablation studies, and demonstrate good performance through quantitative comparisons on the Atari 100k benchmark. The code is available at https://github.com/jrobine/sgf.",
      "authors": [
        "Jan Robine",
        "Marc Höftmann",
        "Stefan Harmeling"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=yFGR36PLDJ",
      "cdate": 1727443487523,
      "mdate": 1743416434400,
      "matched_keywords": [
        "transformer",
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836334"
    },
    {
      "id": "N4NhVN30ph",
      "title": "TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning",
      "abstract": "This work introduces Transformer-based Off-Policy Episodic Reinforcement Learning (TOP-ERL), a novel algorithm that enables off-policy updates in the ERL framework. In ERL, policies predict entire action trajectories over multiple time steps instead of single actions at every time step. These trajectories are typically parameterized by trajectory generators such as  Movement Primitives (MP), allowing for smooth and efficient exploration over long horizons while capturing high-level temporal correlations. However, ERL methods are often constrained to on-policy frameworks due to the difficulty of evaluating state-action values for entire action sequences, limiting their sample efficiency and preventing the use of more efficient off-policy architectures. TOP-ERL addresses this shortcoming by segmenting long action sequences and estimating the state-action values for each segment using a transformer-based critic architecture alongside an n-step return estimation. These contributions result in efficient and stable training that is reflected in the empirical results conducted on sophisticated robot learning environments. TOP-ERL significantly outperforms state-of-the-art RL methods. Thorough ablation studies additionally show the impact of key design choices on the model performance.",
      "authors": [
        "Ge Li",
        "Dong Tian",
        "Hongyi Zhou",
        "Xinkai Jiang",
        "Rudolf Lioutikov",
        "Gerhard Neumann"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=N4NhVN30ph",
      "cdate": 1727443379141,
      "mdate": 1742039134531,
      "matched_keywords": [
        "reinforcement learning",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836341"
    },
    {
      "id": "NHMuM84tRT",
      "title": "Long-Short Decision Transformer: Bridging Global and Local Dependencies for Generalized Decision-Making",
      "abstract": "Decision Transformers (DTs) effectively capture long-range dependencies using self-attention but struggle with fine-grained local relationships, especially the Markovian properties in many offline-RL datasets. Conversely, Decision Convformer (DC) utilizes convolutional filters for capturing local patterns but shows limitations in tasks demanding long-term dependencies, such as Maze2d. To address these limitations and leverage both strengths, we propose the Long-Short Decision Transformer (LSDT), a general-purpose architecture to effectively capture global and local dependencies across two specialized parallel branches (self-attention and convolution). We explore how these branches complement each other by modeling various ranged dependencies across different environments, and compare it against other baselines. Experimental results demonstrate our LSDT achieves state-of-the-art performance and notable gains over the standard DT in D4RL offline RL benchmark. Leveraging the parallel architecture, LSDT performs consistently on diverse datasets, including Markovian and non-Markovian. We also demonstrate the flexibility of LSDT's architecture, where its specialized branches can be replaced or integrated into models like DC to improve their performance in capturing diverse dependencies. Finally, we also highlight the role of goal states in improving decision-making for goal-reaching tasks like Antmaze.",
      "authors": [
        "Jincheng Wang",
        "Penny Karanasou",
        "Pengyuan Wei",
        "Elia Gatti",
        "Diego Martinez Plasencia",
        "Dimitrios Kanoulas"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=NHMuM84tRT",
      "cdate": 1727443282444,
      "mdate": 1740420919267,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836346"
    },
    {
      "id": "uREg3OHjLL",
      "title": "On the Expressiveness of Rational ReLU Neural Networks With Bounded Depth",
      "abstract": "To confirm that the expressive power of ReLU neural networks grows with their depth, the function $F_n = \\max (0,x_1,\\ldots,x_n )$ has been considered in the literature.\n  A conjecture by Hertrich, Basu, Di Summa, and Skutella [NeurIPS 2021] states that any ReLU network that exactly represents $F_n$ has at least $\\lceil \\log_2 (n+1) \\rceil$ hidden layers.\n  The conjecture has recently been confirmed for networks with integer weights by Haase, Hertrich, and Loho [ICLR 2023].\n\n  We follow up on this line of research and show that, within ReLU networks whose weights are decimal fractions, $F_n$ can only be represented by networks with at least $\\lceil \\log_3 (n+1) \\rceil$ hidden layers.\n  Moreover, if all weights are $N$-ary fractions, then $F_n$ can only be represented by networks with at least $\\Omega( \\frac{\\ln n}{\\ln \\ln N})$ layers.\n  These results are a  partial confirmation of the above conjecture for rational ReLU networks, and provide the first non-constant lower bound on the depth of practically relevant ReLU networks.",
      "authors": [
        "Gennadiy Averkov",
        "Christopher Hojny",
        "Maximilian Merkert"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=uREg3OHjLL",
      "cdate": 1727443174107,
      "mdate": 1740767846347,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836351"
    },
    {
      "id": "U42TkrEDzb",
      "title": "Audio Large Language Models Can Be Descriptive Speech Quality Evaluators",
      "abstract": "An ideal multimodal agent should be aware of the quality of its input modalities. Recent advances have enabled large language models (LLMs) to incorporate auditory systems for handling various speech-related tasks. However, most audio LLMs remain unaware of the quality of the speech they process. This limitation arises because speech quality evaluation is typically excluded from multi-task training due to the lack of suitable datasets. To address this, we introduce the first natural language-based speech evaluation corpus, generated from authentic human ratings. In addition to the overall Mean Opinion Score (MOS), this corpus offers detailed analysis across multiple dimensions and identifies causes of quality degradation. It also enables descriptive comparisons between two speech samples (A/B tests) with human-like judgment. Leveraging this corpus, we propose an alignment approach with LLM distillation (ALLD) to guide the audio LLM in extracting relevant information from raw speech and generating meaningful responses. Experimental results demonstrate that ALLD outperforms the previous state-of-the-art regression model in MOS prediction, with a mean square error of 0.17 and an A/B test accuracy of 98.6%. Additionally, the generated responses achieve BLEU scores of 25.8 and 30.2 on two tasks, surpassing the capabilities of task-specific models. This work advances the comprehensive perception of speech signals by audio LLMs, contributing to the development of real-world auditory and sensory intelligent agents.",
      "authors": [
        "Chen Chen",
        "Yuchen Hu",
        "Siyin Wang",
        "Helin Wang",
        "Zhehuai Chen",
        "Chao Zhang",
        "Chao-Han Huck Yang",
        "EngSiong Chng"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=U42TkrEDzb",
      "cdate": 1727443034178,
      "mdate": 1742005902582,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.836357"
    },
    {
      "id": "H0qIWXXLUR",
      "title": "Learn Your Reference Model for Real Good Alignment",
      "abstract": "Despite the fact that offline methods for Large Language Models (LLMs) alignment do not require a direct reward model, they remain susceptible to overoptimization. This issue arises when the trained model deviates excessively from the reference policy, leading to a decrease in sample quality. We propose a novel approach of offline alignment methods, called Trust Region (including variants TR-DPO, TR-IPO, TR-KTO), which dynamically updates the reference policy throughout the training process. Our results show that TR alignment methods effectively mitigate overoptimization, enabling models to maintain strong performance even when substantially deviating from the initial reference policy. We demonstrate the efficacy of these approaches not only through toy examples that exhibit reduced overoptimization, but also through direct, side-by-side comparisons in specific tasks such as helpful and harmless dialogue, as well as summarization, where they surpass conventional methods. Additionally, we report significant improvements in general-purpose assistant setups with the Llama3 model on the AlpacaEval 2 and Arena-Hard benchmarks, highlighting the advantages of Trust Region methods over classical approaches.",
      "authors": [
        "Alexey Gorbatovski",
        "Boris Shaposhnikov",
        "Alexey Malakhov",
        "Nikita Surnachev",
        "Yaroslav Aksenov",
        "Ian Maksimov",
        "Nikita Balagansky",
        "Daniil Gavrilov"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=H0qIWXXLUR",
      "cdate": 1727442874204,
      "mdate": 1740477675866,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836362"
    },
    {
      "id": "DL9txImSzm",
      "title": "Noise-conditioned Energy-based Annealed Rewards (NEAR): A Generative Framework for Imitation Learning from Observation",
      "abstract": "This paper introduces a new imitation learning framework based on energy-based generative models capable of learning complex, physics-dependent, robot motion policies through state-only expert motion trajectories. Our algorithm, called Noise-conditioned Energy-based Annealed Rewards (NEAR), constructs several perturbed versions of the expert's motion data distribution and learns smooth, and well-defined representations of the data distribution's energy function using denoising score matching. We propose to use these learnt energy functions as reward functions to learn imitation policies via reinforcement learning. We also present a strategy to gradually switch between the learnt energy functions, ensuring that the learnt rewards are always well-defined in the manifold of policy-generated samples. We evaluate our algorithm on complex humanoid tasks such as locomotion and martial arts and compare it with state-only adversarial imitation learning algorithms like Adversarial Motion Priors (AMP). Our framework sidesteps the optimisation challenges of adversarial imitation learning techniques and produces results comparable to AMP in several quantitative metrics across multiple imitation settings.",
      "authors": [
        "Anish Abhijit Diwan",
        "Julen Urain",
        "Jens Kober",
        "Jan Peters"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=DL9txImSzm",
      "cdate": 1727442711897,
      "mdate": 1740291981548,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836367"
    },
    {
      "id": "iDcWYtYUwX",
      "title": "EcoFace: Audio-Visual Emotional Co-Disentanglement Speech-Driven 3D Talking Face Generation",
      "abstract": "Speech-driven 3D facial animation has attracted significant attention due to its wide range of applications in animation production and virtual reality. Recent research has explored speech-emotion disentanglement to enhance facial expressions rather than manually assigning emotions. However, this approach face issues such as feature confusion, emotions weakening and mean-face. To address these issues, we present EcoFace, a framework that (1) proposes a novel collaboration objective to provide a explicit signal for emotion representation learning from the speaker's expressive movements and produced sounds, constructing an audio-visual joint and coordinated emotion space that is independent of speech content. (2) constructs a universal facial motion distribution space determined by speech features and implement speaker-specific generation. Extensive experiments show that our method achieves more generalized and emotionally realistic talking face generation compared to previous methods.",
      "authors": [
        "Jiajian Xie",
        "Shengyu Zhang",
        "Mengze Li",
        "chengfei lv",
        "Zhou Zhao",
        "Fei Wu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=iDcWYtYUwX",
      "cdate": 1727442565547,
      "mdate": 1747269191223,
      "matched_keywords": [
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836375"
    },
    {
      "id": "YwzxpZW3p7",
      "title": "Elliptic Loss Regularization",
      "abstract": "Regularizing neural networks is important for anticipating model behavior in regions of the data space that are not well represented. In this work, we propose a regularization technique for enforcing a level of smoothness in the mapping between the input space and the loss. We specify the level of regularity by requiring that the loss of the network satisfies an elliptic operator over the data domain. To do this, we modify the usual empirical risk minimization objective such that we instead minimize a new objective that satisfies an elliptic operator over points within the domain. This allows us to use existing theory on elliptic operators to anticipate the behavior of the error for points outside the training set. We propose a tractable computational method that approximates the behavior of the elliptic operator while being computationally efficient. Finally, we analyze the properties of the proposed regularization to understand the performance on common problems of distribution shift and group imbalance. Numerical experiments empirically confirm the promise of the proposed regularization technique.",
      "authors": [
        "Ali Hasan",
        "Haoming Yang",
        "Yuting Ng",
        "Vahid Tarokh"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=YwzxpZW3p7",
      "cdate": 1727442324110,
      "mdate": 1740505986540,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836380"
    },
    {
      "id": "Xj66fkrlTk",
      "title": "Optimizing Backward Policies in GFlowNets via Trajectory Likelihood Maximization",
      "abstract": "Generative Flow Networks (GFlowNets) are a family of generative models that learn to sample objects with probabilities proportional to a given reward function. The key concept behind GFlowNets is the use of two stochastic policies: a forward policy, which incrementally constructs compositional objects, and a backward policy, which sequentially deconstructs them. Recent results show a close relationship between GFlowNet training and entropy-regularized reinforcement learning (RL) problems with a particular reward design. However, this connection applies only in the setting of a fixed backward policy, which might be a significant limitation. As a remedy to this problem, we introduce a simple backward policy optimization algorithm that involves direct maximization of the value function in an entropy-regularized Markov Decision Process (MDP) over intermediate rewards. We provide an extensive experimental evaluation of the proposed approach across various benchmarks in combination with both RL and GFlowNet algorithms and demonstrate its faster convergence and mode discovery in complex environments.",
      "authors": [
        "Timofei Gritsaev",
        "Nikita Morozov",
        "Sergey Samsonov",
        "Daniil Tiapkin"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Xj66fkrlTk",
      "cdate": 1727442316829,
      "mdate": 1740758280534,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836385"
    },
    {
      "id": "IssPhpUsKt",
      "title": "Improving Reasoning Performance in Large Language Models via Representation Engineering",
      "abstract": "Recent advancements in large language models (LLMs) have resulted in increasingly anthropomorphic language concerning the ability of LLMs to reason. Whether \\textit{reasoning} in LLMs should be understood to be inherently different is, however, widely debated. We propose utilizing a representation engineering approach wherein model activations are read from the residual stream of an LLM when processing a reasoning task. The activations are used to derive a control vector that is applied to the model as an inference-time intervention, modulating the representational space of the model, to improve performance on the specified task. We publish the code for deriving control vectors and analyzing model representations. The method allows us to improve performance on reasoning benchmarks and assess how control vectors influence the final logit distribution of a model via metrics such as KL divergence and entropy. We apply control vectors to Mistral-7B-Instruct and a range of Pythia models on an inductive, a deductive and mathematical reasoning task. We show that an LLM can, to a certain degree, be controlled to improve its perceived reasoning ability by modulating activations. The intervention is dependent upon the ability to reliably extract the model's typical state when correctly solving a task. Our results suggest that reasoning performance can be modulated in the same manner as other information-processing tasks performed by LLMs and demonstrate that we are capable of improving performance on specific tasks via a simple intervention on the residual stream with no additional training.",
      "authors": [
        "Bertram Højer",
        "Oliver Simon Jarvis",
        "Stefan Heinrich"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=IssPhpUsKt",
      "cdate": 1727442199378,
      "mdate": 1740907624745,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836390"
    },
    {
      "id": "HSi4VetQLj",
      "title": "Improving Uncertainty Estimation through Semantically Diverse Language Generation",
      "abstract": "Large language models (LLMs) can suffer from hallucinations when generating text. These hallucinations impede various applications in society and industry by making LLMs untrustworthy. Current LLMs generate text in an autoregressive fashion by predicting and appending text tokens. When an LLM is uncertain about the semantic meaning of the next tokens to generate, it is likely to start hallucinating. Thus, it has been suggested that predictive uncertainty is one of the main causes of hallucinations. We introduce Semantically Diverse Language Generation (SDLG) to quantify predictive uncertainty in LLMs. SDLG steers the LLM to generate semantically diverse yet likely alternatives for an initially generated text. This approach provides a precise measure of aleatoric semantic uncertainty, detecting whether the initial text is likely to be hallucinated. Experiments on question-answering tasks demonstrate that SDLG consistently outperforms existing methods while being the most computationally efficient, setting a new standard for uncertainty estimation in LLMs.",
      "authors": [
        "Lukas Aichberger",
        "Kajetan Schweighofer",
        "Mykyta Ielanskyi",
        "Sepp Hochreiter"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=HSi4VetQLj",
      "cdate": 1727442089893,
      "mdate": 1740851987496,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836395"
    },
    {
      "id": "CbfsKHiWEn",
      "title": "Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization",
      "abstract": "This study addresses the challenge of noise in training datasets for Direct Preference Optimization (DPO), a method for aligning Large Language Models (LLMs) with human preferences. We categorize noise into pointwise noise, which includes low-quality data points, and pairwise noise, which encompasses erroneous data pair associations that affect preference rankings. Utilizing Distributionally Robust Optimization (DRO), we enhance DPO's resilience to these types of noise. Our theoretical insights reveal that DPO inherently embeds DRO principles, conferring robustness to pointwise noise, with the regularization coefficient $\\beta$ playing a critical role in its noise resistance. Extending this framework, we introduce Distributionally Robustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing against worst-case pairwise scenarios. The novel hyperparameter $\\beta'$ in Dr. DPO allows for fine-tuned control over data pair reliability, providing a strategic balance between exploration and exploitation in noisy training environments. Empirical evaluations demonstrate that Dr. DPO substantially improves the quality of generated text and response accuracy in preference datasets, showcasing enhanced performance in both noisy and noise-free settings.",
      "authors": [
        "Junkang Wu",
        "Yuexiang Xie",
        "Zhengyi Yang",
        "Jiancan Wu",
        "Jiawei Chen",
        "Jinyang Gao",
        "Bolin Ding",
        "Xiang Wang",
        "Xiangnan He"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=CbfsKHiWEn",
      "cdate": 1727441968868,
      "mdate": 1740212120828,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836400"
    },
    {
      "id": "97rOQDPmk2",
      "title": "On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent",
      "abstract": "The Adam optimizer is widely used for transformer optimization in practice, which makes understanding the underlying optimization mechanisms an important problem.\nHowever, due to the Adam's complexity, theoretical analysis of how it optimizes transformers remains a challenging task. \nFortunately, Sign Gradient Descent (SignGD) serves as an effective surrogate for Adam.\nDespite its simplicity, theoretical understanding of how SignGD optimizes transformers still lags behind.\nIn this work, we study how SignGD optimizes a two-layer transformer -- consisting of a softmax attention layer with trainable query-key parameterization followed by a linear layer -- on \na linearly separable noisy dataset.\nWe identify four stages in the training dynamics, each exhibiting intriguing behaviors.\nBased on the training dynamics, we prove the fast convergence but poor generalization of the learned transformer on the noisy dataset.\nWe also show that Adam behaves similarly to SignGD in terms of both optimization and generalization in this setting.\nAdditionally, we find that the poor generalization of SignGD is not solely due to data noise,\nsuggesting that both SignGD and Adam requires high-quality data for real-world tasks.\nFinally, experiments on synthetic and real-world datasets empirically support our theoretical results.",
      "authors": [
        "Bingrui Li",
        "Wei Huang",
        "Andi Han",
        "Zhanpeng Zhou",
        "Taiji Suzuki",
        "Jun Zhu",
        "Jianfei Chen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=97rOQDPmk2",
      "cdate": 1727441914980,
      "mdate": 1746091895380,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836405"
    },
    {
      "id": "V4K9h1qNxE",
      "title": "Attention as a Hypernetwork",
      "abstract": "Transformers can under some circumstances generalize to novel problem instances whose constituent parts might have been encountered during training, but whose compositions have not.\nWhat mechanisms underlie this ability for compositional generalization?\nBy reformulating multi-head attention as a hypernetwork, we reveal that a composable, low-dimensional latent code specifies key-query specific operations.\nWe find empirically that this latent code is predictive of the subtasks the network performs on unseen task compositions, revealing that latent codes acquired during training are reused to solve unseen problem instances.\nTo further examine the hypothesis that the intrinsic hypernetwork of multi-head attention supports compositional generalization, we ablate whether making the hypernetwork-generated linear value network nonlinear strengthens compositionality.\nWe find that this modification improves compositional generalization on abstract reasoning tasks.\nIn particular, we introduce a symbolic version of the Raven's Progressive Matrices human intelligence test, which gives us precise control over the problem compositions encountered during training and evaluation.\nWe demonstrate on this task how scaling model size and data enables compositional generalization in transformers and gives rise to a functionally structured latent space.",
      "authors": [
        "Simon Schug",
        "Seijin Kobayashi",
        "Yassir Akram",
        "Joao Sacramento",
        "Razvan Pascanu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=V4K9h1qNxE",
      "cdate": 1727441895854,
      "mdate": 1739807078270,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836410"
    },
    {
      "id": "9chRqsPOGL",
      "title": "SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models",
      "abstract": "Instruction-following is a fundamental capability of language models, requiring the model to recognize even the most subtle requirements in the instructions and accurately reflect them in its output.\nSuch an ability is well-suited for and often optimized by preference learning.\nHowever, existing methods often directly sample multiple independent responses from the model when creating preference pairs.\nSuch practice can introduce content variations irrelevant to whether the instruction is precisely followed (e.g., different expressions about the same semantic), interfering with the goal of teaching models to recognize the key differences that lead to improved instruction following.\nIn light of this, we introduce SPaR, a self-play framework integrating tree-search self-refinement to yield valid and comparable preference pairs free from distractions.\nBy playing against itself, an LLM employs a tree-search strategy to refine its previous responses with respect to the instruction while minimizing unnecessary variations.\nOur experiments show that a LLaMA3-8B model, trained over three iterations guided by SPaR, surpasses GPT-4-Turbo on the IFEval benchmark without losing general capabilities. \nFurthermore, SPaR demonstrates promising scalability, greatly enhancing models like GLM-4-9B and LLaMA3-70B.\nWe also identify how inference scaling in tree search would impact model performance.\nOur code and data are publicly available at https://github.com/thu-coai/SPaR.",
      "authors": [
        "Jiale Cheng",
        "Xiao Liu",
        "Cunxiang Wang",
        "Xiaotao Gu",
        "Yida Lu",
        "Dan Zhang",
        "Yuxiao Dong",
        "Jie Tang",
        "Hongning Wang",
        "Minlie Huang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=9chRqsPOGL",
      "cdate": 1727441806670,
      "mdate": 1740722576187,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836416"
    },
    {
      "id": "Ev4iw23gdI",
      "title": "EMMA: Empowering Multi-modal Mamba with Structural and Hierarchical Alignment",
      "abstract": "Mamba-based architectures have shown to be a promising new direction for deep learning models owing to their competitive performance and sub-quadratic deployment speed. However, current Mamba multi-modal large language models (MLLM) are insufficient in extracting visual features, leading to imbalanced cross-modal alignment between visual and textural latents, negatively impacting performance on multi-modal tasks. In this work, we propose Empowering Multi-modal Mamba with Structural and Hierarchical Alignment (EMMA), which enables the MLLM to extract fine-grained visual information. Specifically, we propose a pixel-wise alignment module to autoregressively optimize the learning and processing of spatial image-level features along with textual tokens, enabling structural alignment at the image level. In addition, to prevent the degradation of visual information during the cross-model alignment process, we propose a multi-scale feature fusion (MFF) module to combine multi-scale visual features from intermediate layers, enabling hierarchical alignment at the feature level. Extensive experiments are conducted across a variety of multi-modal benchmarks. Our model shows lower latency than other Mamba-based MLLMs and is nearly four times faster than transformer-based MLLMs of similar scale during inference. Due to better cross-modal alignment, our model exhibits lower degrees of hallucination and enhanced sensitivity to visual details, which manifests in superior performance across diverse multi-modal benchmarks. Code provided at https://github.com/xingyifei2016/EMMA.",
      "authors": [
        "Yifei Xing",
        "Xiangyuan Lan",
        "Ruiping Wang",
        "Dongmei Jiang",
        "Wenjun Huang",
        "Zheng Qingfang",
        "Yaowei Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=Ev4iw23gdI",
      "cdate": 1727441735968,
      "mdate": 1740708338806,
      "matched_keywords": [
        "large language model",
        "transformer",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836421"
    },
    {
      "id": "rySLejeB1k",
      "title": "Emergent Orientation Maps —— Mechanisms, Coding Efficiency and Robustness",
      "abstract": "Extensive experimental studies have shown that in lower mammals, neuronal orientation preference in the primary visual cortex is organized in disordered \"salt-and-pepper\" organizations. In contrast, higher-order mammals display a continuous variation in orientation preference, forming pinwheel-like structures. Despite these observations, the spiking mechanisms underlying the emergence of these distinct topological structures and their functional roles in visual processing remain poorly understood. To address this, we developed a self-evolving spiking neural network model with Hebbian plasticity, trained using physiological parameters characteristic of rodents, cats, and primates, including retinotopy, neuronal morphology, and connectivity patterns. Our results identify critical factors, such as the degree of input visual field overlap, neuronal connection range, and the balance between localized connectivity and long-range competition, that determine the emergence of either salt-and-pepper or pinwheel-like topologies. Furthermore, we demonstrate that pinwheel structures exhibit lower wiring costs and enhanced sparse coding capabilities compared to salt-and-pepper organizations. They also maintain greater coding robustness against noise in naturalistic visual stimuli. These findings suggest that such topological structures confer significant computational advantages in visual processing and highlight their potential application in the design of brain-inspired deep learning networks and algorithms.",
      "authors": [
        "Haixin Zhong",
        "Haoyu Wang",
        "Wei P Dai",
        "Yuchao Huang",
        "Mingyi Huang",
        "Rubin Wang",
        "Anna Wang Roe",
        "yuguo yu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=rySLejeB1k",
      "cdate": 1727441690287,
      "mdate": 1743520966137,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836429"
    },
    {
      "id": "9tKC0YM8sX",
      "title": "Exact Computation of Any-Order Shapley Interactions for Graph Neural Networks",
      "abstract": "Albeit the ubiquitous use of Graph Neural Networks (GNNs) in machine learning (ML) prediction tasks involving graph-structured data, their interpretability remains challenging. In explainable artificial intelligence (XAI), the Shapley Value (SV) is the predominant method to quantify contributions of individual features to a ML model’s output. Addressing the limitations of SVs in complex prediction models, Shapley Interactions (SIs) extend the SV to groups of features. In this work, we explain single graph predictions of GNNs with SIs that quantify node contributions and interactions among multiple nodes. By exploiting the GNN architecture, we show that the structure of interactions in node embeddings are preserved for graph prediction. As a result, the exponential complexity of SIs depends only on the receptive fields, i.e. the message-passing ranges determined by the connectivity of the graph and the number of convolutional layers. Based on our theoretical results, we introduce GraphSHAP-IQ, an efficient approach to compute any-order SIs exactly. GraphSHAP-IQ is applicable to popular message passing techniques in conjunction with a linear global pooling and output layer. We showcase that GraphSHAP-IQ substantially reduces the exponential complexity of computing exact SIs on multiple benchmark datasets. Beyond exact computation, we evaluate GraphSHAP-IQ’s approximation of SIs on popular GNN architectures and compare with existing baselines. Lastly, we visualize SIs of real-world water distribution networks and molecule structures using a SI-Graph.",
      "authors": [
        "Maximilian Muschalik",
        "Fabian Fumagalli",
        "Paolo Frazzetto",
        "Janine Strotherm",
        "Luca Hermes",
        "Alessandro Sperduti",
        "Eyke Hüllermeier",
        "Barbara Hammer"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=9tKC0YM8sX",
      "cdate": 1727441550196,
      "mdate": 1740890350583,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836434"
    },
    {
      "id": "nt8gBX58Kh",
      "title": "Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in Large Models",
      "abstract": "In recent years, there has been increasing attention on the capabilities of large-scale models, particularly in handling complex tasks that small-scale models are unable to perform. Notably, large language models (LLMs) have demonstrated ``intelligent'' abilities such as complex reasoning and abstract language comprehension, reflecting cognitive-like behaviors. However, current research on emergent abilities in large models predominantly focuses on the relationship between model performance and size, leaving a significant gap in the systematic quantitative analysis of the internal structures and mechanisms driving these emergent abilities. Drawing inspiration from neuroscience research on brain network structure and self-organization, we propose (i) a general network representation of large models, (ii) a new analytical framework — *Neuron-based Multifractal Analysis (NeuroMFA)* - for structural analysis, and (iii) a novel structure-based metric as a proxy for emergent abilities of large models. By linking structural features to the capabilities of large models, *NeuroMFA* provides a quantitative framework for analyzing emergent phenomena in large models. Our experiments show that the proposed method yields a comprehensive measure of the network's evolving heterogeneity and organization, offering theoretical foundations and a new perspective for investigating emergence in large models.",
      "authors": [
        "Xiongye Xiao",
        "Heng Ping",
        "Chenyu Zhou",
        "Defu Cao",
        "Yaxing Li",
        "Yi-Zhuo Zhou",
        "Shixuan Li",
        "Nikos Kanakaris",
        "Paul Bogdan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=nt8gBX58Kh",
      "cdate": 1727441286411,
      "mdate": 1740901736505,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836439"
    },
    {
      "id": "bMC1t7eLRc",
      "title": "Harnessing Diversity for Important Data Selection in Pretraining Large Language Models",
      "abstract": "Data selection is of great significance in  pretraining large language models, given the  variation in quality within the large-scale available training corpora. \nTo achieve this, researchers are currently investigating the use of data influence to measure the importance of data instances, $i.e.,$ a high influence score indicates that incorporating this instance to the training set is likely to enhance the model performance. Consequently, they select the top-$k$ instances with the highest scores.  However, this approach has several limitations. \n(1) Calculating the accurate influence of all available data is time-consuming.\n(2) The selected data instances are not diverse enough, which may hinder the pretrained model's ability to generalize effectively to various downstream tasks.\nIn this paper, we introduce $\\texttt{Quad}$, a data selection approach that considers both quality and diversity by using data influence to achieve state-of-the-art pretraining results.\nTo compute the influence ($i.e.,$ the quality) more accurately and efficiently, we incorporate the attention layers to capture more semantic details, which can be accelerated through the Kronecker product. \nFor the diversity, $\\texttt{Quad}$ clusters the dataset into similar data instances within each cluster and diverse instances across different clusters. For each cluster, if we opt to select data from it, we take some samples to evaluate the influence to prevent processing all instances. Overall, we favor clusters with highly influential instances (ensuring high quality) or clusters that have been selected less frequently (ensuring diversity), thereby well balancing between quality and diversity.  Experiments on Slimpajama and FineWeb over 7B large language models demonstrate that $\\texttt{Quad}$ significantly outperforms other data selection methods with a low FLOPs consumption. Further analysis also validates the effectiveness of our influence calculation.",
      "authors": [
        "Chi Zhang",
        "Huaping Zhong",
        "Kuan Zhang",
        "Chengliang Chai",
        "Rui Wang",
        "Xinlin Zhuang",
        "Tianyi Bai",
        "Qiu Jiantao",
        "Lei Cao",
        "Ju Fan",
        "Ye Yuan",
        "Guoren Wang",
        "Conghui He"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=bMC1t7eLRc",
      "cdate": 1727440959575,
      "mdate": 1740669521928,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836444"
    },
    {
      "id": "PDnEDS244P",
      "title": "Magnetic Preference Optimization: Achieving Last-iterate Convergence for Language Model Alignment",
      "abstract": "Self-play methods have demonstrated remarkable success in enhancing model capabilities across various domains. In the context of Reinforcement Learning from Human Feedback (RLHF), self-play not only boosts Large Language Model (LLM) performance but also overcomes the limitations of traditional Bradley-Terry (BT) model assumptions by finding the Nash equilibrium (NE) of a preference-based, two-player constant-sum game. However, existing methods either guarantee only average-iterate convergence, incurring high storage and inference costs, or converge to the NE of a regularized game, failing to accurately reflect true human preferences. In this paper, we introduce Magnetic Preference Optimization (MPO), a novel approach capable of achieving last-iterate convergence to the NE of the original game, effectively overcoming the limitations of existing methods. Building upon Magnetic Mirror Descent (MMD), MPO attains a linear convergence rate, making it particularly suitable for fine-tuning LLMs. To ensure our algorithm is both theoretically sound and practically viable, we present a simple yet effective implementation that adapts the theoretical insights to the RLHF setting. Empirical results demonstrate that MPO can significantly enhance the performance of LLMs, highlighting the potential of self-play methods in alignment.",
      "authors": [
        "Mingzhi Wang",
        "Chengdong Ma",
        "Qizhi Chen",
        "Linjian Meng",
        "Yang Han",
        "Jiancong Xiao",
        "Zhaowei Zhang",
        "Jing Huo",
        "Weijie J Su",
        "Yaodong Yang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=PDnEDS244P",
      "cdate": 1727440700864,
      "mdate": 1746012259759,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836449"
    },
    {
      "id": "EcrdmRT99M",
      "title": "The Effectiveness of Curvature-Based Rewiring and the Role of Hyperparameters in GNNs Revisited",
      "abstract": "Message passing is the dominant paradigm in Graph Neural Networks (GNNs). The efficiency of message passing, however, can be limited by the topology of the graph. This happens when information is lost during propagation due to being oversquashed when travelling through bottlenecks. To remedy this, recent efforts have focused on graph rewiring techniques, which disconnect the input graph originating from the data and the computational graph, on which message passing is performed. A prominent approach for this is to use discrete graph curvature measures, of which several variants have been proposed, to identify and rewire around bottlenecks, facilitating information propagation. While oversquashing has been demonstrated in synthetic datasets, in this work we reevaluate the performance gains that curvature-based rewiring brings to real-world datasets. We show that in these datasets, edges selected during the rewiring process are not in line with theoretical criteria identifying bottlenecks. This implies they do not necessarily oversquash information during message passing. Subsequently, we demonstrate that SOTA accuracies on these datasets are outliers originating from sweeps of hyperparameters—both the ones for training and dedicated ones related to the rewiring algorithm—instead of consistent performance gains. In conclusion, our analysis nuances the effectiveness of curvature-based rewiring in real-world datasets and brings a new perspective on the methods to evaluate GNN accuracy improvements.",
      "authors": [
        "Floriano Tori",
        "Vincent Holst",
        "Vincent Ginis"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=EcrdmRT99M",
      "cdate": 1727440632675,
      "mdate": 1739887693063,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836454"
    },
    {
      "id": "J9eKm7j6KD",
      "title": "Words in Motion: Extracting Interpretable Control Vectors for Motion Transformers",
      "abstract": "Transformer-based models generate hidden states that are difficult to interpret. In this work, we analyze hidden states and modify them at inference, with a focus on motion forecasting. We use linear probing to analyze whether interpretable features are embedded in hidden states. Our experiments reveal high probing accuracy, indicating latent space regularities with functionally important directions. Building on this, we use the directions between hidden states with opposing features to fit control vectors. At inference, we add our control vectors to hidden states and evaluate their impact on predictions. Remarkably, such modifications preserve the feasibility of predictions. We further refine our control vectors using sparse autoencoders (SAEs). This leads to more linear changes in predictions when scaling control vectors. Our approach enables mechanistic interpretation as well as zero-shot generalization to unseen dataset characteristics with negligible computational overhead.",
      "authors": [
        "Omer Sahin Tas",
        "Royden Wagner"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=J9eKm7j6KD",
      "cdate": 1727440567569,
      "mdate": 1747415311668,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836460"
    },
    {
      "id": "KvaDHPhhir",
      "title": "Sketch2Diagram: Generating Vector Diagrams from Hand-Drawn Sketches",
      "abstract": "We address the challenge of automatically generating high-quality vector diagrams from hand-drawn sketches. \nVector diagrams are essential for communicating complex ideas across various fields, offering flexibility and scalability. \nWhile recent research has progressed in generating diagrams from text descriptions, converting hand-drawn sketches into vector diagrams remains largely unexplored due to the lack of suitable datasets. \nTo address this gap, we introduce SketikZ, a dataset comprising 3,231 pairs of hand-drawn sketches and thier corresponding TikZ codes as well as reference diagrams.\nOur evaluations reveal the limitations of state-of-the-art vision and language models (VLMs), positioning SketikZ as a key benchmark for future research in sketch-to-diagram conversion.\nAlong with SketikZ, we present ImgTikZ, an image-to-TikZ model that integrates a 6.7B parameter code-specialized open-source large language model (LLM) with a pre-trained vision encoder. \nDespite its relatively compact size, ImgTikZ performs comparably to GPT-4o.\nThis success is driven by using our two data augmentation techniques and a multi-candidate inference strategy.\nOur findings open promising directions for future research in sketch-to-diagram conversion and broader image-to-code generation tasks. SketikZ is publicly available.",
      "authors": [
        "Itsumi Saito",
        "Haruto Yoshida",
        "Keisuke Sakaguchi"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=KvaDHPhhir",
      "cdate": 1727440350455,
      "mdate": 1742113102297,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836465"
    },
    {
      "id": "n2NidsYDop",
      "title": "Transformers Provably Solve Parity Efficiently with Chain of Thought",
      "abstract": "This work provides the first theoretical analysis of training transformers to solve complex problems by recursively generating intermediate states, analogous to fine-tuning for chain-of-thought (CoT) reasoning. We consider training a one-layer transformer to solve the fundamental $k$-parity problem, extending the work on RNNs by \\citet{Wies23}. We establish three key results: (1) any finite-precision gradient-based algorithm, without intermediate supervision, requires substantial iterations to solve parity with finite samples. (2) In contrast, when intermediate parities are incorporated into the loss function, our model can learn parity in one gradient update when aided by \\emph{teacher forcing}, where ground-truth labels of the reasoning chain are provided at each generation step. (3) Even without teacher forcing, where the model must generate CoT chains end-to-end, parity can be learned efficiently if augmented data is employed to internally verify the soundness of intermediate steps. Our findings, supported by numerical experiments, show that task decomposition and stepwise reasoning naturally arise from optimizing transformers with CoT; moreover, self-consistency checking can improve multi-step reasoning ability, aligning with empirical studies of CoT.",
      "authors": [
        "Juno Kim",
        "Taiji Suzuki"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=n2NidsYDop",
      "cdate": 1727440308105,
      "mdate": 1746089531429,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836470"
    },
    {
      "id": "HfWcFs7XLR",
      "title": "Agents' Room:  Narrative Generation through Multi-step Collaboration",
      "abstract": "Writing compelling fiction is a multifaceted process combining elements such as crafting a plot, developing interesting characters, and using evocative language. While large language models (LLMs) show promise for story writing, they currently rely heavily on intricate prompting, which limits their use. We propose Agents' Room, a generation framework inspired by narrative theory, that decomposes narrative writing into subtasks tackled by specialized agents. To illustrate our method, we introduce Tell Me A Story, a high-quality dataset of complex writing prompts and human-written stories, and a novel evaluation framework designed specifically for assessing long narratives. We show that Agents' Room generates stories that are preferred by expert evaluators over those produced by baseline systems by leveraging collaboration and specialization to decompose the complex story writing task into tractable components. We provide extensive analysis with automated and human-based metrics of the generated output.",
      "authors": [
        "Fantine Huot",
        "Reinald Kim Amplayo",
        "Jennimaria Palomaki",
        "Alice Shoshana Jakobovits",
        "Elizabeth Clark",
        "Mirella Lapata"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=HfWcFs7XLR",
      "cdate": 1727440169497,
      "mdate": 1741971884410,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836478"
    },
    {
      "id": "eXB5TCrAu9",
      "title": "How Does Vision-Language Adaptation Impact the Safety of Vision Language Models?",
      "abstract": "Vision-Language adaptation (VL adaptation) transforms Large Language Models (LLMs) into Large Vision-Language Models (LVLMs) for multimodal tasks, but this process often compromises the inherent safety capabilities embedded in the original LLMs. Despite potential harmfulness due to weakened safety measures, in-depth analysis on the effects of VL adaptation on safety remains under-explored. This study examines how VL adaptation influences safety and evaluates the impact of safety fine-tuning methods. Our analysis reveals that safety degradation occurs during VL adaptation, even when the training data is safe. While safety tuning techniques like supervised fine-tuning with safety datasets or reinforcement learning from human feedback mitigate some risks, they still lead to safety degradation and a reduction in helpfulness due to over-rejection issues. Further analysis of internal model weights suggests that VL adaptation may impact certain safety-related layers, potentially lowering overall safety levels. Additionally, our findings demonstrate that the objectives of VL adaptation and safety tuning are divergent, which often results in their simultaneous application being suboptimal. To address this, we suggest the weight merging approach as an optimal solution effectively reducing safety degradation while maintaining helpfulness. These insights help guide the development of more reliable and secure LVLMs for real-world applications.",
      "authors": [
        "Seongyun Lee",
        "Geewook Kim",
        "Jiyeon Kim",
        "Hyunji Lee",
        "Hoyeon Chang",
        "Sue Hyun Park",
        "Minjoon Seo"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=eXB5TCrAu9",
      "cdate": 1727440136039,
      "mdate": 1740973863795,
      "matched_keywords": [
        "large language model",
        "reinforcement learning",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.836486"
    },
    {
      "id": "GRMfXcAAFh",
      "title": "Oscillatory State-Space Models",
      "abstract": "We propose Linear Oscillatory State-Space models (LinOSS) for efficiently learning on long sequences. Inspired by cortical dynamics of biological neural networks, we base our proposed LinOSS model on a system of forced harmonic oscillators. A stable discretization, integrated over time using fast associative parallel scans, yields the proposed state-space model. We prove that LinOSS produces stable dynamics only requiring nonnegative diagonal state matrix. This is in stark contrast to many previous state-space models relying heavily on restrictive parameterizations. Moreover, we rigorously show that LinOSS is universal, i.e., it can approximate any continuous and causal operator mapping between time-varying functions, to desired accuracy. In addition, we show that an implicit-explicit discretization of LinOSS perfectly conserves the symmetry of time reversibility of the underlying dynamics. Together, these properties enable efficient modeling of long-range interactions, while ensuring stable and accurate long-horizon forecasting. Finally, our empirical results, spanning a wide range of time-series tasks from mid-range to very long-range classification and regression, as well as long-horizon forecasting, demonstrate that our proposed LinOSS model consistently outperforms state-of-the-art sequence models. Notably, LinOSS outperforms Mamba and LRU by nearly 2x on a sequence modeling task with sequences of length 50k.",
      "authors": [
        "T. Konstantin Rusch",
        "Daniela Rus"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=GRMfXcAAFh",
      "cdate": 1727440112313,
      "mdate": 1740773570985,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836491"
    },
    {
      "id": "whaO3482bs",
      "title": "ChroKnowledge: Unveiling Chronological Knowledge of Language Models in Multiple Domains",
      "abstract": "Large language models (LLMs) have brought significant changes to many aspects of our lives.\nHowever, assessing and ensuring their chronological knowledge remains challenging.\nExisting approaches fall short in addressing the temporal adaptability of knowledge, often relying on a fixed time-point view. \nTo overcome this, we introduce ChroKnowBench, a benchmark dataset designed to evaluate chronologically accumulated knowledge across three key aspects: multiple domains, time dependency, temporal state.\nOur benchmark distinguishes between knowledge that evolves (e.g., personal history, scientific discoveries, amended laws) and knowledge that remain constant (e.g., mathematical truths, commonsense facts). \nBuilding on this benchmark, we present ChroKnowledge (Chronological Categorization of Knowledge), a novel sampling-based framework for evaluating LLMs' non-parametric chronological knowledge.\nOur evaluation led to the following observations: \n(1) The ability of eliciting temporal knowledge varies depending on the data format that model was trained on.\n(2) LLMs partially recall knowledge or show a cut-off at temporal boundaries rather than recalling all aspects of knowledge correctly.\nThus, we apply our ChroKnowPrompt, an in-depth prompting to elicit chronological knowledge by traversing step-by-step through the surrounding time spans.\nWe observe that it successfully recalls objects across both open-source and proprietary LLMs, demonstrating versatility, though it faces challenges with dynamic datasets and unstructured formats.",
      "authors": [
        "Yein Park",
        "Chanwoong Yoon",
        "Jungwoo Park",
        "Donghyeon Lee",
        "Minbyul Jeong",
        "Jaewoo Kang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=whaO3482bs",
      "cdate": 1727439812653,
      "mdate": 1740728758050,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836497"
    },
    {
      "id": "CI4sCBMXjP",
      "title": "ELICIT: LLM Augmentation Via External In-context Capability",
      "abstract": "Enhancing the adaptive capabilities of large language models is a critical pursuit in both research and application.\nTraditional fine-tuning methods require substantial data, computational resources, and specific capabilities, while in-context learning is limited by the need for appropriate demonstrations and efficient token usage.\n    Inspired by the expression of in-context learned capabilities through task vectors and the concept of modular capability or knowledge, we propose ELICIT, a framework consisting of two modules designed to effectively store and reuse task vectors to enhance the diverse adaptive capabilities of models without additional training or inference tokens.\n    Our comprehensive experiments and analysis demonstrate that our pipeline is highly transferable across different input formats, tasks, and model architectures.\n    Externally storing and reusing vectors that represent in-context learned capabilities not only shows the potential to extract modular capabilities but also significantly enhances the performance, versatility, adaptability, and scalability of large language models, paving the way for more efficient and effective use of these models in a wide range of applications.",
      "authors": [
        "Futing Wang",
        "Jianhao Yan",
        "Yue Zhang",
        "Tao Lin"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=CI4sCBMXjP",
      "cdate": 1727439552557,
      "mdate": 1740540650323,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836502"
    },
    {
      "id": "dTPz4rEDok",
      "title": "Offline Hierarchical Reinforcement Learning via Inverse Optimization",
      "abstract": "Hierarchical policies enable strong performance in many sequential decision-making problems, such as those with high-dimensional action spaces, those requiring long-horizon planning, and settings with sparse rewards. \nHowever, learning hierarchical policies from static offline datasets presents a significant challenge.\nCrucially, actions taken by higher-level policies may not be directly observable within hierarchical controllers, and the offline dataset might have been generated using a different policy structure, hindering the use of standard offline learning algorithms.\nIn this work, we propose $\\textit{OHIO}$: a framework for offline reinforcement learning (RL) of hierarchical policies. \nOur framework leverages knowledge of the policy structure to solve the $\\textit{inverse problem}$, recovering the unobservable high-level actions that likely generated the observed data under our hierarchical policy.\nThis approach constructs a dataset suitable for off-the-shelf offline training.\nWe demonstrate our framework on robotic and network optimization problems and show that it substantially outperforms end-to-end RL methods and improves robustness.  We investigate a variety of instantiations of our framework, both in direct deployment of policies trained offline and when online fine-tuning is performed. Code and data are available at https://ohio-offline-hierarchical-rl.github.io.",
      "authors": [
        "Carolin Schmidt",
        "Daniele Gammelli",
        "James Harrison",
        "Marco Pavone",
        "Filipe Rodrigues"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=dTPz4rEDok",
      "cdate": 1727439408727,
      "mdate": 1741958652147,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836507"
    },
    {
      "id": "D2hhkU5O48",
      "title": "QA-Calibration of Language Model Confidence Scores",
      "abstract": "To use generative question-and-answering (QA) systems for decision-making and in any critical application, these systems need to provide well-calibrated confidence scores that reflect the correctness of their answers. Existing calibration methods aim to ensure that the confidence score is, *on average*, indicative of the likelihood that the answer is correct.  We argue, however, that this standard (average-case) notion of calibration is difficult to interpret for decision-making in generative QA. To address this, we generalize the standard notion of average calibration and introduce QA-calibration, which ensures calibration holds across different question-and-answer groups. We then propose discretized posthoc calibration schemes for achieving QA-calibration. We establish distribution-free guarantees on the performance of this method and validate our method on confidence scores returned by elicitation prompts across multiple QA benchmarks and large language models (LLMs).",
      "authors": [
        "Putra Manggala",
        "Atalanti A. Mastakouri",
        "Elke Kirschbaum",
        "Shiva Kasiviswanathan",
        "Aaditya Ramdas"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=D2hhkU5O48",
      "cdate": 1727439333125,
      "mdate": 1740890350107,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836512"
    },
    {
      "id": "8pusxkLEQO",
      "title": "ARLON: Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation",
      "abstract": "Text-to-video (T2V) models have recently undergone rapid and substantial advancements. Nevertheless, due to limitations in data and computational resources, achieving efficient generation of long videos with rich motion dynamics remains a significant challenge. \nTo generate high-quality, dynamic, and temporally consistent long videos, this paper presents ARLON,  a novel framework that boosts diffusion Transformers with autoregressive (\\textbf{AR}) models for long (\\textbf{LON}) video generation, by integrating the coarse spatial and long-range temporal information provided by the AR model to guide the DiT model effectively.\nSpecifically, ARLON incorporates several key innovations: \n1) A latent Vector Quantized Variational Autoencoder (VQ-VAE) compresses the input latent space of the DiT model into compact and highly quantized visual tokens, bridging the AR and DiT models and balancing the learning complexity and information density;\n2) An adaptive norm-based semantic injection module integrates the coarse discrete visual units from the AR model into the DiT model, ensuring effective guidance during video generation; \n3) To enhance the tolerance capability of noise introduced from the AR inference, the DiT model is trained with coarser visual latent tokens incorporated with an uncertainty sampling module. \nExperimental results demonstrate that ARLON significantly outperforms the baseline OpenSora-V1.2 on eight out of eleven metrics selected from VBench, with notable improvements in dynamic degree and aesthetic quality, while delivering competitive results on the remaining three and simultaneously accelerating the generation process. In addition, ARLON achieves state-of-the-art performance in long video generation, outperforming other open-source models in this domain. \nDetailed analyses of the improvements in inference efficiency are presented, alongside a practical application that demonstrates the generation of long videos using progressive text prompts. Project page: \\url{http://aka.ms/arlon}.",
      "authors": [
        "Zongyi Li",
        "Shujie HU",
        "Shujie LIU",
        "Long Zhou",
        "Jeongsoo Choi",
        "Lingwei Meng",
        "Xun Guo",
        "Jinyu Li",
        "Hefei Ling",
        "Furu Wei"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=8pusxkLEQO",
      "cdate": 1727439221034,
      "mdate": 1740132229373,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836520"
    },
    {
      "id": "b24n2LS2BJ",
      "title": "Rethinking Shapley Value for Negative Interactions in Non-convex Games",
      "abstract": "We study causal interactions for payoff allocation in cooperative game theory, including quantifying feature attribution for deep learning models. Most feature attribution methods mainly stem from the criteria of the Shapley value, which assigns fair payoffs to players based on their expected contribution in a cooperative game. However, interactions between players in the game do not explicitly appear in the original formulation of the Shapley value. In this work, we reformulate the Shapley value to clarify the role of interactions and discuss implicit assumptions from a game-theoretical perspective. Our theoretical analysis demonstrates that when negative interactions exist—common in deep learning models—the efficiency axiom can lead to the undervaluation of attributions or payoffs. We suggest a new allocation rule that decomposes contributions into interactions and aggregates positive parts for non-convex games. Furthermore, we propose an approximation algorithm to reduce the cost of interaction computation which can be applied to differentiable functions such as deep learning models. Our approach mitigates counterintuitive attribution outcomes observed in existing methods, ensuring that features critical to a model’s decision receive appropriate attribution.",
      "authors": [
        "Wonjoon Chang",
        "Myeongjin Lee",
        "Jaesik Choi"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=b24n2LS2BJ",
      "cdate": 1727439089203,
      "mdate": 1740745135849,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836525"
    },
    {
      "id": "0ov0dMQ3mN",
      "title": "CO-MOT: Boosting End-to-end Transformer-based Multi-Object Tracking via Coopetition Label Assignment and Shadow Sets",
      "abstract": "Existing end-to-end Multi-Object Tracking (e2e-MOT) methods have not surpassed non-end-to-end tracking-by-detection methods. One possible reason lies in the training label assignment strategy that consistently binds the tracked objects with tracking queries and assigns few newborns to detection queries. Such an assignment, with one-to-one bipartite matching, yields an unbalanced training, _i.e._, scarce positive samples for detection queries, especially for an enclosed scene with the majority of the newborns at the beginning of videos. As such, e2e-MOT will incline to generate a tracking terminal without renewal or re-initialization, compared to other tracking-by-detection methods.\nTo alleviate this problem, we propose **Co-MOT**, a simple yet effective method to facilitate e2e-MOT by a novel coopetition label assignment with a shadow concept. Specifically, we add tracked objects to the matching targets for detection queries when performing the label assignment for training the intermediate decoders. For query initialization, we expand each query by a set of shadow counterparts with limited disturbance to itself.\nWith extensive ablation studies, Co-MOT achieves superior performances without extra costs, _e.g._, 69.4% HOTA on DanceTrack and 52.8% TETA on BDD100K. Impressively, Co-MOT only requires 38% FLOPs of MOTRv2 with comparable performances, resulting in the 1.4× faster inference speed. Source code is publicly available at [GitHub](https://github.com/BingfengYan/CO-MOT).",
      "authors": [
        "Feng yan",
        "Weixin Luo",
        "Yujie Zhong",
        "Yiyang Gan",
        "Lin Ma"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=0ov0dMQ3mN",
      "cdate": 1727439009100,
      "mdate": 1739868266396,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836531"
    },
    {
      "id": "41uZB8bDFh",
      "title": "Durable Quantization Conditioned Misalignment Attack on Large Language Models",
      "abstract": "As large language models (LLMs) are increasingly deployed on resource-constrained edge devices, quantization techniques have been widely adopted to reduce model size and computational requirements. However, this process can expose models to new vulnerabilities. In this work, we introduce the Quantization Conditioned Misalignment (Q-Misalign) attack, a novel threat in which safety misalignment remains dormant in a full-precision LLM but becomes exploitable post-quantization. We demonstrate that our Q-Misalign attack effectively bypasses safety mechanisms and enables the generation of harmful content in quantized models while maintaining full-precision performance. Furthermore, we propose a contrastive task vector-based approach to enhance attack durability, ensuring that vulnerabilities persist even after downstream fine-tuning. Experimental results show that Q-Misalign attack significantly increases jailbreak success rates in quantized models, while preserving model utility and safety alignment in full precision. Our findings highlight a critical gap in current LLM safety measures and call for more robust defenses in quantization-aware scenarios.",
      "authors": [
        "Peiran Dong",
        "Haowei Li",
        "Song Guo"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=41uZB8bDFh",
      "cdate": 1727438413277,
      "mdate": 1747455363235,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836539"
    },
    {
      "id": "t9lS1lX9FQ",
      "title": "Node Identifiers: Compact, Discrete Representations for Efficient Graph Learning",
      "abstract": "We present a novel end-to-end framework that generates highly compact (typically 6-15 dimensions), discrete (int4 type), and interpretable node representations—termed node identifiers (node IDs)—to tackle inference challenges on large-scale graphs. By employing vector quantization, we compress continuous node embeddings from multiple layers of a Graph Neural Network (GNN) into discrete codes, applicable under both self-supervised and supervised learning paradigms. These node IDs capture high-level abstractions of graph data and offer interpretability that traditional GNN embeddings lack. Extensive experiments on 34 datasets, encompassing node classification, graph classification, link prediction, and attributed graph clustering tasks, demonstrate that the generated node IDs significantly enhance speed and memory efficiency while achieving competitive performance compared to current state-of-the-art methods. Our source code is available at https://github.com/LUOyk1999/NodeID.",
      "authors": [
        "Yuankai Luo",
        "Hongkang Li",
        "Qijiong Liu",
        "Lei Shi",
        "Xiao-Ming Wu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=t9lS1lX9FQ",
      "cdate": 1727437971826,
      "mdate": 1746189905467,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836547"
    },
    {
      "id": "eLLBILFRsA",
      "title": "UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation",
      "abstract": "We present UniDetox, a universally applicable method designed to mitigate toxicity across various large language models (LLMs).\nPrevious detoxification methods are typically model-specific, addressing only individual models or model families, and require careful hyperparameter tuning due to the trade-off between detoxification efficacy and language modeling performance. \nIn contrast, UniDetox provides a detoxification technique that can be universally applied to a wide range of LLMs without the need for separate model-specific tuning. \nSpecifically, we propose a novel and efficient dataset distillation technique for detoxification using contrastive decoding. \nThis approach distills detoxifying representations in the form of synthetic text data, enabling universal detoxification of any LLM through fine-tuning with the distilled text. \nOur experiments demonstrate that the detoxifying text distilled from GPT-2 can effectively detoxify larger models, including OPT, Falcon, and LLaMA-2. \nFurthermore, UniDetox eliminates the need for separate hyperparameter tuning for each model, as a single hyperparameter configuration can be seamlessly applied across different models. \nAdditionally, analysis of the detoxifying text reveals a reduction in politically biased content, providing insights into the attributes necessary for effective detoxification of LLMs.",
      "authors": [
        "Huimin LU",
        "Masaru Isonuma",
        "Junichiro Mori",
        "Ichiro Sakata"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=eLLBILFRsA",
      "cdate": 1727437782394,
      "mdate": 1740904892643,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836552"
    },
    {
      "id": "ReItdfwMcg",
      "title": "Optimality and Adaptivity of Deep Neural Features for Instrumental Variable Regression",
      "abstract": "We provide a convergence analysis of \\emph{deep feature instrumental variable} (DFIV) regression (Xu et al., 2021), a nonparametric approach to IV regression using data-adaptive features learned by deep neural networks in two stages. We prove that the DFIV algorithm achieves the minimax optimal learning rate when the target structural function lies in a Besov space. This is shown under standard nonparametric IV assumptions, and an additional smoothness assumption on the regularity of the conditional distribution of the covariate given the instrument, which controls the difficulty of Stage 1. We further demonstrate that DFIV, as a data-adaptive algorithm, is superior to fixed-feature (kernel or sieve) IV methods in two ways. First, when the target function possesses low spatial homogeneity (i.e., it has both smooth and spiky/discontinuous regions), DFIV still achieves the optimal rate, while fixed-feature methods are shown to be strictly suboptimal. Second, comparing with kernel-based two-stage regression estimators, DFIV is provably more data efficient in the Stage 1 samples.",
      "authors": [
        "Juno Kim",
        "Dimitri Meunier",
        "Arthur Gretton",
        "Taiji Suzuki",
        "Zhu Li"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=ReItdfwMcg",
      "cdate": 1727437486096,
      "mdate": 1746089015057,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836557"
    },
    {
      "id": "v1B4aet9ct",
      "title": "Schur's Positive-Definite Network: Deep Learning in the SPD cone with structure",
      "abstract": "Estimating matrices in the symmetric positive-definite (SPD) cone is of interest for many applications ranging from computer vision to graph learning. While there exist various convex optimization-based estimators, they remain limited in expressivity due to their model-based approach. The success of deep learning motivates the use of learning-based approaches to estimate SPD matrices with neural networks in a data-driven fashion. However, designing effective neural architectures for SPD learning is challenging, particularly when the task requires\nadditional structural constraints, such as element-wise sparsity. Current approaches either do not ensure that the output meets all desired properties or lack expressivity. In this paper, we introduce SpodNet, a novel and generic learning module that guarantees SPD outputs and supports additional structural constraints. Notably, it solves the challenging task of learning jointly SPD and\nsparse matrices. Our experiments illustrate the versatility and relevance of SpodNet layers for such applications.",
      "authors": [
        "Can Pouliquen",
        "Mathurin Massias",
        "Titouan Vayer"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=v1B4aet9ct",
      "cdate": 1727436625222,
      "mdate": 1740580560949,
      "matched_keywords": [
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836563"
    },
    {
      "id": "q87GUkdQBm",
      "title": "SFESS: Score Function Estimators for $k$-Subset Sampling",
      "abstract": "Are score function estimators a viable approach to learning with $k$-subset sampling? Sampling $k$-subsets is a fundamental operation that is not amenable to differentiable parametrization, impeding gradient-based optimization. Previous work has favored approximate pathwise gradients or relaxed sampling, dismissing score function estimators because of their high variance. Inspired by the success of score function estimators in variational inference and reinforcement learning, we revisit them for $k$-subset sampling. We demonstrate how to efficiently compute the distribution's score function using a discrete Fourier transform and reduce the estimator's variance with control variates. The resulting estimator provides both $k$-hot samples and unbiased gradient estimates while being applicable to non-differentiable downstream models, unlike existing methods. We validate our approach experimentally and find that it produces results comparable to those of recent state-of-the-art pathwise gradient estimators across a range of tasks.",
      "authors": [
        "Klas Wijk",
        "Ricardo Vinuesa",
        "Hossein Azizpour"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=q87GUkdQBm",
      "cdate": 1727436619519,
      "mdate": 1743587857619,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836568"
    },
    {
      "id": "CbpWPbYHuv",
      "title": "Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models",
      "abstract": "Transformers have found extensive applications across various domains due to their powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have explored alternative modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment representational capacity. In this paper, we propose a novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers. Theoretically, we provide a comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions. Notably, we demonstrate that networks incorporating PolyCom achieve the **optimal approximation rate**, indicating that PolyCom networks require minimal parameters to approximate general smooth functions in Sobolev spaces. We conduct empirical experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. By substituting conventional activation functions with PolyCom, we enable LLMs to capture higher-order interactions within the data, thus improving performance metrics in terms of accuracy and convergence rates.  Extensive experimental results demonstrate the effectiveness of our method, showing substantial improvements over other activation functions. Code is available at https://github.com/BryceZhuo/PolyCom.",
      "authors": [
        "Zhijian Zhuo",
        "Ya Wang",
        "Yutao Zeng",
        "Xiaoqing Li",
        "Xun Zhou",
        "Jinwen Ma"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=CbpWPbYHuv",
      "cdate": 1727436567887,
      "mdate": 1742463164459,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836574"
    },
    {
      "id": "vqbd2OQnGp",
      "title": "Param$\\Delta$ for Direct Mixing: Post-Train Large Language Model At Zero Cost",
      "abstract": "The post-training phase of large language models is essential for enhancing capabilities such as instruction-following, reasoning, and alignment with human preferences. However, it demands extensive high-quality data and poses risks like overfitting, alongside significant computational costs due to repeated post-training and evaluation after each base model update. This paper introduces Param$\\Delta$, a novel method that streamlines post-training by transferring knowledge from an existing post-trained model to a newly updated base model with \\textbf{zero} additional training. By computing the difference between post-trained model weights ($\\Theta_\\text{post}$) and base model weights ($\\Theta_\\text{base}$), and adding this to the updated base model ($\\Theta_\\text{base}'$), we define Param$\\Delta$ Model as: $\\Theta_{\\text{Param}\\Delta} = \\Theta_\\text{post} - \\Theta_\\text{base} + \\Theta_\\text{base}'$. This approach surprisingly equips the new base model with post-trained capabilities, achieving performance comparable to direct post-training. We did analysis on LLama3, Llama3.1, Qwen, and DeepSeek-distilled models. Results indicate Param$\\Delta$ Model effectively replicates traditional post-training. For example, the Param$\\Delta$ Model obtained from 70B Llama3-inst, Llama3-base, Llama3.1-base models attains approximately 95\\% of Llama3.1-inst model's performance on average. Param$\\Delta$ brings a new perspective on how to fully leverage models in the open-weight community, where checkpoints for base and instruct models are readily available and frequently updated, by providing a cost-free framework to accelerate the iterative cycle of model development.",
      "authors": [
        "Sheng Cao",
        "Mingrui Wu",
        "Karthik Prasad",
        "Yuandong Tian",
        "Zechun Liu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=vqbd2OQnGp",
      "cdate": 1727436293370,
      "mdate": 1743576795598,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836580"
    },
    {
      "id": "RQz7szbVDs",
      "title": "A Theory of Initialisation's Impact on Specialisation",
      "abstract": "Prior work has demonstrated a consistent tendency in neural networks engaged in continual learning tasks, wherein intermediate task similarity results in the highest levels of catastrophic interference. This phenomenon is attributed to the network's tendency to reuse learned features across tasks. However, this explanation heavily relies on the premise that neuron specialisation occurs, i.e. the emergence of  localised representations. Our investigation challenges the validity of this assumption. Using theoretical frameworks for the analysis of neural networks, we show a strong dependence of specialisation on the initial condition. More precisely, we show that weight imbalance and high weight entropy can favour specialised solutions. We then apply these insights in the context of continual learning, first showing the emergence of a monotonic relation between task-similarity and forgetting in non-specialised networks. Finally, we show that  specialization by weight imbalance is beneficial on the commonly employed elastic weight consolidation regularisation technique.",
      "authors": [
        "Devon Jarvis",
        "Sebastian Lee",
        "Clémentine Carla Juliette Dominé",
        "Andrew M Saxe",
        "Stefano Sarao Mannelli"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=RQz7szbVDs",
      "cdate": 1727436174832,
      "mdate": 1740830085165,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836585"
    },
    {
      "id": "mPdmDYIQ7f",
      "title": "AgentSquare: Automatic LLM Agent Search in Modular Design Space",
      "abstract": "Recent advancements in Large Language Models (LLMs) have led to a rapid growth of agentic systems capable of handling a wide range of complex tasks. However, current research largely relies on manual, task-specific design, limiting their adaptability to novel tasks. In this paper, we introduce a new research problem: Modularized LLM Agent Search (MoLAS). We propose a modular design space that abstracts existing LLM agent designs into four fundamental modules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory. Building on this design space, we present a novel LLM agent search framework called AgentSquare, which introduces two core mechanisms, i.e., module evolution and recombination, to efficiently search for optimized LLM agents. To further accelerate the process, we design a performance predictor that uses in-context surrogate models to skip unpromising agent designs. Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Moreover, AgentSquare can generate interpretable design insights, enabling a deeper understanding of agentic architecture and its impact on task performance. We believe that the modular design space and AgentSquare search framework offer a platform for fully exploiting the potential of prior successful designs and consolidate the collective efforts of research community. Code repo is available at https://github.com/tsinghua-fib-lab/AgentSquare.",
      "authors": [
        "Yu Shang",
        "Yu Li",
        "Keyu Zhao",
        "Likai Ma",
        "Jiahe Liu",
        "Fengli Xu",
        "Yong Li"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=mPdmDYIQ7f",
      "cdate": 1727435978220,
      "mdate": 1741014817217,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836590"
    },
    {
      "id": "WEQL5ksDnB",
      "title": "Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation",
      "abstract": "Audio-visual speech recognition (AVSR) incorporates auditory and visual modalities to improve recognition accuracy, particularly in noisy environments where audio-only speech systems are insufficient. While previous research has largely addressed audio disruptions, few studies have dealt with visual corruptions, e.g., lip occlusions or blurred videos, which are also detrimental. To address this real-world challenge, we propose CAV2vec, a novel self-supervised speech representation learning framework particularly designed to handle audio-visual joint corruption. CAV2vec employs a self-distillation approach with a corrupted prediction task, where the student model learns to predict clean targets, generated by the teacher model, with corrupted input frames. Specifically, we suggest a unimodal multi-task learning, which distills cross-modal knowledge and aligns the corrupted modalities, by predicting clean audio targets with corrupted videos, and clean video targets with corrupted audios. This strategy mitigates the dispersion in the representation space caused by corrupted modalities, leading to more reliable and robust audio-visual fusion. Our experiments on robust AVSR benchmarks demonstrate that the corrupted representation learning method significantly enhances recognition accuracy across generalized environments involving various types of corruption. Our code is available at https://github.com/sungnyun/cav2vec.",
      "authors": [
        "Sungnyun Kim",
        "Sungwoo Cho",
        "Sangmin Bae",
        "Kangwook Jang",
        "Se-Young Yun"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=WEQL5ksDnB",
      "cdate": 1727435792640,
      "mdate": 1742627880083,
      "matched_keywords": [
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836596"
    },
    {
      "id": "QaTBHSqmH9",
      "title": "Size-Generalizable RNA Structure Evaluation by Exploring Hierarchical Geometries",
      "abstract": "Understanding the 3D structure of RNA is essential for deciphering its function and developing RNA-based therapeutics. Geometric Graph Neural Networks (GeoGNNs) that conform to the $\\mathrm{E}(3)$-symmetry have advanced RNA structure evaluation, a crucial step toward RNA structure prediction. However, existing GeoGNNs are still defective in two aspects: 1. inefficient or incapable of capturing the full geometries of RNA; 2. limited generalization ability when the size of RNA significantly differs between training and test datasets. In this paper, we propose EquiRNA, a novel equivariant GNN model by exploring the three-level hierarchical geometries of RNA. At its core, EquiRNA effectively addresses the size generalization challenge by reusing the representation of nucleotide, the common building block shared across RNAs of varying sizes. Moreover, by adopting a scalarization-based equivariant GNN as the backbone, our model maintains directional information while offering higher computational efficiency compared to existing GeoGNNs. Additionally, we propose a size-insensitive $K$-nearest neighbor sampling strategy to enhance the model's robustness to RNA size shifts. We test our approach on our created benchmark as well as an existing dataset. The results show that our method significantly outperforms other state-of-the-art methods, providing a robust baseline for RNA 3D structure modeling and evaluation.",
      "authors": [
        "Zongzhao Li",
        "Jiacheng Cen",
        "Wenbing Huang",
        "Taifeng Wang",
        "Le Song"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=QaTBHSqmH9",
      "cdate": 1727435646877,
      "mdate": 1740659161556,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836605"
    },
    {
      "id": "xvhV3LvYTc",
      "title": "InstantSplamp: Fast and Generalizable Stenography Framework for Generative Gaussian Splatting",
      "abstract": "With the rapid development of large generative models for 3D, especially the evolution from NeRF representations to more efficient Gaussian Splatting, the synthesis of 3D assets has become increasingly fast and efficient, enabling the large-scale publication and sharing of generated 3D objects. However, while existing methods can add watermarks or steganographic information to individual 3D assets, they often require time-consuming per-scene training and optimization, leading to watermarking overheads that can far exceed the time required for asset generation itself, making deployment impractical for generating large collections of 3D objects. To address this, we propose InstantSplamp a framework that seamlessly integrates the 3D steganography pipeline into large 3D generative models without introducing explicit additional time costs. Guided by visual foundation models,InstantSplamp subtly injects hidden information like copyright tags during asset generation, enabling effective embedding and recovery of watermarks within generated 3D assets while preserving original visual quality. Experiments across various potential deployment scenarios demonstrate that \\model~strikes an optimal balance between rendering quality and hiding fidelity, as well as between hiding performance and speed. Compared to existing per-scene optimization techniques for 3D assets, InstantSplamp reduces their watermarking training overheads that are multiples of generation time to nearly zero, paving the way for real-world deployment at scale. Project page: https://gaussian-stego.github.io/.",
      "authors": [
        "Chenxin Li",
        "Hengyu Liu",
        "Zhiwen Fan",
        "Wuyang Li",
        "Yifan Liu",
        "Panwang Pan",
        "Yixuan Yuan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=xvhV3LvYTc",
      "cdate": 1727434950727,
      "mdate": 1739503805487,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836610"
    },
    {
      "id": "8WQ7VTfPTl",
      "title": "Semantics-Adaptive Activation Intervention for LLMs via Dynamic Steering Vectors",
      "abstract": "Large language models (LLMs) have achieved remarkable performance across many tasks, yet aligning them with desired behaviors remains challenging. Activation intervention has emerged as an effective and economical method to modify the behavior of LLMs. Despite considerable interest in this area, current intervention methods exclusively employ a fixed steering vector to modify model activations, lacking adaptability to diverse input semantics. To address this limitation, we propose Semantics-Adaptive Dynamic Intervention (SADI), a novel method that constructs a dynamic steering vector to intervene model activations at inference time. More specifically, SADI utilizes activation differences in contrastive pairs to precisely identify critical elements of an LLM (i.e., attention heads, hidden states, and neurons) for targeted intervention. During inference, SADI dynamically steers model behavior by scaling element-wise activations based on the directions of input semantics. Experimental results show that SADI outperforms established baselines by substantial margins, improving task performance without training. SADI's cost-effectiveness and generalizability across various LLM backbones and tasks highlight its potential as a versatile alignment technique. We will release the code to foster research in this area.",
      "authors": [
        "Weixuan Wang",
        "JINGYUAN YANG",
        "Wei Peng"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=8WQ7VTfPTl",
      "cdate": 1727434832185,
      "mdate": 1740385030999,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836616"
    },
    {
      "id": "BZrSCv2SBq",
      "title": "ADAM Optimization with Adaptive Batch Selection",
      "abstract": "Adam is a widely used optimizer in neural network training due to its adaptive learning rate. However, because different data samples influence model updates to varying degrees, treating them equally can lead to inefficient convergence. To address this, a prior work proposed adapting the sampling distribution using a bandit framework to select samples adaptively. While promising, the bandit-based variant of Adam suffers from limited theoretical guarantees.\nIn this paper, we introduce \\textit{Adam with Combinatorial Bandit Sampling} (AdamCB), which integrates combinatorial bandit techniques into Adam to resolve these issues. AdamCB is able to fully utilize feedback from multiple samples at once, enhancing both theoretical guarantees and practical performance. Our regret analysis shows that AdamCB achieves faster convergence than Adam-based methods including the previous bandit-based variant. Numerical experiments demonstrate that AdamCB consistently outperforms existing methods.",
      "authors": [
        "Gyu Yeol Kim",
        "Min-hwan Oh"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=BZrSCv2SBq",
      "cdate": 1727434515650,
      "mdate": 1747499885308,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836621"
    },
    {
      "id": "owP2mymrTD",
      "title": "Facilitating Multi-turn Function Calling for LLMs via Compositional Instruction Tuning",
      "abstract": "Large Language Models (LLMs) have exhibited significant potential in performing diverse tasks, including the ability to call functions or use external tools to enhance their performance. While current research on function calling by LLMs primarily focuses on single-turn interactions, this paper addresses the overlooked necessity for LLMs to engage in multi-turn function calling—critical for handling compositional, real-world queries that require planning with functions but not only use functions. To facilitate this, we introduce an approach, BUTTON, which generates synthetic compositional instruction tuning data via bottom-up instruction construction and top-down trajectory generation. In the bottom-up phase, we generate simple atomic tasks based on real-world scenarios and build compositional tasks using heuristic strategies based on atomic tasks. Corresponding function definitions are then synthesized for these compositional tasks. The top-down phase features a multi-agent environment where interactions among simulated humans, assistants, and tools are utilized to gather multi-turn function calling trajectories. This approach ensures task compositionality and allows for effective function and trajectory generation by examining atomic tasks within compositional tasks. We produce a dataset BUTTONInstruct comprising 8k data points and demonstrate its effectiveness through extensive experiments across various LLMs.",
      "authors": [
        "Mingyang Chen",
        "sunhaoze",
        "Tianpeng Li",
        "Fan Yang",
        "Hao Liang",
        "KeerLu",
        "Bin CUI",
        "Wentao Zhang",
        "Zenan Zhou",
        "weipeng chen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=owP2mymrTD",
      "cdate": 1727434130311,
      "mdate": 1740738899102,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836627"
    },
    {
      "id": "xjKz6IxgCX",
      "title": "SafeWatch: An Efficient Safety-Policy Following Video Guardrail Model with Transparent Explanations",
      "abstract": "With the rise of generative AI and rapid growth of high-quality video generation, video guardrails have become more crucial than ever to ensure safety and security across platforms. Current video guardrails, however, are either overly simplistic, relying on pure classification models trained on simple policies with limited unsafe categories, which lack detailed explanations, or prompting multimodal large language models (MLLMs) with long safety guidelines, which are inefficient and impractical for guardrailing real-world content. To bridge this gap, we propose SafeWatch, an efficient MLLM-based video guardrail model designed to follow customized safety policies and provide multi-label video guardrail outputs with content-specific explanations in a zero-shot manner. In particular, unlike traditional MLLM-based guardrails that encode all safety policies autoregressively, causing inefficiency and bias, SafeWatch uniquely encodes each policy chunk in parallel and eliminates their position bias such that all policies are attended simultaneously with equal importance. In addition, to improve efficiency and accuracy, SafeWatch incorporates a policy-aware visual token pruning algorithm that adaptively selects the most relevant video tokens for each policy, discarding noisy or irrelevant information. This allows for more focused, policy-compliant guardrail with significantly reduced computational overhead. Considering the limitations of existing video guardrail benchmarks, we propose SafeWatch-Bench, a large-scale video guardrail benchmark comprising over 2M videos spanning six safety categories which covers over 30 tasks to ensure a comprehensive coverage of all potential safety scenarios. We have conducted extensive experiments, showing that SafeWatch outperforms all SOTA video guardrails on SafeWatch-Bench by 28.2%, and achieves a 13.6% improvement on existing benchmarks, all while reducing inference costs by an average of 10%. SafeWatch also demonstrates strong policy-following abilities and outperforms previous SOTAs by 5.6% and 15.6% in zero-shot generalizability to new policies and new prompting tasks. Additionally, both LLM-as-a-judge and human evaluators confirm the high quality of the explanations provided by SafeWatch. Our project is open-sourced at https://safewatch-aiguard.github.io.",
      "authors": [
        "Zhaorun Chen",
        "Francesco Pinto",
        "Minzhou Pan",
        "Bo Li"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=xjKz6IxgCX",
      "cdate": 1727434008281,
      "mdate": 1741014816933,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.836632"
    },
    {
      "id": "0oWGVvC6oq",
      "title": "On Bits and Bandits: Quantifying the Regret-Information Trade-off",
      "abstract": "In many sequential decision problems, an agent performs a repeated task. He then suffers regret and obtains information that he may use in the following rounds. However, sometimes the agent may also obtain information and avoid suffering regret by querying external sources. We study the trade-off between the information an agent accumulates and the regret it suffers. We invoke information-theoretic methods for obtaining regret lower bounds, that also allow us to easily re-derive several known lower bounds. We introduce the first Bayesian regret lower bounds that depend on the information an agent accumulates. We also prove regret upper bounds using the amount of information the agent accumulates. These bounds show that information measured in bits, can be traded off for regret, measured in reward. Finally, we demonstrate the utility of these bounds in improving the performance of a question-answering task with large language models, allowing us to obtain valuable insights.",
      "authors": [
        "Itai Shufaro",
        "Nadav Merlis",
        "Nir Weinberger",
        "Shie Mannor"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=0oWGVvC6oq",
      "cdate": 1727433994814,
      "mdate": 1740129113975,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836638"
    },
    {
      "id": "1yJP5TVWih",
      "title": "Lambda-Skip Connections: the architectural component that prevents Rank Collapse",
      "abstract": "Rank collapse, a phenomenon where embedding vectors in sequence models\nrapidly converge to a uniform token or equilibrium state, has recently gained at-\ntention in the deep learning literature. This phenomenon leads to reduced expres-\nsivity and potential training instabilities due to vanishing gradients. Empirical ev-\nidence suggests that architectural components like skip connections, LayerNorm,\nand MultiLayer Perceptrons (MLPs) play critical roles in mitigating rank collapse.\nWhile this issue is well-documented for transformers, alternative sequence mod-\nels, such as State Space Models (SSMs), which have recently gained prominence,\nhave not been thoroughly examined for similar vulnerabilities. This paper extends\nthe theory of rank collapse from transformers to SSMs using a unifying frame-\nwork that captures both architectures. We introduce a modification in the skip\nconnection component, termed lambda-skip connections, that provides guaran-\ntees for rank collapse prevention. We present, via analytical results, a sufficient\ncondition to achieve the guarantee for all of the aforementioned architectures. We\nalso study the necessity of this condition via ablation studies and analytical exam-\nples. To our knowledge, this is the first study that provides a general guarantee to\nprevent rank collapse, and that investigates rank collapse in the context of SSMs,\noffering valuable understanding for both theoreticians and practitioners. Finally,\nwe validate our findings with experiments demonstrating the crucial role of archi-\ntectural components in preventing rank collapse.",
      "authors": [
        "Federico Arangath Joseph",
        "Jerome Sieber",
        "Melanie Zeilinger",
        "Carmen Amo Alonso"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=1yJP5TVWih",
      "cdate": 1727433194053,
      "mdate": 1739436272313,
      "matched_keywords": [
        "transformer",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836643"
    },
    {
      "id": "cADpvQgnqg",
      "title": "Foundation Models Secretly Understand Neural Network Weights: Enhancing Hypernetwork Architectures with Foundation Models",
      "abstract": "Large pre-trained models, or foundation models, have shown impressive performance when adapted to a variety of downstream tasks, often out-performing specialized models. Hypernetworks, neural networks that generate some or all of the parameters of another neural network, have become an increasingly important technique for conditioning and generalizing implicit neural representations (INRs), which represent signals or objects such as audio or 3D shapes using a neural network. However, despite the potential benefits of incorporating foundation models in hypernetwork methods, this research direction has not been investigated, likely due to the dissimilarity of the weight generation task with other visual tasks. To address this gap, we (1) show how foundation models can improve hypernetworks with Transformer-based architectures, (2) provide an empirical analysis of the benefits of foundation models for hypernetworks through the lens of the generalizable INR task, showing that leveraging foundation models improves performance, generalizability, and data efficiency across a variety of algorithms and modalities. We also provide further analysis in examining the design space of foundation model-based hypernetworks, including examining the choice of foundation models, algorithms, and the effect of scaling foundation models.",
      "authors": [
        "Jeffrey Gu",
        "Serena Yeung-Levy"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=cADpvQgnqg",
      "cdate": 1727433071954,
      "mdate": 1740909113333,
      "matched_keywords": [
        "foundation model",
        "transformer",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836649"
    },
    {
      "id": "71XtUhazG0",
      "title": "Mini-Monkey: Alleviating the Semantic Sawtooth Effect for Lightweight MLLMs via Complementary Image Pyramid",
      "abstract": "Recently, scaling images to high resolution has received much attention in multimodal large language models (MLLMs). Most existing practices adopt a sliding-window-style cropping strategy to adapt to resolution increase. Such a cropping strategy, however, can easily cut off objects and connected regions, which introduces semantic discontinuity and therefore impedes MLLMs from recognizing small or irregularly shaped objects or text, leading to a phenomenon we call the semantic sawtooth effect. This effect is particularly evident in lightweight MLLMs. To address this issue, we introduce a Complementary Image Pyramid (CIP), a simple, effective, and plug-and-play solution designed to mitigate semantic discontinuity during high-resolution image processing. In particular, CIP dynamically constructs an image pyramid to provide complementary semantic information for the cropping-based MLLMs, enabling it rich acquire semantics at all levels. Furthermore, we introduce a Scale Compression Mechanism (SCM) to reduce the additional computational overhead by compressing the redundant visual tokens. Our experiments demonstrate that CIP can consistently enhance the performance across diverse architectures (e.g., MiniCPM-V-2, InternVL2, and LLaVA-OneVision), various model capacity (1B$\\rightarrow$8B), and different usage configurations (training-free and fine-tuning). Leveraging the proposed CIP and SCM, we introduce a lightweight MLLM, Mini-Monkey, which achieves remarkable performance in both general multimodal understanding and document understanding. On the OCRBench, the 2B-version Mini-Monkey even surpasses the 8B model InternVL2-8B by 12 score. Additionally, training Mini-Monkey is cheap, requiring only eight RTX 3090 GPUs. Code and models are available at \nhttps://github.com/Yuliang-Liu/Monkey.",
      "authors": [
        "Mingxin Huang",
        "Yuliang Liu",
        "Dingkang Liang",
        "Lianwen Jin",
        "Xiang Bai"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=71XtUhazG0",
      "cdate": 1727432976815,
      "mdate": 1739945651428,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.836654"
    },
    {
      "id": "2zmO1GVT0Y",
      "title": "NL-Eye: Abductive NLI For Images",
      "abstract": "Will a Visual Language Model (VLM)-based bot warn us about slipping if it detects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet their ability to infer outcomes and causes remains underexplored. To address this, we introduce NL-Eye, a benchmark designed to assess VLMs' visual abductive reasoning skills. NL-Eye adapts the abductive Natural Language Inference (NLI) task to the visual domain, requiring models to evaluate the plausibility of hypothesis images based on a premise image and explain their decisions. NL-Eye consists of 350 carefully curated triplet examples (1,050 images) spanning diverse reasoning categories: physical, functional, logical, emotional, cultural, and social. The data curation process involved two steps—writing textual descriptions and generating images using text-to-image models, both requiring substantial human involvement to ensure high-quality and challenging scenes. Our experiments show that VLMs struggle significantly on NL-Eye, often performing at random baseline levels, while humans excel in both plausibility prediction and explanation quality. This demonstrates a deficiency in the abductive reasoning capabilities of modern VLMs. NL-Eye represents a crucial step toward developing VLMs capable of robust multimodal reasoning for real-world applications, including accident-prevention bots and generated video verification.",
      "authors": [
        "Mor Ventura",
        "Michael Toker",
        "Nitay Calderon",
        "Zorik Gekhman",
        "Yonatan Bitton",
        "Roi Reichart"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=2zmO1GVT0Y",
      "cdate": 1727432789349,
      "mdate": 1740743240064,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.836662"
    },
    {
      "id": "LO4MEPoqrG",
      "title": "Does Safety Training of LLMs Generalize to Semantically Related Natural Prompts?",
      "abstract": "Large Language Models (LLMs) are known to be susceptible to crafted adversarial attacks or jailbreaks that lead to the generation of objectionable content despite being aligned to human preferences using safety fine-tuning methods. While the large dimensionality of input token space makes it inevitable to find *adversarial* prompts that can jailbreak these models, we aim to evaluate whether safety fine-tuned LLMs are safe against *natural* prompts which are semantically related to toxic seed prompts that elicit safe responses after alignment. We surprisingly find that popular aligned LLMs such as GPT-4 can be compromised using naive prompts that are NOT even crafted with an objective of jailbreaking the model. Furthermore, we empirically show that given a seed prompt that elicits a toxic response from an unaligned model, one can systematically generate several semantically related *natural* prompts that can jailbreak aligned LLMs. Towards this, we propose a method of *Response Guided Question Augmentation (ReG-QA)* to evaluate the generalization of safety aligned LLMs to natural prompts, that first generates several toxic answers given a seed question using an unaligned LLM (Q to A), and further leverages an LLM to generate questions that are likely to produce these answers (A to Q). We interestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to producing natural jailbreak *questions* from unsafe content (without denial) and can thus be used for the latter (A to Q) step. We obtain attack success rates that are comparable to/ better than leading adversarial attack methods on the JailbreakBench leaderboard, while being significantly more stable against defenses such as Smooth-LLM and Synonym Substitution, which are effective against existing all attacks on the leaderboard.",
      "authors": [
        "Sravanti Addepalli",
        "Yerram Varun",
        "Arun Suggala",
        "Karthikeyan Shanmugam",
        "Prateek Jain"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=LO4MEPoqrG",
      "cdate": 1727432484290,
      "mdate": 1740912501409,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836668"
    },
    {
      "id": "semTHoVGsJ",
      "title": "Density estimation with LLMs: a geometric investigation of in-context learning trajectories",
      "abstract": "Large language models (LLMs) demonstrate remarkable emergent abilities to perform in-context learning across various tasks, including time series forecasting. \nThis work investigates LLMs' ability to estimate probability density functions (PDFs) from data observed in-context; \nsuch density estimation (DE) is a fundamental task underlying many probabilistic modeling problems. \nWe leverage the Intensive Principal Component Analysis (InPCA) to visualize and analyze the in-context learning dynamics of LLaMA-2 models. \nOur main finding is that these LLMs all follow similar learning trajectories in a low-dimensional InPCA space, which are distinct from those of traditional density estimation methods like histograms and Gaussian kernel density estimation (KDE). \nWe interpret the LLaMA in-context DE process as a KDE with an adaptive kernel width and shape. \nThis custom kernel model captures a significant portion of LLaMA's behavior despite having only two parameters. \nWe further speculate on why LLaMA's kernel width and shape differs from classical algorithms, providing insights into the mechanism of in-context probabilistic reasoning in LLMs.\nOur codebase, along with a 3D visualization of an LLM's in-context learning trajectory, is publicly available at https://github.com/AntonioLiu97/LLMICL_inPCA.",
      "authors": [
        "Toni J.B. Liu",
        "Nicolas Boulle",
        "Raphaël Sarfati",
        "Christopher Earls"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=semTHoVGsJ",
      "cdate": 1727432192802,
      "mdate": 1740528665626,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836674"
    },
    {
      "id": "v7YrIjpkTF",
      "title": "Multimodal Quantitative Language for Generative Recommendation",
      "abstract": "Generative recommendation has emerged as a promising paradigm aiming at directly generating the identifiers of the target candidates.\nMost existing methods attempt to leverage prior knowledge embedded in Pre-trained Language Models (PLMs) to improve the recommendation performance. However, they often fail to accommodate the differences between the general linguistic knowledge of PLMs and the specific needs of recommendation systems. Moreover, they rarely consider the complementary knowledge between the multimodal information of items, which represents the multi-faceted preferences of users.  To facilitate efficient recommendation knowledge transfer, we propose a novel approach called Multimodal Quantitative Language for Generative Recommendation (MQL4GRec). Our key idea is to transform items from different domains and modalities into a unified language, which can serve as a bridge for transferring recommendation knowledge. Specifically, we first introduce quantitative translators to convert the text and image content of items from various domains into a new and concise language, known as quantitative language, with all items sharing the same vocabulary. Then, we design a series of quantitative language generation tasks to enrich quantitative language with semantic information and prior knowledge.  Finally, we achieve the transfer of recommendation knowledge from different domains and modalities to the recommendation task through pre-training and fine-tuning. We evaluate the effectiveness of MQL4GRec through extensive experiments and comparisons with existing methods, achieving improvements over the baseline by 11.18\\%, 14.82\\%, and 7.95\\% on the NDCG metric across three different datasets, respectively.",
      "authors": [
        "Jianyang Zhai",
        "Zi-Feng Mai",
        "Chang-Dong Wang",
        "Feidiao Yang",
        "Xiawu Zheng",
        "Hui Li",
        "Yonghong Tian"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=v7YrIjpkTF",
      "cdate": 1727432118224,
      "mdate": 1743503618307,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.836679"
    },
    {
      "id": "ehr4oTe6XI",
      "title": "Disentangled Representation Learning with the Gromov-Monge Gap",
      "abstract": "Learning disentangled representations from unlabelled data is a fundamental challenge in machine learning. Solving it may unlock other problems, such as generalization, interpretability, or fairness. Although remarkably challenging to solve in theory, disentanglement is often achieved in practice through prior matching. Furthermore, recent works have shown that prior matching approaches can be enhanced by leveraging geometrical considerations, e.g., by learning representations that preserve geometric features of the data, such as distances or angles between points. However, matching the prior while preserving geometric features is challenging, as a mapping that *fully* preserves these features while aligning the data distribution with the prior does not exist in general. To address these challenges, we introduce a novel approach to disentangled representation learning based on quadratic optimal transport. We formulate the problem using Gromov-Monge maps that transport one distribution onto another with minimal distortion of predefined geometric features, preserving them *as much as can be achieved*. To compute such maps, we propose the Gromov-Monge-Gap (GMG), a regularizer quantifying whether a map moves a reference distribution with minimal geometry distortion. We demonstrate the effectiveness of our approach for disentanglement across four standard benchmarks, outperforming other methods leveraging geometric considerations.",
      "authors": [
        "Théo Uscidda",
        "Luca Eyring",
        "Karsten Roth",
        "Fabian J Theis",
        "Zeynep Akata",
        "marco cuturi"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=ehr4oTe6XI",
      "cdate": 1727431998998,
      "mdate": 1742250064634,
      "matched_keywords": [
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836684"
    },
    {
      "id": "RoN6M3i7gJ",
      "title": "A Riemannian Framework for Learning Reduced-order Lagrangian Dynamics",
      "abstract": "By incorporating physical consistency as inductive bias, deep neural networks display increased generalization capabilities and data efficiency in learning nonlinear dynamic models. However, the complexity of these models generally increases with the system dimensionality, requiring larger datasets, more complex deep networks, and significant computational effort.\nWe propose a novel geometric network architecture to learn physically-consistent reduced-order dynamic parameters that accurately describe the original high-dimensional system behavior.\nThis is achieved by building on recent advances in model-order reduction and by adopting a Riemannian perspective to jointly learn a non-linear structure-preserving latent space and the associated low-dimensional dynamics.\nOur approach enables accurate long-term predictions of the high-dimensional dynamics of rigid and deformable systems with increased data efficiency by inferring interpretable and physically-plausible reduced Lagrangian models.",
      "authors": [
        "Katharina Friedl",
        "Noémie Jaquier",
        "Jens Lundell",
        "Tamim Asfour",
        "Danica Kragic"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=RoN6M3i7gJ",
      "cdate": 1727431556531,
      "mdate": 1740746512234,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836691"
    },
    {
      "id": "VQwI055flA",
      "title": "CARTS: Advancing Neural Theorem Proving with Diversified Tactic Calibration and Bias-Resistant Tree Search",
      "abstract": "Recent advancements in neural theorem proving integrate large language models with tree search algorithms like Monte Carlo Tree Search (MCTS), where the language model suggests tactics and the tree search finds the complete proof path. However, many tactics proposed by the language model converge to semantically or strategically similar, reducing diversity and increasing search costs by expanding redundant proof paths. This issue exacerbates as computation scales and more tactics are explored per state. Furthermore, the trained value function suffers from false negatives, label imbalance, and domain gaps due to biased data construction.  To address these challenges, we propose CARTS (diversified tactic CAlibration and bias-Resistant Tree Search), which balances tactic diversity and importance while calibrating model confidence. CARTS also introduce preference modeling and an adjustment term related to the ratio of valid tactics to improve the bias-resistance of the value function. Experimental results demonstrate that CARTS consistently outperforms previous methods achieving a pass@l rate of 49.6\\% on the miniF2F-test benchmark. Further analysis confirms that CARTS improves tactic diversity and leads to a more balanced tree search.",
      "authors": [
        "Xiao-Wen Yang",
        "Zhi Zhou",
        "Haiming Wang",
        "Aoxue Li",
        "Wen-Da Wei",
        "Hui Jin",
        "Zhenguo Li",
        "Yu-Feng Li"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=VQwI055flA",
      "cdate": 1727431452435,
      "mdate": 1740643457872,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836696"
    },
    {
      "id": "WttfQGwpES",
      "title": "A Theoretical Perspective: How to Prevent Model Collapse in Self-consuming Training Loops",
      "abstract": "High-quality data is essential for training large generative models, yet the vast reservoir of real data available online has become nearly depleted. Consequently, models increasingly generate their own data for further training, forming Self-consuming Training Loops (STLs). However, the empirical results have been strikingly inconsistent: some models degrade or even collapse, while others successfully avoid these failures, leaving a significant gap in theoretical understanding to explain this discrepancy. This paper introduces the intriguing notion of *recursive stability* and presents the first theoretical generalization analysis, revealing how both model architecture and the proportion between real and synthetic data influence the success of STLs. We further extend this analysis to transformers in in-context learning, showing that even a constant-sized proportion of real data ensures convergence, while also providing insights into optimal synthetic data sizing.",
      "authors": [
        "Shi Fu",
        "Yingjie Wang",
        "Yuzhu Chen",
        "Xinmei Tian",
        "Dacheng Tao"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=WttfQGwpES",
      "cdate": 1727431391507,
      "mdate": 1740890349221,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836701"
    },
    {
      "id": "icDoYdUhRa",
      "title": "Pareto Low-Rank Adapters: Efficient Multi-Task Learning with Preferences",
      "abstract": "Multi-task trade-offs in machine learning can be addressed via Pareto Front Learning (PFL) methods that parameterize the Pareto Front (PF) with a single model. PFL permits to select the desired operational point during inference, contrary to traditional Multi-Task Learning (MTL) that optimizes for a single trade-off decided prior to training. However, recent PFL methodologies suffer from limited scalability, slow convergence, and excessive memory requirements, while exhibiting inconsistent mappings from preference to objective space. We introduce PaLoRA, a novel parameter-efficient method that addresses these limitations in two ways. First, we augment any neural network architecture with task-specific low-rank adapters and continuously parameterize the Pareto Front in their convex hull. Our approach steers the original model and the adapters towards learning general and task-specific features, respectively. Second, we propose a deterministic sampling schedule of preference vectors that reinforces this division of labor, enabling faster convergence and strengthening the validity of the mapping from preference to objective space throughout training. Our experiments show that PaLoRA outperforms state-of-the-art MTL and PFL baselines across various datasets, scales to large networks, reducing the memory overhead $23.8-31.7$ times compared with competing PFL baselines in scene understanding benchmarks.",
      "authors": [
        "Nikolaos Dimitriadis",
        "Pascal Frossard",
        "François Fleuret"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=icDoYdUhRa",
      "cdate": 1727431349128,
      "mdate": 1740588953490,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836707"
    },
    {
      "id": "uOb7rij7sR",
      "title": "CryoGEN: Generative Energy-based Models for Cryogenic Electron Tomography Reconstruction",
      "abstract": "Cryogenic electron tomography (Cryo-ET) is a powerful technique for visualizing subcellular structures in their native states. Nonetheless, its effectiveness is compromised by anisotropic resolution artifacts caused by the missing-wedge effect. To address this, IsoNet, a deep learning-based method, proposes iteratively reconstructing the missing-wedge information. While successful, IsoNet's dependence on recursive prediction updates often leads to training instability and model divergence. In this study, we introduce CryoGEN—an energy-based probabilistic model that not only mitigates resolution anisotropy but also removes the need for recursive subtomogram averaging, delivering an approximate *10*$\\times$ speedup for training. Evaluations across various biological datasets, including immature HIV-1 virions and ribosomes, demonstrate that CryoGEN significantly enhances structural completeness and interpretability of the reconstructed samples.",
      "authors": [
        "Yunfei Teng",
        "Yuxuan Ren",
        "Kai Chen",
        "Xi Chen",
        "Zhaoming Chen",
        "Qiwei Ye"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=uOb7rij7sR",
      "cdate": 1727431196225,
      "mdate": 1747560938403,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836713"
    },
    {
      "id": "TmCcNuo03f",
      "title": "Measuring And Improving Engagement of Text-to-Image Generation Models",
      "abstract": "Recent advances in text-to-image generation have achieved impressive aesthetic quality, making these models usable for both personal and commercial purposes. However, in the fields of marketing and advertising, images are often created to be more engaging, as reflected in user behaviors such as increasing clicks, likes, and purchases, in addition to being aesthetically pleasing. To this end, we introduce the challenge of optimizing the image generation process for improved viewer engagement. In order to study image engagement and utility in real-world marketing scenarios, we collect *EngagingImageNet*, the first large-scale dataset of images, along with associated user engagement metrics. Further, we find that existing image evaluation metrics like aesthetics, CLIPScore, PickScore, ImageReward, *etc.* are unable to capture viewer engagement. To address the lack of reliable metrics for assessing image utility, we use the *EngagingImageNet* dataset to train *EngageNet*, an engagement-aware Vision Language Model (VLM) that predicts viewer engagement of images by leveraging contextual information about the tweet content, enterprise details, and posting time. We then explore methods to enhance the engagement of text-to-image models, making initial strides in this direction. These include conditioning image generation on improved prompts, supervised fine-tuning of stable diffusion on high-performing images, and reinforcement learning to align stable diffusion with *EngageNet*-based reward signals, all of which lead to the generation of images with higher viewer engagement. Finally, we propose the *Engagement Arena*, to benchmark text-to-image models based on their ability to generate engaging images, using *EngageNet* as the evaluator, thereby encouraging the research community to measure further advances in the engagement of text-to-image modeling. These contributions provide a new pathway for advancing utility-driven image generation, with significant implications for the commercial application of image generation. We have released our code and dataset on [behavior-in-the-wild.github.io/image-engagement](https://behavior-in-the-wild.github.io/image-engagement).",
      "authors": [
        "Varun Khurana",
        "Yaman Kumar Singla",
        "Jayakumar Subramanian",
        "Changyou Chen",
        "Rajiv Ratn Shah",
        "zhiqiang xu",
        "Balaji Krishnamurthy"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=TmCcNuo03f",
      "cdate": 1727431116036,
      "mdate": 1741014816289,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836722"
    },
    {
      "id": "K4FAFNRpko",
      "title": "VLAS: Vision-Language-Action Model with Speech Instructions for Customized Robot Manipulation",
      "abstract": "Vision-language-action models (VLAs) have recently become highly prevalent in robot manipulation due to its end-to-end architecture and impressive performance. However, current VLAs are limited to processing human instructions in textual form, neglecting the more natural speech modality for human interaction. A typical approach of incorporating speech modality into VLA necessitates a separate speech recognition system to transcribe spoken instructions into text. Such a cascading pipeline raises two major concerns for robotic systems. First, the entire model grows in size and complexity, potentially resulting in redundant computations and increased memory consumption. Second, the transcription procedure would lose non-semantic information in the raw speech, such as voiceprint, which is crucial for a robot to successfully understand and complete customized tasks. To this end, we propose VLAS, the fisrt end-to-end policy model that seamlessly integrates speech modality for robot manipulation. We present a three-stage speech instruction tuning strategy leveraging multimodal datasets, including our manually curated SQA and CSI datasets. Furthermore, to facilitate personalized operations, we develop a voice retrieval-augmented generation (RAG) approach to enhance the robot's performance in tasks requiring individual-specific knowledge. Experimental results show that the proposed VLAS, following either textual or speech instructions, can achieve performance comparable to traditional VLAs on the CALVIN benchmark. In addition, we created a benchmark consisting of customization tasks, where our VLAS demonstrates absolute superiority by fully leveraging the auxiliary information in speech.",
      "authors": [
        "Wei Zhao",
        "Pengxiang Ding",
        "Zhang Min",
        "Zhefei Gong",
        "Shuanghao Bai",
        "Han Zhao",
        "Donglin Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=K4FAFNRpko",
      "cdate": 1727430610135,
      "mdate": 1747470048612,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:02.836727"
    },
    {
      "id": "sahQq2sH5x",
      "title": "Benchmarking Predictive Coding Networks -- Made Simple",
      "abstract": "In this work, we tackle the problems of efficiency and scalability for predictive coding networks (PCNs) in machine learning. To do so, we  propose a library that focuses on performance and simplicity, and use it to implement a large set of standard benchmarks for the community to use for their experiments. As most works in the field propose their own tasks and architectures, do not compare one against each other, and focus on small-scale tasks, a simple and fast open-source library, and a comprehensive set of benchmarks, would address all of these concerns. Then, we perform extensive tests on such benchmarks using both existing algorithms for PCNs, as well as adaptations of other methods popular in the bio-plausible deep learning community. All of this has allowed us to (i) test architectures much larger than commonly used in the literature, on more complex datasets; (ii) reach new state-of-the-art results in all of the tasks and dataset provided; (iii) clearly highlight what the current limitations of PCNs are, allowing us to state important future research directions. With the hope of galvanizing community efforts towards one of the main open problems in the field, scalability, we will release the code, tests, and benchmarks.",
      "authors": [
        "Luca Pinchetti",
        "Chang Qi",
        "Oleh Lokshyn",
        "Cornelius Emde",
        "Amine M'Charrak",
        "Mufeng Tang",
        "Simon Frieder",
        "Bayar Menzat",
        "Gaspard Oliviers",
        "Rafal Bogacz",
        "Thomas Lukasiewicz",
        "Tommaso Salvatori"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=sahQq2sH5x",
      "cdate": 1727430395897,
      "mdate": 1740938574845,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836733"
    },
    {
      "id": "mun3bGqdDM",
      "title": "Atomas: Hierarchical Adaptive Alignment on Molecule-Text for Unified Molecule Understanding and Generation",
      "abstract": "Molecule-and-text cross-modal representation learning has emerged as a promising direction for enhancing the quality of molecular representation, thereby improving performance in various scientific fields. However, most approaches employ a global alignment approach to learn the knowledge from different modalities that may fail to capture fine-grained information, such as molecule-and-text fragments and stereoisomeric nuances, which is crucial for downstream tasks. Furthermore, it is incapable of modeling such information using a similar global alignment strategy due to the lack of annotations about the fine-grained fragments in the existing dataset.\nIn this paper, we propose Atomas, a hierarchical molecular representation learning framework that jointly learns representations from SMILES strings and text. We design a Hierarchical Adaptive Alignment model to automatically learn the fine-grained fragment correspondence between two modalities and align these representations at three semantic levels. \nAtomas's end-to-end training framework supports understanding and generating molecules, enabling a wider range of downstream tasks. Atomas achieves superior performance across 12 tasks on 11 datasets, outperforming 11 baseline models thus highlighting the effectiveness and versatility of our method. Scaling experiments further demonstrate Atomas’s robustness and scalability. Moreover, visualization and qualitative analysis, validated by human experts, confirm the chemical relevance of our approach. Codes are released on ~\\url{https://github.com/yikunpku/Atomas}.",
      "authors": [
        "Yikun Zhang",
        "Geyan Ye",
        "Chaohao Yuan",
        "Bo Han",
        "Long-Kai Huang",
        "Jianhua Yao",
        "Wei Liu",
        "Yu Rong"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=mun3bGqdDM",
      "cdate": 1727430231673,
      "mdate": 1741018870106,
      "matched_keywords": [
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836738"
    },
    {
      "id": "pZiyCaVuti",
      "title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory",
      "abstract": "Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses. However, their long-term memory capabilities in sustained interactions remain underexplored. We introduce LongMemEval, a comprehensive benchmark designed to evaluate five core long-term memory abilities of chat assistants: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. With 500 meticulously curated questions embedded within freely scalable user-assistant chat histories, LongMemEval presents a significant challenge to existing long-term memory systems, with commercial chat assistants and long-context LLMs showing a 30% accuracy drop on memorizing information across sustained interactions. We then present a unified framework that breaks down the long-term memory design into three stages: indexing, retrieval, and reading. Built upon key experimental insights, we propose several memory design optimizations including session decomposition for value granularity, fact-augmented key expansion for indexing, and time-aware query expansion for refining the search scope. Extensive experiments show that these optimizations greatly improve both memory recall and downstream question answering on LongMemEval. Overall, our study provides valuable resources and guidance for advancing the long-term memory capabilities of LLM-based chat assistants, paving the way toward more personalized and reliable conversational AI. Our benchmark and code are publicly available at https://github.com/xiaowu0162/LongMemEval.",
      "authors": [
        "Di Wu",
        "Hongwei Wang",
        "Wenhao Yu",
        "Yuwei Zhang",
        "Kai-Wei Chang",
        "Dong Yu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=pZiyCaVuti",
      "cdate": 1727430130517,
      "mdate": 1740724206740,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836744"
    },
    {
      "id": "JTji0Jfh5a",
      "title": "Reinforcement Learning from Imperfect Corrective Actions and Proxy Rewards",
      "abstract": "In practice, reinforcement learning (RL) agents are often trained with a possibly imperfect proxy reward function, which may lead to a human-agent alignment issue (i.e., the learned policy either converges to non-optimal performance with low cumulative rewards, or achieves high cumulative rewards but in an undesired manner). To tackle this issue, we consider a framework where a human labeler can provide additional feedback in the form of corrective actions, which expresses the labeler's action preferences although this feedback may possibly be imperfect as well. \nIn this setting, to obtain a better-aligned policy guided by both learning signals, we propose a novel value-based deep RL algorithm called **I**terative learning from **Co**rrective actions and **Pro**xy rewards (ICoPro), which cycles through three phases: \n(1) Solicit sparse corrective actions from a human labeler on the agent's demonstrated trajectories; \n(2) Incorporate these corrective actions into the Q-function using a margin loss to enforce adherence to labeler's preferences; \n(3) Train the agent with standard RL losses regularized with a margin loss to learn from proxy rewards and propagate the Q-values learned from human feedback. Moreover, another novel design in our approach is to integrate pseudo-labels from the target Q-network to reduce human labor and further stabilize training. \nWe experimentally validate our proposition on a variety of tasks (Atari games and autonomous driving on highway). On the one hand, using proxy rewards with different levels of imperfection, our method can better align with human and is more sample-efficient than baseline methods. On the other hand, facing corrective actions with different types of imperfection, our method can overcome the non-optimality of this feedback thanks to the guidance from proxy rewards.",
      "authors": [
        "Zhaohui JIANG",
        "Xuening Feng",
        "Paul Weng",
        "Yifei Zhu",
        "Yan Song",
        "Tianze Zhou",
        "Yujing Hu",
        "Tangjie Lv",
        "Changjie Fan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=JTji0Jfh5a",
      "cdate": 1727430100425,
      "mdate": 1742038197477,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836749"
    },
    {
      "id": "NfCEVihkdC",
      "title": "Measuring And Improving Persuasiveness Of Large Language Models",
      "abstract": "Large Language Models (LLMs) are increasingly being used in workflows involving generating content to be consumed by humans (*e.g.,* marketing) and also in directly interacting with humans (*e.g.,* through chatbots). The development of such systems that are capable of generating verifiably persuasive messages presents both opportunities and challenges for society. On the one hand, such systems could positively impact domains like advertising and social good, such as addressing drug addiction, and on the other, they could be misused for spreading misinformation and shaping political opinions. To channel LLMs' impact on society, we need to develop systems to measure and benchmark their persuasiveness. With this motivation, we introduce **PersuasionBench** and **PersuasionArena**, the first large-scale benchmark and arena containing a battery of tasks to automatically measure the simulative and generative persuasion abilities of large language models. We introduce **transsuasion** (trans = carrying across, suasion = the act of persuading), a novel task of transforming non-persuasive language into persuasive content while preserving other factors determining persuasiveness (sender, receiver, time, and channel). Our findings indicate that the simulative persuasion capabilities of LLMs are barely above random; however, their generative persuasion capabilities are much better. For instance, GPT-4o loses only 36% of the time when playing against the best human persuader. Further, we find that LLMs' persuasiveness correlates positively with model size, but smaller models can also be made to have a higher persuasiveness than much larger models. Notably, targeted training using synthetic and natural datasets significantly enhances smaller models' persuasive capabilities, challenging scale-dependent assumptions. Our findings carry key implications for both model developers and policymakers. For instance, while the EU AI Act and California's SB-1047 aim to regulate AI models based on the number of floating point operations, we demonstrate that simple metrics like this alone fail to capture the full scope of AI's societal impact. We invite the community to explore and contribute to PersuasionArena and PersuasionBench, available at [behavior-in-the-wild.github.io/measure-persuasion](https://behavior-in-the-wild.github.io/measure-persuasion), to advance our understanding of AI-driven persuasion and its societal implications.",
      "authors": [
        "Somesh Kumar Singh",
        "Yaman Kumar Singla",
        "Harini S I",
        "Balaji Krishnamurthy"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=NfCEVihkdC",
      "cdate": 1727429677708,
      "mdate": 1744114879125,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836756"
    },
    {
      "id": "HZVIQE1MsJ",
      "title": "Learning LLM-as-a-Judge for Preference Alignment",
      "abstract": "Learning from preference feedback is a common practice for aligning large language models (LLMs) with human value. Conventionally, preference data is learned and encoded into a scalar reward model that connects a value head with an LLM to produce a scalar score as preference. However, scalar models lack interpretability and are known to be susceptible to biases in datasets. This paper investigates leveraging LLM itself to learn from such preference data and serve as a judge to address both limitations in one shot. Specifically, we prompt the pre-trained LLM to generate initial judgment pairs with contrastive preference in natural language form. The self-generated contrastive judgment pairs are used to train the LLM-as-a-Judge with Direct Preference Optimization (DPO) and incentivize its reasoning capability as a judge. This proposal of learning the LLMas-a-Judge using self-generated Contrastive judgments (Con-J) ensures natural interpretability through the generated rationales supporting the judgments, and demonstrates higher robustness against bias compared to scalar models. Experimental results show that Con-J outperforms the scalar reward model trained on the same collection of preference data, and outperforms a series of open-source and closed-source generative LLMs. We open-source the training process and model weights of Con-J at https://github.com/YeZiyi1998/Con-J.",
      "authors": [
        "Ziyi Ye",
        "Xiangsheng Li",
        "Qiuchi Li",
        "Qingyao Ai",
        "Yujia Zhou",
        "Wei Shen",
        "Dong Yan",
        "Yiqun LIU"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=HZVIQE1MsJ",
      "cdate": 1727429388500,
      "mdate": 1740890348989,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836761"
    },
    {
      "id": "BL4WBIfyrz",
      "title": "Lightweight Neural App Control",
      "abstract": "This paper introduces a novel mobile phone control architecture, Lightweight Multi-modal App Control (LiMAC), for efficient interactions and control across various Android apps. LiMAC  takes as input a textual goal and a sequence of past mobile observations, such as screenshots and corresponding UI trees, to generate precise actions. To address the computational constraints inherent to smartphones, we introduce a small Action Transformer (AcT) integrated with a fine-tuned vision-language model (VLM) for real-time decision-making and task execution.  We evaluate LiMAC on two open-source mobile control datasets, demonstrating the superior performance of our small-form-factor approach against fine-tuned versions of open-source VLMs, such as Florence2 and Qwen2-VL. It also significantly outperforms prompt engineering baselines utilising closed-source foundation models like GPT-4o. More specifically, LiMAC increases the overall action accuracy by up to 19% compared to fine-tuned VLMs, and up to 42% compared to prompt-engineering baselines.",
      "authors": [
        "Filippos Christianos",
        "Georgios Papoudakis",
        "Thomas Coste",
        "Jianye HAO",
        "Jun Wang",
        "Kun Shao"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=BL4WBIfyrz",
      "cdate": 1727429387959,
      "mdate": 1743084196785,
      "matched_keywords": [
        "foundation model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836775"
    },
    {
      "id": "gkUyYcY1W9",
      "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
      "abstract": "Long-context Large Language Models (LLMs) have enabled numerous downstream applications but also introduced significant challenges related to computational and memory efficiency. To address these challenges, optimizations for long-context inference have been developed, centered around the KV cache. However, existing benchmarks often evaluate in single-request, neglecting the full lifecycle of the KV cache in real-world use. This oversight is particularly critical, as KV cache reuse has become widely adopted in LLMs inference frameworks, such as vLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft, Google, and Anthropic. To address this gap, we introduce SCBENCH (SharedContextBENCH), a comprehensive benchmark for evaluating long-context methods from a KV cache centric perspective: 1) KV cache generation, 2) KV cache compression, 3) KV cache retrieval, and 4) KV cache loading. Specifically, SCBench uses test examples with shared context, ranging 12 tasks with two shared context modes, covering four categories of long-context capabilities: string retrieval, semantic retrieval, global information, and multi-task. With SCBench, we provide an extensive KV cache-centric analysis of eight categories long-context solutions, including Gated Linear RNNs (Codestal-Mamba), Mamba-Attention hybrids (Jamba-1.5-Mini), and efficient methods such as sparse attention, KV cache dropping, quantization, retrieval, loading, and prompt compression. The evaluation is conducted on six Transformer-based long-context LLMs: Llama-3.1-8B/70B, Qwen2.5-72B/32B, Llama-3-8B-262K, and GLM-4-9B. Our findings show that sub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding with O(n) memory and sub-O(n^2) pre-filling computation perform robustly. Dynamic sparsity yields more expressive KV caches than static patterns, and layer-level sparsity in hybrid architectures reduces memory usage with strong performance. Additionally, we identify attention distribution shift issues in long-generation scenarios.",
      "authors": [
        "YUCHENG LI",
        "Huiqiang Jiang",
        "Qianhui Wu",
        "Xufang Luo",
        "Surin Ahn",
        "Chengruidong Zhang",
        "Amir H. Abdi",
        "Dongsheng Li",
        "Jianfeng Gao",
        "Yuqing Yang",
        "Lili Qiu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=gkUyYcY1W9",
      "cdate": 1727429340520,
      "mdate": 1742364826933,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836780"
    },
    {
      "id": "1Ogw1SHY3p",
      "title": "Monet: Mixture of Monosemantic Experts for Transformers",
      "abstract": "Understanding the internal computations of large language models (LLMs) is crucial for aligning them with human values and preventing undesirable behaviors like toxic content generation. However, mechanistic interpretability is hindered by *polysemanticity*—where individual neurons respond to multiple, unrelated concepts. While Sparse Autoencoders (SAEs) have attempted to disentangle these features through sparse dictionary learning,  they have compromised LLM performance due to reliance on post-hoc reconstruction loss. To address this issue, we introduce **Mixture of Monosemantic Experts for Transformers (Monet)** architecture, which incorporates sparse dictionary learning directly into end-to-end Mixture-of-Experts pretraining. Our novel expert decomposition method enables scaling the expert count to 262,144 per layer while total parameters scale proportionally to the square root of the number of experts. Our analyses demonstrate mutual exclusivity of knowledge across experts and showcase the parametric knowledge encapsulated within individual experts. Moreover, **Monet** allows knowledge manipulation over domains, languages, and toxicity mitigation without degrading general performance. Our pursuit of transparent LLMs highlights the potential of scaling expert counts to enhance mechanistic interpretability and directly resect the internal knowledge to fundamentally adjust model behavior.",
      "authors": [
        "Jungwoo Park",
        "Ahn Young Jin",
        "Kee-Eung Kim",
        "Jaewoo Kang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=1Ogw1SHY3p",
      "cdate": 1727429306067,
      "mdate": 1746971885330,
      "matched_keywords": [
        "large language model",
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836790"
    },
    {
      "id": "gjRhw5S3A4",
      "title": "GraphBridge: Towards Arbitrary Transfer Learning in GNNs",
      "abstract": "Graph neural networks (GNNs) are conventionally trained on a per-domain, per-task basis. It creates a significant barrier in transferring the acquired knowledge to different, heterogeneous data setups. This paper introduces **GraphBridge**, a novel framework to enable knowledge transfer across disparate tasks and domains in GNNs, circumventing the need for modifications to task configurations or graph structures. Specifically, GraphBridge allows for the augmentation of any pre-trained GNN with prediction heads and a bridging network that connects the input to the output layer. This architecture not only preserves the intrinsic knowledge of the original model but also supports outputs of arbitrary dimensions. To mitigate the negative transfer problem, GraphBridge merges the source model with a concurrently trained model, thereby reducing the source bias when applied to the target domain. Our method is thoroughly evaluated across diverse transfer learning scenarios, including Graph2Graph, Node2Node, Graph2Node, and graph2point-cloud. Empirical validation, conducted over 16 datasets representative of these scenarios, confirms the framework's capacity for task- and domain-agnostic transfer learning within graph-like data, marking a significant advancement in the field of GNNs. Code is available at https://github.com/jujulili888/GraphBridge.",
      "authors": [
        "Li Ju",
        "Xingyi Yang",
        "Qi Li",
        "Xinchao Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=gjRhw5S3A4",
      "cdate": 1727429278186,
      "mdate": 1742736935335,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836796"
    },
    {
      "id": "INow59Vurm",
      "title": "Towards Explaining the Power of Constant-depth Graph Neural Networks for Structured Linear Programming",
      "abstract": "Graph neural networks (GNNs) have recently emerged as powerful tools for solving complex optimization problems, often being employed to approximate solution mappings. Empirical evidence shows that even shallow GNNs (with fewer than ten layers) can achieve strong performance in predicting optimal solutions to linear programming (LP) problems. This finding is somewhat counter-intuitive, as LPs are global optimization problems, while shallow GNNs predict based on local information. Although previous theoretical results suggest that GNNs have the expressive power to solve LPs, they require deep architectures whose depth grows at least polynomially with the problem size, and thus leave the underlying principle of this empirical phenomenon still unclear. In this paper, we examine this phenomenon through the lens of distributed computing and average-case analysis. We establish that the expressive power of GNNs for LPs is closely related to well-studied distributed algorithms for LPs. Specifically, we show that any $d$-round distributed LP algorithm can be simulated by a $d$-depth GNN, and vice versa. In particular, by designing a new distributed LP algorithm and then unrolling it, we prove that constant-depth, constant-width GNNs suffice to solve sparse binary LPs effectively. Here, in contrast with previous analyses focusing on worst-case scenarios, in which we show that GNN depth must increase with problem size by leveraging an impossibility result about distributed LP algorithms, our analysis shifts the focus to the average-case performance, and shows that constant GNN depth then becomes sufficient no matter how large the problem size is. Our theory is validated by numerical results.",
      "authors": [
        "Qian Li",
        "Minghui Ouyang",
        "Tian Ding",
        "Yuyi Wang",
        "Qingjiang Shi",
        "Ruoyu Sun"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=INow59Vurm",
      "cdate": 1727429006930,
      "mdate": 1740890348873,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836802"
    },
    {
      "id": "k2uUeLCrQq",
      "title": "RelCon: Relative Contrastive Learning for a Motion Foundation Model for Wearable Data",
      "abstract": "We present RelCon, a novel self-supervised Relative Contrastive learning approach for training a motion foundation model from wearable accelerometry sensors. First, a learnable distance measure is trained to capture motif similarity and domain-specific semantic information such as rotation invariance. Then, the learned distance provides a measurement of semantic similarity between a pair of accelerometry time-series, which we use to train our foundation model to model relative relationships across time and across subjects. The foundation model is trained on 1 billion segments from 87,376 participants, and achieves strong performance across multiple downstream tasks, including human activity recognition and gait metric regression. To our knowledge, we are the first to show the generalizability of a foundation model with motion data from wearables across distinct evaluation tasks.",
      "authors": [
        "Maxwell A Xu",
        "Jaya Narain",
        "Gregory Darnell",
        "Haraldur T Hallgrimsson",
        "Hyewon Jeong",
        "Darren Forde",
        "Richard Andres Fineman",
        "Karthik Jayaraman Raghuram",
        "James Matthew Rehg",
        "Shirley You Ren"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=k2uUeLCrQq",
      "cdate": 1727428835713,
      "mdate": 1742092964821,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836807"
    },
    {
      "id": "1tBvzOYTLF",
      "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
      "abstract": "With significant efforts in recent studies, LLM-as-a-Judge has become a cost-effective alternative to human evaluation for assessing text generation quality in a wide range of tasks. However, there still remains a reliability gap between LLM-as-a-Judge and human evaluation. One important reason is the lack of guided oracles in the evaluation process. Motivated by the role of reference pervasively used in classic text evaluation, we introduce RevisEval, a novel text generation evaluation paradigm via the response-adapted references. RevisEval is driven by the key observation that an ideal reference should maintain the necessary relevance to the response to be evaluated. Specifically, RevisEval leverages the text revision capabilities of large language models (LLMs) to adaptively revise the response, then treat the revised text as the reference (response-adapted reference) for the subsequent evaluation. Extensive experiments demonstrate that RevisEval outperforms traditional reference-free and reference-based evaluation paradigms that use LLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks. More importantly, our response-adapted references can further boost the classical text metrics, e.g., BLEU and BERTScore, compared to traditional references and even rival the LLM-as-a-Judge. A detailed analysis is also conducted to confirm RevisEval's effectiveness in bias reduction, the impact of inference cost, and reference relevance.",
      "authors": [
        "Qiyuan Zhang",
        "Yufei Wang",
        "Tiezheng YU",
        "Yuxin Jiang",
        "Chuhan Wu",
        "Liangyou Li",
        "Yasheng Wang",
        "Xin Jiang",
        "Lifeng Shang",
        "Ruiming Tang",
        "Fuyuan Lyu",
        "Chen Ma"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=1tBvzOYTLF",
      "cdate": 1727428681636,
      "mdate": 1740034000990,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836813"
    },
    {
      "id": "20qZK2T7fa",
      "title": "Neuroplastic Expansion in Deep Reinforcement Learning",
      "abstract": "The loss of plasticity in learning agents, analogous to the solidification of neural pathways in biological brains, significantly impedes learning and adaptation in reinforcement learning due to its non-stationary nature. To address this fundamental challenge, we propose a novel approach, *Neuroplastic Expansion* (NE), inspired by cortical expansion in cognitive science. NE maintains learnability and adaptability throughout the entire training process by dynamically growing the network from a smaller initial size to its full dimension. Our method is designed with three key components: (1) elastic neuron generation based on potential gradients, (2) dormant neuron pruning to optimize network expressivity, and (3) neuron consolidation via experience review to strike a balance in the plasticity-stability dilemma. Extensive experiments demonstrate that NE effectively mitigates plasticity loss and outperforms state-of-the-art methods across various tasks in MuJoCo and DeepMind Control Suite environments. NE enables more adaptive learning in complex, dynamic environments, which represents a crucial step towards transitioning deep reinforcement learning from static, one-time training paradigms to more flexible, continually adapting models.",
      "authors": [
        "Jiashun Liu",
        "Johan Samir Obando Ceron",
        "Aaron Courville",
        "Ling Pan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=20qZK2T7fa",
      "cdate": 1727428418263,
      "mdate": 1747536851834,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836818"
    },
    {
      "id": "IjbXZdugdj",
      "title": "Bio-xLSTM: Generative modeling, representation and in-context learning of biological and chemical sequences",
      "abstract": "Language models for biological and chemical sequences enable crucial applications such as drug discovery, protein engineering, and precision medicine. Currently, these language models are predominantly based on Transformer architectures. While Transformers have yielded impressive results, their quadratic runtime dependency on sequence length complicates their use for long genomic sequences and in-context learning on proteins and chemical sequences. Recently, the recurrent xLSTM architecture has been shown to perform favorably compared to Transformers and modern state-space models (SSMs) in the natural language domain. Similar to SSMs, xLSTMs have linear runtime dependency and allow for constant-memory decoding at inference time, which makes them prime candidates for modeling long-range dependencies in biological and chemical sequences. In this work, we tailor xLSTM towards these domains and we propose a suite of language models called Bio-xLSTM. Extensive experiments in three large domains, genomics, proteins, and chemistry, were performed to assess xLSTM’s ability to model biological and chemical sequences. The results show that Bio-xLSTM is a highly proficient generative model for DNA, protein, and chemical sequences, learns rich representations, and can perform in-context learning for proteins and small molecules.",
      "authors": [
        "Niklas Schmidinger",
        "Lisa Schneckenreiter",
        "Philipp Seidl",
        "Johannes Schimunek",
        "Pieter-Jan Hoedt",
        "Johannes Brandstetter",
        "Andreas Mayr",
        "Sohvi Luukkonen",
        "Sepp Hochreiter",
        "Günter Klambauer"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=IjbXZdugdj",
      "cdate": 1727428168194,
      "mdate": 1741014815861,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836824"
    },
    {
      "id": "bwOndfohRK",
      "title": "Neural networks on Symmetric Spaces of Noncompact Type",
      "abstract": "Recent works have demonstrated promising performances of neural networks on hyperbolic spaces and symmetric positive definite (SPD) manifolds. These spaces belong to a family of Riemannian manifolds referred to as symmetric spaces of noncompact type. In this paper, we propose a novel approach for developing neural networks on such spaces. Our approach relies on a unified formulation of the distance from a point to a hyperplane on the considered spaces. We show that some existing formulations of the point-to-hyperplane distance can be recovered by our approach under specific settings. Furthermore, we derive a closed-form expression for the point-to-hyperplane distance in higher-rank symmetric spaces of noncompact type equipped with G-invariant Riemannian metrics. The derived distance then serves as a tool to design fully-connected (FC) layers and an attention mechanism for neural networks on the considered spaces. Our approach is validated on challenging benchmarks for image classification, electroencephalogram (EEG) signal classification, image generation, and natural language inference.",
      "authors": [
        "Xuan Son Nguyen",
        "Shuo Yang",
        "Aymeric Histace"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=bwOndfohRK",
      "cdate": 1727428056131,
      "mdate": 1740900178506,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836829"
    },
    {
      "id": "q1UyoY3MgJ",
      "title": "Rethinking Invariance in In-context Learning",
      "abstract": "In-Context Learning (ICL) has emerged as a pivotal capability of auto-regressive large language models, yet it is hindered by a notable sensitivity to the ordering of context examples regardless of their mutual independence. To address this issue, recent studies have introduced several variant algorithms of ICL that achieve permutation invariance. However, many of these do not exhibit comparable performance with the standard auto-regressive ICL algorithm. In this work, we identify two crucial elements in the design of an invariant ICL algorithm: information non-leakage and context interdependence, which are not simultaneously achieved by any of the existing methods. These investigations lead us to the proposed \\emph{Invariant ICL (InvICL)}, a methodology designed to achieve invariance in ICL while ensuring the two properties. Empirically, our findings reveal that InvICL surpasses previous models, both invariant and non-invariant, in most benchmark datasets, showcasing superior generalization capabilities across varying input lengths. Code is available at https://github.com/PKU-ML/InvICL.",
      "authors": [
        "Lizhe Fang",
        "Yifei Wang",
        "Khashayar Gatmiry",
        "Lei Fang",
        "Yisen Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=q1UyoY3MgJ",
      "cdate": 1727427768459,
      "mdate": 1740836156934,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836835"
    },
    {
      "id": "TVQLu34bdw",
      "title": "Proteina: Scaling Flow-based Protein Structure Generative Models",
      "abstract": "Recently, diffusion- and flow-based generative models of protein structures have emerged as a powerful tool for de novo protein design. Here, we develop *Proteina*, a new large-scale flow-based protein backbone generator that utilizes hierarchical fold class labels for conditioning and relies on a tailored scalable transformer architecture with up to $5\\times$ as many parameters as previous models. To meaningfully quantify performance, we introduce a new set of metrics that directly measure the distributional similarity of generated proteins with reference sets, complementing existing metrics. We further explore scaling training data to millions of synthetic protein structures and explore improved training and sampling recipes adapted to protein backbone generation. This includes fine-tuning strategies like LoRA for protein backbones, new guidance methods like classifier-free guidance and autoguidance for protein backbones, and new adjusted training objectives. Proteina achieves state-of-the-art performance on de novo protein backbone design and produces diverse and designable proteins at unprecedented length, up to 800 residues. The hierarchical conditioning offers novel control, enabling high-level secondary-structure guidance as well as low-level fold-specific generation.",
      "authors": [
        "Tomas Geffner",
        "Kieran Didi",
        "Zuobai Zhang",
        "Danny Reidenbach",
        "Zhonglin Cao",
        "Jason Yim",
        "Mario Geiger",
        "Christian Dallago",
        "Emine Kucukbenli",
        "Arash Vahdat",
        "Karsten Kreis"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=TVQLu34bdw",
      "cdate": 1727427767245,
      "mdate": 1740881848227,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836840"
    },
    {
      "id": "wUtXB43Chi",
      "title": "FlashMask: Efficient and Rich Mask Extension of FlashAttention",
      "abstract": "The computational and memory demands of vanilla attention scale quadratically with the sequence length $N$, posing significant challenges for processing long sequences in Transformer models. FlashAttention alleviates these challenges by eliminating the $\\mathcal{O}(N^2)$ memory dependency and reducing attention latency through IO-aware memory optimizations. However, its native support for certain attention mask types is limited, and it does not inherently accommodate more complex masking requirements. Previous approaches resort to using dense masks with $\\mathcal{O}(N^2)$ memory complexity, leading to inefficiencies. In this paper, we propose \\ours{}, an extension of FlashAttention that introduces a column-wise sparse representation of attention masks. This approach efficiently represents a wide range of mask types and facilitates the development of optimized kernel implementations. By adopting this novel representation, \\ours{} achieves linear memory complexity $\\mathcal{O}(N)$, making it suitable for modeling long-context sequences. Moreover, this representation enables kernel optimizations that eliminate unnecessary computations by leveraging sparsity in the attention mask, without sacrificing computational accuracy, resulting in higher computational efficiency. We evaluate \\ours{}'s performance in fine-tuning and alignment training of LLMs such as SFT, LoRA, DPO, and RM. \\ours{} achieves significant throughput improvements, with end-to-end speedups ranging from 1.65x to 3.22x compared to existing FlashAttention dense method. Additionally, our kernel-level comparisons demonstrate that \\ours{} surpasses the latest counterpart, FlexAttention, by 12.1\\% to 60.7\\% in terms of kernel TFLOPs/s, achieving 37.8\\% to 62.3\\% of the theoretical maximum FLOPs/s on the A100 GPU. The code is open-sourced on PaddlePaddle\\footnote{\\url{https://github.com/PaddlePaddle/Paddle}} and integrated into PaddleNLP\\footnote{\\url{https://github.com/PaddlePaddle/PaddleNLP}}, supporting models with over 100 billion parameters for contexts extending up to 128K tokens.",
      "authors": [
        "Guoxia Wang",
        "Jinle Zeng",
        "Xiyuan Xiao",
        "Siming Wu",
        "Jiabin Yang",
        "Lujing Zheng",
        "Zeyu Chen",
        "Jiang Bian",
        "Dianhai Yu",
        "Haifeng Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=wUtXB43Chi",
      "cdate": 1727427749868,
      "mdate": 1740674323943,
      "matched_keywords": [
        "transformer"
      ],
      "fetched_at": "2025-08-10T23:47:02.836851"
    },
    {
      "id": "HZgZrtIreg",
      "title": "Generalizing Weisfeiler-Lehman Kernels to Subgraphs",
      "abstract": "Subgraph representation learning has been effective in solving various real-world problems. However, current graph neural networks (GNNs) produce suboptimal results for subgraph-level tasks due to their inability to capture complex interactions within and between subgraphs. To provide a more expressive and efficient alternative, we propose WLKS, a Weisfeiler-Lehman (WL) kernel generalized for subgraphs by applying the WL algorithm on induced $k$-hop neighborhoods. We combine kernels across different $k$-hop levels to capture richer structural information that is not fully encoded in existing models. Our approach can balance expressiveness and efficiency by eliminating the need for neighborhood sampling. In experiments on eight real-world and synthetic benchmarks, WLKS significantly outperforms leading approaches on five datasets while reducing training time, ranging from 0.01x to 0.25x compared to the state-of-the-art.",
      "authors": [
        "Dongkwan Kim",
        "Alice Oh"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=HZgZrtIreg",
      "cdate": 1727427643363,
      "mdate": 1739261718535,
      "matched_keywords": [
        "neural network",
        "representation learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836857"
    },
    {
      "id": "8jvVNPHtVJ",
      "title": "Automated Filtering of Human Feedback Data for Aligning Text-to-Image Diffusion Models",
      "abstract": "Fine-tuning text-to-image diffusion models with human feedback is an effective method for aligning model behavior with human intentions. However, this alignment process often suffers from slow convergence due to the large size and noise present in human feedback datasets. In this work, we propose FiFA, a novel automated data filtering algorithm designed to enhance the fine-tuning of diffusion models using human feedback datasets with direct preference optimization (DPO). Specifically, our approach selects data by solving an optimization problem to maximize three components: preference margin, text quality, and text diversity. The concept of preference margin is used to identify samples that are highly informative in addressing the noisy nature of feedback dataset, which is calculated using a proxy reward model. Additionally, we incorporate text quality, assessed by large language models to prevent harmful contents, and consider text diversity through a k-nearest neighbor entropy estimator to improve generalization. Finally, we integrate all these components into an optimization process, with approximating the solution by assigning importance score to each data pair and selecting the most important ones. As a result, our method efficiently filters data automatically, without the need for manual intervention, and can be applied to any large-scale dataset. Experimental results show that FiFA significantly enhances training stability and achieves better performance, being preferred by humans 17% more, while using less than 0.5% of the full data and thus 1% of the GPU hours compared to utilizing full human feedback datasets.",
      "authors": [
        "Yongjin Yang",
        "Sihyeon Kim",
        "Hojung Jung",
        "Sangmin Bae",
        "SangMook Kim",
        "Se-Young Yun",
        "Kimin Lee"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=8jvVNPHtVJ",
      "cdate": 1727427520898,
      "mdate": 1740902530666,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836865"
    },
    {
      "id": "fL4qWkSmtM",
      "title": "What is Wrong with Perplexity for Long-context Language Modeling?",
      "abstract": "Handling long-context inputs is crucial for large language models (LLMs) in tasks such as extended conversations, document summarization, and many-shot in-context learning. While recent approaches have extended the context windows of LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has proven unreliable for assessing long-context capabilities. The underlying cause of this limitation has remained unclear. In this work, we provide a comprehensive explanation for this issue. We find that PPL overlooks key tokens, which are essential for long-context understanding, by averaging across all tokens and thereby obscuring the true performance of models in long-context scenarios. To address this, we propose \\textbf{LongPPL}, a novel metric that focuses on key tokens by employing a long-short context contrastive method to identify them. Our experiments demonstrate that LongPPL strongly correlates with performance on various long-context benchmarks (e.g., Pearson correlation of -0.96), significantly outperforming traditional PPL in predictive accuracy. Additionally, we introduce \\textbf{LongCE} (Long-context Cross-Entropy) loss, a re-weighting strategy for fine-tuning that prioritizes key tokens, leading to consistent improvements across diverse benchmarks. In summary, these contributions offer deeper insights into the limitations of PPL and present effective solutions for accurately evaluating and enhancing the long-context capabilities of LLMs. Code is available at https://github.com/PKU-ML/LongPPL.",
      "authors": [
        "Lizhe Fang",
        "Yifei Wang",
        "Zhaoyang Liu",
        "Chenheng Zhang",
        "Stefanie Jegelka",
        "Jinyang Gao",
        "Bolin Ding",
        "Yisen Wang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=fL4qWkSmtM",
      "cdate": 1727427481257,
      "mdate": 1741060815769,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836870"
    },
    {
      "id": "dsP91M4hDL",
      "title": "TC-MoE: Augmenting Mixture of Experts with Ternary Expert Choice",
      "abstract": "The Mixture of Experts (MoE) architecture has emerged as a promising solution to reduce computational overhead by selectively activating subsets of model parameters.\nThe effectiveness of MoE models depends primarily on their routing mechanisms, with the widely adopted Top-K routing scheme used for activating experts.\nHowever, the Top-K scheme has notable limitations,\nincluding unnecessary activations and underutilization of experts.\nIn this work, \nrather than modifying the routing mechanism as done in previous studies,\nwe propose the Ternary Choice MoE (TC-MoE),\na novel approach that expands the expert space by applying the ternary set {-1, 0, 1} to each expert.\nThis expansion allows more efficient and effective expert activations without incurring significant computational costs.\nAdditionally, \ngiven the unique characteristics of the expanded expert space,\nwe introduce a new load balance loss and reward loss to ensure workload balance and achieve a flexible trade-off between effectiveness and efficiency.\nExtensive experiments demonstrate that TC-MoE achieves an average improvement of over 1.1% compared with traditional approaches,\nwhile reducing the average number of activated experts by up to 9%.\nThese results confirm that TC-MoE effectively addresses the inefficiencies of conventional routing schemes,\noffering a more efficient and scalable solution for MoE-based large language models.\nCode and models are available at https://github.com/stiger1000/TC-MoE.",
      "authors": [
        "Shen Yan",
        "Xingyan Bin",
        "Sijun Zhang",
        "Yisen Wang",
        "Zhouchen Lin"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=dsP91M4hDL",
      "cdate": 1727427346419,
      "mdate": 1740810314828,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836876"
    },
    {
      "id": "ujpAYpFDEA",
      "title": "Can Watermarked LLMs be Identified by Users via Crafted Prompts?",
      "abstract": "Text watermarking for Large Language Models (LLMs) has made significant progress in detecting LLM outputs and preventing misuse. Current watermarking techniques offer high detectability, minimal impact on text quality, and robustness to text editing. \n    However, current researches lack investigation into the imperceptibility of watermarking techniques in LLM services.\n    This is crucial as LLM providers may not want to disclose the presence of watermarks in real-world scenarios, as it could reduce user willingness to use the service and make watermarks more vulnerable to attacks. This work is the first to investigate the imperceptibility of watermarked LLMs. We design an identification algorithm called Water-Probe that detects watermarks through well-designed prompts to the LLM. Our key motivation is that current watermarked LLMs expose consistent biases under the same watermark key, resulting in similar differences across prompts under different watermark keys. Experiments show that almost all mainstream watermarking algorithms are easily identified with our well-designed prompts, \n    while Water-Probe demonstrates a minimal false positive rate for non-watermarked LLMs. \n    Finally, we propose that the key to enhancing the imperceptibility of watermarked LLMs is to increase the randomness of watermark key selection. Based on this, we introduce the Water-Bag strategy, which significantly improves watermark imperceptibility by merging multiple watermark keys.",
      "authors": [
        "Aiwei Liu",
        "Sheng Guan",
        "Yiming Liu",
        "Leyi Pan",
        "Yifei Zhang",
        "Liancheng Fang",
        "Lijie Wen",
        "Philip S. Yu",
        "Xuming Hu"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=ujpAYpFDEA",
      "cdate": 1727427271066,
      "mdate": 1740149814445,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836881"
    },
    {
      "id": "pHe4P1IVnb",
      "title": "Bayesian WeakS-to-Strong from Text Classification to Generation",
      "abstract": "Advances in large language models raise the question of how alignment techniques will adapt as models become increasingly complex and humans will only be able to supervise them weakly. Weak-to-Strong mimics such a scenario where weak model supervision attempts to harness the full capabilities of a much stronger model. This work extends Weak-to-Strong to WeakS-to-Strong by exploring an ensemble of weak models which simulate the variability in human opinions. Confidence scores are estimated using a Bayesian approach to guide the WeakS-to-Strong generalization. Furthermore, we extend the application of WeakS-to-Strong from text classification tasks to text generation tasks where more advanced strategies are investigated for supervision. Moreover, direct preference optimization is applied to advance the student model's preference learning, beyond the basic learning framework of teacher forcing. Results demonstrate the effectiveness of the proposed approach for the reliability of a strong student model, showing potential for superalignment.",
      "authors": [
        "Ziyun Cui",
        "Ziyang Zhang",
        "Guangzhi Sun",
        "Wen Wu",
        "Chao Zhang"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=pHe4P1IVnb",
      "cdate": 1727427219382,
      "mdate": 1740890348538,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836887"
    },
    {
      "id": "URPwT55i6O",
      "title": "Polyrating: A Cost-Effective and Bias-Aware Rating System for LLM Evaluation",
      "abstract": "Rating-based human evaluation has become an essential tool to accurately evaluate the impressive performance of large language models (LLMs). However, current rating systems suffer from several important limitations: first, they fail to account for biases that significantly influence evaluation results, second, they require large and expensive preference datasets to obtain accurate ratings, and third, they do not facilitate meaningful comparisons of model ratings across different tasks. To address these issues, we introduce Polyrating, an expressive and flexible rating system based on maximum a posteriori estimation that enables a more nuanced and thorough analysis of model performance at lower costs. Polyrating can detect and quantify biases affecting human preferences, ensuring fairer model comparisons. Further, Polyrating can reduce the cost of human evaluations by up to $41$% for new models and up to $77$% for new tasks by leveraging existing benchmark scores. Lastly, Polyrating enables direct comparisons of ratings across different tasks, providing a comprehensive understanding of an LLMs' strengths, weaknesses, and relative performance across different applications.",
      "authors": [
        "Jasper Dekoninck",
        "Maximilian Baader",
        "Martin Vechev"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=URPwT55i6O",
      "cdate": 1727426984860,
      "mdate": 1739276185450,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836893"
    },
    {
      "id": "upoxXRRTQ2",
      "title": "The impact of allocation strategies in subset learning on the expressive power of neural networks",
      "abstract": "In traditional machine learning, models are defined by a set of parameters, which are optimized to perform specific tasks. In neural networks, these parameters correspond to the synaptic weights. However, in reality, it is often infeasible to control or update all weights. This challenge is not limited to artificial networks but extends to biological networks, such as the brain, where the extent of distributed synaptic weight modification during learning remains unclear. Motivated by these insights, we theoretically investigate how different allocations of a fixed number of learnable weights influence the capacity of neural networks. Using a teacher-student setup, we introduce a benchmark to quantify the expressivity associated with each allocation. We establish conditions under which allocations have \\`maximal' or \\`minimal' expressive power in linear recurrent neural networks and linear multi-layer feedforward networks. For suboptimal allocations, we propose heuristic principles to estimate their expressivity. These principles extend to shallow ReLU networks as well. Finally, we validate our theoretical findings with empirical experiments. Our results emphasize the critical role of strategically distributing learnable weights across the network, showing that a more widespread allocation generally enhances the network’s expressive power.",
      "authors": [
        "Ofir Schlisselberg",
        "Ran Darshan"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=upoxXRRTQ2",
      "cdate": 1727426979901,
      "mdate": 1743407110608,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836898"
    },
    {
      "id": "tj5xJInWty",
      "title": "Temporal Heterogeneous Graph Generation with Privacy, Utility, and Efficiency",
      "abstract": "Nowadays, temporal heterogeneous graphs attract much research and industrial attention for building the next-generation Relational Deep Learning models and applications, due to their informative structures and features. While providing timely and precise services like personalized recommendations and question answering, this rich information also introduces extra exposure risk for each node in the graph. The distinctive local topology, the abundant heterogeneous features, and the time dimension of the graph data are more prone to expose sensitive information and narrow down the scope of victim candidates, which calls for well-defined protection techniques on graphs. To this end, we propose a Temporal Heterogeneous Graph Generator balancing Privacy, Utility, and Efficiency, named THePUff. More specifically, we first propose a differential privacy algorithm to perturb the input temporal heterogeneous graph for protecting privacy, and then utilize both the perturbed graph and the original one in a generative adversarial setting for THePUff to learn and generate privacy-guaranteed and utility-preserved graph data in an efficient manner. We further propose 6 new metrics in the temporal setting to measure heterogeneous graph utility and privacy. Finally, based on temporal heterogeneous graph datasets with up to 1 million nodes and 20 million edges, the experiments show that THePUff generates utilizable temporal heterogeneous graphs with privacy protected, compared with state-of-the-art baselines.",
      "authors": [
        "Xinyu He",
        "Dongqi Fu",
        "Hanghang Tong",
        "Ross Maciejewski",
        "Jingrui He"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=tj5xJInWty",
      "cdate": 1727426923123,
      "mdate": 1740644539503,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836903"
    },
    {
      "id": "oVKEAFjEqv",
      "title": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning",
      "abstract": "Large language models (LLMs) have shown remarkable potential as autonomous agents, particularly in web-based tasks. \nHowever, existing LLM web agents face significant limitations: high-performing agents rely on expensive proprietary LLM APIs, while open LLMs lack the necessary decision-making capabilities. \nThis paper introduces WebRL, a novel self-evolving online curriculum reinforcement learning framework designed to train high-performance web agents using open LLMs. \nOur approach addresses key challenges in this domain, including the scarcity of training tasks, sparse feedback signals, and policy distribution drift in online learning. \nWebRL incorporates a self-evolving curriculum that generates new tasks from unsuccessful attempts, a robust outcome-supervised reward model (ORM), and adaptive reinforcement learning strategies to ensure consistent improvement. \nWe apply WebRL to transform Llama-3.1 models into proficient web agents, achieving remarkable results on the WebArena-Lite benchmark. \nOur Llama-3.1-8B agent improves from an initial 4.8\\% success rate to 42.4\\%, while the Llama-3.1-70B agent achieves a 47.3\\% success rate across five diverse websites. \nThese results surpass the performance of GPT-4-Turbo (17.6\\%) by over 160\\% relatively and significantly outperform previous state-of-the-art web agents trained on open LLMs (AutoWebGLM, 18.2\\%). \nOur findings demonstrate WebRL's effectiveness in bridging the gap between open and proprietary LLM-based web agents, paving the way for more accessible and powerful autonomous web interaction systems.",
      "authors": [
        "Zehan Qi",
        "Xiao Liu",
        "Iat Long Iong",
        "Hanyu Lai",
        "Xueqiao Sun",
        "Jiadai Sun",
        "Xinyue Yang",
        "Yu Yang",
        "Shuntian Yao",
        "Wei Xu",
        "Jie Tang",
        "Yuxiao Dong"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=oVKEAFjEqv",
      "cdate": 1727426887728,
      "mdate": 1740906088706,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:02.836912"
    },
    {
      "id": "ww3CLRhF1v",
      "title": "Adaptive Methods through the Lens of SDEs: Theoretical Insights on the Role of Noise",
      "abstract": "Despite the vast empirical evidence supporting the efficacy of adaptive optimization methods in deep learning, their theoretical understanding is far from complete. This work introduces novel SDEs for commonly used adaptive optimizers: SignSGD, RMSprop(W), and Adam(W). These SDEs offer a quantitatively accurate description of these optimizers and help illuminate an intricate relationship between adaptivity, gradient noise, and curvature. Our novel analysis of SignSGD highlights a noteworthy and precise contrast to SGD in terms of convergence speed, stationary distribution, and robustness to heavy-tail noise. We extend this analysis to AdamW and RMSpropW, for which we observe that the role of noise is much more complex. Crucially, we support our theoretical analysis with experimental evidence by verifying our insights: this includes numerically integrating our SDEs using Euler-Maruyama discretization on various neural network architectures such as MLPs, CNNs, ResNets, and Transformers. Our SDEs accurately track the behavior of the respective optimizers, especially when compared to previous SDEs derived for Adam and RMSprop. We believe our approach can provide valuable insights into best training practices and novel scaling rules.",
      "authors": [
        "Enea Monzio Compagnoni",
        "Tianlin Liu",
        "Rustem Islamov",
        "Frank Norbert Proske",
        "Antonio Orvieto",
        "Aurelien Lucchi"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=ww3CLRhF1v",
      "cdate": 1727426849790,
      "mdate": 1742072280044,
      "matched_keywords": [
        "transformer",
        "deep learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836917"
    },
    {
      "id": "4R71pdPBZp",
      "title": "Self-Evolving Multi-Agent Collaboration Networks for Software Development",
      "abstract": "LLM-driven multi-agent collaboration (MAC) systems have demonstrated impressive capabilities in automatic software development at the function level. However, their heavy reliance on human design limits their adaptability to the diverse demands of real-world software development.\nTo address this limitation, we introduce EvoMAC, a novel self-evolving paradigm for MAC networks. Inspired by traditional neural network training, EvoMAC obtains text-based environmental feedback by verifying the MAC network's output against a target proxy and leverages a novel textual backpropagation to update the network.\nTo extend coding capabilities beyond function-level tasks to more challenging software-level development, we further propose RSD-Bench, a requirement-oriented software development benchmark, which features complex and diverse software requirements along with automatic evaluation of requirement correctness.\nOur experiments show that:\ni) The automatic requirement-aware evaluation in RSD-Bench closely aligns with human evaluations, validating its reliability as a software-level coding benchmark.\nii) EvoMAC outperforms previous SOTA methods on both the software-level RSD-Bench and the function-level HumanEval benchmarks, reflecting its superior coding capabilities.",
      "authors": [
        "Yue Hu",
        "Yuzhu Cai",
        "Yaxin Du",
        "Xinyu Zhu",
        "Xiangrui Liu",
        "Zijie Yu",
        "Yuchen Hou",
        "Shuo Tang",
        "Siheng Chen"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=4R71pdPBZp",
      "cdate": 1727426656580,
      "mdate": 1740890348416,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836923"
    },
    {
      "id": "HAwZGLcye3",
      "title": "BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments",
      "abstract": "Agents based on large language models have shown great potential in accelerating scientific discovery by leveraging their rich background knowledge and reasoning capabilities. In this paper, we introduce BioDiscoveryAgent, an agent that designs new experiments, reasons about their outcomes, and efficiently navigates the hypothesis space to reach desired solutions. We demonstrate our agent on the problem of designing genetic perturbation experiments, where the aim is to find a small subset out of many possible genes that, when perturbed, result in a specific phenotype (e.g., cell growth). Utilizing its biological knowledge, BioDiscoveryAgent can uniquely design new experiments without the need to train a machine learning model or explicitly design an acquisition function as in Bayesian optimization. Moreover, BioDiscoveryAgent using Claude 3.5 Sonnet achieves an average of 21% improvement in predicting relevant genetic perturbations across six datasets, and a 46% improvement in the harder task of non-essential gene perturbation, compared to existing Bayesian optimization baselines specifically trained for this task. Our evaluation includes one dataset that is unpublished, ensuring it is not part of the language model's training data. Additionally, BioDiscoveryAgent predicts gene combinations to perturb more than twice as accurately as a random baseline, a task so far not explored in the context of closed-loop experiment design. The agent also has access to tools for searching the biomedical literature, executing code to analyze biological datasets, and prompting another agent to critically evaluate its predictions. Overall, BioDiscoveryAgent is interpretable at every stage, representing an accessible new paradigm in the computational design of biological experiments with the potential to augment scientists' efficacy.",
      "authors": [
        "Yusuf H Roohani",
        "Andrew H. Lee",
        "Qian Huang",
        "Jian Vora",
        "Zachary Steinhart",
        "Kexin Huang",
        "Alexander Marson",
        "Percy Liang",
        "Jure Leskovec"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=HAwZGLcye3",
      "cdate": 1727426454605,
      "mdate": 1740800736305,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:02.836928"
    },
    {
      "id": "UstOpZCESc",
      "title": "Privacy-Aware Lifelong Learning",
      "abstract": "Lifelong learning algorithms enable models to incrementally acquire new knowledge without forgetting previously learned information. Contrarily, the field of machine unlearning focuses on explicitly forgetting certain previous knowledge from pretrained models when requested, in order to comply with data privacy regulations on the right-to-be-forgotten. Enabling efficient lifelong learning with the capability to selectively unlearn sensitive information from models presents a critical and largely unaddressed challenge with contradicting objectives. We address this problem from the perspective of simultaneously preventing catastrophic forgetting and allowing forward knowledge transfer during task-incremental learning, while ensuring exact task unlearning and minimizing memory requirements, based on a single neural network model to be adapted. Our proposed solution, privacy-aware lifelong learning (PALL), involves optimization of task-specific sparse subnetworks with parameter sharing within a single architecture. We additionally utilize an episodic memory rehearsal mechanism to facilitate exact unlearning without performance degradations. We empirically demonstrate the scalability of PALL across various architectures in image classification, and provide a state-of-the-art solution that uniquely integrates lifelong learning and privacy-aware unlearning mechanisms for responsible AI applications.",
      "authors": [
        "Ozan Ozdenizci",
        "Elmar Rueckert",
        "Robert Legenstein"
      ],
      "conference": "ICLR 2025",
      "venue_id": "ICLR.cc/2025/Conference",
      "url": "https://openreview.net/forum?id=UstOpZCESc",
      "cdate": 1727426390112,
      "mdate": 1739276881367,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:02.836933"
    }
  ]
}