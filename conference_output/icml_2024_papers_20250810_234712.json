{
  "conference": "ICML 2024",
  "papers_count": 490,
  "timestamp": "2025-08-10T23:47:12.530738",
  "papers": [
    {
      "id": "Fw4fBE2rqW",
      "title": "On Discrete Prompt Optimization for Diffusion Models",
      "abstract": "This paper introduces the first gradient-based framework for prompt optimization in text-to-image diffusion models. We formulate prompt engineering as a discrete optimization problem over the language space. Two major challenges arise in efficiently finding a solution to this problem: (1) Enormous Domain Space: Setting the domain to the entire language space poses significant difficulty to the optimization process. (2) Text Gradient: Efficiently computing the text gradient is challenging, as it requires backpropagating through the inference steps of the diffusion model and a non-differentiable embedding lookup table. Beyond the problem formulation, our main technical contributions lie in solving the above challenges. First, we design a family of dynamically generated compact subspaces comprised of only the most relevant words to user input, substantially restricting the domain space. Second, we introduce ``Shortcut Text Gradient\" --- an effective replacement for the text gradient that can be obtained with constant memory and runtime. Empirical evaluation on prompts collected from diverse sources (DiffusionDB, ChatGPT, COCO) suggests that our method can discover prompts that substantially improve (prompt enhancement) or destroy (adversarial attack) the faithfulness of images generated by the text-to-image diffusion model.",
      "authors": [
        "Ruochen Wang",
        "Ting Liu",
        "Cho-Jui Hsieh",
        "Boqing Gong"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Fw4fBE2rqW",
      "cdate": 1706876283950,
      "mdate": 1719287297883,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525087"
    },
    {
      "id": "uEx2bSAJu8",
      "title": "Multi-View Clustering by Inter-cluster Connectivity Guided Reward",
      "abstract": "Multi-view clustering has been widely explored for its effectiveness in harmonizing heterogeneity along with consistency in different views of data. Despite the significant progress made by recent works, the performance of most existing methods is heavily reliant on strong priori information regarding the true cluster number $\\textit{K}$, which is rarely feasible in real-world scenarios. In this paper, we propose a novel graph-based multi-view clustering algorithm to infer unknown $\\textit{K}$ through a graph consistency reward mechanism. To be specific, we evaluate the cluster indicator matrix during each iteration with respect to diverse $\\textit{K}$. We formulate the inference process of unknown $\\textit{K}$ as a parsimonious reinforcement learning paradigm, where the reward is measured by inter-cluster connectivity. As a result, our approach is capable of independently producing the final clustering result, free from the input of a predefined cluster number. Experimental results on multiple benchmark datasets demonstrate the effectiveness of our proposed approach in comparison to existing state-of-the-art methods.",
      "authors": [
        "Hao Dai",
        "Yang Liu",
        "Peng Su",
        "Hecheng Cai",
        "Shudong Huang",
        "Jiancheng Lv"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=uEx2bSAJu8",
      "cdate": 1706875254062,
      "mdate": 1719287297876,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525112"
    },
    {
      "id": "5vZzmCeTYu",
      "title": "Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning",
      "abstract": "Recent advancements in off-policy Reinforcement Learning (RL) have significantly improved sample efficiency, primarily due to the incorporation of various forms of regularization that enable more gradient update steps than traditional agents. However, many of these techniques have been tested in limited settings, often on tasks from single simulation benchmarks and against well-known algorithms rather than a range of regularization approaches. This limits our understanding of the specific mechanisms driving RL improvements. To address this, we implemented over 60 different off-policy agents, each integrating established regularization techniques from recent state-of-the-art algorithms. We tested these agents across 14 diverse tasks from 2 simulation benchmarks, measuring training metrics related to overestimation, overfitting, and plasticity loss — issues that motivate the examined regularization techniques. Our findings reveal that while the effectiveness of a specific regularization setup varies with the task, certain combinations consistently demonstrate robust and superior performance. Notably, a simple Soft Actor-Critic agent, appropriately regularized, reliably finds a better-performing policy within the training regime, which previously was achieved mainly through model-based approaches.",
      "authors": [
        "Michal Nauman",
        "Michał Bortkiewicz",
        "Piotr Miłoś",
        "Tomasz Trzcinski",
        "Mateusz Ostaszewski",
        "Marek Cygan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5vZzmCeTYu",
      "cdate": 1706875176729,
      "mdate": 1719287297848,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525119"
    },
    {
      "id": "LDq1JPdc55",
      "title": "Copyright Traps for Large Language Models",
      "abstract": "Questions of fair use of copyright-protected content to train Large Language Models (LLMs) are being actively debated. Document-level inference has been proposed as a new task: inferring from black-box access to the trained model whether a piece of content has been seen during training. SOTA methods however rely on naturally occurring memorization of (part of) the content. While very effective against models that memorize significantly, we hypothesize - and later confirm - that they will not work against models that do not naturally memorize, e.g. medium-size 1B models. We here propose to use copyright traps, the inclusion of fictitious entries in original content, to detect the use of copyrighted materials in LLMs with a focus on models where memorization does not naturally occur. We carefully design a randomized controlled experimental setup, inserting traps into original content (books) and train a 1.3B LLM from scratch. We first validate that the use of content in our target model would be undetectable using existing methods. We then show, contrary to intuition, that even medium-length trap sentences repeated a significant number of times (100) are not detectable using existing methods. However, we show that longer sequences repeated a large number of times can be reliably detected (AUC=0.75) and used as copyright traps. Beyond copyright applications, our findings contribute to the study of LLM memorization: the randomized controlled setup enables us to draw causal relationships between memorization and certain sequence properties such as repetition in model training data and perplexity.",
      "authors": [
        "Matthieu Meeus",
        "Igor Shilov",
        "Manuel Faysse",
        "Yves-Alexandre de Montjoye"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LDq1JPdc55",
      "cdate": 1706875176105,
      "mdate": 1719287297810,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525125"
    },
    {
      "id": "Fzp1DRzCIN",
      "title": "Implicit meta-learning may lead language models to trust more reliable sources",
      "abstract": "We demonstrate that large language models (LLMs) may learn indicators of document usefulness and modulate their updates accordingly. We introduce random strings (\"tags\") as indicators of usefulness in a synthetic fine-tuning dataset. Fine-tuning on this dataset leads to **implicit meta-learning (IML)**: in further fine-tuning, the model updates to make more use of text that is tagged as useful. We perform a thorough empirical investigation of this phenomenon, finding (among other things) that (i) it occurs in both pretrained LLMs and those trained from scratch, as well as on a vision task, and (ii) larger models and smaller batch sizes tend to give more IML. We also use probing to examine how IML changes the way models store knowledge in their parameters. Finally, we reflect on what our results might imply about the capabilities, risks, and controllability of future AI systems.",
      "authors": [
        "Dmitrii Krasheninnikov",
        "Egor Krasheninnikov",
        "Bruno Kacper Mlodozeniec",
        "Tegan Maharaj",
        "David Krueger"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Fzp1DRzCIN",
      "cdate": 1706875174288,
      "mdate": 1721109270606,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525131"
    },
    {
      "id": "HsseRq2FAx",
      "title": "Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming",
      "abstract": "Model-based reinforcement learning (MBRL) has been a primary approach to ameliorating the sample efficiency issue as well as to make a generalist agent. However, there has not been much effort toward enhancing the strategy of dreaming itself. Therefore, it is a question *whether and how an agent can ``*dream better*''* in a more structured and strategic way. In this paper, inspired by the observation from cognitive science suggesting that humans use a spatial divide-and-conquer strategy in planning, we propose a new MBRL agent, called **Dr. Strategy**, which is equipped with a novel **Dr**eaming **Strategy**. The proposed agent realizes a version of divide-and-conquer-like strategy in dreaming. This is achieved by learning a set of latent landmarks and then utilizing these to learn a landmark-conditioned highway policy. With the highway policy, the agent can first learn in the dream to move to a landmark, and from there it tackles the exploration and achievement task in a more focused way. In experiments, we show that the proposed model outperforms prior pixel-based MBRL methods in various visually complex and partially observable navigation tasks.",
      "authors": [
        "Hany Hamed",
        "Subin Kim",
        "Dongyeong Kim",
        "Jaesik Yoon",
        "Sungjin Ahn"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=HsseRq2FAx",
      "cdate": 1706875161526,
      "mdate": 1719287297734,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525137"
    },
    {
      "id": "60vC1FY0dZ",
      "title": "When Will Gradient Regularization Be Harmful?",
      "abstract": "Gradient regularization (GR), which aims to penalize the gradient norm atop the loss function, has shown promising results in training modern over-parameterized deep neural networks. However, can we trust this powerful technique? This paper reveals that GR can cause performance degeneration in adaptive optimization scenarios, particularly with learning rate warmup. Our empirical and theoretical analyses suggest this is due to GR inducing instability and divergence in gradient statistics of adaptive optimizers at the initial training stage. Inspired by the warmup heuristic, we propose three GR warmup strategies, each relaxing the regularization effect to a certain extent during the warmup course to ensure the accurate and stable accumulation of gradients. With experiments on Vision Transformer family, we confirm the three GR warmup strategies can effectively circumvent these issues, thereby largely improving the model performance. Meanwhile, we note that scalable models tend to rely more on the GR warmup, where the performance can be improved by up to 3% on Cifar10 compared to baseline GR. Code is available at https://github.com/zhaoyang-0204/gnp.",
      "authors": [
        "Yang Zhao",
        "Hao Zhang",
        "Xiuyuan Hu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=60vC1FY0dZ",
      "cdate": 1706875158970,
      "mdate": 1719287297742,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525144"
    },
    {
      "id": "pOJbk4Nzmi",
      "title": "Efficient Algorithms for Empirical Group Distributionally Robust Optimization and Beyond",
      "abstract": "In this paper, we investigate the empirical counterpart of Group Distributionally Robust Optimization (GDRO), which aims to minimize the maximal empirical risk across $m$ distinct groups. We formulate empirical GDRO as a *two-level* finite-sum convex-concave minimax optimization problem and develop an algorithm called ALEG to benefit from its special structure. ALEG is a double-looped stochastic primal-dual algorithm that incorporates variance reduction techniques into a modified mirror prox routine. To exploit the two-level finite-sum structure, we propose a simple group sampling strategy to construct the stochastic gradient with a smaller Lipschitz constant and then perform variance reduction for all groups. Theoretical analysis shows that ALEG achieves $\\varepsilon$-accuracy within a computation complexity of $\\mathcal{O}\\left(\\frac{m\\sqrt{\\bar{n}\\ln{m}}}{\\varepsilon}\\right)$, where $\\bar n$ is the average number of samples among $m$ groups. Notably, our approach outperforms the state-of-the-art method by a factor of $\\sqrt{m}$. Based on ALEG, we further develop a two-stage optimization algorithm called ALEM to deal with the empirical Minimax Excess Risk Optimization (MERO) problem. The computation complexity of ALEM nearly matches that of ALEG, surpassing the rates of existing methods.",
      "authors": [
        "Dingzhi Yu",
        "Yunuo Cai",
        "Wei Jiang",
        "Lijun Zhang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pOJbk4Nzmi",
      "cdate": 1706875098358,
      "mdate": 1719287297657,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525152"
    },
    {
      "id": "hZ0fWhgVch",
      "title": "Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion",
      "abstract": "As a dominant force in text-to-image generation tasks, Diffusion Probabilistic Models (DPMs) face a critical challenge in controllability, struggling to adhere strictly to complex, multi-faceted instructions. In this work, we aim to address this alignment challenge for conditional generation tasks. First, we provide an alternative view of state-of-the-art DPMs as a way of inverting advanced Vision-Language Models (VLMs). With this formulation, we naturally propose a training-free approach that bypasses the conventional sampling process associated with DPMs. By directly optimizing images with the supervision of discriminative VLMs, the proposed method can potentially achieve a better text-image alignment. As proof of concept, we demonstrate the pipeline with the pre-trained BLIP-2 model and identify several key designs for improved image generation. To further enhance the image fidelity, a Score Distillation Sampling module of Stable Diffusion is incorporated. By carefully balancing the two components during optimization, our method can produce high-quality images with near state-of-the-art performance on T2I-Compbench. The code is available at https://github.com/Pepper-lll/VLMinv.",
      "authors": [
        "Xuantong LIU",
        "Tianyang Hu",
        "Wenjia Wang",
        "Kenji Kawaguchi",
        "Yuan Yao"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hZ0fWhgVch",
      "cdate": 1706875055614,
      "mdate": 1719287297582,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525159"
    },
    {
      "id": "f3TUipYU3U",
      "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
      "abstract": "Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.",
      "authors": [
        "Mantas Mazeika",
        "Long Phan",
        "Xuwang Yin",
        "Andy Zou",
        "Zifan Wang",
        "Norman Mu",
        "Elham Sakhaee",
        "Nathaniel Li",
        "Steven Basart",
        "Bo Li",
        "David Forsyth",
        "Dan Hendrycks"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=f3TUipYU3U",
      "cdate": 1706875052745,
      "mdate": 1719287297553,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525165"
    },
    {
      "id": "LVF4P1NNwO",
      "title": "Exact Conversion of In-Context Learning to Model Weights in Linearized-Attention Transformers",
      "abstract": "In-Context Learning (ICL) has been a powerful emergent property of large language models that has attracted increasing attention in recent years. In contrast to regular gradient-based learning, ICL is highly interpretable and does not require parameter updates. In this paper, we show that, for linearized transformer networks, ICL can be made explicit and permanent through the inclusion of bias terms. We mathematically demonstrate the equivalence between a model with ICL demonstration prompts and the same model with the additional bias terms. Our algorithm (ICLCA) allows for exact conversion in an inexpensive manner. Existing methods are not exact and require expensive parameter updates. We demonstrate the efficacy of our approach through experiments that show the exact incorporation of ICL tokens into a linear transformer. We further suggest how our method can be adapted to achieve cheap approximate conversion of ICL tokens, even in regular transformer networks that are not linearized. Our experiments on GPT-2 show that, even though the conversion is only approximate, the model still gains valuable context from the included bias terms.",
      "authors": [
        "Brian K Chen",
        "Tianyang Hu",
        "Hui Jin",
        "Hwee Kuan Lee",
        "Kenji Kawaguchi"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LVF4P1NNwO",
      "cdate": 1706875015148,
      "mdate": 1719287297502,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525171"
    },
    {
      "id": "Gp0xZDmrA2",
      "title": "Learning in Feature Spaces via Coupled Covariances: Asymmetric Kernel SVD and Nyström method",
      "abstract": "In contrast with Mercer kernel-based approaches as used e.g. in Kernel Principal Component Analysis (KPCA), it was previously shown that Singular Value Decomposition (SVD) inherently relates to asymmetric kernels and Asymmetric Kernel Singular Value Decomposition (KSVD) has been proposed. However, the existing formulation to KSVD cannot work with infinite-dimensional feature mappings, the variational objective can be unbounded, and needs further numerical evaluation and exploration towards machine learning. In this work, i) we introduce a new asymmetric learning paradigm based on coupled covariance eigenproblem (CCE) through covariance operators, allowing infinite-dimensional feature maps. The solution to CCE is ultimately obtained from the SVD of the induced asymmetric kernel matrix, providing links to KSVD. ii) Starting from the integral equations corresponding to a pair of coupled adjoint eigenfunctions, we formalize the asymmetric Nyström method through a finite sample approximation to speed up training. iii) We provide the first empirical evaluations verifying the practical utility and benefits of KSVD and compare with methods resorting to symmetrization or linear SVD across multiple tasks.",
      "authors": [
        "Qinghua Tao",
        "Francesco Tonin",
        "Alex Lambert",
        "Yingyi Chen",
        "Panagiotis Patrinos",
        "Johan Suykens"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Gp0xZDmrA2",
      "cdate": 1706874994790,
      "mdate": 1719287297469,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525177"
    },
    {
      "id": "frA0NNBS1n",
      "title": "Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo",
      "abstract": "Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence. In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems. In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences. We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning. As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function. These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions. We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks.",
      "authors": [
        "Stephen Zhao",
        "Rob Brekelmans",
        "Alireza Makhzani",
        "Roger Baker Grosse"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=frA0NNBS1n",
      "cdate": 1706874982624,
      "mdate": 1719287297435,
      "matched_keywords": [
        "reinforcement learning",
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525183"
    },
    {
      "id": "OF7e0w1uon",
      "title": "Q-Star Meets Scalable Posterior Sampling: Bridging Theory and Practice via HyperAgent",
      "abstract": "We propose HyperAgent, a reinforcement learning (RL) algorithm based on the hypermodel framework for exploration in RL. HyperAgent allows for the efficient incremental approximation of posteriors associated with an optimal action-value function ($Q^\\star$) without the need for conjugacy and follows the greedy policies w.r.t. these approximate posterior samples. We demonstrate that HyperAgent offers robust performance in large-scale deep RL benchmarks. It can solve Deep Sea hard exploration problems with episodes that optimally scale with problem size and exhibits significant efficiency gains in the Atari suite. Implementing HyperAgent requires minimal code addition to well-established deep RL frameworks like DQN. We theoretically prove that, under tabular assumptions, HyperAgent achieves logarithmic per-step computational complexity while attaining sublinear regret, matching the best known randomized tabular RL algorithm.",
      "authors": [
        "Yingru Li",
        "Jiawei Xu",
        "Lei Han",
        "Zhi-Quan Luo"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OF7e0w1uon",
      "cdate": 1706874959863,
      "mdate": 1719287297401,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525189"
    },
    {
      "id": "nU1mtFDtMX",
      "title": "STEER: Assessing the Economic Rationality of Large Language Models",
      "abstract": "There is increasing interest in using LLMs as decision-making \"agents\". Doing so includes many degrees of freedom: which model should be used; how should it be prompted; should it be asked to introspect, conduct chain-of-thought reasoning, etc? Settling these questions---and more broadly, determining whether an LLM agent is reliable enough to be trusted---requires a methodology for assessing such an agent's economic rationality. In this paper, we provide one. We begin by surveying the economic literature on rational decision making, taxonomizing a large set of fine-grained \"elements\" that an agent should exhibit, along with dependencies between them. We then propose a benchmark distribution that quantitatively scores an LLMs performance on these elements and, combined with a user-provided rubric, produces a \"rationality report card\". Finally, we describe the results of a large-scale empirical experiment with 14 different LLMs, characterizing the both current state of the art and the impact of different model sizes on models' ability to exhibit rational behavior.",
      "authors": [
        "Narun Krishnamurthi Raman",
        "Taylor Lundy",
        "Samuel Joseph Amouyal",
        "Yoav Levine",
        "Kevin Leyton-Brown",
        "Moshe Tennenholtz"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nU1mtFDtMX",
      "cdate": 1706874926230,
      "mdate": 1719287297368,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525195"
    },
    {
      "id": "0bmXrtTDUu",
      "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
      "abstract": "Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand ($\\sim$1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges.",
      "authors": [
        "Nikhil Sardana",
        "Jacob Portes",
        "Sasha Doubov",
        "Jonathan Frankle"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0bmXrtTDUu",
      "cdate": 1706874918755,
      "mdate": 1719287297320,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525200"
    },
    {
      "id": "edHLN40DWu",
      "title": "One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts",
      "abstract": "Large Language Models (LLMs) exhibit strong generalization capabilities to novel tasks when prompted with language instructions and in-context demos. Since this ability sensitively depends on the quality of prompts, various methods have been explored to automate the instruction design. While these methods demonstrated promising results, they also restricted the searched prompt to one instruction. Such simplification significantly limits their capacity, as a single demo-free instruction might not be able to cover the entire complex problem space of the targeted task. To alleviate this issue, we adopt the Mixture-of-Expert paradigm and divide the problem space into a set of sub-regions; Each sub-region is governed by a specialized expert, equipped with both an instruction and a set of demos. A two-phase process is developed to construct the specialized expert for each region: (1) demo assignment: Inspired by the theoretical connection between in-context learning and kernel regression, we group demos into experts based on their semantic similarity; (2) instruction assignment: A region-based joint search of an instruction per expert complements the demos assigned to it, yielding a synergistic effect. The resulting method, codenamed Mixture-of-Prompts (MoP), achieves an average win rate of 81% against prior arts across several major benchmarks.",
      "authors": [
        "Ruochen Wang",
        "Sohyun An",
        "Minhao Cheng",
        "Tianyi Zhou",
        "Sung Ju Hwang",
        "Cho-Jui Hsieh"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=edHLN40DWu",
      "cdate": 1706874913218,
      "mdate": 1719287297298,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525206"
    },
    {
      "id": "nUVForc3VP",
      "title": "Double-Step Alternating Extragradient with Increasing Timescale Separation for Finding Local Minimax Points: Provable Improvements",
      "abstract": "In nonconvex-nonconcave minimax optimization, two-timescale gradient methods have shown their potential to find local minimax (optimal) points, provided that the timescale separation between the min and the max player is sufficiently large. However, existing two-timescale variants of gradient descent ascent and extragradient methods face two shortcomings, especially when we search for non-strict local minimax points that are prevalent in modern overparameterized setting. In specific, (1) these methods can be unstable at some non-strict local minimax points even with sufficiently large timescale separation, and even (2) computing a proper amount of timescale separation is infeasible in practice. To remedy these two issues, we propose to incorporate two simple but provably effective schemes, double-step alternating update and increasing timescale separation, into the two-timescale extragradient method, respectively. Under mild conditions, we show that the proposed methods converge to non-strict local minimax points that all existing two-timescale methods fail to converge.",
      "authors": [
        "Kyuwon Kim",
        "Donghwan Kim"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nUVForc3VP",
      "cdate": 1706874871196,
      "mdate": 1719287297260,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525213"
    },
    {
      "id": "DLTjFFiuUJ",
      "title": "Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration",
      "abstract": "Attention is a fundamental component behind the remarkable achievements of large language models (LLMs). However, our current understanding of the attention mechanism, especially regarding how attention distributions are established, remains limited. Inspired by recent studies that explore the presence of attention sink in the initial token, which receives disproportionately large attention scores despite their lack of semantic importance, this work delves deeper into this phenomenon. We aim to provide a more profound understanding of the existence of attention sinks within LLMs and to uncover ways to enhance the achievable accuracy of LLMs by directly optimizing the attention distributions, without the need for weight finetuning. Specifically, this work begins with comprehensive visualizations of the attention distributions in LLMs during inference across various inputs and tasks. Based on these visualizations, to the best of our knowledge, we are the first to discover that (1) attention sinks occur not only at the start of sequences but also within later tokens of the input, and (2) not all attention sinks have a positive impact on the achievable accuracy of LLMs. Building upon our findings, we propose a training-free Attention Calibration Technique (ACT) that automatically optimizes the attention distributions on the fly during inference in an input-adaptive manner. Extensive experiments validate that ACT consistently enhances the accuracy of various LLMs across different applications. Specifically, ACT achieves an average improvement of up to $7.30\\%$ in accuracy across different datasets when applied to Llama-30B.",
      "authors": [
        "Zhongzhi Yu",
        "Zheng Wang",
        "Yonggan Fu",
        "Huihong Shi",
        "Khalid Shaikh",
        "Yingyan Celine Lin"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DLTjFFiuUJ",
      "cdate": 1706874869857,
      "mdate": 1719287297238,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525219"
    },
    {
      "id": "v1I4zRAjMb",
      "title": "TENG: Time-Evolving Natural Gradient for Solving PDEs With Deep Neural Nets Toward Machine Precision",
      "abstract": "Partial differential equations (PDEs) are instrumental for modeling dynamical systems in science and engineering. The advent of neural networks has initiated a significant shift in tackling these complexities though challenges in accuracy persist, especially for initial value problems. In this paper, we introduce the *Time-Evolving Natural Gradient (TENG)*, generalizing time-dependent variational principles and optimization-based time integration, leveraging natural gradient optimization to obtain high accuracy in neural-network-based PDE solutions. Our comprehensive development includes algorithms like TENG-Euler and its high-order variants, such as TENG-Heun, tailored for enhanced precision and efficiency. TENG's effectiveness is further validated through its performance, surpassing current leading methods and achieving *machine precision* in step-by-step optimizations across a spectrum of PDEs, including the heat equation, Allen-Cahn equation, and Burgers' equation.",
      "authors": [
        "Zhuo Chen",
        "Jacob McCarran",
        "Esteban Vizcaino",
        "Marin Soljacic",
        "Di Luo"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=v1I4zRAjMb",
      "cdate": 1706874853741,
      "mdate": 1719287297205,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525225"
    },
    {
      "id": "Z0S6fUdW68",
      "title": "Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption",
      "abstract": "This study tackles the challenges of adversarial corruption in model-based reinforcement learning (RL), where the transition dynamics can be corrupted by an adversary. Existing studies on corruption-robust RL mostly focus on the setting of model-free RL, where robust least-square regression is often employed for value function estimation. However, these techniques cannot be directly applied to model-based RL. In this paper, we focus on model-based RL and take the maximum likelihood estimation (MLE) approach to learn transition model. Our work encompasses both online and offline settings. In the online setting, we introduce an algorithm called corruption-robust optimistic MLE (CR-OMLE), which leverages total-variation (TV)-based information ratios as uncertainty weights for MLE. We prove that CR-OMLE achieves a regret of $\\tilde{\\mathcal{O}}(\\sqrt{T} + C)$, where $C$ denotes the cumulative corruption level after $T$ episodes. We also prove a lower bound to show that the additive dependence on $C$ is optimal. We extend our weighting technique to the offline setting, and propose an algorithm named corruption-robust pessimistic MLE (CR-PMLE). Under a uniform coverage condition, CR-PMLE exhibits suboptimality worsened by $\\mathcal{O}(C/n)$, nearly matching the lower bound. To the best of our knowledge, this is the first work on corruption-robust model-based RL algorithms with provable guarantees.",
      "authors": [
        "Chenlu Ye",
        "Jiafan He",
        "Quanquan Gu",
        "Tong Zhang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Z0S6fUdW68",
      "cdate": 1706874805205,
      "mdate": 1719287297120,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525231"
    },
    {
      "id": "fdroxYsgzQ",
      "title": "Prompting is a Double-Edged Sword: Improving Worst-Group Robustness of Foundation Models",
      "abstract": "Machine learning models fail catastrophically under distribution shift, but a surprisingly effective way to empirically improve robustness to some types of shift (*e.g.*, Imagenet-A/C) is to use stronger open-vocabulary classifiers derived from foundation models. In this work, we first note that for shifts governed by spurious correlations (features spuriously correlated with the label on the training data, but not on test), the zero-shot and few-shot performance of foundation models is no better than ERM models, and remains unchanged when pretrained data/model size is scaled. Secondly, even in these situations, foundation models are quite accurate at predicting the value of the spurious feature. In a simplified setup, we theoretically analyze both these findings. Specifically, we show that during contrastive pretraining, the simplicity bias of foundation models tends to result in the learning of features that mostly rely on the spurious attribute, compared to more robust features. We leverage these observations to propose Prompting for Robustness (PfR) which first uses foundation models to zero-shot predict the spurious attribute on labeled examples, and then learns a classifier with balanced performance across different groups of labels and spurious attribute. Across 5 vision and language tasks, we show that PfR's performance nearly equals that of an oracle algorithm (group DRO) that leverages human labeled spurious attributes.",
      "authors": [
        "Amrith Setlur",
        "Saurabh Garg",
        "Virginia Smith",
        "Sergey Levine"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fdroxYsgzQ",
      "cdate": 1706874792494,
      "mdate": 1719287297084,
      "matched_keywords": [
        "machine learning",
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525237"
    },
    {
      "id": "haUOhXo70o",
      "title": "Hard Tasks First: Multi-Task Reinforcement Learning Through Task Scheduling",
      "abstract": "Multi-task reinforcement learning (RL) faces the significant challenge of varying task difficulties, often leading to negative transfer when simpler tasks overshadow the learning of more complex ones. To overcome this challenge, we propose a novel algorithm, Scheduled Multi-Task Training (SMT), that strategically prioritizes more challenging tasks, thereby enhancing overall learning efficiency. SMT introduces a dynamic task prioritization strategy, underpinned by an effective metric for assessing task difficulty. This metric ensures an efficient and targeted allocation of training resources, significantly improving learning outcomes. Additionally, SMT incorporates a reset mechanism that periodically reinitializes key network parameters to mitigate the simplicity bias, further enhancing the adaptability and robustness of the learning process across diverse tasks. The efficacy of SMT's scheduling method is validated by significantly improving performance on challenging Meta-World benchmarks.",
      "authors": [
        "Myungsik Cho",
        "Jongeui Park",
        "Suyoung Lee",
        "Youngchul Sung"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=haUOhXo70o",
      "cdate": 1706874745900,
      "mdate": 1719287297023,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525244"
    },
    {
      "id": "Ht20wtgaty",
      "title": "Fault Tolerant ML: Efficient Meta-Aggregation and Synchronous Training",
      "abstract": "In this paper, we investigate the challenging framework of Byzantine-robust training in distributed machine learning (ML) systems, focusing on enhancing both efficiency and practicality. As distributed ML systems become integral for complex ML tasks, ensuring resilience against Byzantine failures—where workers may contribute incorrect updates due to malice or error—gains paramount importance. Our first contribution is the introduction of the Centered Trimmed Meta Aggregator (CTMA), an efficient meta-aggregator that upgrades baseline aggregators to optimal performance levels, while requiring low computational demands. Additionally, we propose harnessing a recently developed gradient estimation technique based on a double-momentum strategy within the Byzantine context. Our paper highlights its theoretical and practical advantages for Byzantine-robust training, especially in simplifying the tuning process and reducing the reliance on numerous hyperparameters. The effectiveness of this technique is supported by theoretical insights within the stochastic convex optimization (SCO) framework and corroborated by empirical evidence.",
      "authors": [
        "Tehila Dahan",
        "Kfir Yehuda Levy"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Ht20wtgaty",
      "cdate": 1706874734836,
      "mdate": 1719287296999,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525250"
    },
    {
      "id": "xye7iNsgXn",
      "title": "Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations",
      "abstract": "Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute. Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (``Generative Recommenders''), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data. HSTU outperforms baselines over synthetic and public datasets by up to 65.8% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion parameters, improve metrics in online A/B tests by 12.4% and have been deployed on multiple surfaces of a large internet platform with billions of users. More importantly, the model quality of Generative Recommenders empirically scales as a power-law of training compute across three orders of magnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for future model developments, and further paves the way for the first foundation models in recommendations.",
      "authors": [
        "Jiaqi Zhai",
        "Lucy Liao",
        "Xing Liu",
        "Yueming Wang",
        "Rui Li",
        "Xuan Cao",
        "Leon Gao",
        "Zhaojie Gong",
        "Fangda Gu",
        "Jiayuan He",
        "Yinghai Lu",
        "Yu Shi"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xye7iNsgXn",
      "cdate": 1706874715269,
      "mdate": 1719287296966,
      "matched_keywords": [
        "deep learning",
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525255"
    },
    {
      "id": "XwnABAdH5y",
      "title": "Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning",
      "abstract": "Trustworthiness is an essential prerequisite for the real-world application of large language models. In this paper, we focus on the trustworthiness of language models with respect to retrieval augmentation. Despite being supported with external evidence, retrieval-augmented generation still suffers from hallucinations, one primary cause of which is the conflict between contextual and parametric knowledge. We deem that retrieval-augmented language models have the inherent capabilities of supplying response according to both contextual and parametric knowledge. Inspired by aligning language models with human preference, we take the first step towards aligning retrieval-augmented language models to a status where it responds relying merely on the external evidence and disregards the interference of parametric knowledge. Specifically, we propose a reinforcement learning based algorithm Trustworthy-Alignment, theoretically and experimentally demonstrating large language models' capability of reaching a trustworthy status without explicit supervision on how to respond. Our work highlights the potential of large language models on exploring its intrinsic abilities by its own and expands the application scenarios of alignment from fulfilling human preference to creating trustworthy agents.",
      "authors": [
        "Zongmeng Zhang",
        "Yufeng Shi",
        "Jinhua Zhu",
        "Wengang Zhou",
        "Xiang Qi",
        "peng zhang",
        "Houqiang Li"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=XwnABAdH5y",
      "cdate": 1706874711106,
      "mdate": 1719287296947,
      "matched_keywords": [
        "reinforcement learning",
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525262"
    },
    {
      "id": "MgTzMaYHvG",
      "title": "Instruction Tuning for Secure Code Generation",
      "abstract": "Modern language models (LMs) have gained widespread acceptance in everyday and professional contexts, particularly in programming. An essential procedure enabling this adoption is instruction tuning, which substantially enhances LMs' practical utility by training them to follow user instructions and human preferences. However, existing instruction tuning schemes overlook a crucial aspect: the security of generated code. As a result, even the state-of-the-art instruction-tuned LMs frequently produce unsafe code, posing significant security risks. In this work, we introduce SafeCoder to address this gap. SafeCoder performs security-centric fine-tuning using a diverse and high-quality dataset that we collected using an automated pipeline. We integrate the security fine-tuning with standard instruction tuning, to facilitate a joint optimization of both security and utility. Despite its simplicity, we show that SafeCoder is effective across a variety of popular LMs and datasets. It is able to drastically improve security (by about 30%), while preserving utility.",
      "authors": [
        "Jingxuan He",
        "Mark Vero",
        "Gabriela Krasnopolska",
        "Martin Vechev"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MgTzMaYHvG",
      "cdate": 1706874672463,
      "mdate": 1719287296927,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525268"
    },
    {
      "id": "Bc4vZ2CX7E",
      "title": "Position: Open-Endedness is Essential for Artificial Superhuman Intelligence",
      "abstract": "In recent years there has been a tremendous surge in the general capabilities of AI systems, mainly fuelled by training foundation models on internet-scale data. Nevertheless, the creation of open-ended, ever self-improving AI remains elusive. **In this position paper, we argue that the ingredients are now in place to achieve *open-endedness* in AI systems with respect to a human observer. Furthermore, we claim that such open-endedness is an essential property of any artificial superhuman intelligence (ASI).** We begin by providing a concrete formal definition of open-endedness through the lens of novelty and learnability. We then illustrate a path towards ASI via open-ended systems built on top of foundation models, capable of making novel, human-relevant discoveries. We conclude by examining the safety implications of generally-capable open-ended AI. We expect that open-ended foundation models will prove to be an increasingly fertile and safety-critical area of research in the near future.",
      "authors": [
        "Edward Hughes",
        "Michael D Dennis",
        "Jack Parker-Holder",
        "Feryal Behbahani",
        "Aditi Mavalankar",
        "Yuge Shi",
        "Tom Schaul",
        "Tim Rocktäschel"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Bc4vZ2CX7E",
      "cdate": 1706874563451,
      "mdate": 1719287296767,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525273"
    },
    {
      "id": "s5PLISyNyP",
      "title": "Meta-Learners for Partially-Identified Treatment Effects Across Multiple Environments",
      "abstract": "Estimating the conditional average treatment effect (CATE) from observational data is relevant for many applications such as personalized medicine. Here, we focus on the widespread setting where the observational data come from multiple environments, such as different hospitals, physicians, or countries. Furthermore, we allow for violations of standard causal assumptions, namely, overlap within the environments and unconfoundedness. To this end, we move away from point identification and focus on partial identification. Specifically, we show that current assumptions from the literature on multiple environments allow us to interpret the environment as an instrumental variable (IV). This allows us to adapt bounds from the IV literature for partial identification of CATE by leveraging treatment assignment mechanisms across environments. Then, we propose different model-agnostic learners (so-called meta-learners) to estimate the bounds that can be used in combination with arbitrary machine learning models. We further demonstrate the effectiveness of our meta-learners across various experiments using both simulated and real-world data. Finally, we discuss the applicability of our meta-learners to partial identification in instrumental variable settings, such as randomized controlled trials with non-compliance.",
      "authors": [
        "Jonas Schweisthal",
        "Dennis Frauen",
        "Mihaela van der Schaar",
        "Stefan Feuerriegel"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=s5PLISyNyP",
      "cdate": 1706874553456,
      "mdate": 1719287296715,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525279"
    },
    {
      "id": "h3SGdpI4Ta",
      "title": "Infinite-Horizon Distributionally Robust Regret-Optimal Control",
      "abstract": "We study the infinite-horizon distributionally robust (DR) control of linear systems with quadratic costs, where disturbances have unknown, possibly time-correlated distribution within a Wasserstein-2 ambiguity set. We aim to minimize the worst-case expected regret—the excess cost of a causal policy compared to a non-causal one with access to future disturbance. Though the optimal policy lacks a finite-order state-space realization (i.e., it is non-rational), it can be characterized by a finite-dimensional parameter. Leveraging this, we develop an efficient frequency-domain algorithm to compute this optimal control policy and present a convex optimization method to construct a near-optimal state-space controller that approximates the optimal non-rational controller in the $\\mathit{H}_\\infty$-norm. This approach avoids solving a computationally expensive semi-definite program (SDP) that scales with the time horizon in the finite-horizon setting.",
      "authors": [
        "Taylan Kargin",
        "Joudi Hajar",
        "Vikrant Malik",
        "Babak Hassibi"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=h3SGdpI4Ta",
      "cdate": 1706874515472,
      "mdate": 1719287296624,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525285"
    },
    {
      "id": "F3936hVwQa",
      "title": "Conformal Validity Guarantees Exist for Any Data Distribution (and How to Find Them)",
      "abstract": "As artificial intelligence (AI) / machine learning (ML) gain widespread adoption, practitioners are increasingly seeking means to quantify and control the risk these systems incur. This challenge is especially salient when such systems have autonomy to collect their own data, such as in black-box optimization and active learning, where their actions induce sequential feedback-loop shifts in the data distribution. Conformal prediction is a promising approach to uncertainty and risk quantification, but prior variants' validity guarantees have assumed some form of ``quasi-exchangeability'' on the data distribution, thereby excluding many types of sequential shifts. In this paper we prove that conformal prediction can theoretically be extended to *any* joint data distribution, not just exchangeable or quasi-exchangeable ones. Although the most general case is exceedingly impractical to compute, for concrete practical applications we outline a procedure for deriving specific conformal algorithms for any data distribution, and we use this procedure to derive tractable algorithms for a series of AI/ML-agent-induced covariate shifts. We evaluate the proposed algorithms empirically on synthetic black-box optimization and active learning tasks.",
      "authors": [
        "Drew Prinster",
        "Samuel Don Stanton",
        "Anqi Liu",
        "Suchi Saria"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=F3936hVwQa",
      "cdate": 1706874505568,
      "mdate": 1719287296591,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525291"
    },
    {
      "id": "BrZPj9rEpN",
      "title": "Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics",
      "abstract": "Developing policies that can adapt to non-stationary environments is essential for real-world reinforcement learning applications. Nevertheless, learning such adaptable policies in offline settings, with only a limited set of pre-collected trajectories, presents significant challenges. A key difficulty arises because the limited offline data makes it hard for the context encoder to differentiate between changes in the environment dynamics and shifts in the behavior policy, often leading to context misassociations. To address this issue, we introduce a novel approach called debiased offline representation learning for fast online adaptation (DORA). DORA incorporates an information bottleneck principle that maximizes mutual information between the dynamics encoding and the environmental data, while minimizing mutual information between the dynamics encoding and the actions of the behavior policy. We present a practical implementation of DORA, leveraging tractable bounds of the information bottleneck principle. Our experimental evaluation across six benchmark MuJoCo tasks with variable parameters demonstrates that DORA not only achieves a more precise dynamics encoding but also significantly outperforms existing baselines in terms of performance.",
      "authors": [
        "Xinyu Zhang",
        "Wenjie Qiu",
        "Yi-Chen Li",
        "Lei Yuan",
        "Chengxing Jia",
        "Zongzhang Zhang",
        "Yang Yu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BrZPj9rEpN",
      "cdate": 1706874475881,
      "mdate": 1719287296531,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525297"
    },
    {
      "id": "rJVjQSQ8ye",
      "title": "Linguistic Calibration of Long-Form Generations",
      "abstract": "Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce long-form text with calibrated confidence statements. Through the lens of decision-making, we define linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as \"I estimate a 30% chance of...\" or \"I am certain that...\", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and human evaluations of long-form generations that it is significantly more calibrated than strong finetuned factuality baselines with comparable accuracy. These findings generalize under significant domain shifts to scientific and biomedical questions and to an entirely held-out person biography generation task. Our results demonstrate that long-form generations may be calibrated end-to-end by constructing an objective in the space of the predictions that users make in downstream decision-making.",
      "authors": [
        "Neil Band",
        "Xuechen Li",
        "Tengyu Ma",
        "Tatsunori Hashimoto"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rJVjQSQ8ye",
      "cdate": 1706874451113,
      "mdate": 1719287296511,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525303"
    },
    {
      "id": "kXHgEYFyf3",
      "title": "R2E: Turning any Github Repository into a Programming Agent Environment",
      "abstract": "While Large Language Models’ (LLMs) coding capabilities have advanced rapidly, corresponding evaluation benchmarks on real-world programming setups are yet to catch up. Building a scalable and interactive testbed for evaluating general-purpose AI coding agents for real-world code has been challenging, particularly due to a lack of high-quality test suites available. In this paper, we present Repository to Environment (R2E), a framework that can turn any GitHub repository into a test environment to evaluate the performance of code-generating systems, both static and interactive. R2E is powered by a synergistic combination of program analysis and LLMs to construct equivalence test harnesses for any GitHub function. We instantiate our framework to build the first large-scale benchmark, R2E-Eval1, for building realistic environments for AI coding assistants. Our results demonstrate that even when SOTA models cannot generate correct solutions with advanced prompting techniques, they can effectively use environment feedback highlighting the need to move from static functional coding to interactive programming paradigm. We hope that our framework (and the instantiated benchmark) can motivate research directions by providing web-scale open-ended coding environments. R2E code is available at https://r2e.dev/",
      "authors": [
        "Naman Jain",
        "Manish Shetty",
        "Tianjun Zhang",
        "King Han",
        "Koushik Sen",
        "Ion Stoica"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kXHgEYFyf3",
      "cdate": 1706874438875,
      "mdate": 1719287296484,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525309"
    },
    {
      "id": "disVlUOH4b",
      "title": "Efficient Adaptation in Mixed-Motive Environments via Hierarchical Opponent Modeling and Planning",
      "abstract": "Despite the recent successes of multi-agent reinforcement learning (MARL) algorithms, efficiently adapting to co-players in mixed-motive environments remains a significant challenge. One feasible approach is to hierarchically model co-players' behavior based on inferring their characteristics. However, these methods often encounter difficulties in efficient reasoning and utilization of inferred information. To address these issues, we propose Hierarchical Opponent modeling and Planning (HOP), a novel multi-agent decision-making algorithm that enables few-shot adaptation to unseen policies in mixed-motive environments. HOP is hierarchically composed of two modules: an opponent modeling module that infers others' goals and learns corresponding goal-conditioned policies, and a planning module that employs Monte Carlo Tree Search (MCTS) to identify the best response. Our approach improves efficiency by updating beliefs about others' goals both across and within episodes and by using information from the opponent modeling module to guide planning. Experimental results demonstrate that in mixed-motive environments, HOP exhibits superior few-shot adaptation capabilities when interacting with various unseen agents, and excels in self-play scenarios. Furthermore, the emergence of social intelligence during our experiments underscores the potential of our approach in complex multi-agent environments.",
      "authors": [
        "Yizhe Huang",
        "Anji Liu",
        "Fanqi Kong",
        "Yaodong Yang",
        "Song-Chun Zhu",
        "Xue Feng"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=disVlUOH4b",
      "cdate": 1706874352076,
      "mdate": 1719287296428,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525315"
    },
    {
      "id": "LbcNAIgNnB",
      "title": "How to Explore with Belief: State Entropy Maximization in POMDPs",
      "abstract": "Recent works have studied *state entropy maximization* in reinforcement learning, in which the agent's objective is to learn a policy inducing high entropy over states visitation (Hazan et al., 2019). They typically assume full observability of the state of the system, so that the entropy of the observations is maximized. In practice, the agent may only get *partial* observations, e.g., a robot perceiving the state of a physical space through proximity sensors and cameras. A significant mismatch between the entropy over observations and true states of the system can arise in those settings. In this paper, we address the problem of entropy maximization over the *true states* with a decision policy conditioned on partial observations *only*. The latter is a generalization of POMDPs, which is intractable in general. We develop a memory and computationally efficient *policy gradient* method to address a first-order relaxation of the objective defined on *belief* states, providing various formal characterizations of approximation gaps, the optimization landscape, and the *hallucination* problem. This paper aims to generalize state entropy maximization to more realistic domains that meet the challenges of applications.",
      "authors": [
        "Riccardo Zamboni",
        "Duilio Cirino",
        "Marcello Restelli",
        "Mirco Mutti"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LbcNAIgNnB",
      "cdate": 1706874345844,
      "mdate": 1719287296410,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525321"
    },
    {
      "id": "bfQCO9Vqhk",
      "title": "Submodular framework for structured-sparse optimal transport",
      "abstract": "Unbalanced optimal transport (UOT) has recently gained much attention due to its flexible framework for handling un-normalized measures and its robustness properties. In this work, we explore learning (structured) sparse transport plans in the UOT setting, i.e., transport plans have an upper bound on the number of non-sparse entries in each column (structured sparse pattern) or in the whole plan (general sparse pattern). We propose novel sparsity-constrained UOT formulations building on the recently explored maximum mean discrepancy based UOT. We show that the proposed optimization problem is equivalent to the maximization of a weakly submodular function over a uniform matroid or a partition matroid. We develop efficient gradient-based discrete greedy algorithms and provide the corresponding theoretical guarantees. Empirically, we observe that our proposed greedy algorithms select a diverse support set and we illustrate the efficacy of the proposed approach in various applications.",
      "authors": [
        "Piyushi Manupriya",
        "Pratik Jawanpuria",
        "Karthik S. Gurumoorthy",
        "SakethaNath Jagarlapudi",
        "Bamdev Mishra"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=bfQCO9Vqhk",
      "cdate": 1706874326644,
      "mdate": 1719287296389,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525327"
    },
    {
      "id": "59MYoLghyk",
      "title": "Breadth-First Exploration on Adaptive Grid for Reinforcement Learning",
      "abstract": "Graph-based planners have gained significant attention for goal-conditioned reinforcement learning (RL), where they construct a graph consisting of confident transitions between *subgoals* as edges and run shortest path algorithms to exploit the confident edges. Meanwhile, identifying and avoiding unattainable transitions are also crucial yet overlooked by the previous graph-based planners, leading to wasting an excessive number of attempts at unattainable subgoals. To address this oversight, we propose a graph construction method that efficiently manages all the achieved and unattained subgoals on a grid graph adaptively discretizing the goal space. This enables a breadth-first exploration strategy, grounded in the local adaptive grid refinement, that prioritizes broad probing of subgoals on a coarse grid over meticulous one on a dense grid. We conducted a theoretical analysis and demonstrated the effectiveness of our approach through empirical evidence, showing that only BEAG succeeds in complex environments under the proposed fixed-goal setting.",
      "authors": [
        "Youngsik Yoon",
        "Gangbok Lee",
        "Sungsoo Ahn",
        "Jungseul Ok"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=59MYoLghyk",
      "cdate": 1706874315258,
      "mdate": 1719287296323,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525334"
    },
    {
      "id": "pOgMluzEIH",
      "title": "SILVER: Single-loop variance reduction and application to federated learning",
      "abstract": "Most variance reduction methods require multiple times of full gradient computation, which is time-consuming and hence a bottleneck in application to distributed optimization. We present a single-loop variance-reduced gradient estimator named SILVER (SIngle-Loop VariancE-Reduction) for the finite-sum non-convex optimization, which does not require multiple full gradients but nevertheless achieves the optimal gradient complexity. Notably, unlike existing methods, SILVER provably reaches second-order optimality, with exponential convergence in the Polyak-Łojasiewicz (PL) region, and achieves further speedup depending on the data heterogeneity. Owing to these advantages, SILVER serves as a new base method to design communication-efficient federated learning algorithms: we combine SILVER with local updates which gives the best communication rounds and number of communicated gradients across all range of Hessian heterogeneity, and, at the same time, guarantees second-order optimality and exponential convergence in the PL region.",
      "authors": [
        "Kazusato Oko",
        "Shunta Akiyama",
        "Denny Wu",
        "Tomoya Murata",
        "Taiji Suzuki"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pOgMluzEIH",
      "cdate": 1706874297759,
      "mdate": 1719287296313,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525341"
    },
    {
      "id": "EWt5wsEdvc",
      "title": "Cell2Sentence: Teaching Large Language Models the Language of Biology",
      "abstract": "We introduce Cell2Sentence (C2S), a novel method to directly adapt large language models to a biological context, specifically single-cell transcriptomics. By transforming gene expression data into \"cell sentences,\" C2S bridges the gap between natural language processing and biology. We demonstrate cell sentences enable the fine-tuning of language models for diverse tasks in biology, including cell generation, complex cell-type annotation, and direct data-driven text generation. Our experiments reveal that GPT-2, when fine-tuned with C2S, can generate biologically valid cells based on cell type inputs, and accurately predict cell types from cell sentences. This illustrates that language models, through C2S fine-tuning, can acquire a significant understanding of single-cell biology while maintaining robust text generation capabilities. C2S offers a flexible, accessible framework to integrate natural language processing with transcriptomics, utilizing existing models and libraries for a wide range of biological applications.",
      "authors": [
        "Daniel Levine",
        "Syed A Rizvi",
        "Sacha Lévy",
        "Nazreen Pallikkavaliyaveetil",
        "David Zhang",
        "Xingyu Chen",
        "Sina Ghadermarzi",
        "Ruiming Wu",
        "Zihe Zheng",
        "Ivan Vrkic",
        "Anna Zhong",
        "Daphne Raskin",
        "Insu Han",
        "Antonio Henrique de Oliveira Fonseca",
        "Josue Ortega Caro",
        "Amin Karbasi",
        "Rahul Madhav Dhodapkar",
        "David van Dijk"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EWt5wsEdvc",
      "cdate": 1706874292596,
      "mdate": 1719287296242,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525347"
    },
    {
      "id": "Wp054bnPq9",
      "title": "Watermark Stealing in Large Language Models",
      "abstract": "LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying *watermark stealing* (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical *spoofing attacks*, as hypothesized in prior work, but also greatly boosts *scrubbing* attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80\\%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We make all our code and additional examples available at https://watermark-stealing.org.",
      "authors": [
        "Nikola Jovanović",
        "Robin Staab",
        "Martin Vechev"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Wp054bnPq9",
      "cdate": 1706874270376,
      "mdate": 1719287296204,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525353"
    },
    {
      "id": "dVpFKfqF3R",
      "title": "Stop Regressing: Training Value Functions via Classification for Scalable Deep RL",
      "abstract": "Value functions are an essential component in deep reinforcement learning (RL), that are typically trained via mean squared error regression to match bootstrapped target values. However, scaling value-based RL methods to large networks has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We show that training value functions with categorical cross-entropy significantly enhances performance and scalability across various domains, including single-task RL on Atari 2600 games, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving *state-of-the-art results* on these domains. Through careful analysis, we show that categorical cross-entropy mitigates issues inherent to value-based RL, such as noisy targets and non-stationarity. We argue that shifting to categorical cross-entropy for training value functions can substantially improve the scalability of deep RL at little-to-no cost.",
      "authors": [
        "Jesse Farebrother",
        "Jordi Orbay",
        "Quan Vuong",
        "Adrien Ali Taiga",
        "Yevgen Chebotar",
        "Ted Xiao",
        "Alex Irpan",
        "Sergey Levine",
        "Pablo Samuel Castro",
        "Aleksandra Faust",
        "Aviral Kumar",
        "Rishabh Agarwal"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=dVpFKfqF3R",
      "cdate": 1706874259638,
      "mdate": 1719287296173,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525360"
    },
    {
      "id": "rTBR0eqE4G",
      "title": "Decomposing and Editing Predictions by Modeling Model Computation",
      "abstract": "*How does the internal computation of a machine learning model transform inputs into predictions?* To tackle this question, we introduce a framework called *component modeling* for decomposing a model prediction in terms of its components---architectural \"building blocks\" such as convolution filters or attention heads. We focus on a special case of this framework, *component attribution*, where the goal is to estimate the counterfactual impact of individual components on a given prediction. We then present COAR, a scalable algorithm for estimating component attributions, and demonstrate its effectiveness across models, datasets and modalities. Finally, we show that COAR directly enables effective model editing. Our code is available at [github.com/MadryLab/modelcomponents]([https://github.com/MadryLab/modelcomponents]).",
      "authors": [
        "Harshay Shah",
        "Andrew Ilyas",
        "Aleksander Madry"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rTBR0eqE4G",
      "cdate": 1706874180521,
      "mdate": 1719287296076,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525366"
    },
    {
      "id": "dMhF96PfQi",
      "title": "Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced Optimal Transport",
      "abstract": "Wasserstein gradient flow (WGF) describes the gradient dynamics of probability density within the Wasserstein space. WGF provides a promising approach for conducting optimization over the probability distributions. Numerically approximating the continuous WGF requires the time discretization method. The most well-known method for this is the JKO scheme. In this regard, previous WGF models employ the JKO scheme and parametrized transport map for each JKO step. However, this approach results in quadratic training complexity $O(K^2)$ with the number of JKO step $K$. This severely limits the scalability of WGF models. In this paper, we introduce a scalable WGF-based generative model, called Semi-dual JKO (S-JKO). Our model is based on the semi-dual form of the JKO step, derived from the equivalence between the JKO step and the Unbalanced Optimal Transport. Our approach reduces the training complexity to $O(K)$. We demonstrate that our model significantly outperforms existing WGF-based generative models, achieving FID scores of 2.62 on CIFAR-10 and 6.42 on CelebA-HQ-256, which are comparable to state-of-the-art image generative models.",
      "authors": [
        "Jaemoo Choi",
        "Jaewoong Choi",
        "Myungjoo Kang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=dMhF96PfQi",
      "cdate": 1706874179529,
      "mdate": 1719287296064,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525373"
    },
    {
      "id": "pXaEYzrFae",
      "title": "Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation",
      "abstract": "To ensure that text generated by large language models (LLMs) is in an expected format, constrained decoding methods propose to enforce strict formal language constraints during generation. However, as we show in this work, not only do such methods often incur performance overhead during generation, but many of them also significantly impair task accuracy, if they do not correctly align the underlying LLM sub-word vocabularies with external constraints. To address this, we present a novel decoding algorithm, DOMINO, that can enforce constraints in a fully subword-aligned fashion, while leveraging pre-computation and speculative decoding to achieve virtually no overhead and in some cases even almost 2$\\times$ speedup over unconstrained decoding -- thereby outperforming existing approaches by a wide margin. We release DOMINO as open source at https://github.com/eth-sri/domino.",
      "authors": [
        "Luca Beurer-Kellner",
        "Marc Fischer",
        "Martin Vechev"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pXaEYzrFae",
      "cdate": 1706874144764,
      "mdate": 1719287296044,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525380"
    },
    {
      "id": "pwfcwEqdUz",
      "title": "Latent Logic Tree Extraction for Event Sequence Explanation from LLMs",
      "abstract": "Modern high-stakes systems, such as healthcare or robotics, often generate vast streaming event sequences. Our goal is to design an efficient, plug-and-play tool to elicit logic tree-based explanations from Large Language Models (LLMs) to provide customized insights into each observed event sequence. Built on the temporal point process model for events, our method employs the likelihood function as a score to evaluate generated logic trees. We propose an amortized Expectation-Maximization (EM) learning framework and treat the logic tree as latent variables. In the E-step, we evaluate the posterior distribution over the latent logic trees using an LLM prior and the likelihood of the observed event sequences. LLM provides a high-quality prior for the latent logic trees, however, since the posterior is built over a discrete combinatorial space, we cannot get the closed-form solution. We propose to generate logic tree samples from the posterior using a learnable GFlowNet, which is a diversity-seeking generator for structured discrete variables. The M-step employs the generated logic rules to approximate marginalization over the posterior, facilitating the learning of model parameters and refining the tunable LLM prior parameters. In the online setting, our locally built, lightweight model will iteratively extract the most relevant rules from LLMs for each sequence using only a few iterations. Empirical demonstrations showcase the promising performance and adaptability of our framework.",
      "authors": [
        "Zitao Song",
        "Chao Yang",
        "Chaojie Wang",
        "Bo An",
        "Shuang Li"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pwfcwEqdUz",
      "cdate": 1706874047853,
      "mdate": 1719287295894,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525386"
    },
    {
      "id": "1tRLxQzdep",
      "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
      "abstract": "Despite the remarkable capabilities, Large Language Models (LLMs) face deployment challenges due to their extensive size. Pruning methods drop a subset of weights to accelerate, but many of them require retraining, which is prohibitively expensive and computationally demanding. Recently, post-training pruning approaches introduced novel metrics, enabling the pruning of LLMs without retraining. However, these metrics require the involvement of human experts and tedious trial and error. To efficiently identify superior pruning metrics, we develop an automatic framework for searching symbolic pruning metrics using genetic programming. In particular, we devise an elaborate search space encompassing the existing pruning metrics to discover the potential symbolic pruning metric. We propose an opposing operation simplification strategy to increase the diversity of the population. In this way, Pruner-Zero allows auto-generation of symbolic pruning metrics. Based on the searched results, we explore the correlation between pruning metrics and performance after pruning and summarize some principles. Extensive experiments on LLaMA and LLaMA-2 on language modeling and zero-shot tasks demonstrate that our Pruner-Zero obtains superior performance than SOTA post-training pruning methods. Code at: https://github.com/pprp/Pruner-Zero.",
      "authors": [
        "Peijie Dong",
        "Lujun Li",
        "Zhenheng Tang",
        "Xiang Liu",
        "Xinglin Pan",
        "Qiang Wang",
        "Xiaowen Chu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=1tRLxQzdep",
      "cdate": 1706874032449,
      "mdate": 1719287295833,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525392"
    },
    {
      "id": "qAml3FpfhG",
      "title": "tinyBenchmarks: evaluating LLMs with fewer examples",
      "abstract": "The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models’ abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.",
      "authors": [
        "Felipe Maia Polo",
        "Lucas Weber",
        "Leshem Choshen",
        "Yuekai Sun",
        "Gongjun Xu",
        "Mikhail Yurochkin"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qAml3FpfhG",
      "cdate": 1706874023041,
      "mdate": 1719287295812,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525397"
    },
    {
      "id": "eJFQROkaj0",
      "title": "RoboMP$^2$: A Robotic Multimodal Perception-Planning Framework with Multimodal Large Language Models",
      "abstract": "Multimodal Large Language Models (MLLMs) have shown impressive reasoning abilities and general intelligence in various domains. It inspires researchers to train end-to-end MLLMs or utilize large models to generate policies with human-selected prompts for embodied agents. However, these methods exhibit limited generalization capabilities on unseen tasks or scenarios, and overlook the multimodal environment information which is critical for robots to make decisions. In this paper, we introduce a novel **Robo**tic **M**ultimodal **P**erception-**P**lanning (**RoboMP$^2$**) framework for robotic manipulation which consists of a Goal-Conditioned Multimodal Preceptor (GCMP) and a Retrieval-Augmented Multimodal Planner (RAMP). Specially, GCMP captures environment states by employing a tailored MLLMs for embodied agents with the abilities of semantic reasoning and localization. RAMP utilizes coarse-to-fine retrieval method to find the $k$ most-relevant policies as in-context demonstrations to enhance the planner. Extensive experiments demonstrate the superiority of RoboMP$^2$ on both VIMA benchmark and real-world tasks, with around 10% improvement over the baselines.",
      "authors": [
        "Qi Lv",
        "Hao Li",
        "Xiang Deng",
        "Rui Shao",
        "Michael Y Wang",
        "Liqiang Nie"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=eJFQROkaj0",
      "cdate": 1706873995713,
      "mdate": 1719287295740,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525403"
    },
    {
      "id": "1RZKuvqYCR",
      "title": "Token-level Direct Preference Optimization",
      "abstract": "Fine-tuning pre-trained Large Language Models (LLMs) is essential to align them with human values and intentions. This process often utilizes methods like pairwise comparisons and KL divergence against a reference LLM, focusing on the evaluation of full answers generated by the models. However, the generation of these responses occurs in a token level, following a sequential, auto-regressive fashion. In this paper, we introduce Token-level Direct Preference Optimization (TDPO), a novel approach to align LLMs with human preferences by optimizing policy at the token level. Unlike previous methods, which face challenges in divergence efficiency, TDPO integrates forward KL divergence constraints for each token, improving alignment and diversity. Utilizing the Bradley-Terry model for a token-based reward system, our method enhances the regulation of KL divergence, while preserving simplicity without the need for explicit reward modeling. Experimental results across various text tasks demonstrate TDPO’s superior performance in balancing alignment with generation diversity. Notably, fine-tuning with TDPO strikes a better balance than DPO in the controlled sentiment generation and single-turn dialogue datasets, and significantly improves the quality of generated responses compared to both DPO and PPO-based RLHF methods.",
      "authors": [
        "Yongcheng Zeng",
        "Guoqing Liu",
        "Weiyu Ma",
        "Ning Yang",
        "Haifeng Zhang",
        "Jun Wang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=1RZKuvqYCR",
      "cdate": 1706873966000,
      "mdate": 1719287295690,
      "matched_keywords": [
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525409"
    },
    {
      "id": "FOJE1kRcHG",
      "title": "Mean Field Langevin Actor-Critic: Faster Convergence and Global Optimality beyond Lazy Learning",
      "abstract": "This work explores the feature learning capabilities of deep reinforcement learning algorithms in the pursuit of optimal policy determination. We particularly examine an over-parameterized neural actor-critic framework within the mean-field regime, where both actor and critic components undergo updates via policy gradient and temporal-difference (TD) learning, respectively. We introduce the *mean-field Langevin TD learning* (MFLTD) method, enhancing mean-field Langevin dynamics with proximal TD updates for critic policy evaluation, and assess its performance against conventional approaches through numerical analysis. Additionally, for actor policy updates, we present the *mean-field Langevin policy gradient* (MFLPG), employing policy gradient techniques augmented by Wasserstein gradient flows for parameter space exploration. Our findings demonstrate that MFLTD accurately identifies the true value function, while MFLPG ensures linear convergence of actor sequences towards the globally optimal policy, considering a Kullback-Leibler divergence regularized framework. Through both time particle and discretized analysis, we substantiate the linear convergence guarantees of our neural actor-critic algorithms, representing a notable contribution to neural reinforcement learning focusing on *global optimality* and *feature learning*, extending the existing understanding beyond the conventional scope of lazy training.",
      "authors": [
        "Kakei Yamamoto",
        "Kazusato Oko",
        "Zhuoran Yang",
        "Taiji Suzuki"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FOJE1kRcHG",
      "cdate": 1706873957538,
      "mdate": 1719287295653,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525415"
    },
    {
      "id": "CbbTF6tDhW",
      "title": "Improving Robustness to Multiple Spurious Correlations by Multi-Objective Optimization",
      "abstract": "We study the problem of training an unbiased and accurate model given a dataset with multiple biases. This problem is challenging since the multiple biases cause multiple undesirable shortcuts during training, and even worse, mitigating one may exacerbate the other. We propose a novel training method to tackle this challenge. Our method first groups training data so that different groups induce different shortcuts, and then optimizes a linear combination of group-wise losses while adjusting their weights dynamically to alleviate conflicts between the groups in performance; this approach, rooted in the multi-objective optimization theory, encourages to achieve the minimax Pareto solution. We also present a new benchmark with multiple biases, dubbed MultiCelebA, for evaluating debiased training methods under realistic and challenging scenarios. Our method achieved the best on three datasets with multiple biases, and also showed superior performance on conventional single-bias datasets.",
      "authors": [
        "Nayeong Kim",
        "Juwon Kang",
        "Sungsoo Ahn",
        "Jungseul Ok",
        "Suha Kwak"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=CbbTF6tDhW",
      "cdate": 1706873873676,
      "mdate": 1719287295568,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525421"
    },
    {
      "id": "l6Hef6FVd0",
      "title": "PIPER: Primitive-Informed Preference-based Hierarchical Reinforcement Learning via Hindsight Relabeling",
      "abstract": "In this work, we introduce PIPER: Primitive-Informed Preference-based Hierarchical reinforcement learning via Hindsight Relabeling, a novel approach that leverages preference-based learning to learn a reward model, and subsequently uses this reward model to relabel higher-level replay buffers. Since this reward is unaffected by lower primitive behavior, our relabeling-based approach is able to mitigate non-stationarity, which is common in existing hierarchical approaches, and demonstrates impressive performance across a range of challenging sparse-reward tasks. Since obtaining human feedback is typically impractical, we propose to replace the human-in-the-loop approach with our primitive-in-the-loop approach, which generates feedback using sparse rewards provided by the environment. Moreover, in order to prevent infeasible subgoal prediction and avoid degenerate solutions, we propose primitive-informed regularization that conditions higher-level policies to generate feasible subgoals for lower-level policies. We perform extensive experiments to show that PIPER mitigates non-stationarity in hierarchical reinforcement learning and achieves greater than 50$\\\\%$ success rates in challenging, sparse-reward robotic environments, where most other baselines fail to achieve any significant progress.",
      "authors": [
        "Utsav Singh",
        "Wesley A Suttle",
        "Brian M. Sadler",
        "Vinay P. Namboodiri",
        "Amrit Bedi"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=l6Hef6FVd0",
      "cdate": 1706873813718,
      "mdate": 1719287295456,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525427"
    },
    {
      "id": "m4dO5L6eCp",
      "title": "Smooth Tchebycheff Scalarization for Multi-Objective Optimization",
      "abstract": "Multi-objective optimization problems can be found in many real-world applications, where the objectives often conflict each other and cannot be optimized by a single solution. In the past few decades, numerous methods have been proposed to find Pareto solutions that represent optimal trade-offs among the objectives for a given problem. However, these existing methods could have high computational complexity or may not have good theoretical properties for solving a general differentiable multi-objective optimization problem. In this work, by leveraging the smooth optimization technique, we propose a lightweight and efficient smooth Tchebycheff scalarization approach for gradient-based multi-objective optimization. It has good theoretical properties for finding all Pareto solutions with valid trade-off preferences, while enjoying significantly lower computational complexity compared to other methods. Experimental results on various real-world application problems fully demonstrate the effectiveness of our proposed method.",
      "authors": [
        "Xi Lin",
        "Xiaoyuan Zhang",
        "Zhiyuan Yang",
        "Fei Liu",
        "Zhenkun Wang",
        "Qingfu Zhang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=m4dO5L6eCp",
      "cdate": 1706873770370,
      "mdate": 1719287295335,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525434"
    },
    {
      "id": "tu5fCCuua2",
      "title": "DNCs Require More Planning Steps",
      "abstract": "Many recent works use machine learning models to solve various complex algorithmic problems. However, these models attempt to reach a solution without considering the problem's required computational complexity, which can be detrimental to their ability to solve it correctly. In this work we investigate the effect of computational time and memory on generalization of implicit algorithmic solvers. To do so, we focus on the Differentiable Neural Computer (DNC), a general problem solver that also lets us reason directly about its usage of time and memory. In this work, we argue that the number of planning steps the model is allowed to take, which we call ”planning budget”, is a constraint that can cause the model to generalize poorly and hurt its ability to fully utilize its external memory. We evaluate our method on Graph Shortest Path, Convex Hull, Graph MinCut and Associative Recall, and show how the planning budget can drastically change the behavior of the learned algorithm, in terms of learned time complexity, training time, stability and generalization to inputs larger than those seen during training.",
      "authors": [
        "Yara Shamshoum",
        "Nitzan Hodos",
        "Yuval Sieradzki",
        "Assaf Schuster"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tu5fCCuua2",
      "cdate": 1706873767168,
      "mdate": 1719287295312,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525440"
    },
    {
      "id": "3AuoStfUIH",
      "title": "Offline Multi-Objective Optimization",
      "abstract": "Offline optimization aims to maximize a black-box objective function with a static dataset and has wide applications. In addition to the objective function being black-box and expensive to evaluate, numerous complex real-world problems entail optimizing multiple conflicting objectives, i.e., multi-objective optimization (MOO). Nevertheless, offline MOO has not progressed as much as offline single-objective optimization (SOO), mainly due to the lack of benchmarks like Design-Bench for SOO. To bridge this gap, we propose a first benchmark for offline MOO, covering a range of problems from synthetic to real-world tasks. This benchmark provides tasks, datasets, and open-source examples, which can serve as a foundation for method comparisons and advancements in offline MOO. Furthermore, we analyze how the current related methods can be adapted to offline MOO from four fundamental perspectives, including data, model architecture, learning algorithm, and search algorithm. Empirical results show improvements over the best value of the training set, demonstrating the effectiveness of offline MOO methods. As no particular method stands out significantly, there is still an open challenge in further enhancing the effectiveness of offline MOO. We finally discuss future challenges for offline MOO, with the hope of shedding some light on this emerging field. Our code is available at https://github.com/lamda-bbo/offline-moo.",
      "authors": [
        "Ke Xue",
        "Rongxi Tan",
        "Xiaobin Huang",
        "Chao Qian"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3AuoStfUIH",
      "cdate": 1706873717418,
      "mdate": 1719287295260,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525446"
    },
    {
      "id": "Yug1IEkvcb",
      "title": "Model-Free Robust $\\phi$-Divergence Reinforcement Learning Using Both Offline and Online Data",
      "abstract": "The robust $\\phi$-regularized Markov Decision Process (RRMDP) framework focuses on designing control policies that are robust against parameter uncertainties due to mismatches between the simulator (nominal) model and real-world settings. This work makes *two* important contributions. First, we propose a *model-free* algorithm called *Robust $\\phi$-regularized fitted Q-iteration* for learning an $\\epsilon$-optimal robust policy that uses only the historical data collected by rolling out a behavior policy (with *robust exploratory* requirement) on the nominal model. To the best of our knowledge, we provide the *first* unified analysis for a class of $\\phi$-divergences achieving robust optimal policies in high-dimensional systems of arbitrary large state space with general function approximation. Second, we introduce the *hybrid robust $\\phi$-regularized reinforcement learning* framework to learn an optimal robust policy using both historical data and online sampling. Towards this framework, we propose a model-free algorithm called *Hybrid robust Total-variation-regularized Q-iteration*. To the best of our knowledge, we provide the *first* improved out-of-data-distribution assumption in large-scale problems of arbitrary large state space with general function approximation under the hybrid robust $\\phi$-regularized reinforcement learning framework.",
      "authors": [
        "Kishan Panaganti",
        "Adam Wierman",
        "Eric Mazumdar"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Yug1IEkvcb",
      "cdate": 1706873627474,
      "mdate": 1719287295227,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525452"
    },
    {
      "id": "uQiFsBil3p",
      "title": "Random matrix theory improved Fréchet mean of symmetric positive definite matrices",
      "abstract": "In this study, we consider the realm of covariance matrices in machine learning, particularly focusing on computing Fréchet means on the manifold of symmetric positive definite matrices, commonly referred to as Karcher or geometric means. Such means are leveraged in numerous machine learning tasks. Relying on advanced statistical tools, we introduce a random matrix theory based method that estimates Fréchet means, which is particularly beneficial when dealing with low sample support and a high number of matrices to average. Our experimental evaluation, involving both synthetic and real-world EEG and hyperspectral datasets, shows that we largely outperform state-of-the-art methods.",
      "authors": [
        "Florent Bouchard",
        "Ammar Mian",
        "Malik Tiomoko",
        "Guillaume Ginolhac",
        "Frederic Pascal"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=uQiFsBil3p",
      "cdate": 1706873596589,
      "mdate": 1719287295198,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525458"
    },
    {
      "id": "ngcZhfXCBW",
      "title": "RLVF: Learning from Verbal Feedback without Overgeneralization",
      "abstract": "The diversity of contexts in which large language models (LLMs) are deployed requires the ability to modify or customize default model behaviors to incorporate nuanced requirements and preferences. A convenient interface to specify such model adjustments is high-level verbal feedback, such as “Don’t use emojis when drafting emails to my boss.” However, while writing high-level feedback is far simpler than collecting annotations for reinforcement learning from human feedback (RLHF), we find that simply prompting a model with such feedback leads to $\\textbf{overgeneralization}$–applying feedback in contexts where it is not relevant. We propose a new method Contextualized Critiques with Constrained Preference Optimization (C3PO) to learn from high-level verbal feedback while reducing overgeneralization compared to current work. C3PO uses a piece of high-level feedback to generate a small synthetic preference dataset to specify when and how the feedback should (and should not) be applied. It then fine-tunes the model in accordance with the synthetic preference data while minimizing the divergence from the original model for prompts where the feedback does not apply. Our experimental results indicate that our approach effectively applies verbal feedback to relevant scenarios while preserving existing behaviors for other contexts more than current methods. For both human- and GPT-4-generated high-level feedback, C3PO effectively adheres to the given feedback comparably to in-context baselines while reducing overgeneralization by 30%.",
      "authors": [
        "Moritz Pascal Stephan",
        "Alexander Khazatsky",
        "Eric Mitchell",
        "Annie S Chen",
        "Sheryl Hsu",
        "Archit Sharma",
        "Chelsea Finn"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ngcZhfXCBW",
      "cdate": 1706873543268,
      "mdate": 1719287295083,
      "matched_keywords": [
        "reinforcement learning",
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525464"
    },
    {
      "id": "jdRIaUu3xY",
      "title": "BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models",
      "abstract": "Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive experiments demonstrate BBox-Adapter's effectiveness and cost efficiency. It improves model performance by up to 6.77% across diverse tasks and domains, while reducing training and inference costs by 31.30x and 1.84x, respectively.",
      "authors": [
        "Haotian Sun",
        "Yuchen Zhuang",
        "Wei Wei",
        "Chao Zhang",
        "Bo Dai"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jdRIaUu3xY",
      "cdate": 1706873537641,
      "mdate": 1719287295062,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525470"
    },
    {
      "id": "QAGRPiC3FS",
      "title": "RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content",
      "abstract": "Recent advancements in Large Language Models (LLMs) have showcased remarkable capabilities across various tasks in different domains. However, the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges. Current mitigation strategies, while effective, are not resilient under adversarial attacks. This paper introduces Resilient Guardrails for Large Language Models (RigorLLM), a novel framework designed to efficiently and effectively moderate harmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted approach that includes energy-based training data augmentation through Langevin dynamics, optimizing a safe suffix for inputs via minimax optimization, and integrating a fusion-based model combining robust KNN with LLMs based on our data augmentation, RigorLLM offers a robust solution to harmful content moderation. Our experimental evaluations demonstrate that RigorLLM not only outperforms existing baselines like OpenAI API and Perspective API in detecting harmful content but also exhibits unparalleled resilience to jailbreaking attacks. The innovative use of constrained optimization and a fusion-based guardrail approach represents a significant step forward in developing more secure and reliable LLMs, setting a new standard for content moderation frameworks in the face of evolving digital threats.",
      "authors": [
        "Zhuowen Yuan",
        "Zidi Xiong",
        "Yi Zeng",
        "Ning Yu",
        "Ruoxi Jia",
        "Dawn Song",
        "Bo Li"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=QAGRPiC3FS",
      "cdate": 1706873536974,
      "mdate": 1719287295043,
      "matched_keywords": [
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525476"
    },
    {
      "id": "pVyOchWUBa",
      "title": "Position: Understanding LLMs Requires More Than Statistical Generalization",
      "abstract": "The last decade has seen blossoming research in deep learning theory attempting to answer, ``Why does deep learning generalize?\" A powerful shift in perspective precipitated this progress: the study of overparametrized models in the interpolation regime. In this paper, we argue that another perspective shift is due, since some of the desirable qualities of LLMs are not a consequence of good statistical generalization and require a separate theoretical explanation. Our core argument relies on the observation that AR probabilistic models are inherently non-identifiable: models zero or near-zero KL divergence apart---thus, equivalent test loss---can exhibit markedly different behaviors. We support our position with mathematical examples and empirical observations, illustrating why non-identifiability has practical relevance through three case studies: (1) the non-identifiability of zero-shot rule extrapolation; (2) the approximate non-identifiability of in-context learning; and (3) the non-identifiability of fine-tunability. We review promising research directions focusing on LLM-relevant generalization measures, transferability, and inductive biases.",
      "authors": [
        "Patrik Reizinger",
        "Szilvia Ujváry",
        "Anna Mészáros",
        "Anna Kerekes",
        "Wieland Brendel",
        "Ferenc Huszár"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pVyOchWUBa",
      "cdate": 1706873514802,
      "mdate": 1719287294997,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525483"
    },
    {
      "id": "64I29YeQdt",
      "title": "Quality-Diversity with Limited Resources",
      "abstract": "Quality-Diversity (QD) algorithms have emerged as a powerful optimization paradigm with the aim of generating a set of high-quality and diverse solutions. To achieve such a challenging goal, QD algorithms require maintaining a large archive and a large population in each iteration, which brings two main issues, sample and resource efficiency. Most advanced QD algorithms focus on improving the sample efficiency, while the resource efficiency is overlooked to some extent. Particularly, the resource overhead during the training process has not been touched yet, hindering the wider application of QD algorithms. In this paper, we highlight this important research question, i.e., how to efficiently train QD algorithms with limited resources, and propose a novel and effective method called RefQD to address it. RefQD decomposes a neural network into representation and decision parts, and shares the representation part with all decision parts in the archive to reduce the resource overhead. It also employs a series of strategies to address the mismatch issue between the old decision parts and the newly updated representation part. Experiments on different types of tasks from small to large resource consumption demonstrate the excellent performance of RefQD: it not only uses significantly fewer resources (e.g., 16% GPU memories on QDax and 3.7% on Atari) but also achieves comparable or better performance compared to sample-efficient QD algorithms. Our code is available at [https://github.com/lamda-bbo/RefQD](https://github.com/lamda-bbo/RefQD).",
      "authors": [
        "Ren-Jian Wang",
        "Ke Xue",
        "Cong Guan",
        "Chao Qian"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=64I29YeQdt",
      "cdate": 1706873487080,
      "mdate": 1719287294963,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525489"
    },
    {
      "id": "Gp5F6qzwGK",
      "title": "Iterative Regularized Policy Optimization with Imperfect Demonstrations",
      "abstract": "Imitation learning heavily relies on the quality of provided demonstrations. In scenarios where demonstrations are imperfect and rare, a prevalent approach for refining policies is through online fine-tuning with reinforcement learning, in which a Kullback–Leibler (KL) regularization is often employed to stabilize the learning process. However, our investigation reveals that on the one hand, imperfect demonstrations can bias the online learning process, the KL regularization will further constrain the improvement of online policy exploration. To address the above issues, we propose Iterative Regularized Policy Optimization (IRPO), a framework that involves iterative offline imitation learning and online reinforcement exploration. Specifically, the policy learned online is used to serve as the demonstrator for successive learning iterations, with a demonstration boosting to consistently enhance the quality of demonstrations. Experimental validations conducted across widely used benchmarks and a novel fixed-wing UAV control task consistently demonstrate the effectiveness of IRPO in improving both the demonstration quality and the policy performance. Our code is available at https://github.com/GongXudong/IRPO.",
      "authors": [
        "Gong Xudong",
        "Feng Dawei",
        "Kele Xu",
        "Yuanzhao Zhai",
        "Chengkang Yao",
        "Weijia Wang",
        "Bo Ding",
        "Huaimin Wang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Gp5F6qzwGK",
      "cdate": 1706873363772,
      "mdate": 1719840719575,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525495"
    },
    {
      "id": "B1W712hMBi",
      "title": "NExT: Teaching Large Language Models to Reason about Code Execution",
      "abstract": "A fundamental skill among human developers is the ability to understand and reason about program execution. As an example, a programmer can mentally simulate code execution in natural language to debug and repair code (aka. rubber duck debugging). However, large language models (LLMs) of code are typically trained on the surface textual form of programs, thus may lack a semantic understanding of how programs execute at run-time. To address this issue, we propose NExT, a method to teach LLMs to inspect the execution traces of programs (variable states of executed lines) and reason about their run-time behavior through chain-of-thought (CoT) rationales. Specifically, NExT uses self-training to bootstrap a synthetic training set of execution-aware rationales that lead to correct task solutions (e.g., fixed programs) without laborious manual annotation. Experiments on program repair tasks based on MBPP and HumanEval demonstrate that NExT improves the fix rate of a PaLM 2 model, by 26.1% and 10.3% absolute, respectively, with significantly improved rationale quality as verified by automated metrics and human raters. Our model can also generalize to scenarios where program traces are absent at test-time.",
      "authors": [
        "Ansong Ni",
        "Miltiadis Allamanis",
        "Arman Cohan",
        "Yinlin Deng",
        "Kensen Shi",
        "Charles Sutton",
        "Pengcheng Yin"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=B1W712hMBi",
      "cdate": 1706873323955,
      "mdate": 1719287294878,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525501"
    },
    {
      "id": "37xFIeYgE0",
      "title": "Explaining Probabilistic Models with Distributional Values",
      "abstract": "A large branch of explainable machine learning is grounded in cooperative game theory. However, research indicates that game-theoretic explanations may mislead or be hard to interpret. We argue that often there is a critical mismatch between what one wishes to explain (e.g. the output of a classifier) and what current methods such as SHAP explain (e.g. the scalar probability of a class). This paper addresses such gap for probabilistic models by generalising cooperative games and value operators. We introduce the *distributional values*, random variables that track changes in the model output (e.g. flipping of the predicted class) and derive their analytic expressions for games with Gaussian, Bernoulli and Categorical payoffs. We further establish several characterising properties, and show that our framework provides fine-grained and insightful explanations with case studies on vision and language models.",
      "authors": [
        "Luca Franceschi",
        "Michele Donini",
        "Cedric Archambeau",
        "Matthias Seeger"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=37xFIeYgE0",
      "cdate": 1706873230162,
      "mdate": 1719287294775,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525508"
    },
    {
      "id": "nDps3Q8j2l",
      "title": "Fourier Controller Networks for Real-Time Decision-Making in Embodied Learning",
      "abstract": "Transformer has shown promise in reinforcement learning to model time-varying features for obtaining generalized low-level robot policies on diverse robotics datasets in embodied learning. However, it still suffers from the issues of low data efficiency and high inference latency. In this paper, we propose to investigate the task from a new perspective of the frequency domain. We first observe that the energy density in the frequency domain of a robot's trajectory is mainly concentrated in the low-frequency part. Then, we present the Fourier Controller Network (FCNet), a new network that uses Short-Time Fourier Transform (STFT) to extract and encode time-varying features through frequency domain interpolation. In order to do real-time decision-making, we further adopt FFT and Sliding DFT methods in the model architecture to achieve parallel training and efficient recurrent inference. Extensive results in both simulated (e.g., D4RL) and real-world environments (e.g., robot locomotion) demonstrate FCNet's substantial efficiency and effectiveness over existing methods such as Transformer, e.g., FCNet outperforms Transformer on multi-environmental robotics datasets of all types of sizes (from 1.9M to 120M). The project page and code can be found https://thkkk.github.io/fcnet.",
      "authors": [
        "Hengkai Tan",
        "Songming Liu",
        "Kai Ma",
        "Chengyang Ying",
        "Xingxing Zhang",
        "Hang Su",
        "Jun Zhu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nDps3Q8j2l",
      "cdate": 1706873219215,
      "mdate": 1719287294720,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525514"
    },
    {
      "id": "5kXNMDpUVF",
      "title": "A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization",
      "abstract": "An open problem in differentially private deep learning is hyperparameter optimization (HPO). DP-SGD introduces new hyperparameters and complicates existing ones, forcing researchers to painstakingly tune hyperparameters with hundreds of trials, which in turn makes it impossible to account for the privacy cost of HPO without destroying the utility. We propose an adaptive HPO method that uses cheap trials (in terms of privacy cost and runtime) to estimate optimal hyperparameters and scales them up. We obtain state-of-the-art performance on 22 benchmark tasks, across computer vision and natural language processing, across pretraining and finetuning, across architectures and a wide range of $\\varepsilon \\in [0.01,8.0]$, all while accounting for the privacy cost of HPO.",
      "authors": [
        "Ashwinee Panda",
        "Xinyu Tang",
        "Saeed Mahloujifar",
        "Vikash Sehwag",
        "Prateek Mittal"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5kXNMDpUVF",
      "cdate": 1706873141506,
      "mdate": 1719287294573,
      "matched_keywords": [
        "deep learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525519"
    },
    {
      "id": "FQQ4476dT2",
      "title": "FightLadder: A Benchmark for Competitive Multi-Agent Reinforcement Learning",
      "abstract": "Recent advances in reinforcement learning (RL) heavily rely on a variety of well-designed benchmarks, which provide environmental platforms and consistent criteria to evaluate existing and novel algorithms. Specifically, in multi-agent RL (MARL), a plethora of benchmarks based on cooperative games have spurred the development of algorithms that improve the scalability of cooperative multi-agent systems. However, for the competitive setting, a lightweight and open-sourced benchmark with challenging gaming dynamics and visual inputs has not yet been established. In this work, we present FightLadder, a real-time fighting game platform, to empower competitive MARL research. Along with the platform, we provide implementations of state-of-the-art MARL algorithms for competitive games, as well as a set of evaluation metrics to characterize the performance and exploitability of agents. We demonstrate the feasibility of this platform by training a general agent that consistently defeats 12 built-in characters in single-player mode, and expose the difficulty of training a non-exploitable agent without human knowledge and demonstrations in two-player mode. FightLadder provides meticulously designed environments to address critical challenges in competitive MARL research, aiming to catalyze a new era of discovery and advancement in the field. Videos and code at https://sites.google.com/view/fightladder/home.",
      "authors": [
        "Wenzhe Li",
        "Zihan Ding",
        "Seth Karten",
        "Chi Jin"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FQQ4476dT2",
      "cdate": 1706873037133,
      "mdate": 1719287294456,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525526"
    },
    {
      "id": "L0VoOdjCUb",
      "title": "Learning with 3D rotations, a hitchhiker's guide to SO(3)",
      "abstract": "Many settings in machine learning require the selection of a rotation representation. However, choosing a suitable representation from the many available options is challenging. This paper acts as a survey and guide through rotation representations. We walk through their properties that harm or benefit deep learning with gradient-based optimization. By consolidating insights from rotation-based learning, we provide a comprehensive overview of learning functions with rotation representations. We provide guidance on selecting representations based on whether rotations are in the model’s input or output and whether the data primarily comprises small angles.",
      "authors": [
        "Andreas René Geist",
        "Jonas Frey",
        "Mikel Zhobro",
        "Anna Levina",
        "Georg Martius"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=L0VoOdjCUb",
      "cdate": 1706872943830,
      "mdate": 1719287294317,
      "matched_keywords": [
        "machine learning",
        "deep learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525533"
    },
    {
      "id": "GqsRKEhelH",
      "title": "Indirectly Parameterized Concrete Autoencoders",
      "abstract": "Feature selection is a crucial task in settings where data is high-dimensional or acquiring the full set of features is costly. Recent developments in neural network-based embedded feature selection show promising results across a wide range of applications. Concrete Autoencoders (CAEs), considered state-of-the-art in embedded feature selection, may struggle to achieve stable joint optimization, hurting their training time and generalization. In this work, we identify that this instability is correlated with the CAE learning duplicate selections. To remedy this, we propose a simple and effective improvement: Indirectly Parameterized CAEs (IP-CAEs). IP-CAEs learn an embedding and a mapping from it to the Gumbel-Softmax distributions' parameters. Despite being simple to implement, IP-CAE exhibits significant and consistent improvements over CAE in both generalization and training time across several datasets for reconstruction and classification. Unlike CAE, IP-CAE effectively leverages non-linear relationships and does not require retraining the jointly optimized decoder. Furthermore, our approach is, in principle, generalizable to Gumbel-Softmax distributions beyond feature selection.",
      "authors": [
        "Alfred Nilsson",
        "Klas Wijk",
        "Sai bharath chandra Gutha",
        "Erik Englesson",
        "Alexandra Hotti",
        "Carlo Saccardi",
        "Oskar Kviman",
        "Jens Lagergren",
        "Ricardo Vinuesa Motilva",
        "Hossein Azizpour"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GqsRKEhelH",
      "cdate": 1706872869352,
      "mdate": 1719287294244,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525539"
    },
    {
      "id": "beXQVQorse",
      "title": "High-Dimensional Bayesian Optimization via Semi-Supervised Learning with Optimized Unlabeled Data Sampling",
      "abstract": "We introduce a novel semi-supervised learning approach, named Teacher-Student Bayesian Optimization ($\\texttt{TSBO}$), integrating the teacher-student paradigm into BO to minimize expensive labeled data queries for the first time. $\\texttt{TSBO}$ incorporates a teacher model, an unlabeled data sampler, and a student model. The student is trained on unlabeled data locations generated by the sampler, with pseudo labels predicted by the teacher. The interplay between these three components implements a unique *selective regularization* to the teacher in the form of student feedback. This scheme enables the teacher to predict high-quality pseudo labels, enhancing the generalization of the GP surrogate model in the search space. To fully exploit $\\texttt{TSBO}$, we propose two optimized unlabeled data samplers to construct effective student feedback that well aligns with the objective of Bayesian optimization. Furthermore, we quantify and leverage the uncertainty of the teacher-student model for the provision of reliable feedback to the teacher in the presence of risky pseudo-label predictions. $\\texttt{TSBO}$ demonstrates significantly improved sample-efficiency in several global optimization tasks under tight labeled data budgets. The implementation is available at https://github.com/reminiscenty/TSBO-Official.",
      "authors": [
        "Yuxuan Yin",
        "Yu Wang",
        "Peng Li"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=beXQVQorse",
      "cdate": 1706872723455,
      "mdate": 1719287294111,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525544"
    },
    {
      "id": "9ZkUFSwlUH",
      "title": "Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts",
      "abstract": "Reinforcement learning (RL) is a powerful approach for acquiring a good-performing policy. However, learning diverse skills is challenging in RL due to the commonly used Gaussian policy parameterization. We propose Diverse Skill Learning (Di-SkilL), an RL method for learning diverse skills using Mixture of Experts, where each expert formalizes a skill as a contextual motion primitive. Di-SkilL optimizes each expert and its associate context distribution to a maximum entropy objective that incentivizes learning diverse skills in similar contexts. The per-expert context distribution enables automatic curricula learning, allowing each expert to focus on its best-performing sub-region of the context space. To overcome hard discontinuities and multi-modalities without any prior knowledge of the environment's unknown context probability space, we leverage energy-based models to represent the per-expert context distributions and demonstrate how we can efficiently train them using the standard policy gradient objective. We show on challenging robot simulation tasks that Di-SkilL can learn diverse and performant skills.",
      "authors": [
        "Onur Celik",
        "Aleksandar Taranovic",
        "Gerhard Neumann"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9ZkUFSwlUH",
      "cdate": 1706872683292,
      "mdate": 1719287294049,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525551"
    },
    {
      "id": "Rp8R9C0Sth",
      "title": "AutoOS: Make Your OS More Powerful by Exploiting Large Language Models",
      "abstract": "With the rapid development of Artificial Intelligence of Things (AIoT), customizing and optimizing operating system (OS) kernel configurations for various AIoT application scenarios is crucial for maximizing system performance. However, existing approaches falter due to the overwhelming problem complexity (i.e., over 15,000 configuration options in the Linux kernel), together with the huge evaluation costs and error-prone options that may result in OS boot-up failure, which all make it an unresolved problem to optimize the Linux kernel automatically. In this paper, we introduce AutoOS, a novel framework exploiting Large Language Models for customizing and optimizing OS kernel configurations automatically for various AIoT application scenarios.Inspired by the inherently directory-structured kernel configuration process,  we first formulate our research problem as optimizing on a dynamic tree. We then propose a novel framework integrating a state machine-based traversal algorithm as the observe-prune-propose-act-correct loop, which can effectively refine the optimization space and ensure a successful OS boot-up.Experimental results show that AutoOS can automatically customize and optimize the OS kernel configurations without human effort. More importantly, AutoOS even achieves better performance by up to 25% than vendor-provided configuration.",
      "authors": [
        "Huilai Chen",
        "Yuanbo Wen",
        "Limin Cheng",
        "Shouxu Kuang",
        "Yumeng Liu",
        "Weijia Li",
        "Ling Li",
        "Rui Zhang",
        "Xinkai Song",
        "Wei Li",
        "Qi Guo",
        "Yunji Chen"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Rp8R9C0Sth",
      "cdate": 1706872673994,
      "mdate": 1719287294013,
      "matched_keywords": [
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525557"
    },
    {
      "id": "hKdJPMQvew",
      "title": "Hyperbolic Active Learning for Semantic Segmentation under Domain Shift",
      "abstract": "We introduce a hyperbolic neural network approach to pixel-level active learning for semantic segmentation. Analysis of the data statistics leads to a novel interpretation of the hyperbolic radius as an indicator of data scarcity. In HALO (Hyperbolic Active Learning Optimization), for the first time, we propose the use of epistemic uncertainty as a data acquisition strategy, following the intuition of selecting data points that are the least known. The hyperbolic radius, complemented by the widely-adopted prediction entropy, effectively approximates epistemic uncertainty. We perform extensive experimental analysis based on two established synthetic-to-real benchmarks, i.e. GTAV $\\rightarrow$ Cityscapes and SYNTHIA $\\rightarrow$ Cityscapes. Additionally, we test HALO on Cityscape $\\rightarrow$ ACDC for domain adaptation under adverse weather conditions, and we benchmark both convolutional and attention-based backbones. HALO sets a new state-of-the-art in active learning for semantic segmentation under domain shift and it is the first active learning approach that surpasses the performance of supervised domain adaptation while using only a small portion of labels (i.e., 1%).",
      "authors": [
        "Luca Franco",
        "Paolo Mandica",
        "Konstantinos Kallidromitis",
        "Devin Guillory",
        "Yu-Teng Li",
        "Trevor Darrell",
        "Fabio Galasso"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hKdJPMQvew",
      "cdate": 1706872604242,
      "mdate": 1719287293946,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525563"
    },
    {
      "id": "DkqiId4AuR",
      "title": "Failures Are Fated, But Can Be Faded: Characterizing and Mitigating Unwanted Behaviors in Large-Scale Vision and Language Models",
      "abstract": "In large deep neural networks that seem to perform surprisingly well on many tasks, we also observe a few failures related to accuracy, social biases, and alignment with human values, among others. Therefore, before deploying these models, it is crucial to characterize this failure landscape for engineers to debug and legislative bodies to audit models. Nevertheless, it is infeasible to exhaustively test for all possible combinations of factors that could lead to a model's failure. In this paper, we introduce a post-hoc method that utilizes *deep reinforcement learning* to explore and construct the landscape of failure modes in pre-trained discriminative and generative models. With the aid of limited human feedback, we then demonstrate how to restructure the failure landscape to be more desirable by moving away from the discovered failure modes. We empirically show the effectiveness of the proposed method across common Computer Vision, Natural Language Processing, and Vision-Language tasks.",
      "authors": [
        "Som Sagar",
        "Aditya Taparia",
        "Ransalu Senanayake"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DkqiId4AuR",
      "cdate": 1706872596867,
      "mdate": 1719287293922,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525569"
    },
    {
      "id": "a2uFstsHPb",
      "title": "Efficient Pareto Manifold Learning with Low-Rank Structure",
      "abstract": "Multi-task learning, which optimizes performance across multiple tasks, is inherently a multi-objective optimization problem. Various algorithms are developed to provide discrete trade-off solutions on the Pareto front. Recently, continuous Pareto front approximations using a linear combination of base networks have emerged as a compelling strategy. However, it suffers from scalability issues when the number of tasks is large. To address this issue, we propose a novel approach that integrates a main network with several low-rank matrices to efficiently learn the Pareto manifold. It significantly reduces the number of parameters and facilitates the extraction of shared features. We also introduce orthogonal regularization to further bolster performance. Extensive experimental results demonstrate that the proposed approach outperforms state-of-the-art baselines, especially on datasets with a large number of tasks.",
      "authors": [
        "Weiyu Chen",
        "James Kwok"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=a2uFstsHPb",
      "cdate": 1706872544022,
      "mdate": 1719287293870,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525575"
    },
    {
      "id": "VyoY3Wh9Wd",
      "title": "In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization",
      "abstract": "With the increasing computational costs associated with deep learning, automated hyperparameter optimization methods, strongly relying on black-box Bayesian optimization (BO), face limitations. Freeze-thaw BO offers a promising grey-box alternative,  strategically allocating scarce resources incrementally to different configurations. However, the frequent surrogate model updates inherent to this approach pose challenges for existing methods, requiring retraining or fine-tuning their neural network surrogates online, introducing overhead, instability, and hyper-hyperparameters. In this work, we propose FT-PFN, a novel surrogate for Freeze-thaw style BO. FT-PFN is a prior-data fitted network (PFN) that leverages the transformers' in-context learning ability to efficiently and reliably do Bayesian learning curve extrapolation in a single forward pass. Our empirical analysis across three benchmark suites shows that the predictions made by FT-PFN are more accurate and 10-100 times faster than those of the deep Gaussian process and deep ensemble surrogates used in previous work. Furthermore, we show that, when combined with our novel acquisition mechanism (MFPI-random), the resulting in-context freeze-thaw BO method (ifBO), yields new state-of-the-art performance in the same three families of deep learning HPO benchmarks considered in prior work.",
      "authors": [
        "Herilalaina Rakotoarison",
        "Steven Adriaensen",
        "Neeratyoy Mallik",
        "Samir Garibov",
        "Eddie Bergman",
        "Frank Hutter"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=VyoY3Wh9Wd",
      "cdate": 1706872416576,
      "mdate": 1719287293793,
      "matched_keywords": [
        "deep learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525583"
    },
    {
      "id": "Bq2THeNXRr",
      "title": "Selecting Large Language Model to Fine-tune via Rectified Scaling Law",
      "abstract": "The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with Scaling Law. Unlike pre-training, we find that the fine-tuning scaling curve includes not just the well-known \"power phase\" but also the previously unobserved \"pre-power phase\". We also explain why existing Scaling Law fails to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of \"pre-learned data size\" into our Rectified Scaling Law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundreds of times less resource consumption, while other methods may provide negatively correlated selection. The project page is available at rectified-scaling-law.github.io.",
      "authors": [
        "Haowei Lin",
        "Baizhou Huang",
        "Haotian Ye",
        "Qinyu Chen",
        "Zihao Wang",
        "Sujian Li",
        "Jianzhu Ma",
        "Xiaojun Wan",
        "James Zou",
        "Yitao Liang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Bq2THeNXRr",
      "cdate": 1706872323276,
      "mdate": 1719287293735,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525589"
    },
    {
      "id": "t82Y3fmRtk",
      "title": "Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning",
      "abstract": "In this paper, we propose **R**$^3$: Learning **R**easoning through **R**everse Curriculum **R**einforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. **R**$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, **R**$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, **R**$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$ points on average. Notably, in program-based reasoning, 7B-scale models perform comparably to larger models or closed-source models with our **R**$^3$.",
      "authors": [
        "Zhiheng Xi",
        "Wenxiang Chen",
        "Boyang Hong",
        "Senjie Jin",
        "Rui Zheng",
        "Wei He",
        "Yiwen Ding",
        "Shichun Liu",
        "Xin Guo",
        "Junzhe Wang",
        "Honglin Guo",
        "Wei Shen",
        "Xiaoran Fan",
        "Yuhao Zhou",
        "Shihan Dou",
        "Xiao Wang",
        "Xinbo Zhang",
        "peng sun",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=t82Y3fmRtk",
      "cdate": 1706872078174,
      "mdate": 1719287293521,
      "matched_keywords": [
        "reinforcement learning",
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525596"
    },
    {
      "id": "M4Htd52HMH",
      "title": "Embodied CoT Distillation From LLM To Off-the-shelf Agents",
      "abstract": "We address the challenge of utilizing large language models (LLMs) for complex embodied tasks, in the environment where decision-making systems operate timely on capacity-limited, off-the-shelf devices. We present DeDer, a framework for decomposing and distilling the embodied reasoning capabilities from LLMs to efficient, small language model (sLM)-based policies. In DeDer, the decision-making process of LLM-based strategies is restructured into a hierarchy with a reasoning-policy and planning-policy. The reasoning-policy is distilled from the data that is generated through the embodied in-context learning and self-verification of an LLM, so it can produce effective rationales. The planning-policy, guided by the rationales, can render optimized plans efficiently. In turn, DeDer allows for adopting sLMs for both policies, deployed on off-the-shelf devices. Furthermore, to enhance the quality of intermediate rationales, specific to embodied tasks, we devise the embodied knowledge graph, and to generate multiple rationales timely through a single inference, we also use the contrastively prompted attention model. Our experiments with the ALFRED benchmark demonstrate that DeDer surpasses leading language planning and distillation approaches, indicating the applicability and efficiency of sLM-based embodied policies derived through DeDer.",
      "authors": [
        "Wonje Choi",
        "Woo Kyung Kim",
        "Minjong Yoo",
        "Honguk Woo"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=M4Htd52HMH",
      "cdate": 1706871974152,
      "mdate": 1719287293414,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525601"
    },
    {
      "id": "v7I5FtL2pV",
      "title": "Tabular Insights, Visual Impacts: Transferring Expertise from Tables to Images",
      "abstract": "Transferring knowledge across diverse data modalities is receiving increasing attention in machine learning. This paper tackles the task of leveraging expert-derived, yet expensive, tabular data to enhance image-based predictions when tabular data is unavailable during inference. The primary challenges stem from the inherent complexity of accurately mapping diverse tabular data to visual contexts, coupled with the necessity to devise distinct strategies for numerical and categorical tabular attributes. We propose CHannel tAbulaR alignment with optiMal tranSport (Charms), which establishes an alignment between image channels and tabular attributes, enabling selective knowledge transfer that is pertinent to visual features. Specifically, Charms measures similarity distributions across modalities to effectively differentiate and transfer relevant tabular features, with a focus on morphological characteristics, enhancing the capabilities of visual classifiers. By maximizing the mutual information between image channels and tabular features, knowledge from both numerical and categorical tabular attributes are extracted. Experimental results demonstrate that Charms not only enhances the performance of image classifiers but also improves their interpretability by effectively utilizing tabular knowledge.",
      "authors": [
        "Jun-Peng Jiang",
        "Han-Jia Ye",
        "Leye Wang",
        "Yang Yang",
        "Yuan Jiang",
        "De-Chuan Zhan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=v7I5FtL2pV",
      "cdate": 1706871930488,
      "mdate": 1719287293404,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525607"
    },
    {
      "id": "p1kDNFs62o",
      "title": "Nesting Particle Filters for Experimental Design in Dynamical Systems",
      "abstract": "In this paper, we propose a novel approach to Bayesian experimental design for non-exchangeable data that formulates it as risk-sensitive policy optimization. We develop the Inside-Out SMC$^2$ algorithm, a nested sequential Monte Carlo technique to infer optimal designs, and embed it into a particle Markov chain Monte Carlo framework to perform gradient-based policy amortization. Our approach is distinct from other amortized experimental design techniques, as it does not rely on contrastive estimators. Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies.",
      "authors": [
        "Sahel Iqbal",
        "Adrien Corenflos",
        "Simo Särkkä",
        "Hany Abdulsamad"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=p1kDNFs62o",
      "cdate": 1706871904071,
      "mdate": 1719287293340,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525612"
    },
    {
      "id": "qGEEso256L",
      "title": "Structure-Aware E(3)-Invariant Molecular Conformer Aggregation Networks",
      "abstract": "A molecule’s 2D representation consists of its atoms, their attributes, and the molecule’s covalent bonds. A 3D (geometric) representation of a molecule is called a conformer and consists of its atom types and Cartesian coordinates. Every conformer has a potential energy, and the lower this energy, the more likely it occurs in nature. Most existing machine learning methods for molecular property prediction consider either 2D molecular graphs or 3D conformer structure representations in isolation. Inspired by recent work on using ensembles of conformers in conjunction with 2D graph representations, we propose E(3)-invariant molecular conformer aggregation networks. The method integrates a molecule’s 2D representation with that of multiple of its conformers. Contrary to prior work, we propose a novel 2D–3D aggregation mechanism based on a differentiable solver for the Fused Gromov-Wasserstein Barycenter problem and the use of an efficient conformer generation method based on distance geometry. We show that the proposed aggregation mechanism is E(3) invariant and propose an efficient GPU implementation. Moreover, we demonstrate that the aggregation mechanism helps to significantly outperform state-of-the-art molecule property prediction methods on established datasets.",
      "authors": [
        "Duy Minh Ho Nguyen",
        "Nina Lukashina",
        "Tai Nguyen",
        "An Thai Le",
        "TrungTin Nguyen",
        "Nhat Ho",
        "Jan Peters",
        "Daniel Sonntag",
        "Viktor Zaverkin",
        "Mathias Niepert"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qGEEso256L",
      "cdate": 1706871902076,
      "mdate": 1719287293305,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525619"
    },
    {
      "id": "Lg8nw3ltvX",
      "title": "Layerwise Proximal Replay: A Proximal Point Method for Online Continual Learning",
      "abstract": "In online continual learning, a neural network incrementally learns from a non-i.i.d. data stream. Nearly all online continual learning methods employ experience replay to simultaneously prevent catastrophic forgetting and underfitting on past data. Our work demonstrates a limitation of this approach: neural networks trained with experience replay tend to have unstable optimization trajectories, impeding their overall accuracy. Surprisingly, these instabilities persist even when the replay buffer stores all previous training examples, suggesting that this issue is orthogonal to catastrophic forgetting. We minimize these instabilities through a simple modification of the optimization geometry. Our solution, Layerwise Proximal Replay (LPR), balances learning from new and replay data while only allowing for gradual changes in the hidden activation of past data. We demonstrate that LPR consistently improves replay-based online continual learning across multiple problem settings, regardless of the amount of available replay memory.",
      "authors": [
        "Jinsoo Yoo",
        "Yunpeng Liu",
        "Frank Wood",
        "Geoff Pleiss"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Lg8nw3ltvX",
      "cdate": 1706871861001,
      "mdate": 1719287293226,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525625"
    },
    {
      "id": "CHz7WshPcp",
      "title": "Longitudinal Targeted Minimum Loss-based Estimation with Temporal-Difference Heterogeneous Transformer",
      "abstract": "We propose Deep Longitudinal Targeted Minimum Loss-based Estimation (Deep LTMLE), a novel approach to estimate the counterfactual mean of outcome under dynamic treatment policies in longitudinal problem settings. Our approach utilizes a transformer architecture with heterogeneous type embedding trained using temporal-difference learning. After obtaining an initial estimate using the transformer, following the targeted minimum loss-based likelihood estimation (TMLE) framework, we statistically corrected for the bias commonly associated with machine learning algorithms. Furthermore, our method also facilitates statistical inference by enabling the provision of 95% confidence intervals grounded in asymptotic statistical theory. Simulation results demonstrate our method's superior performance over existing approaches, particularly in complex, long time-horizon scenarios. It remains effective in small-sample, short-duration contexts, matching the performance of asymptotically efficient estimators. To demonstrate our method in practice, we applied our method to estimate counterfactual mean outcomes for standard versus intensive blood pressure management strategies in a real-world cardiovascular epidemiology cohort study.",
      "authors": [
        "Toru Shirakawa",
        "Yi Li",
        "Yulun Wu",
        "Sky Qiu",
        "Yuxuan Li",
        "Mingduo Zhao",
        "Hiroyasu Iso",
        "Mark J. van der Laan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=CHz7WshPcp",
      "cdate": 1706871844213,
      "mdate": 1719287293189,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525631"
    },
    {
      "id": "sDjszMb2Ir",
      "title": "LASER: Linear Compression in Wireless Distributed Optimization",
      "abstract": "Data-parallel SGD is the de facto algorithm for distributed optimization, especially for large scale machine learning. Despite its merits, communication bottleneck is one of its persistent issues. Most compression schemes to alleviate this either assume noiseless communication links, or fail to achieve good performance on practical tasks. In this paper, we close this gap and introduce **LASER**: **L**ine**A**r Compre**S**sion in Wir**E**less Dist**R**ibuted Optimization. LASER capitalizes on the inherent low-rank structure of gradients and transmits them efficiently over the noisy channels. Whilst enjoying theoretical guarantees similar to those of the classical SGD, LASER shows consistent gains over baselines on a variety of practical benchmarks. In particular, it outperforms the state-of-the-art compression schemes on challenging computer vision and GPT language modeling tasks. On the latter, we obtain 50-64% improvement in perplexity over our baselines for noisy channels.",
      "authors": [
        "Ashok Vardhan Makkuva",
        "Marco Bondaschi",
        "Thijs Vogels",
        "Martin Jaggi",
        "Hyeji Kim",
        "Michael Gastpar"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=sDjszMb2Ir",
      "cdate": 1706871812056,
      "mdate": 1719287293102,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525637"
    },
    {
      "id": "LWRI4uPG2X",
      "title": "eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data",
      "abstract": "With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products – a typical out-of-domain generalization challenge. Meanwhile, large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields. Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, large-scale, and high-quality benchmark instruction dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced GPT-4, and the state-of-the-art task-specific models in in-domain evaluation. Moreover, eCeLLM exhibits excellent generalizability to out-of-domain settings, including unseen products and unseen instructions, highlighting its superiority as a generalist e-commerce model. Both the ECInstruct dataset and the eCeLLM models show great potential in empowering versatile and effective LLMs for e-commerce. ECInstruct and eCeLLM models are publicly accessible through this link.",
      "authors": [
        "Bo Peng",
        "Xinyi Ling",
        "Ziru Chen",
        "Huan Sun",
        "Xia Ning"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LWRI4uPG2X",
      "cdate": 1706871740793,
      "mdate": 1719287293011,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525644"
    },
    {
      "id": "1xKgDANODx",
      "title": "Retrieval-Augmented Score Distillation for Text-to-3D Generation",
      "abstract": "Text-to-3D generation has achieved significant success by incorporating powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to the inconsistency of 3D geometry. Recently, since large-scale multi-view datasets have been released, fine-tuning the diffusion model on the multi-view datasets becomes a mainstream to solve the 3D inconsistency problem. However, it has confronted with fundamental difficulties regarding the limited quality and diversity of 3D data, compared with 2D data. To sidestep these trade-offs, we explore a retrieval-augmented approach tailored for score distillation, dubbed ReDream. We postulate that both expressiveness of 2D diffusion models and geometric consistency of 3D assets can be fully leveraged by employing the semantically relevant assets directly within the optimization process. To this end, we introduce novel framework for retrieval-based quality enhancement in text-to-3D generation. We leverage the retrieved asset to incorporate its geometric prior in the variational objective and adapt the diffusion model's 2D prior toward view consistency, achieving drastic improvements in both geometry and fidelity of generated scenes. We conduct extensive experiments to demonstrate that ReDream exhibits superior quality with increased geometric consistency. Project page is available at https://ku-cvlab.github.io/ReDream/.",
      "authors": [
        "Junyoung Seo",
        "Susung Hong",
        "Wooseok Jang",
        "Inès Hyeonsu Kim",
        "Min-Seop Kwak",
        "Doyup Lee",
        "Seungryong Kim"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=1xKgDANODx",
      "cdate": 1706871737830,
      "mdate": 1719287292956,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525649"
    },
    {
      "id": "F3Ds71Xgo1",
      "title": "Entropy-Reinforced Planning with Large Language Models for Drug Discovery",
      "abstract": "The objective of drug discovery is to identify chemical compounds that possess specific pharmaceutical properties toward a binding target. Existing large language models (LLMS) can achieve high token matching scores in terms of likelihood for molecule generation. However, relying solely on LLM decoding often results in the generation of molecules that are either invalid due to a single misused token, or suboptimal due to unbalanced exploration and exploitation as a consequence of the LLM’s prior experience. Here we propose ERP, Entropy-Reinforced Planning for Transformer Decoding, which employs an entropy-reinforced planning algorithm to enhance the Transformer decoding process and strike a balance between exploitation and exploration. ERP aims to achieve improvements in multiple properties compared to direct sampling from the Transformer. We evaluated ERP on the SARS-CoV-2 virus (3CLPro) and human cancer cell target protein (RTCB) benchmarks and demonstrated that, in both benchmarks, ERP consistently outperforms the current state-of-the-art algorithm by 1-5 percent, and baselines by 5-10 percent, respectively. Moreover, such improvement is robust across Transformer models trained with different objectives. Finally, to further illustrate the capabilities of ERP, we tested our algorithm on three code generation benchmarks and outperformed the current state-of-the-art approach as well. Our code is publicly available at: https://github.com/xuefeng-cs/ERP.",
      "authors": [
        "Xuefeng Liu",
        "Chih-chan Tien",
        "Peng Ding",
        "Songhao Jiang",
        "Rick L. Stevens"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=F3Ds71Xgo1",
      "cdate": 1706871717709,
      "mdate": 1719287292937,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525656"
    },
    {
      "id": "yhpDKSw7yA",
      "title": "Provably Robust DPO: Aligning Language Models with Noisy Feedback",
      "abstract": "Learning from preference-based feedback has recently gained traction as a promising approach to align language models with human interests. While these aligned generative models have demonstrated impressive capabilities across various tasks, their dependence on high-quality human preference data poses a bottleneck in practical applications. Specifically, noisy (incorrect and ambiguous) preference pairs in the dataset might restrict the language models from capturing human intent accurately. While practitioners have recently proposed heuristics to mitigate the effect of noisy preferences, a complete theoretical understanding of their workings remain elusive. In this work, we aim to bridge this gap by introducing a general framework for policy optimization in the presence of random preference flips. We focus on the direct preference optimization (DPO) algorithm in particular since it assumes that preferences adhere to the Bradley-Terry-Luce (BTL) model, raising concerns about the impact of noisy data on the learned policy. We design a novel loss function, which de-bias the effect of noise on average, making a policy trained by minimizing that loss robust to the noise. Under log-linear parameterization of the policy class and assuming good feature coverage of the SFT policy, we prove that the sub-optimality gap of the proposed robust DPO (rDPO) policy compared to the optimal policy is of the order $O(\\frac{1}{1-2\\epsilon}\\sqrt{\\frac{d}{n}})$, where $\\epsilon < 1/2$ is flip rate of labels, $d$ is policy parameter dimension and $n$ is size of dataset. Our experiments on IMDb sentiment generation and Anthropic's helpful-harmless dataset shows that rDPO is robust to noise in preference labels compared to vanilla DPO and other heuristics proposed by practitioners.",
      "authors": [
        "Sayak Ray Chowdhury",
        "Anush Kini",
        "Nagarajan Natarajan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=yhpDKSw7yA",
      "cdate": 1706871691176,
      "mdate": 1719287292838,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525661"
    },
    {
      "id": "JVhUR8q27o",
      "title": "Towards AutoAI: Optimizing a Machine Learning System with Black-box and Differentiable Components",
      "abstract": "*Machine learning* (ML) models in the real world typically do not exist in isolation. They are usually part of a complex system (e.g., healthcare systems, self-driving cars) containing multiple ML and *black-box* components. The problem of optimizing such systems, which we refer to as *automated AI* (AutoAI), requires us to *jointly* train all ML components together and presents a significant challenge because the number of system parameters is extremely high and the system has no analytical form. To circumvent this, we introduce a novel algorithm called A-BAD-BO which uses each ML component's local loss as an auxiliary indicator for system performance. A-BAD-BO uses *Bayesian optimization* (BO) to optimize the local loss configuration of a system in a smaller dimensional space and exploits the differentiable structure of ML components to recover optimal system parameters from the optimized configuration. We show A-BAD-BO converges to optimal system parameters by showing that it is *asymptotically no regret*. We use A-BAD-BO to optimize several synthetic and real-world complex systems, including a prompt engineering pipeline for *large language models* containing millions of system parameters. Our results demonstrate that A-BAD-BO yields better system optimality than gradient-driven baselines and is more sample-efficient than pure BO algorithms.",
      "authors": [
        "Zhiliang Chen",
        "Chuan-Sheng Foo",
        "Bryan Kian Hsiang Low"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=JVhUR8q27o",
      "cdate": 1706871523670,
      "mdate": 1719287292497,
      "matched_keywords": [
        "machine learning",
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525667"
    },
    {
      "id": "RPMTNGMq0O",
      "title": "Dealing With Unbounded Gradients in Stochastic Saddle-point Optimization",
      "abstract": "We study the performance of stochastic first-order methods for finding saddle points of convex-concave functions. A notorious challenge faced by such methods is that the gradients can grow arbitrarily large during optimization, which may result in instability and divergence. In this paper, we propose a simple and effective regularization technique that stabilizes the iterates and yields meaningful performance guarantees even if the domain and the gradient noise scales linearly with the size of the iterates (and is thus potentially unbounded). Besides providing a set of general results, we also apply our algorithm to a specific problem in reinforcement learning, where it leads to performance guarantees for finding near-optimal policies in an average-reward MDP without prior knowledge of the bias span.",
      "authors": [
        "Gergely Neu",
        "Nneka Okolo"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=RPMTNGMq0O",
      "cdate": 1706871320352,
      "mdate": 1719287292392,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525673"
    },
    {
      "id": "9zlZuAAb08",
      "title": "Quality Diversity through Human Feedback: Towards Open-Ended Diversity-Driven Optimization",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has shown potential in qualitative tasks where easily defined performance measures are lacking. However, there are drawbacks when RLHF is commonly used to optimize for average human preferences, especially in generative tasks that demand diverse model responses. Meanwhile, Quality Diversity (QD) algorithms excel at identifying diverse and high-quality solutions but often rely on manually crafted diversity metrics. This paper introduces Quality Diversity through Human Feedback (QDHF), a novel approach that progressively infers diversity metrics from human judgments of similarity among solutions, thereby enhancing the applicability and effectiveness of QD algorithms in complex and open-ended domains. Empirical studies show that QDHF significantly outperforms state-of-the-art methods in automatic diversity discovery and matches the efficacy of QD with manually crafted diversity metrics on standard benchmarks in robotics and reinforcement learning. Notably, in open-ended generative tasks, QDHF substantially enhances the diversity of text-to-image generation from a diffusion model and is more favorably received in user studies. We conclude by analyzing QDHF's scalability, robustness, and quality of derived diversity metrics, emphasizing its strength in open-ended optimization tasks. Code and tutorials are available at https://liding.info/qdhf.",
      "authors": [
        "Li Ding",
        "Jenny Zhang",
        "Jeff Clune",
        "Lee Spector",
        "Joel Lehman"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9zlZuAAb08",
      "cdate": 1706871223122,
      "mdate": 1719287292308,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525680"
    },
    {
      "id": "SlRcJvf1yd",
      "title": "Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates",
      "abstract": "We study the problem of efficiently computing the derivative of the fixed-point of a parametric nondifferentiable contraction map. This problem has wide applications in machine learning, including hyperparameter optimization, meta-learning and data poisoning attacks. We analyze two popular approaches: iterative differentiation (ITD) and approximate implicit differentiation (AID). A key challenge behind the nonsmooth setting is that the chain rule does not hold anymore. We build upon the work by Bolte et al. (2022), who prove linear convergence of nonsmooth ITD under a piecewise Lipschitz smooth assumption. In the deterministic case, we provide a linear rate for AID and an improved linear rate for ITD which closely match the ones for the smooth setting. We further introduce NSID, a new stochastic method to compute the implicit derivative when the contraction map is defined as the composition of an outer map and an inner map which is accessible only through a stochastic unbiased estimator. We establish rates for the convergence of NSID, encompassing the best available rates in the smooth setting. We also present illustrative experiments confirming our analysis.",
      "authors": [
        "Riccardo Grazzi",
        "Massimiliano Pontil",
        "Saverio Salzo"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=SlRcJvf1yd",
      "cdate": 1706871054256,
      "mdate": 1740735281566,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525686"
    },
    {
      "id": "9DMMvMTDur",
      "title": "EvIL: Evolution Strategies for Generalisable Imitation Learning",
      "abstract": "Often times in imitation learning (IL), the environment we collect expert demonstrations in and the environment we want to deploy our learned policy in aren't exactly the same (e.g. demonstrations collected in simulation but deployment in the real world). Compared to policy-centric approaches to IL like behavioural cloning, reward-centric approaches like *inverse reinforcement learning* (IRL) often better replicate expert behaviour in new environments. This transfer is usually performed by optimising the recovered reward under the dynamics of the target environment. However, *(a)* we find that modern deep IL algorithms frequently recover rewards which induce policies far weaker than the expert, *even in the same environment the demonstrations were collected in*. Furthermore, *(b)* these rewards are often quite poorly shaped, necessitating extensive environment interaction to optimise effectively. We provide simple and scalable fixes to both of these concerns. For *(a)*, we find that *reward model ensembles* combined with a slightly different training objective significantly improves re-training and transfer performance. For *(b)*, we propose a novel *evolution-strategies* based method (EvIL) to optimise for a reward-shaping term that speeds up re-training in the target environment, closing a gap left open by the classical theory of IRL. On a suite of continuous control tasks, we are able to re-train policies in target (and source) environments more interaction-efficiently than prior work.",
      "authors": [
        "Silvia Sapora",
        "Gokul Swamy",
        "Chris Lu",
        "Yee Whye Teh",
        "Jakob Nicolaus Foerster"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9DMMvMTDur",
      "cdate": 1706871005251,
      "mdate": 1719287292102,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525692"
    },
    {
      "id": "uku9r6RROl",
      "title": "DRED: Zero-Shot Transfer in Reinforcement Learning via Data-Regularised Environment Design",
      "abstract": "Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when these environments share characteristics with the ones they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which assume control over level generation. We find that existing UED methods can significantly shift the training distribution, which translates to low ZSG performance. To prevent both overfitting and distributional shift, we introduce *data-regularised environment design* (DRED). DRED generates levels using a generative model trained to approximate the ground truth distribution of an initial set of level parameters. Through its grounding, DRED achieves significant improvements in ZSG over adaptive level sampling strategies and UED methods.",
      "authors": [
        "Samuel Garcin",
        "James Doran",
        "Shangmin Guo",
        "Christopher G. Lucas",
        "Stefano V Albrecht"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=uku9r6RROl",
      "cdate": 1706870922856,
      "mdate": 1719287292057,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525698"
    },
    {
      "id": "4Vqr8SRfyX",
      "title": "Case-Based or Rule-Based: How Do Transformers Do the Math?",
      "abstract": "Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition. While we can easily learn basic *rules* of addition and apply them to new problems of any length, LLMs struggle to do the same. Instead, they may rely on similar *cases* seen in the training corpus for help. We define these two different reasoning mechanisms as \"*rule-based reasoning*\" and \"*case-based reasoning*\". Since rule-based reasoning is essential for acquiring systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems. Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that transformers use subgraph matching/shortcut learning to reason. To mitigate such problems, we propose a Rule-Following Fine-Tuning (RFFT) technique to teach transformers to perform rule-based reasoning. Specifically, we provide explicit rules in the input and then instruct transformers to recite and follow the rules step by step. Through RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to generalize to up to 12-digit addition with over 95% accuracy, which is over 40% higher than scratchpad. The significant improvement demonstrates that teaching LLMs to use rules explicitly helps them learn rule-based reasoning and generalize better in length. Code is available at https://github.com/GraphPKU/Case_or_Rule.",
      "authors": [
        "Yi Hu",
        "Xiaojuan Tang",
        "Haotong Yang",
        "Muhan Zhang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4Vqr8SRfyX",
      "cdate": 1706870831918,
      "mdate": 1719287291951,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525704"
    },
    {
      "id": "FYvpxyS43U",
      "title": "RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation",
      "abstract": "We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis that jointly trains $\\textit{low-rank}$ and *highly-sparse* components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms LoRA, pure sparse fine-tuning, and alternative hybrid methods at the same parameter budget, and can even recover the performance of FFT on some tasks. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memory- and computationally-efficient training, and show that it is also compatible with low-precision base weights, resulting in the first joint representation combining quantization, low-rank and sparse approximations. Our code is available at https://github.com/IST-DASLab/RoSA.",
      "authors": [
        "Mahdi Nikdan",
        "Soroush Tabesh",
        "Elvir Crnčević",
        "Dan Alistarh"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FYvpxyS43U",
      "cdate": 1706870689039,
      "mdate": 1719287291894,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525710"
    },
    {
      "id": "BwAkaxqiLB",
      "title": "Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model",
      "abstract": "Heuristics are widely used for dealing with complex search and optimization problems. However, manual design of heuristics can be often very labour extensive and requires rich working experience and knowledge. This paper proposes Evolution of Heuristic (EoH), a novel evolutionary paradigm that leverages both Large Language Models (LLMs) and Evolutionary Computation (EC) methods for Automatic Heuristic Design (AHD). EoH represents the ideas of heuristics in natural language, termed thoughts. They are then translated into executable codes by LLMs. The evolution of both thoughts and codes in an evolutionary search framework makes it very effective and efficient for generating high-performance heuristics. Experiments on three widely studied combinatorial optimization benchmark problems demonstrate that EoH outperforms commonly used handcrafted heuristics and other recent AHD methods including FunSearch. Particularly, the heuristic produced by EoH with a low computational budget (in terms of the number of queries to LLMs) significantly outperforms widely-used human hand-crafted baseline algorithms for the online bin packing problem.",
      "authors": [
        "Fei Liu",
        "Tong Xialiang",
        "Mingxuan Yuan",
        "Xi Lin",
        "Fu Luo",
        "Zhenkun Wang",
        "Zhichao Lu",
        "Qingfu Zhang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BwAkaxqiLB",
      "cdate": 1706870593519,
      "mdate": 1719287291806,
      "matched_keywords": [
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525716"
    },
    {
      "id": "Y0sH9HGMwq",
      "title": "Prediction Accuracy of Learning in Games : Follow-the-Regularized-Leader meets Heisenberg",
      "abstract": "We investigate the accuracy of prediction in deterministic learning dynamics of zero-sum games with random initializations, specifically focusing on observer uncertainty and its relationship to the evolution of covariances. Zero-sum games are a prominent field of interest in machine learning due to their various applications. Concurrently, the accuracy of prediction in dynamical systems from mechanics has long been a classic subject of investigation since the discovery of the Heisenberg Uncertainty Principle. This principle employs covariance and standard deviation of particle states to measure prediction accuracy. In this study, we bring these two approaches together to analyze the Follow-the-Regularized-Leader (FTRL) algorithm in two-player zero-sum games. We provide growth rates of covariance information for continuous-time FTRL, as well as its two canonical discretization methods (Euler and Symplectic). A Heisenberg-type inequality is established for FTRL. Our analysis and experiments also show that employing Symplectic discretization enhances the accuracy of prediction in learning dynamics.",
      "authors": [
        "Yi Feng",
        "Georgios Piliouras",
        "Xiao Wang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Y0sH9HGMwq",
      "cdate": 1706870572652,
      "mdate": 1719287291831,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525721"
    },
    {
      "id": "nxzXTLByXO",
      "title": "BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback",
      "abstract": "Distribution matching methods for language model alignment such as Generation with Distributional Control (GDC) and Distributional Policy Gradient (DPG) have not received the same level of attention in reinforcement learning from human feedback (RLHF) as contrastive methods such as Sequence Likelihood Calibration (SLiC), Direct Preference Optimization (DPO) and its variants. We identify high variance of the gradient estimate as the primary reason for the lack of success of these methods and propose a self-normalized baseline to reduce the variance. We further generalize the target distribution in DPG, GDC and DPO by using Bayes' rule to define the reward-conditioned posterior. The resulting approach, referred to as BRAIn - Bayesian Reward-conditioned Amortized Inference acts as a bridge between distribution matching methods and DPO and significantly outperforms prior art in summarization and Antropic HH tasks.",
      "authors": [
        "Gaurav Pandey",
        "Yatin Nandwani",
        "Tahira Naseem",
        "Mayank Mishra",
        "Guangxuan Xu",
        "Dinesh Raghu",
        "Sachindra Joshi",
        "Asim Munawar",
        "Ramón Fernandez Astudillo"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nxzXTLByXO",
      "cdate": 1706870566892,
      "mdate": 1719287291726,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525726"
    },
    {
      "id": "NwYsuFuelg",
      "title": "Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine Workers",
      "abstract": "Byzantine-robust learning has emerged as a prominent fault-tolerant distributed machine learning framework. However, most techniques focus on the *static* setting, wherein the identity of Byzantine workers remains unchanged throughout the learning process. This assumption fails to capture real-world *dynamic* Byzantine behaviors, which may include intermittent malfunctions or targeted, time-limited attacks. Addressing this limitation, we propose DynaBRO -- a new method capable of withstanding any sub-linear number of identity changes across rounds. Specifically, when the number of such changes is $\\mathcal{O}(\\sqrt{T})$ (where $T$ is the total number of training rounds), DynaBRO nearly matches the state-of-the-art asymptotic convergence rate of the static setting. Our method utilizes a multi-level Monte Carlo (MLMC) gradient estimation technique applied at the server to robustly aggregated worker updates. By additionally leveraging an adaptive learning rate, we circumvent the need for prior knowledge of the fraction of Byzantine workers.",
      "authors": [
        "Ron Dorfman",
        "Naseem Amin Yehya",
        "Kfir Yehuda Levy"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NwYsuFuelg",
      "cdate": 1706870518993,
      "mdate": 1719287291712,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525732"
    },
    {
      "id": "1YMjzz2g81",
      "title": "SPABA: A Single-Loop and Probabilistic Stochastic Bilevel Algorithm Achieving Optimal Sample Complexity",
      "abstract": "While stochastic bilevel optimization methods have been extensively studied for addressing large-scale nested optimization problems in machine learning, it remains an open question whether the optimal complexity bounds for solving bilevel optimization are the same as those in single-level optimization. Our main result resolves this question: SPABA, an adaptation of the PAGE method for nonconvex optimization in (Li et al., 2021) to the bilevel setting, can achieve optimal sample complexity in both the finite-sum and expectation settings. We show the optimality of SPABA by proving that there is no gap in complexity analysis between stochastic bilevel and single-level optimization when implementing PAGE. Notably, as indicated by the results of (Dagréou et al., 2022), there might exist a gap in complexity analysis when implementing other stochastic gradient estimators, like SGD and SAGA. In addition to SPABA, we propose several other single-loop stochastic bilevel algorithms, that either match or improve the state-of-the-art sample complexity results, leveraging our convergence rate and complexity analysis. Numerical experiments demonstrate the superior practical performance of the proposed methods.",
      "authors": [
        "Tianshu Chu",
        "Dachuan Xu",
        "Wei Yao",
        "Jin Zhang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=1YMjzz2g81",
      "cdate": 1706870412067,
      "mdate": 1719287291604,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525737"
    },
    {
      "id": "KCVCFsPkrm",
      "title": "Shifted Interpolation for Differential Privacy",
      "abstract": "Noisy gradient descent and its variants are the predominant algorithms for differentially private machine learning. It is a fundamental question to quantify their privacy leakage, yet tight characterizations remain open even in the foundational setting of convex losses. This paper improves over previous analyses by establishing (and refining) the “privacy amplification by iteration” phenomenon in the unifying framework of $f$-differential privacy---which tightly captures all aspects of the privacy loss and immediately implies tighter privacy accounting in other notions of differential privacy, e.g., $(\\varepsilon,\\delta)$-DP and Rényi DP. Our key technical insight is the construction of *shifted interpolated processes* that unravel the popular shifted-divergences argument, enabling generalizations beyond divergence-based relaxations of DP. Notably, this leads to the first *exact* privacy analysis in the foundational setting of strongly convex optimization. Our techniques extend to many settings: convex/strongly convex, constrained/unconstrained, full/cyclic/stochastic batches, and all combinations thereof. As an immediate corollary, we recover the $f$-DP characterization of the exponential mechanism for strongly convex optimization in Gopi et al. (2022), and moreover extend this result to more general settings.",
      "authors": [
        "Jinho Bok",
        "Weijie J Su",
        "Jason Altschuler"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=KCVCFsPkrm",
      "cdate": 1706870386022,
      "mdate": 1719287291573,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525743"
    },
    {
      "id": "8tzjEMF0Vq",
      "title": "MaxMin-RLHF: Alignment with Diverse Human Preferences",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, the single reward model overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. Next, we propose to learn a mixture of reward models via an expectation-maximization algorithm and solve a MaxMin alignment objective inspired by the Egalitarian principle in social choice theory to better honor diverse human preferences. We present comprehensive experimental results on small-scale (GPT-2) and large-scale language (with Tulu2-7B)) and show the efficacy of the proposed approach in the presence of diversity among human preferences. We remark that our findings in this work are not only limited to language models but also extend to reinforcement learning in general.",
      "authors": [
        "Souradip Chakraborty",
        "Jiahao Qiu",
        "Hui Yuan",
        "Alec Koppel",
        "Dinesh Manocha",
        "Furong Huang",
        "Amrit Bedi",
        "Mengdi Wang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=8tzjEMF0Vq",
      "cdate": 1706870359879,
      "mdate": 1719287291511,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525749"
    },
    {
      "id": "VDgfJnOEMV",
      "title": "On the Convergence of Projected Bures-Wasserstein Gradient Descent under Euclidean Strong Convexity",
      "abstract": "The Bures-Wasserstein (BW) gradient descent method has gained considerable attention in various domains, including Gaussian barycenter, matrix recovery and variational inference problems, due to its alignment with the Wasserstein geometry of normal distributions. Despite its popularity, existing convergence analysis are often contingent upon specific loss functions, and the exploration of constrained settings within this framework remains limited. In this work, we make an attempt to bridge this gap by providing a general convergence rate guarantee for BW gradient descent when the Euclidean strong convexity of the loss and the constraints is assumed. In an effort to advance practical implementations, we also derive a closed-form solution for the projection onto BW distance-constrained sets, which enables the fast implementation of projected BW gradient descent for problems that arise in the constrained barycenter and distributionally robust optimization literature. Experimental results demonstrate significant improvements in computational efficiency and convergence speed, underscoring the efficacy of our method in practical scenarios.",
      "authors": [
        "Junyi FAN",
        "Yuxuan Han",
        "Zijian Liu",
        "Jian-Feng Cai",
        "Yang Wang",
        "Zhengyuan Zhou"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=VDgfJnOEMV",
      "cdate": 1706870359326,
      "mdate": 1719287291472,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525755"
    },
    {
      "id": "gtYdvSGMYV",
      "title": "LAGMA: LAtent Goal-guided Multi-Agent Reinforcement Learning",
      "abstract": "In cooperative multi-agent reinforcement learning (MARL), agents collaborate to achieve common goals, such as defeating enemies and scoring a goal. However, learning goal-reaching paths toward such a semantic goal takes a considerable amount of time in complex tasks and the trained model often fails to find such paths. To address this, we present LAtent Goal-guided Multi-Agent reinforcement learning (LAGMA), which generates a goal-reaching trajectory in latent space and provides a latent goal-guided incentive to transitions toward this reference trajectory. LAGMA consists of three major components: (a) quantized latent space constructed via a modified VQ-VAE for efficient sample utilization, (b) goal-reaching trajectory generation via extended VQ codebook, and (c) latent goal-guided intrinsic reward generation to encourage transitions towards the sampled goal-reaching path. The proposed method is evaluated by StarCraft II with both dense and sparse reward settings and Google Research Football. Empirical results show further performance improvement over state-of-the-art baselines.",
      "authors": [
        "Hyungho Na",
        "Il-chul Moon"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=gtYdvSGMYV",
      "cdate": 1706870186544,
      "mdate": 1719287291439,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525761"
    },
    {
      "id": "gAyzjHw2ml",
      "title": "SceneCraft: An LLM Agent for Synthesizing 3D Scenes as Blender Code",
      "abstract": "This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self-improvement without expensive LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses existing LLM-based agents in rendering complex scenes, as shown by its adherence to constraints and favorable human assessments. We also showcase the broader application potential of SceneCraft by reconstructing detailed 3D scenes from the Sintel movie and guiding a video generative model with generated scenes as intermediary control signal.",
      "authors": [
        "Ziniu Hu",
        "Ahmet Iscen",
        "Aashi Jain",
        "Thomas Kipf",
        "Yisong Yue",
        "David A Ross",
        "Cordelia Schmid",
        "Alireza Fathi"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=gAyzjHw2ml",
      "cdate": 1706870158791,
      "mdate": 1719287291393,
      "matched_keywords": [
        "large language model",
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525768"
    },
    {
      "id": "BNAvYSCrLD",
      "title": "In-Context Learning Agents Are Asymmetric Belief Updaters",
      "abstract": "We study the in-context learning dynamics of large language models (LLMs) using three instrumental learning tasks adapted from cognitive psychology. We find that LLMs update their beliefs in an asymmetric manner and learn more from better-than-expected outcomes than from worse-than-expected ones. Furthermore, we show that this effect reverses when learning about counterfactual feedback and disappears when no agency is implied. We corroborate these findings by investigating idealized in-context learning agents derived through meta-reinforcement learning, where we observe similar patterns. Taken together, our results contribute to our understanding of how in-context learning works by highlighting that the framing of a problem significantly influences how learning occurs, a phenomenon also observed in human cognition.",
      "authors": [
        "Johannes A. Schubert",
        "Akshay Kumar Jagadish",
        "Marcel Binz",
        "Eric Schulz"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BNAvYSCrLD",
      "cdate": 1706870054837,
      "mdate": 1719287291339,
      "matched_keywords": [
        "reinforcement learning",
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525774"
    },
    {
      "id": "hJaWoU3Emh",
      "title": "Multiply Robust Estimation for Local Distribution Shifts with Multiple Domains",
      "abstract": "Distribution shifts are ubiquitous in real-world machine learning applications, posing a challenge to the generalization of models trained on one data distribution to another. We focus on scenarios where data distributions vary across multiple segments of the entire population and only make local assumptions about the differences between training and test (deployment) distributions within each segment. We propose a two-stage multiply robust estimation method to improve model performance on each individual segment for tabular data analysis. The method involves fitting a linear combination of the based models, learned using clusters of training data from multiple segments, followed by a refinement step for each segment. Our method is designed to be implemented with commonly used off-the-shelf machine learning models. We establish theoretical guarantees on the generalization bound of the method on the test risk. With extensive experiments on synthetic and real datasets, we demonstrate that the proposed method substantially improves over existing alternatives in prediction accuracy and robustness on both regression and classification tasks. We also assess its effectiveness on a user city prediction dataset from Meta.",
      "authors": [
        "Steven Wilkins-Reeves",
        "Xu Chen",
        "Qi Ma",
        "christine agarwal",
        "Aude Hofleitner"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hJaWoU3Emh",
      "cdate": 1706869954821,
      "mdate": 1719287291292,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525780"
    },
    {
      "id": "cj5HbaX14p",
      "title": "BOtied: Multi-objective Bayesian optimization with tied multivariate ranks",
      "abstract": "Many scientific and industrial applications require the joint optimization of multiple, potentially competing objectives. Multi-objective Bayesian optimization (MOBO) is a sample-efficient framework for identifying Pareto-optimal solutions. At the heart of MOBO is the acquisition function, which determines the next candidate to evaluate by navigating the best compromises among the objectives. Acquisition functions that rely on integrating over the objective space scale poorly to a large number of objectives. In this paper, we show a natural connection between the non-dominated solutions and the highest multivariate rank, which coincides with the extreme level line of the joint cumulative distribution function (CDF). Motivated by this link, we propose the CDF indicator, a Pareto-compliant metric for evaluating the quality of approximate Pareto sets, that can complement the popular hypervolume indicator. We then introduce an acquisition function based on the CDF indicator, called BOtied. BOtied can be implemented efficiently with copulas, a statistical tool for modeling complex, high-dimensional distributions. Our experiments on a variety of synthetic and real-world experiments demonstrate that BOtied outperforms state-of-the-art MOBO algorithms while being computationally efficient for many objectives.",
      "authors": [
        "Ji Won Park",
        "Natasa Tagasovska",
        "Michael Maser",
        "Stephen Ra",
        "Kyunghyun Cho"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=cj5HbaX14p",
      "cdate": 1706869916287,
      "mdate": 1719287291264,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525786"
    },
    {
      "id": "RDofzHLuX4",
      "title": "Estimating Distributional Treatment Effects in Randomized Experiments: Machine Learning for Variance Reduction",
      "abstract": "We propose a novel regression adjustment method designed for estimating distributional treatment effect parameters in randomized experiments. Randomized experiments have been extensively used to estimate treatment effects in various scientific fields. However, to gain deeper insights, it is essential to estimate distributional treatment effects rather than relying solely on average effects. Our approach incorporates pre-treatment covariates into a distributional regression framework, utilizing machine learning techniques to improve the precision of distributional treatment effect estimators. The proposed approach can be readily implemented with off-the-shelf machine learning methods and remains valid as long as the nuisance components are reasonably well estimated. Also, we establish the asymptotic properties of the proposed estimator and present a uniformly valid inference method. Through simulation results and real data analysis, we demonstrate the effectiveness of integrating machine learning techniques in reducing the variance of distributional treatment effect estimators in finite samples.",
      "authors": [
        "Undral Byambadalai",
        "Tatsushi Oka",
        "Shota Yasui"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=RDofzHLuX4",
      "cdate": 1706869683271,
      "mdate": 1719287291096,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525791"
    },
    {
      "id": "wUgTnf918v",
      "title": "An Interpretable Evaluation of Entropy-based Novelty of Generative Models",
      "abstract": "The massive developments of generative model frameworks require principled methods for the evaluation of a model's novelty compared to a reference dataset. While the literature has extensively studied the evaluation of the quality, diversity, and generalizability of generative models, the assessment of a model's novelty compared to a reference model has not been adequately explored in the machine learning community. In this work, we focus on the novelty assessment for multi-modal distributions and attempt to address the following differential clustering task: Given samples of a generative model $P_\\mathcal{G}$ and a reference model $P_\\mathrm{ref}$, how can we discover the sample types expressed by $P_\\mathcal{G}$ more frequently than in $P_\\mathrm{ref}$? We introduce a spectral approach to the differential clustering task and propose the Kernel-based Entropic Novelty (KEN) score to quantify the mode-based novelty of $P_\\mathcal{G}$ with respect to $P_\\mathrm{ref}$. We analyze the KEN score for mixture distributions with well-separable components and develop a kernel-based method to compute the KEN score from empirical data. We support the KEN framework by presenting numerical results on synthetic and real image datasets, indicating the framework's effectiveness in detecting novel modes and comparing generative models. The paper's code is available at: github.com/buyeah1109/KEN.",
      "authors": [
        "Jingwei Zhang",
        "Cheuk Ting Li",
        "Farzan Farnia"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=wUgTnf918v",
      "cdate": 1706869672944,
      "mdate": 1719287291076,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525796"
    },
    {
      "id": "TzqmqZS0nj",
      "title": "Can Machines Learn the True Probabilities?",
      "abstract": "When there exists uncertainty, AI machines are designed to make decisions so as to reach the best expected outcomes. Expectations are based on true facts about the objective environment the machines interact with, and those facts can be encoded into AI models in the form of true objective probability functions. Accordingly, AI models involve probabilistic machine learning in which the probabilities should be objectively interpreted. We prove under some basic assumptions when machines can learn the true objective probabilities, if any, and when machines cannot learn them.",
      "authors": [
        "Jinsook Kim"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=TzqmqZS0nj",
      "cdate": 1706869575205,
      "mdate": 1719287290986,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525802"
    },
    {
      "id": "a6wCNfIj8E",
      "title": "Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings",
      "abstract": "Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a *functional* reward encoding (FRE) as a general, scalable solution to this *zero-shot RL* problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformer-based variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zero-shot manner, given a small number of reward-annotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL and offline RL methods.",
      "authors": [
        "Kevin Frans",
        "Seohong Park",
        "Pieter Abbeel",
        "Sergey Levine"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=a6wCNfIj8E",
      "cdate": 1706869511954,
      "mdate": 1719287290957,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525808"
    },
    {
      "id": "WSpPC1Jm0p",
      "title": "Helpful or Harmful Data? Fine-tuning-free Shapley Attribution for Explaining Language Model Predictions",
      "abstract": "The increasing complexity of foundational models underscores the necessity for explainability, particularly for fine-tuning, the most widely used training method for adapting models to downstream tasks. Instance attribution, one type of explanation, attributes the model prediction to each training example by an instance score. However, the robustness of instance scores, specifically towards dataset resampling, has been overlooked. To bridge this gap, we propose a notion of robustness on the sign of the instance score. We theoretically and empirically demonstrate that the popular leave-one-out-based methods lack robustness, while the Shapley value behaves significantly better, but at a higher computational cost. Accordingly, we introduce an efficient fine-tuning-free approximation of the Shapley value (FreeShap) for instance attribution based on the neural tangent kernel. We empirically demonstrate that FreeShap outperforms other methods for instance attribution and other data-centric applications such as data removal, data selection, and wrong label detection, and further generalize our scale to large language models (LLMs). Our code is available at https://github.com/JTWang2000/FreeShap.",
      "authors": [
        "Jingtan Wang",
        "Xiaoqiang Lin",
        "Rui Qiao",
        "Chuan-Sheng Foo",
        "Bryan Kian Hsiang Low"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=WSpPC1Jm0p",
      "cdate": 1706869464210,
      "mdate": 1719287290906,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525814"
    },
    {
      "id": "LGz7GaUSEB",
      "title": "A Hierarchical Adaptive Multi-Task Reinforcement Learning Framework for Multiplier Circuit Design",
      "abstract": "Multiplier design---which aims to explore a large combinatorial design space to simultaneously optimize multiple conflicting objectives---is a fundamental problem in the integrated circuits industry. Although traditional approaches tackle the multi-objective multiplier optimization problem by manually designed heuristics, reinforcement learning (RL) offers a promising approach to discover high-speed and area-efficient multipliers. However, the existing RL-based methods struggle to find Pareto-optimal circuit designs for all possible preferences, i.e., weights over objectives, in a sample-efficient manner. To address this challenge, we propose a novel hierarchical adaptive (HAVE) multi-task reinforcement learning framework. The hierarchical framework consists of a meta-agent to generate diverse multiplier preferences, and an adaptive multi-task agent to collaboratively optimize multipliers conditioned on the dynamic preferences given by the meta-agent. To the best of our knowledge, HAVE is the first to well approximate Pareto-optimal circuit designs for the entire preference space with high sample efficiency. Experiments on multipliers across a wide range of input widths demonstrate that HAVE significantly Pareto-dominates state-of-the-art approaches, achieving up to 28% larger hypervolume. Moreover, experiments demonstrate that multipliers designed by HAVE can well generalize to large-scale computation-intensive circuits.",
      "authors": [
        "Zhihai Wang",
        "Jie Wang",
        "Dongsheng Zuo",
        "Ji Yunjie",
        "Xilin Xia",
        "Yuzhe Ma",
        "Jianye HAO",
        "Mingxuan Yuan",
        "Yongdong Zhang",
        "Feng Wu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LGz7GaUSEB",
      "cdate": 1706869448891,
      "mdate": 1719287290873,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525821"
    },
    {
      "id": "IejxxE9DO2",
      "title": "A Neural-Guided Dynamic Symbolic Network for Exploring Mathematical Expressions from Data",
      "abstract": "Symbolic regression (SR) is a powerful technique for discovering the underlying mathematical expressions from observed data. Inspired by the success of deep learning, recent deep generative SR methods have shown promising results. However, these methods face difficulties in processing high-dimensional problems and learning constants due to the large search space, and they don't scale well to unseen problems. In this work, we propose DySymNet, a novel neural-guided **Dy**namic **Sym**bolic **Net**work for SR. Instead of searching for expressions within a large search space, we explore symbolic networks with various structures, guided by reinforcement learning, and optimize them to identify expressions that better-fitting the data. Based on extensive numerical experiments on low-dimensional public standard benchmarks and the well-known SRBench with more variables, DySymNet shows clear superiority over several representative baseline models. Open source code is available at https://github.com/AILWQ/DySymNet.",
      "authors": [
        "Wenqiang Li",
        "Weijun Li",
        "Lina Yu",
        "Min Wu",
        "Linjun Sun",
        "Jingyi Liu",
        "Yanjie Li",
        "Shu Wei",
        "Deng Yusong",
        "Meilan Hao"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=IejxxE9DO2",
      "cdate": 1706869338387,
      "mdate": 1719287290764,
      "matched_keywords": [
        "deep learning",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525827"
    },
    {
      "id": "yh6Y7ppf46",
      "title": "Accelerating Legacy Numerical Solvers by Non-intrusive Gradient-based Meta-solving",
      "abstract": "Scientific computing is an essential tool for scientific discovery and engineering design, and its computational cost is always a main concern in practice. To accelerate scientific computing, it is a promising approach to use machine learning (especially meta-learning) techniques for selecting hyperparameters of traditional numerical methods. There have been numerous proposals to this direction, but many of them require automatic-differentiable numerical methods. However, in reality, many practical applications still depend on well-established but non-automatic-differentiable legacy codes, which prevents practitioners from applying the state-of-the-art research to their own problems. To resolve this problem, we propose a non-intrusive methodology with a novel gradient estimation technique to combine machine learning and legacy numerical codes without any modification. We theoretically and numerically show the advantage of the proposed method over other baselines and present applications of accelerating established non-automatic-differentiable numerical solvers implemented in PETSc, a widely used open-source numerical software library.",
      "authors": [
        "Sohei Arisaka",
        "Qianxiao Li"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=yh6Y7ppf46",
      "cdate": 1706869290063,
      "mdate": 1719287290740,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525832"
    },
    {
      "id": "lwTshcWlmB",
      "title": "Degeneration-free Policy Optimization: RL Fine-Tuning for Language Models without Degeneration",
      "abstract": "As the pre-training objectives (e.g., next token prediction) of language models (LMs) are inherently not aligned with task scores, optimizing LMs to achieve higher downstream task scores is essential. One of the promising approaches is to fine-tune LMs through reinforcement learning (RL). However, conventional RL methods based on PPO and a penalty of KL divergence are vulnerable to text degeneration where LMs do not generate natural texts anymore after RL fine-tuning. To address this problem, we provide Degeneration-free Policy Optimization (DfPO) that can fine-tune LMs to generate texts that achieve improved downstream task scores, while preserving the ability to generate natural texts. To achieve this, we introduce KL-masking which masks out the actions that potentially cause deviation from the reference policy when its likelihood is increased or decreased. Then, we devise truncated advantage functions for separately performing likelihood maximization and minimization to improve the task performance. In the experiments, we provide the results of DfPO and baseline algorithms on various generative NLP tasks including text continuation, text detoxification, and commonsense generation. Our experiments demonstrate that DfPO successfully improves the downstream task scores while preserving the ability to generate natural texts, without requiring additional hyperparameter search.",
      "authors": [
        "Youngsoo Jang",
        "Geon-Hyeong Kim",
        "Byoungjip Kim",
        "Yu Jin Kim",
        "Honglak Lee",
        "Moontae Lee"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lwTshcWlmB",
      "cdate": 1706869239243,
      "mdate": 1719287290683,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525838"
    },
    {
      "id": "QZd3rvlP76",
      "title": "Polynomial-based Self-Attention for Table Representation Learning",
      "abstract": "Structured data, which constitutes a significant portion of existing data types, has been a long-standing research topic in the field of machine learning. Various representation learning methods for tabular data have been proposed, ranging from encoder-decoder structures to Transformers. Among these, Transformer-based methods have achieved state-of-the-art performance not only in tabular data but also in various other fields, including computer vision and natural language processing. However, recent studies have revealed that self-attention, a key component of Transformers, can lead to an oversmoothing issue. We show that Transformers for tabular data also face this problem. To tackle the problem, we suggest a novel self-attention layer for tabular data, leveraging matrix polynomials. This proposed layer serves as a replacement for the original self-attention layer, contributing to the improvement of model scalability. In our experiments with three representative table learning models equipped with our proposed layer, we illustrate that the layer effectively mitigates the oversmoothing problem and enhances the representation performance of the existing methods, outperforming the state-of-the-art table representation methods.",
      "authors": [
        "Jayoung Kim",
        "Yehjin Shin",
        "Jeongwhan Choi",
        "Hyowon Wi",
        "Noseong Park"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=QZd3rvlP76",
      "cdate": 1706869024684,
      "mdate": 1719287290607,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525844"
    },
    {
      "id": "mUT1biz09t",
      "title": "Privacy-Preserving Instructions for Aligning Large Language Models",
      "abstract": "Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions. These instructions, which potentially contain sensitive information, are annotated by human workers in the process. This poses a new privacy risk not addressed by the typical private optimization. To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning. Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators. Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones. In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by showing comparable results to real instructions. In supervised fine-tuning, models trained with private synthetic instructions outperform leading open-source models such as Vicuna.",
      "authors": [
        "Da Yu",
        "Peter Kairouz",
        "Sewoong Oh",
        "Zheng Xu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=mUT1biz09t",
      "cdate": 1706869001643,
      "mdate": 1719287290583,
      "matched_keywords": [
        "reinforcement learning",
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525850"
    },
    {
      "id": "tQPkzTdaaN",
      "title": "PARDEN, Can You Repeat That? Defending against Jailbreaks via Repetition",
      "abstract": "Large language models (LLMs) have shown success in many natural language processing tasks. Despite rigorous safety alignment processes, supposedly safety-aligned LLMs like Llama 2 and Claude 2 are still susceptible to jailbreaks, leading to security risks and abuse of the models. One option to mitigate such risks is to augment the LLM with a dedicated \"safeguard\", which checks the LLM's inputs or outputs for undesired behaviour. A promising approach is to use the LLM itself as the safeguard. Nonetheless, baseline methods, such as prompting the LLM to self-classify toxic content, demonstrate limited efficacy. We hypothesise that this is due to domain shift: the alignment training imparts a self-censoring behaviour to the model (\"Sorry I can't do that\"), while the self-classify approach shifts it to a classification format (\"Is this prompt malicious\"). In this work, we propose PARDEN, which avoids this domain shift by simply asking the model to repeat its own outputs. PARDEN neither requires finetuning nor white box access to the model. We empirically verify the effectiveness of our method and show that PARDEN significantly outperforms existing jailbreak detection baselines for Llama-2 and Claude-2. We find that PARDEN is particularly powerful in the relevant regime of high True Positive Rate (TPR) and low False Positive Rate (FPR). For instance, for Llama2-7B, at TPR equal to 90%, PARDEN accomplishes a roughly 11x reduction in the FPR from 24.8% to 2.0% on the harmful behaviours dataset. Code and data are available at https://github.com/Ed-Zh/PARDEN.",
      "authors": [
        "Ziyang Zhang",
        "Qizhen Zhang",
        "Jakob Nicolaus Foerster"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tQPkzTdaaN",
      "cdate": 1706868957973,
      "mdate": 1719287290339,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525856"
    },
    {
      "id": "xSkIxKdO08",
      "title": "CF-OPT: Counterfactual Explanations for Structured Prediction",
      "abstract": "Optimization layers in deep neural networks have enjoyed a growing popularity in structured learning, improving the state of the art on a variety of applications. Yet, these pipelines lack interpretability since they are made of two opaque layers: a highly non-linear prediction model, such as a deep neural network, and an optimization layer, which is typically a complex black-box solver. Our goal is to improve the transparency of such methods by providing counterfactual explanations. We build upon variational autoencoders a principled way of obtaining counterfactuals: working in the latent space leads to a natural notion of plausibility of explanations. We finally introduce a variant of the classic loss for VAE training that improves their performance in our specific structured context. These provide the foundations of CF-OPT, a first-order optimization algorithm that can find counterfactual explanations for a broad class of structured learning architectures. Our numerical results show that both close and plausible explanations can be obtained for problems from the recent literature.",
      "authors": [
        "Germain Vivier-Ardisson",
        "Alexandre Forel",
        "Axel Parmentier",
        "Thibaut Vidal"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xSkIxKdO08",
      "cdate": 1706868720750,
      "mdate": 1719287290058,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525861"
    },
    {
      "id": "WofwaWjIf7",
      "title": "Cross-domain Open-world Discovery",
      "abstract": "In many real-world applications, test data may commonly exhibit categorical shifts, characterized by the emergence of novel classes, as well as distribution shifts arising from feature distributions different from the ones the model was trained on. However, existing methods either discover novel classes in the open-world setting or assume domain shifts without the ability to discover novel classes. In this work, we consider a cross-domain open-world discovery setting, where the goal is to assign samples to seen classes and discover unseen classes under a domain shift. To address this challenging problem, we present CROW, a prototype-based approach that introduces a cluster-then-match strategy enabled by a well-structured representation space of foundation models. In this way, CROW discovers novel classes by robustly matching clusters with previously seen classes, followed by fine-tuning the representation space using an objective designed for cross-domain open-world discovery. Extensive experimental results on image classification benchmark datasets demonstrate that CROW outperforms alternative baselines, achieving an 8% average performance improvement across 75 experimental settings.",
      "authors": [
        "Shuo Wen",
        "Maria Brbic"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=WofwaWjIf7",
      "cdate": 1706868672137,
      "mdate": 1719287289984,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525867"
    },
    {
      "id": "rSfzchjIYu",
      "title": "Effective Federated Graph Matching",
      "abstract": "Graph matching in the setting of federated learning is still an open problem. This paper proposes an unsupervised federated graph matching algorithm, UFGM, for inferring matched node pairs on different graphs across clients while maintaining privacy requirement, by leveraging graphlet theory and trust region optimization. First, the nodes' graphlet features are captured to generate pseudo matched node pairs on different graphs across clients as pseudo training data for tackling the dilemma of unsupervised graph matching in federated setting and leveraging the strength of supervised graph matching. An approximate graphlet enumeration method is proposed to sample a small number of graphlets and capture nodes' graphlet features. Theoretical analysis is conducted to demonstrate that the approximate method is able to maintain the quality of graphlet estimation while reducing its expensive cost. Second, we propose a separate trust region algorithm for pseudo supervised federated graph matching while maintaining the privacy constraints. In order to avoid expensive cost of the second-order Hessian computation in the trust region algorithm, we propose two weak quasi-Newton conditions to construct a positive definite scalar matrix as the Hessian approximation with only first-order gradients. We theoretically derive the error introduced by the separate trust region due to the Hessian approximation and conduct the convergence analysis of the approximation method.",
      "authors": [
        "Yang Zhou",
        "Zijie Zhang",
        "Zeru Zhang",
        "Lingjuan Lyu",
        "Wei-Shinn Ku"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rSfzchjIYu",
      "cdate": 1706868667818,
      "mdate": 1719287289931,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525873"
    },
    {
      "id": "q0lxAs5GGO",
      "title": "Disentanglement Learning via Topology",
      "abstract": "We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding a multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art methods are based on VAE and encourage the joint distribution of latent variables to be factorized. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement learning. Our experiments have shown that the proposed TopDis loss improves disentanglement scores such as MIG, FactorVAE score, SAP score, and DCI disentanglement score with respect to state-of-the-art results while preserving the reconstruction quality. Our method works in an unsupervised manner, permitting us to apply it to problems without labeled factors of variation. The TopDis loss works even when factors of variation are correlated. Additionally, we show how to use the proposed topological loss to find disentangled directions in a trained GAN.",
      "authors": [
        "Nikita Balabin",
        "Daria Voronkova",
        "Ilya Trofimov",
        "Evgeny Burnaev",
        "Serguei Barannikov"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=q0lxAs5GGO",
      "cdate": 1706868628214,
      "mdate": 1719287289890,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525878"
    },
    {
      "id": "jP8mf34iCW",
      "title": "Training Greedy Policy for Proposal Batch Selection in Expensive Multi-Objective Combinatorial Optimization",
      "abstract": "Active learning is increasingly adopted for expensive multi-objective combinatorial optimization problems, but it involves a challenging subset selection problem, optimizing the batch acquisition score that quantifies the goodness of a batch for evaluation. Due to the excessively large search space of the subset selection problem, prior methods optimize the batch acquisition on the latent space, which has discrepancies with the actual space, or optimize individual acquisition scores without considering the dependencies among candidates in a batch instead of directly optimizing the batch acquisition. To manage the vast search space, a simple and effective approach is the greedy method, which decomposes the problem into smaller subproblems, yet it has difficulty in parallelization since each subproblem depends on the outcome from the previous ones. To this end, we introduce a novel greedy-style subset selection algorithm that optimizes batch acquisition directly on the combinatorial space by sequential greedy sampling from the greedy policy, specifically trained to address all greedy subproblems concurrently. Notably, our experiments on the red fluorescent proteins design task show that our proposed method achieves the baseline performance in 1.69x fewer queries, demonstrating its efficiency.",
      "authors": [
        "Deokjae Lee",
        "Hyun Oh Song",
        "Kyunghyun Cho"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jP8mf34iCW",
      "cdate": 1706868371556,
      "mdate": 1719371304491,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525885"
    },
    {
      "id": "Vw4Yar2fmW",
      "title": "Self-Consistency Training for Density-Functional-Theory Hamiltonian Prediction",
      "abstract": "Predicting the mean-field Hamiltonian matrix in density functional theory is a fundamental formulation to leverage machine learning for solving molecular science problems. Yet, its applicability is limited by insufficient labeled data for training. In this work, we highlight that Hamiltonian prediction possesses a self-consistency principle, based on which we propose self-consistency training, an exact training method that does not require labeled data. It distinguishes the task from predicting other molecular properties by the following benefits: (1) it enables the model to be trained on a large amount of unlabeled data, hence addresses the data scarcity challenge and enhances generalization; (2) it is more efficient than running DFT to generate labels for supervised training, since it amortizes DFT calculation over a set of queries. We empirically demonstrate the better generalization in data-scarce and out-of-distribution scenarios, and the better efficiency over DFT labeling. These benefits push forward the applicability of Hamiltonian prediction to an ever-larger scale.",
      "authors": [
        "He Zhang",
        "Chang Liu",
        "Zun Wang",
        "Xinran Wei",
        "Siyuan Liu",
        "Nanning Zheng",
        "Bin Shao",
        "Tie-Yan Liu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Vw4Yar2fmW",
      "cdate": 1706868364205,
      "mdate": 1719287289754,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525891"
    },
    {
      "id": "znz261CQK7",
      "title": "Bias of Stochastic Gradient Descent or the Architecture: Disentangling the Effects of Overparameterization of Neural Networks",
      "abstract": "Neural networks typically generalize well when fitting the data perfectly, even though they are heavily overparameterized. Many factors have been pointed out as the reason for this phenomenon, including an implicit bias of stochastic gradient descent (SGD) and a possible simplicity bias arising from the neural network architecture. The goal of this paper is to disentangle the factors that influence generalization stemming from optimization and architectural choices by studying *random* and *SGD-optimized* networks that achieve zero training error. We experimentally show, in the low sample regime, that overparameterization in terms of increasing width is beneficial for generalization, and this benefit is due to the bias of SGD and not due to an architectural bias. In contrast, for increasing depth, overparameterization is detrimental for generalization, but random and SGD-optimized networks behave similarly, so this can be attributed to an architectural bias.",
      "authors": [
        "Amit Peleg",
        "Matthias Hein"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=znz261CQK7",
      "cdate": 1706868245109,
      "mdate": 1719287289605,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525897"
    },
    {
      "id": "pvg1OdUtDQ",
      "title": "DiNADO: Norm-Disentangled Neurally-Decomposed Oracles for Controlling Language Models",
      "abstract": "NeurAlly-Decomposed Oracle (NADO) is a powerful approach for controllable generation with large language models. It is designed to avoid catastrophic forgetting while achieving guaranteed convergence to an entropy-maximized closed-form optimal solution with reasonable modeling capacity. Despite the success, several challenges arise when apply NADO to a wide range of scenarios. Vanilla NADO suffers from gradient vanishing for low-probability control signals and is highly reliant on a regularization to satisfy the stochastic version of Bellman equation. In addition, the vanilla implementation of NADO introduces a few additional transformer layers, suffering from a limited capacity especially compared to other finetune-based model adaptation methods like LoRA. In this paper, we propose a improved version of the NADO algorithm, namely DiNADO (norm-**Di**sentangled **N**eur**A**lly-**D**ecomposed **O**racles), which improves the performance of the NADO algorithm through disentangling the step-wise global norm over the approximated oracle $R$-value for all potential next-tokens, allowing DiNADO to be combined with finetuning methods like LoRA. We discuss in depth how DiNADO achieves better capacity, stability and flexibility with both empirical and theoretical results. Experiments on formality control in machine translation and the lexically constrained generation task CommonGen demonstrates the significance of the improvements.",
      "authors": [
        "Sidi Lu",
        "Wenbo Zhao",
        "Chenyang Tao",
        "Arpit Gupta",
        "Shanchan Wu",
        "Tagyoung Chung",
        "Nanyun Peng"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pvg1OdUtDQ",
      "cdate": 1706868237463,
      "mdate": 1719287289545,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525902"
    },
    {
      "id": "FVvf69a5rx",
      "title": "MOMENT: A Family of Open Time-series Foundation Models",
      "abstract": "We introduce MOMENT, a family of open-source foundation models for general-purpose time series analysis. Pre-training large models on time series data is challenging due to (1) the absence of a large and cohesive public time series repository, and (2) diverse time series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time series, called the Time series Pile, and systematically tackle time series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning. Finally, we present several interesting empirical observations about large pre-trained time series models. Pre-trained models (AutonLab/MOMENT-1-large) and Time Series Pile (AutonLab/Timeseries-PILE) are available on [Huggingface](https://huggingface.co/AutonLab).",
      "authors": [
        "Mononito Goswami",
        "Konrad Szafer",
        "Arjun Choudhry",
        "Yifu Cai",
        "Shuo Li",
        "Artur Dubrawski"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FVvf69a5rx",
      "cdate": 1706868216406,
      "mdate": 1719287289439,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525908"
    },
    {
      "id": "ETNx4SekbY",
      "title": "Observable Propagation: Uncovering Feature Vectors in Transformers",
      "abstract": "A key goal of current mechanistic interpretability research in NLP is to find *linear features* (also called \"feature vectors\") for transformers: directions in activation space corresponding to concepts that are used by a given model in its computation. Present state-of-the-art methods for finding linear features require large amounts of labelled data -- both laborious to acquire and computationally expensive to utilize. In this work, we introduce a novel method, called \"observable propagation\" (in short: ObProp), for finding linear features used by transformer language models in computing a given task -- *using almost no data*. Our paradigm centers on the concept of \"observables\", linear functionals corresponding to given tasks. We then introduce a mathematical theory for the analysis of feature vectors, including a similarity metric between feature vectors called the *coupling coefficient* which estimates the degree to which one feature's output correlates with another's. We use ObProp to perform extensive qualitative investigations into several tasks, including gendered occupational bias, political party prediction, and programming language detection. Our results suggest that ObProp surpasses traditional approaches for finding feature vectors in the low-data regime, and that ObProp can be used to better understand the mechanisms responsible for bias in large language models.",
      "authors": [
        "Jacob Dunefsky",
        "Arman Cohan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ETNx4SekbY",
      "cdate": 1706868208795,
      "mdate": 1719287289418,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525913"
    },
    {
      "id": "upO8FUwf92",
      "title": "Towards Compositionality in Concept Learning",
      "abstract": "Concept-based interpretability methods offer a lens into the internals of foundation models by decomposing their embeddings into high-level concepts. These concept representations are most useful when they are *compositional*, meaning that the individual concepts compose to explain the full sample. We show that existing unsupervised concept extraction methods find concepts which are not compositional. To automatically discover compositional concept representations, we identify two salient properties of such representations, and propose Compositional Concept Extraction (CCE) for finding concepts which obey these properties. We evaluate CCE on five different datasets over image and text data. Our evaluation shows that CCE finds more compositional concept representations than baselines and yields better accuracy on four downstream classification tasks.",
      "authors": [
        "Adam Stein",
        "Aaditya Naik",
        "Yinjun Wu",
        "Mayur Naik",
        "Eric Wong"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=upO8FUwf92",
      "cdate": 1706868197009,
      "mdate": 1719287289361,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525919"
    },
    {
      "id": "pEWAcejiU2",
      "title": "Better & Faster Large Language Models via Multi-token Prediction",
      "abstract": "Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following $n$ tokens using $n$ independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12% more problems on Human Eval and 17% more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to $3\\times$ faster at inference, even with large batch sizes.",
      "authors": [
        "Fabian Gloeckle",
        "Badr Youbi Idrissi",
        "Baptiste Roziere",
        "David Lopez-Paz",
        "Gabriel Synnaeve"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pEWAcejiU2",
      "cdate": 1706868173505,
      "mdate": 1719287289267,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525924"
    },
    {
      "id": "qwQVV5R8Y7",
      "title": "$S^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting",
      "abstract": "Recently, there has been a growing interest in leveraging pre-trained large language models (LLMs) for various time series applications. However, the semantic space of LLMs, established through the pre-training, is still underexplored and may help yield more distinctive and informative representations to facilitate time series forecasting. To this end, we propose Semantic Space Informed Prompt learning with LLM ($S^2$IP-LLM) to align the pre-trained semantic space with time series embedding space and perform time series forecasting based on learned prompts from the joint space. We first design a tokenization module tailored for cross-modality alignment, which explicitly concatenates patches of decomposed time series components to create embeddings that effectively encode the temporal dynamics. Next, we leverage the pre-trained word token embeddings to derive semantic anchors and align selected anchors with time series embeddings by maximizing the cosine similarity in the joint space. This way, $S^2$IP-LLM can retrieve relevant semantic anchors as prompts to provide strong indicators (context) for time series that exhibit different temporal dynamics. With thorough empirical studies on multiple benchmark datasets, we demonstrate that the proposed $S^2$IP-LLM can achieve superior forecasting performance over state-of-the-art baselines. Furthermore, our ablation studies and visualizations verify the necessity of prompt learning informed by semantic space.",
      "authors": [
        "Zijie Pan",
        "Yushan Jiang",
        "Sahil Garg",
        "Anderson Schneider",
        "Yuriy Nevmyvaka",
        "Dongjin Song"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qwQVV5R8Y7",
      "cdate": 1706868073892,
      "mdate": 1719287289223,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525931"
    },
    {
      "id": "2T00oYk54P",
      "title": "Explaining Graph Neural Networks via Structure-aware Interaction Index",
      "abstract": "The Shapley value is a prominent tool for interpreting black-box machine learning models thanks to its strong theoretical foundation. However, for models with structured inputs, such as graph neural networks, existing Shapley-based explainability approaches either focus solely on node-wise importance or neglect the graph structure when perturbing the input instance. This paper introduces the Myerson-Taylor interaction index that internalizes the graph structure into attributing the node values and the interaction values among nodes. Unlike the Shapley-based methods, the Myerson-Taylor index decomposes coalitions into components satisfying a pre-chosen connectivity criterion. We prove that the Myerson-Taylor index is the unique one that satisfies a system of five natural axioms accounting for graph structure and high-order interaction among nodes. Leveraging these properties, we propose Myerson-Taylor Structure-Aware Graph Explainer (MAGE), a novel explainer that uses the second-order Myerson-Taylor index to identify the most important motifs influencing the model prediction, both positively and negatively. Extensive experiments on various graph datasets and models demonstrate that our method consistently provides superior subgraph explanations compared to state-of-the-art methods.",
      "authors": [
        "Ngoc Bui",
        "Hieu Trung Nguyen",
        "Viet Anh Nguyen",
        "Rex Ying"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=2T00oYk54P",
      "cdate": 1706868058816,
      "mdate": 1719287289136,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525937"
    },
    {
      "id": "If4xW9vF7U",
      "title": "Training-Free Long-Context Scaling of Large Language Models",
      "abstract": "The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of finetuning large-scale models with longer sequences, we propose a training-free approach named Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of up to 100k tokens. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of models built through continual training. All code and data used in this work are released at https://github.com/HKUNLP/ChunkLlama.",
      "authors": [
        "Chenxin An",
        "Fei Huang",
        "Jun Zhang",
        "Shansan Gong",
        "Xipeng Qiu",
        "Chang Zhou",
        "Lingpeng Kong"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=If4xW9vF7U",
      "cdate": 1706867854027,
      "mdate": 1719287289016,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525942"
    },
    {
      "id": "TN3fi7dwPo",
      "title": "Tandem Transformers for Inference Efficient LLMs",
      "abstract": "The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative (Leviathan et al., 2023) and parallel (Stern et al., 2018) decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations. We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream performance. We further incorporate the Tandem model within the speculative decoding (SPEED) framework where the large model validates tokens from the small model. This ensures that the tandem of PaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster than using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream task accuracy.",
      "authors": [
        "Aishwarya P S",
        "Pranav Ajit Nair",
        "Yashas Samaga B L",
        "Toby James Boyd",
        "Sanjiv Kumar",
        "Prateek Jain",
        "Praneeth Netrapalli"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=TN3fi7dwPo",
      "cdate": 1706867800782,
      "mdate": 1719287288826,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525948"
    },
    {
      "id": "1V50J0emll",
      "title": "Physics and Lie symmetry informed Gaussian processes",
      "abstract": "Physics-informed machine learning (PIML) has established itself as a new scientific paradigm which enables the seamless integration of observational data with partial differential equation (PDE) based physics models. A powerful tool for the analysis, reduction and solution of PDEs is the Lie symmetry method. Nevertheless, only recently has the integration of such symmetries into PIML frameworks begun to be explored. The present work adds to this growing literature by introducing an approach for incorporating a Lie symmetry into a physics-informed Gaussian process (GP) model. The symmetry is introduced as a constraint on the GP; either in a soft manner via virtual observations of an induced PDE called the invariant surface condition, or explicitly through the design of the kernel. Experimental results demonstrate that the use of symmetry constraints improves the performance of the GP for both forward and inverse problems, and that our approach offers competitive performance with neural networks in the low-data environment.",
      "authors": [
        "David Dalton",
        "Dirk Husmeier",
        "Hao Gao"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=1V50J0emll",
      "cdate": 1706867760954,
      "mdate": 1719287288825,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525954"
    },
    {
      "id": "Xgrey8uQhr",
      "title": "Graph Structure Extrapolation for Out-of-Distribution Generalization",
      "abstract": "Out-of-distribution (OOD) generalization deals with the prevalent learning scenario where test distribution shifts from training distribution. With rising application demands and inherent complexity, graph OOD problems call for specialized solutions. While data-centric methods exhibit performance enhancements on many generic machine learning tasks, there is a notable absence of data augmentation methods tailored for graph OOD generalization. In this work, we propose to achieve graph OOD generalization with the novel design of non-Euclidean-space linear extrapolation. The proposed augmentation strategy extrapolates structure spaces to generate OOD graph data. Our design tailors OOD samples for specific shifts without corrupting underlying causal mechanisms. Theoretical analysis and empirical results evidence the effectiveness of our method in solving target shifts, showing substantial and constant improvements across various graph OOD tasks.",
      "authors": [
        "Xiner Li",
        "Shurui Gui",
        "Youzhi Luo",
        "Shuiwang Ji"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Xgrey8uQhr",
      "cdate": 1706867732829,
      "mdate": 1719287288768,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525959"
    },
    {
      "id": "CrUmgUaAQp",
      "title": "Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs",
      "abstract": "Recent advancements in large language models (LLMs) underscore their potential for responding to inquiries in various domains. However, ensuring that generative agents provide accurate and reliable answers remains an ongoing challenge. In this context, multi-agent debate (MAD) has emerged as a promising strategy for enhancing the truthfulness of LLMs. We benchmark a range of debating and prompting strategies to explore the trade-offs between cost, time, and accuracy. Importantly, we find that multi-agent debating systems, in their current form, do not reliably outperform other proposed prompting strategies, such as self-consistency and ensembling using multiple reasoning paths. However, when performing hyperparameter tuning, several MAD systems, such as Multi-Persona, perform better. This suggests that MAD protocols might not be inherently worse than other approaches, but that they are more sensitive to different hyperparameter settings and difficult to optimize. We build on these results to offer insights into improving debating strategies, such as adjusting agent agreement levels, which can significantly enhance performance and even surpass all other non-debate protocols we evaluated. We provide an open-source repository to the community with several state-of-the-art protocols together with evaluation scripts to benchmark across popular research datasets.",
      "authors": [
        "Andries Petrus Smit",
        "Nathan Grinsztajn",
        "Paul Duckworth",
        "Thomas D Barrett",
        "Arnu Pretorius"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=CrUmgUaAQp",
      "cdate": 1706867720599,
      "mdate": 1719287288727,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525965"
    },
    {
      "id": "jKYyFbH8ap",
      "title": "Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks",
      "abstract": "We introduce **S**yntax-**A**ware **F**ill-**i**n-the-**M**iddle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future research in effective pretraining strategies for code LLMs. The evaluation toolkit and dataset are available at https://github.com/gonglinyuan/safim, and the leaderboard is available at https://safimbenchmark.com.",
      "authors": [
        "Linyuan Gong",
        "Sida Wang",
        "Mostafa Elhoushi",
        "Alvin Cheung"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jKYyFbH8ap",
      "cdate": 1706867715263,
      "mdate": 1719287288727,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525970"
    },
    {
      "id": "D5IRvFF1lN",
      "title": "Learning-Efficient Yet Generalizable Collaborative Filtering for Item Recommendation",
      "abstract": "The weighted squared loss is a common component in several Collaborative Filtering (CF) algorithms for item recommendation, including the representative implicit Alternating Least Squares (iALS). Despite its widespread use, this loss function lacks a clear connection to ranking objectives such as Discounted Cumulative Gain (DCG), posing a fundamental challenge in explaining the exceptional ranking performance observed in these algorithms. In this work, we make a breakthrough by establishing a connection between squared loss and ranking metrics through a Taylor expansion of the DCG-consistent surrogate loss—softmax loss. We also discover a new surrogate squared loss function, namely Ranking-Generalizable Squared (RG$^2$) loss, and conduct thorough theoretical analyses on the DCG-consistency of the proposed loss function. Later, we present an example of utilizing the RG$^2$ loss with Matrix Factorization (MF), coupled with a generalization upper bound and an ALS optimization algorithm that leverages closed-form solutions over all items. Experimental results over three public datasets demonstrate the effectiveness of the RG$^2$ loss, exhibiting ranking performance on par with, or even surpassing, the softmax loss while achieving faster convergence.",
      "authors": [
        "Yuanhao Pu",
        "Xiaolong Chen",
        "Xu Huang",
        "Jin Chen",
        "Defu Lian",
        "Enhong Chen"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=D5IRvFF1lN",
      "cdate": 1706867503742,
      "mdate": 1719287288581,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525976"
    },
    {
      "id": "Vdr87ZUfnl",
      "title": "Reinforcement Learning and Regret Bounds for Admission Control",
      "abstract": "The expected regret of any reinforcement learning algorithm is lower bounded by $\\Omega\\left(\\sqrt{DXAT}\\right)$ for undiscounted returns, where $D$ is the diameter of the Markov decision process, $X$ the size of the state space, $A$ the size of the action space and $T$ the number of time steps. However, this lower bound is general. A smaller regret can be obtained by taking into account some specific knowledge of the problem structure. In this article, we consider an admission control problem to an $M/M/c/S$ queue with $m$ job classes and class-dependent rewards and holding costs. Queuing systems often have a diameter that is exponential in the buffer size $S$, making the previous lower bound prohibitive for any practical use. We propose an algorithm inspired by UCRL2, and use the structure of the problem to upper bound the expected total regret by $O(S\\log T + \\sqrt{mT \\log T})$ in the finite server case. In the infinite server case, we prove that the dependence of the regret on $S$ disappears.",
      "authors": [
        "Lucas Weber",
        "Ana Busic",
        "Jiamin Zhu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Vdr87ZUfnl",
      "cdate": 1706867463261,
      "mdate": 1719287288494,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.525983"
    },
    {
      "id": "cBWVJh5Fvf",
      "title": "AST-T5: Structure-Aware Pretraining for Code Generation and Understanding",
      "abstract": "Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids complex program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks including HumanEval and MBPP. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in CodeXGLUE. Our code and model are publicly available at https://github.com/gonglinyuan/ast_t5.",
      "authors": [
        "Linyuan Gong",
        "Mostafa Elhoushi",
        "Alvin Cheung"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=cBWVJh5Fvf",
      "cdate": 1706867390382,
      "mdate": 1719287288461,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.525989"
    },
    {
      "id": "0tuwdgBiSN",
      "title": "Complexity Matters: Feature Learning in the Presence of Spurious Correlations",
      "abstract": "Existing research often posits spurious features as **easier** to learn than core features in neural network optimization, but the impact of their relative simplicity remains under-explored. Moreover, studies mainly focus on end performance rather than the learning dynamics of feature learning. In this paper, we propose a theoretical framework and an associated synthetic dataset grounded in boolean function analysis. This setup allows for fine-grained control over the relative complexity (compared to core features) and correlation strength (with respect to the label) of spurious features to study the dynamics of feature learning under spurious correlations. Our findings uncover several interesting phenomena: (1) stronger spurious correlations or simpler spurious features slow down the learning rate of the core features, (2) two distinct subnetworks are formed to learn core and spurious features separately, (3) learning phases of spurious and core features are not always separable, (4) spurious features are not forgotten even after core features are fully learned. We demonstrate that our findings justify the success of retraining the last layer to remove spurious correlation and also identifies limitations of popular debiasing algorithms that exploit early learning of spurious features. We support our empirical findings with theoretical analyses for the case of learning XOR features with a one-hidden-layer ReLU network.",
      "authors": [
        "GuanWen Qiu",
        "Da Kuang",
        "Surbhi Goel"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0tuwdgBiSN",
      "cdate": 1706867383023,
      "mdate": 1719287288432,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.525995"
    },
    {
      "id": "mxjB0LIgpT",
      "title": "Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?",
      "abstract": "Trustworthy ML systems should not only return accurate predictions, but also a reliable representation of their uncertainty. Bayesian methods are commonly used to quantify both aleatoric and epistemic uncertainty, but alternative approaches, such as evidential deep learning methods, have become popular in recent years. The latter group of methods in essence extends empirical risk minimization (ERM) for predicting second-order probability distributions over outcomes, from which measures of epistemic (and aleatoric) uncertainty can be extracted. This paper presents novel theoretical insights of evidential deep learning, highlighting the difficulties in optimizing second-order loss functions and interpreting the resulting epistemic uncertainty measures. With a systematic setup that covers a wide range of approaches for classification, regression and counts, it provides novel insights into issues of identifiability and convergence in second-order loss minimization, and the relative (rather than absolute) nature of epistemic uncertainty measures.",
      "authors": [
        "Mira Juergens",
        "Nis Meinert",
        "Viktor Bengs",
        "Eyke Hüllermeier",
        "Willem Waegeman"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=mxjB0LIgpT",
      "cdate": 1706867367976,
      "mdate": 1719287288387,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526001"
    },
    {
      "id": "DwwI9L67B5",
      "title": "State-Free Inference of State-Space Models: The *Transfer Function* Approach",
      "abstract": "We approach designing a state-space model for deep learning applications through its dual representation, the *transfer function*, and uncover a highly efficient sequence parallel inference algorithm that is *state-free*: unlike other proposed algorithms, state-free inference does not incur any significant memory or computational cost with an increase in state size. We achieve this using properties of the proposed frequency domain transfer function parametrization, which enables direct computation of its corresponding convolutional kernel's spectrum via a single Fast Fourier Transform. Our experimental results across multiple sequence lengths and state sizes illustrates, on average, a 35% training speed improvement over S4 layers -- parametrized in time-domain -- on the Long Range Arena benchmark, while delivering state-of-the-art downstream performances over other attention-free approaches. Moreover, we report improved perplexity in language modeling over a long convolutional Hyena baseline, by simply introducing our transfer function parametrization. Our code is available at https://github.com/ruke1ire/RTF.",
      "authors": [
        "Rom Parnichkun",
        "Stefano Massaroli",
        "Alessandro Moro",
        "Jimmy T.H. Smith",
        "Ramin Hasani",
        "Mathias Lechner",
        "Qi An",
        "Christopher Re",
        "Hajime Asama",
        "Stefano Ermon",
        "Taiji Suzuki",
        "Michael Poli",
        "Atsushi Yamashita"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DwwI9L67B5",
      "cdate": 1706867309201,
      "mdate": 1719287288372,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526007"
    },
    {
      "id": "olbTrkWo1D",
      "title": "Mitigating Catastrophic Forgetting in Online Continual Learning by Modeling Previous Task Interrelations via Pareto Optimization",
      "abstract": "Catastrophic forgetting remains a core challenge in continual learning (CL), where the models struggle to retain previous knowledge when learning new tasks. While existing replay-based CL methods have been proposed to tackle this challenge by utilizing a memory buffer to store data from previous tasks, they generally overlook the interdependence between previously learned tasks and fail to encapsulate the optimally integrated knowledge in previous tasks, leading to sub-optimal performance of the previous tasks. Against this issue, we first reformulate replay-based CL methods as a unified hierarchical gradient aggregation framework. We then incorporate the Pareto optimization to capture the interrelationship among previously learned tasks and design a Pareto-Optimized CL algorithm (POCL), which effectively enhances the overall performance of past tasks while ensuring the performance of the current task. Comprehensive empirical results demonstrate that the proposed POCL outperforms current state-of-the-art CL methods across multiple datasets and different settings.",
      "authors": [
        "Yichen Wu",
        "Hong Wang",
        "Peilin Zhao",
        "Yefeng Zheng",
        "Ying Wei",
        "Long-Kai Huang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=olbTrkWo1D",
      "cdate": 1706867251969,
      "mdate": 1719287288296,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526012"
    },
    {
      "id": "XT6iF8FDZx",
      "title": "Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States",
      "abstract": "In modern machine learning, models can often fit training data in numerous ways, some of which perform well on unseen (test) data, while others do not. Remarkably, in such cases gradient descent frequently exhibits an implicit bias that leads to excellent performance on unseen data. This implicit bias was extensively studied in supervised learning, but is far less understood in optimal control (reinforcement learning). There, learning a controller applied to a system via gradient descent is known as policy gradient, and a question of prime importance is the extent to which a learned controller extrapolates to unseen initial states. This paper theoretically studies the implicit bias of policy gradient in terms of extrapolation to unseen initial states. Focusing on the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the extent of extrapolation depends on the degree of exploration induced by the system when commencing from initial states included in training. Experiments corroborate our theory, and demonstrate its conclusions on problems beyond LQR, where systems are non-linear and controllers are neural networks. We hypothesize that real-world optimal control may be greatly improved by developing methods for informed selection of initial states to train on.",
      "authors": [
        "Noam Razin",
        "Yotam Alexander",
        "Edo Cohen-Karlik",
        "Raja Giryes",
        "Amir Globerson",
        "Nadav Cohen"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=XT6iF8FDZx",
      "cdate": 1706867223534,
      "mdate": 1719287288251,
      "matched_keywords": [
        "machine learning",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526099"
    },
    {
      "id": "OJTKlubFk1",
      "title": "Error Feedback Can Accurately Compress Preconditioners",
      "abstract": "Leveraging second-order information about the loss at the scale of deep networks is one of the main lines of approach for improving the performance of current optimizers for deep learning. Yet, existing approaches for accurate full-matrix preconditioning, such as Full-Matrix Adagrad (GGT) or Matrix-Free Approximate Curvature (M-FAC) suffer from massive storage costs when applied even to small-scale models, as they must store a sliding window of gradients, whose memory requirements are multiplicative in the model dimension. In this paper, we address this issue via a novel and efficient error-feedback technique that can be applied to compress preconditioners by up to two orders of magnitude in practice, without loss of convergence. Specifically, our approach compresses the gradient information via sparsification or low-rank compression before it is fed into the preconditioner, feeding the compression error back into future iterations. Extensive experiments on deep neural networks show that this approach can compress full-matrix preconditioners to up to 99% sparsity without accuracy loss, effectively removing the memory overhead of fullmatrix preconditioners such as GGT and M-FAC.",
      "authors": [
        "Ionut-Vlad Modoranu",
        "Aleksei Kalinov",
        "Eldar Kurtic",
        "Elias Frantar",
        "Dan Alistarh"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OJTKlubFk1",
      "cdate": 1706867152565,
      "mdate": 1719287288174,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526106"
    },
    {
      "id": "OQ97v7uRGc",
      "title": "On The Complexity of First-Order Methods in Stochastic Bilevel Optimization",
      "abstract": "We consider the problem of finding stationary points in Bilevel optimization when the lower-level problem is unconstrained and strongly convex. The problem has been extensively studied in recent years; the main technical challenge is to keep track of lower-level solutions $y^*(x)$ in response to the changes in the upper-level variables $x$. Subsequently, all existing approaches tie their analyses to a genie algorithm that knows lower-level solutions and, therefore, need not query any points far from them. We consider a dual question to such approaches: suppose we have an oracle, which we call $y^*$-aware, that returns an $O(\\epsilon)$-estimate of the lower-level solution, in addition to first-order gradient estimators *locally unbiased* within the $\\Theta(\\epsilon)$-ball around $y^*(x)$. We study the complexity of finding stationary points with such an $y^*$-aware oracle: we propose a simple first-order method that converges to an $\\epsilon$ stationary point using $O(\\epsilon^{-6}), O(\\epsilon^{-4})$ access to first-order $y^*$-aware oracles. Our upper bounds also apply to standard unbiased first-order oracles, improving the best-known complexity of first-order methods by $O(\\epsilon)$ with minimal assumptions. We then provide the matching $\\Omega(\\epsilon^{-6})$, $\\Omega(\\epsilon^{-4})$ lower bounds without and with an additional smoothness assumption, respectively. Our results imply that any approach that simulates an algorithm with an $y^*$-aware oracle must suffer the same lower bounds.",
      "authors": [
        "Jeongyeol Kwon",
        "Dohyun Kwon",
        "Hanbaek Lyu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OQ97v7uRGc",
      "cdate": 1706867043101,
      "mdate": 1719287288061,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526111"
    },
    {
      "id": "9CCoVyFuEp",
      "title": "Loss Shaping Constraints for Long-Term Time Series Forecasting",
      "abstract": "Several applications in time series forecasting require predicting multiple steps ahead. Despite the vast amount of literature in the topic, both classical and recent deep learning based approaches have mostly focused on minimising performance averaged over the predicted window. We observe that this can lead to disparate distributions of errors across forecasting steps, especially for recent transformer architectures trained on popular forecasting benchmarks. That is, optimising performance on average can lead to undesirably large errors at specific time-steps. In this work, we present a Constrained Learning approach for long-term time series forecasting that aims to find the best model in terms of average performance that respects a user-defined upper bound on the loss at each time-step. We call our approach loss shaping constraints because it imposes constraints on the loss at each time step, and leverage recent duality results to show that despite its non-convexity, the resulting problem has a bounded duality gap. We propose a practical primal-dual algorithm to tackle it, and demonstrate that the proposed approach exhibits competitive average performance in time series forecasting benchmarks, while shaping the distribution of errors across the predicted window.",
      "authors": [
        "Ignacio Hounie",
        "Javier Porras-Valenzuela",
        "Alejandro Ribeiro"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9CCoVyFuEp",
      "cdate": 1706866989852,
      "mdate": 1719287287985,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526116"
    },
    {
      "id": "hunSEjeCPE",
      "title": "Energy-Guided Diffusion Sampling for Offline-to-Online Reinforcement Learning",
      "abstract": "Combining offline and online reinforcement learning (RL) techniques is indeed crucial for achieving efficient and safe learning where data acquisition is expensive. Existing methods replay offline data directly in the online phase, resulting in a significant challenge of data distribution shift and subsequently causing inefficiency in online fine-tuning. To address this issue, we introduce an innovative approach, **E**nergy-guided **DI**ffusion **S**ampling (EDIS), which utilizes a diffusion model to extract prior knowledge from the offline dataset and employs energy functions to distill this knowledge for enhanced data generation in the online phase. The theoretical analysis demonstrates that EDIS exhibits reduced suboptimality compared to solely utilizing online data or directly reusing offline data. EDIS is a plug-in approach and can be combined with existing methods in offline-to-online RL setting. By implementing EDIS to off-the-shelf methods Cal-QL and IQL, we observe a notable 20% average improvement in empirical performance on MuJoCo, AntMaze, and Adroit environments. Code is available at https://github.com/liuxhym/EDIS.",
      "authors": [
        "Xu-Hui Liu",
        "Tian-Shuo Liu",
        "Shengyi Jiang",
        "Ruifeng Chen",
        "Zhilong Zhang",
        "Xinwei Chen",
        "Yang Yu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hunSEjeCPE",
      "cdate": 1706866905421,
      "mdate": 1719287288020,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526122"
    },
    {
      "id": "ytz2naZoDB",
      "title": "Stabilizing Policy Gradients for Stochastic Differential Equations via Consistency with Perturbation Process",
      "abstract": "Considering generating samples with high rewards, we focus on optimizing deep neural networks parameterized stochastic differential equations (SDEs), the advanced generative models with high expressiveness, with policy gradient, the leading algorithm in reinforcement learning. Nevertheless, when applying policy gradients to SDEs, since the policy gradient is estimated on a finite set of trajectories, it can be ill-defined, and the policy behavior in data-scarce regions may be uncontrolled. This challenge compromises the stability of policy gradients and negatively impacts sample complexity. To address these issues, we propose constraining the SDE to be consistent with its associated perturbation process. Since the perturbation process covers the entire space and is easy to sample, we can mitigate the aforementioned problems. Our framework offers a general approach allowing for a versatile selection of policy gradient methods to effectively and efficiently train SDEs. We evaluate our algorithm on the task of structure-based drug design and optimize the binding affinity of generated ligand molecules. Our method achieves the best Vina score (-9.07) on the CrossDocked2020 dataset.",
      "authors": [
        "Xiangxin Zhou",
        "Liang Wang",
        "Yichi Zhou"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ytz2naZoDB",
      "cdate": 1706866778962,
      "mdate": 1719287287942,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526128"
    },
    {
      "id": "aZnZOqUOHq",
      "title": "Predicting Lagrangian Multipliers for Mixed Integer Linear Programs",
      "abstract": "Lagrangian Relaxation stands among the most efficient approaches for solving Mixed Integer Linear Programs (MILPs) with difficult constraints. Given any duals for these constraints, called Lagrangian Multipliers (LMs), it returns a bound on the optimal value of the MILP, and Lagrangian methods seek the LMs giving the best such bound. But these methods generally rely on iterative algorithms resembling gradient descent to maximize the concave piecewise linear dual function: the computational burden grows quickly with the number of relaxed constraints. We introduce a deep learning approach that bypasses the descent, effectively amortizing per instance optimization. A probabilistic encoder based on a graph neural network computes, given a MILP instance and its Continuous Relaxation (CR) solution, high-dimensional representations of relaxed constraints, which are turned into LMs by a decoder. We train the encoder and the decoder jointly by directly optimizing the bound obtained from the predicted multipliers. Our method is applicable to any problem with a compact MILP formulation, and to any Lagrangian Relaxation providing a tighter bound than CR. Experiments on two widely known problems, Multi-Commodity Network Design and Generalized Assignment, show that our approach closes up to 85% of the gap between the continuous relaxation and the best Lagrangian bound, and provides a high-quality warm-start for descent-based Lagrangian methods.",
      "authors": [
        "Francesco Demelas",
        "Joseph Le Roux",
        "Mathieu Lacroix",
        "Axel Parmentier"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=aZnZOqUOHq",
      "cdate": 1706866632233,
      "mdate": 1719287287886,
      "matched_keywords": [
        "deep learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526134"
    },
    {
      "id": "rHylzxK3HU",
      "title": "Restoring balance: principled under/oversampling of data for optimal classification",
      "abstract": "Class imbalance in real-world data poses a common bottleneck for machine learning tasks, since achieving good generalization on under-represented examples is often challenging. Mitigation strategies, such as under or oversampling the data depending on their abundances, are routinely proposed and tested empirically, but how they should adapt to the data statistics remains poorly understood. In this work, we determine exact analytical expressions of the generalization curves in the high-dimensional regime for linear classifiers (Support Vector Machines). We also provide a sharp prediction of the effects of under/oversampling strategies depending on class imbalance, first and second moments of the data, and the metrics of performance considered. We show that mixed strategies involving under and oversampling of data lead to performance improvement. Through numerical experiments, we show the relevance of our theoretical predictions on real datasets, on deeper architectures and with sampling strategies based on unsupervised probabilistic models.",
      "authors": [
        "Emanuele Loffredo",
        "Mauro Pastore",
        "Simona Cocco",
        "Remi Monasson"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rHylzxK3HU",
      "cdate": 1706866578019,
      "mdate": 1719287287821,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526140"
    },
    {
      "id": "iUwHnoENnl",
      "title": "Model Alignment as Prospect Theoretic Optimization",
      "abstract": "Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases---the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call $\\textit{human-aware losses}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration.",
      "authors": [
        "Kawin Ethayarajh",
        "Winnie Xu",
        "Niklas Muennighoff",
        "Dan Jurafsky",
        "Douwe Kiela"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iUwHnoENnl",
      "cdate": 1706866567373,
      "mdate": 1719287287794,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526146"
    },
    {
      "id": "qkhbyDqlNI",
      "title": "From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems",
      "abstract": "In this work, from a theoretical lens, we aim to understand why large language model (LLM) empowered agents are able to solve decision-making problems in the physical world. To this end, consider a hierarchical reinforcement learning (RL) model where the LLM Planner and the Actor perform high-level task planning and low-level execution, respectively. Under this model, the LLM Planner navigates a partially observable Markov decision process (POMDP) by iteratively generating language-based subgoals via prompting. Under proper assumptions on the pretraining data, we prove that the pretrained LLM Planner effectively performs Bayesian aggregated imitation learning (BAIL) through in-context learning. Additionally, we highlight the necessity for exploration beyond the subgoals derived from BAIL by proving that naively executing the subgoals returned by LLM leads to a linear regret. As a remedy, we introduce an $\\epsilon$-greedy exploration strategy to BAIL, which is proven to incur sublinear regret when the pretraining error is small. Finally, we extend our theoretical framework to include scenarios where the LLM Planner serves as a world model for inferring the transition model of the environment and to multi-agent settings, enabling coordination among multiple Actors.",
      "authors": [
        "Jianliang He",
        "Siyu Chen",
        "Fengzhuo Zhang",
        "Zhuoran Yang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qkhbyDqlNI",
      "cdate": 1706866558135,
      "mdate": 1719287287750,
      "matched_keywords": [
        "reinforcement learning",
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526152"
    },
    {
      "id": "t8mt4YrPsq",
      "title": "Larimar: Large Language Models with Episodic Memory Control",
      "abstract": "Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed---yielding speed-ups of 8-10x depending on the base LLM ---as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting, information leakage prevention, and input context length generalization with Larimar and show their effectiveness. Our code is available at https://github.com/IBM/larimar.",
      "authors": [
        "Payel Das",
        "Subhajit Chaudhury",
        "Elliot Nelson",
        "Igor Melnyk",
        "Sarathkrishna Swaminathan",
        "Sihui Dai",
        "Aurelie Lozano",
        "Georgios Kollias",
        "Vijil Chenthamarakshan",
        "Jiri Navratil",
        "Soham Dan",
        "Pin-Yu Chen"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=t8mt4YrPsq",
      "cdate": 1706866461334,
      "mdate": 1719287287656,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526158"
    },
    {
      "id": "xB6YJZOKyT",
      "title": "RVI-SAC: Average Reward Off-Policy Deep Reinforcement Learning",
      "abstract": "In this paper, we propose an off-policy deep reinforcement learning (DRL) method utilizing the average reward criterion. While most existing DRL methods employ the discounted reward criterion, this can potentially lead to a discrepancy between the training objective and performance metrics in continuing tasks, making the average reward criterion a recommended alternative. We introduce RVI-SAC, an extension of the state-of-the-art off-policy DRL method, Soft Actor-Critic (SAC), to the average reward criterion. Our proposal consists of (1) Critic updates based on RVI Q-learning, (2) Actor updates introduced by the average reward soft policy improvement theorem, and (3) automatic adjustment of Reset Cost enabling the average reward reinforcement learning to be applied to tasks with termination. We apply our method to the Gymnasium's Mujoco tasks, a subset of locomotion tasks, and demonstrate that RVI-SAC shows competitive performance compared to existing methods.",
      "authors": [
        "Yukinari Hisaki",
        "Isao Ono"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xB6YJZOKyT",
      "cdate": 1706866084538,
      "mdate": 1719287287431,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526164"
    },
    {
      "id": "69RewQwWA9",
      "title": "An Iterative Min-Min Optimization Method for Sparse Bayesian Learning",
      "abstract": "As a well-known machine learning algorithm, sparse Bayesian learning (SBL) can find sparse representations in linearly probabilistic models by imposing a sparsity-promoting prior on model coefficients. However, classical SBL algorithms lack the essential theoretical guarantees of global convergence. To address this issue, we propose an iterative Min-Min optimization method to solve the marginal likelihood function (MLF) of SBL based on the concave-convex procedure. The method can optimize the hyperparameters related to both the prior and noise level analytically at each iteration by re-expressing MLF using auxiliary functions. Particularly, we demonstrate that the method globally converges to a local minimum or saddle point of MLF. With rigorous theoretical guarantees, the proposed novel SBL algorithm outperforms classical ones in finding sparse representations on simulation and real-world examples, ranging from sparse signal recovery to system identification and kernel regression.",
      "authors": [
        "Yasen Wang",
        "Junlin Li",
        "Zuogong Yue",
        "ye yuan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=69RewQwWA9",
      "cdate": 1706866012813,
      "mdate": 1719287287427,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526169"
    },
    {
      "id": "RZHRnnGcEx",
      "title": "Let Go of Your Labels with Unsupervised Transfer",
      "abstract": "Foundation vision-language models have enabled remarkable zero-shot transferability of the pre-trained representations to a wide range of downstream tasks. However, to solve a new task, zero-shot transfer still necessitates human guidance to define visual categories that appear in the data. Here, we show that fully unsupervised transfer emerges when searching for the labeling of a dataset that induces maximal margin classifiers in representation spaces of different foundation models. We present TURTLE, a fully unsupervised method that effectively employs this guiding principle to uncover the underlying labeling of a downstream dataset without any supervision and task-specific representation learning. We evaluate TURTLE on a diverse benchmark suite of 26 datasets and show that it achieves new state-of-the-art unsupervised performance. Furthermore, TURTLE, although being fully unsupervised, outperforms zero-shot transfer baselines on a wide range of datasets. In particular, TURTLE matches the average performance of CLIP zero-shot on 26 datasets by employing the same representation space, spanning a wide range of architectures and model sizes. By guiding the search for the underlying labeling using the representation spaces of two foundation models, TURTLE surpasses zero-shot transfer and unsupervised prompt tuning baselines, demonstrating the surprising power and effectiveness of unsupervised transfer.",
      "authors": [
        "Artyom Gadetsky",
        "Yulun Jiang",
        "Maria Brbic"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=RZHRnnGcEx",
      "cdate": 1706865973752,
      "mdate": 1719287287401,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526175"
    },
    {
      "id": "dqpg8jdA2w",
      "title": "Offline Transition Modeling via Contrastive Energy Learning",
      "abstract": "Learning a high-quality transition model is of great importance for sequential decision-making tasks, especially in offline settings. Nevertheless, the complex behaviors of transition dynamics in real-world environments pose challenges for the standard forward models because of their inductive bias towards smooth regressors, conflicting with the inherent nature of transitions such as discontinuity or large curvature. In this work, we propose to model the transition probability implicitly through a scalar-value energy function, which enables not only flexible distribution prediction but also capturing complex transition behaviors. The Energy-based Transition Models (ETM) are shown to accurately fit the discontinuous transition functions and better generalize to out-of-distribution transition data. Furthermore, we demonstrate that energy-based transition models improve the evaluation accuracy and significantly outperform other off-policy evaluation methods in DOPE benchmark. Finally, we show that energy-based transition models also benefit reinforcement learning and outperform prior offline RL algorithms in D4RL Gym-Mujoco tasks.",
      "authors": [
        "Ruifeng Chen",
        "Chengxing Jia",
        "Zefang Huang",
        "Tian-Shuo Liu",
        "Xu-Hui Liu",
        "Yang Yu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=dqpg8jdA2w",
      "cdate": 1706865813103,
      "mdate": 1719287287168,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526181"
    },
    {
      "id": "DlR8fWgJRl",
      "title": "Model-based Reinforcement Learning for Confounded POMDPs",
      "abstract": "We propose a model-based offline reinforcement learning (RL) algorithm for confounded partially observable Markov decision processes (POMDPs) under general function approximations and show it is provably efficient under some technical conditions such as the partial coverage imposed on the offline data distribution. Specifically, we first establish a novel model-based identification result for learning the effect of any action on the reward and future transitions in the confounded POMDP. Using this identification result, we then design a nonparametric two-stage estimation procedure to construct an estimator for off-policy evaluation (OPE), which permits general function approximations. Finally, we learn the optimal policy by performing a conservative policy optimization within the confidence regions based on the proposed estimation procedure for OPE. Under some mild conditions, we establish a finite-sample upper bound on the suboptimality of the learned policy in finding the optimal one, which depends on the sample size and the length of horizons polynomially.",
      "authors": [
        "Mao Hong",
        "Zhengling Qi",
        "Yanxun Xu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DlR8fWgJRl",
      "cdate": 1706865776707,
      "mdate": 1719287287133,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526187"
    },
    {
      "id": "DbyHDYslM7",
      "title": "BiE: Bi-Exponent Block Floating-Point for Large Language Models Quantization",
      "abstract": "Nowadays, Large Language Models (LLMs) mostly possess billions of parameters, bringing significant challenges to hardware platforms. Although quantization is an efficient approach to reduce computation and memory overhead for inference optimization, we stress the challenge that mainstream low-bit quantization approaches still suffer from either various data distribution outliers or a lack of hardware efficiency. We also find that low-bit data format has further potential expressiveness to cover the atypical language data distribution. In this paper, we propose a novel numerical representation, Bi-Exponent Block Floating Point (BiE), and a new quantization flow. BiE quantization shows accuracy superiority and hardware friendliness on various models and benchmarks.",
      "authors": [
        "Lancheng Zou",
        "Wenqian Zhao",
        "Shuo Yin",
        "Chen Bai",
        "Qi Sun",
        "Bei Yu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DbyHDYslM7",
      "cdate": 1706865477109,
      "mdate": 1719287287093,
      "matched_keywords": [
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526193"
    },
    {
      "id": "dslUyy1rN4",
      "title": "Position: Automatic Environment Shaping is the Next Frontier in RL",
      "abstract": "Many roboticists dream of presenting a robot with a task in the evening and returning the next morning to find the robot capable of solving the task. What is preventing us from achieving this? Sim-to-real reinforcement learning (RL) has achieved impressive performance on challenging robotics tasks, but requires substantial human effort to set up the task in a way that is amenable to RL. It's our position that algorithmic improvements in policy optimization and other ideas should be guided towards resolving the primary bottleneck of shaping the training environment, i.e., designing observations, actions, rewards and simulation dynamics. Most practitioners don't tune the RL algorithm, but other environment parameters to obtain a desirable controller. We posit that scaling RL to diverse robotic tasks will only be achieved if the community focuses on automating environment shaping procedures.",
      "authors": [
        "Younghyo Park",
        "Gabriel B. Margolis",
        "Pulkit Agrawal"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=dslUyy1rN4",
      "cdate": 1706865428583,
      "mdate": 1719287287050,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526200"
    },
    {
      "id": "TujtZgdRxB",
      "title": "Online Matching with Stochastic Rewards: Provable Better Bound via Adversarial Reinforcement Learning",
      "abstract": "For a specific online optimization problem, for example, online bipartite matching (OBM), research efforts could be made in two directions before it is finally closed, i.e., the optimal competitive online algorithm is found. One is to continuously design algorithms with better performance. To this end, reinforcement learning (RL) has demonstrated great success in literature. However, little is known on the other direction: whether RL helps explore how hard an online problem is. In this paper, we study a generalized model of OBM, named online matching with stochastic rewards (OMSR, FOCS 2012), for which the optimal competitive ratio is still unknown. We adopt an adversarial RL approach that trains two RL agents adversarially and iteratively: the algorithm agent learns for algorithms with larger competitive ratios, while the adversarial agent learns to produce a family of hard instances. Through such a framework, agents converge at the end with a robust algorithm, which empirically outperforms the state of the art (STOC 2020). Much more significantly, it allows to track how the hard instances are generated. We succeed in distilling two structural properties from the learned graph patterns, which remarkably reduce the action space, and further enable theoretical improvement on the best-known hardness result of OMSR, from $0.621$ (FOCS 2012) to $0.597$. To the best of our knowledge, this gives the first evidence that RL can help enhance the theoretical understanding of an online problem.",
      "authors": [
        "Qiankun Zhang",
        "Aocheng Shen",
        "Boyu Zhang",
        "Hanrui Jiang",
        "Bingqian Du"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=TujtZgdRxB",
      "cdate": 1706865396454,
      "mdate": 1719287287029,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526205"
    },
    {
      "id": "ie3vXkMvRY",
      "title": "On the Unexpected Effectiveness of Reinforcement Learning for Sequential Recommendation",
      "abstract": "In recent years, Reinforcement Learning (RL) has shown great promise in session-based recommendation. Sequential models that use RL have reached state-of-the-art performance for the Next-item Prediction (NIP) task. This result is intriguing, as the NIP task only evaluates how well the system can correctly recommend the next item to the user, while the goal of RL is to find a policy that optimizes rewards in the long term -- sometimes at the expense of suboptimal short-term performance. Then, how can RL improve the system's performance on short-term metrics? This article investigates this question by exploring proxy learning objectives, which we identify as goals RL models might be following, and thus could explain the performance boost. We found that RL -- when used as an auxiliary loss -- promotes the learning of embeddings that capture information about the user's previously interacted items. Subsequently, we replaced the RL objective with a straightforward auxiliary loss designed to predict the number of items the user interacted with. This substitution results in performance gains comparable to RL. These findings pave the way to improve performance and understanding of RL methods for recommender systems.",
      "authors": [
        "Álvaro Labarca Silva",
        "Denis Parra",
        "Rodrigo Toro Icarte"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ie3vXkMvRY",
      "cdate": 1706865366062,
      "mdate": 1719287286989,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526210"
    },
    {
      "id": "XDz9leJ9iK",
      "title": "Position: Cracking the Code of Cascading Disparity Towards Marginalized Communities",
      "abstract": "The rise of foundation models holds immense promise for advancing AI, but this progress may amplify existing risks and inequalities, leaving marginalized communities behind. In this position paper, we discuss that disparities towards marginalized communities – performance, representation, privacy, robustness, interpretability and safety – are not isolated concerns but rather interconnected elements of a cascading disparity phenomenon. We contrast foundation models with traditional models and highlight the potential for exacerbated disparity against marginalized communities. Moreover, we emphasize the unique threat of cascading impacts in foundation models, where interconnected disparities can trigger long-lasting negative consequences, specifically to the people on the margin. We define marginalized communities within the machine learning context and explore the multifaceted nature of disparities. We analyze the sources of these disparities, tracing them from data creation, training and deployment procedures to highlight the complex technical and socio-technical landscape. To mitigate the pressing crisis, we conclude with a set of calls to action to mitigate disparity at its source.",
      "authors": [
        "Golnoosh Farnadi",
        "Mohammad Havaei",
        "Negar Rostamzadeh"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=XDz9leJ9iK",
      "cdate": 1706865173026,
      "mdate": 1719287286931,
      "matched_keywords": [
        "machine learning",
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526216"
    },
    {
      "id": "zFHaB7KESM",
      "title": "Guarantees for Nonlinear Representation Learning: Non-identical Covariates, Dependent Data, Fewer Samples",
      "abstract": "A driving force behind the diverse applicability of modern machine learning is the ability to extract meaningful features across many sources. However, many practical domains involve data that are non-identically distributed across sources, and possibly statistically dependent within its source, violating vital assumptions in existing theoretical studies of representation learning. Toward addressing these issues, we establish statistical guarantees for learning general *nonlinear* representations from multiple data sources that admit different input distributions and possibly dependent data. Specifically, we study the sample-complexity of learning $T+1$ functions $f_\\star^{(t)} \\circ g_\\star$ from a function class $\\mathcal{F} \\times \\mathcal{G}$, where $f_\\star^{(t)}$ are task specific linear functions and $g_\\star$ is a shared non-linear representation. An approximate representation $\\hat g$ is estimated using $N$ samples from each of $T$ source tasks, and a fine-tuning function $\\hat f^{(0)}$ is fit using $N'$ samples from a target task passed through $\\hat g$. Our results show that the excess risk of the estimate $\\hat f^{(0)} \\circ \\hat g$ on the target task decays as $\\tilde{\\mathcal{O}}\\Big(\\frac{\\mathrm{C}(\\mathcal{G})}{N T} + \\frac{\\text{dim}(\\mathcal{F})}{N'}\\Big)$, where $\\mathrm{C}(\\mathcal{G})$ denotes the complexity of $\\mathcal{G}$. Notably, our rates match that of the iid setting, while requiring fewer samples per task than prior analysis and admitting *no dependence on the mixing time*. We support our analysis with numerical experiments performing imitation learning over non-linear dynamical systems.",
      "authors": [
        "Thomas TCK Zhang",
        "Bruce D Lee",
        "Ingvar Ziemann",
        "George J. Pappas",
        "Nikolai Matni"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=zFHaB7KESM",
      "cdate": 1706865149118,
      "mdate": 1719287286854,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526221"
    },
    {
      "id": "ULleq1Dtaw",
      "title": "Towards General Algorithm Discovery for Combinatorial Optimization: Learning Symbolic Branching Policy from Bipartite Graph",
      "abstract": "Machine learning (ML) approaches have been successfully applied to accelerating exact combinatorial optimization (CO) solvers. However, many of them fail to explain what patterns they have learned that accelerate the CO algorithms due to the black-box nature of ML models like neural networks, and thus they prevent researchers from further understanding the tasks they are interested in. To tackle this problem, we propose the *first* graph-based algorithm discovery framework---namely, graph symbolic discovery for exact combinatorial optimization solver (GS4CO)---that learns interpretable branching policies directly from the *general* bipartite graph representation of CO problems. Specifically, we design a unified representation for symbolic policies with graph inputs, and then we employ a Transformer with multiple tree-structural encodings to generate symbolic trees end-to-end, which effectively reduces the cumulative error from iteratively distilling graph neural networks. Experiments show that GS4CO learned interpretable and lightweight policies outperform all the baselines on CPU machines, including both the human-designed and the learning-based. GS4CO shows an encouraging step towards general algorithm discovery on modern CO solvers.",
      "authors": [
        "Yufei Kuang",
        "Jie Wang",
        "Yuyan Zhou",
        "Xijun Li",
        "Fangzhou Zhu",
        "Jianye HAO",
        "Feng Wu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ULleq1Dtaw",
      "cdate": 1706865107953,
      "mdate": 1719287286854,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526227"
    },
    {
      "id": "X8uQ1TslUc",
      "title": "OT-CLIP: Understanding and Generalizing CLIP via Optimal Transport",
      "abstract": "We propose to understand Contrastive Language-Image Pretraining model (CLIP) from the Optimal Transport (OT) perspective. Specifically, we show that training of CLIP is an embodiment of inverse OT and the adopted two InfoNCE losses in CLIP correspond to a special case of bilevel optimization of modified entropic OT. We then generalize the original CLIP loss to an OT-based loss family using variants of Regularized OT (e.g. Fused Gromov OT, unbalanced OT, etc.), and demonstrate their superior performance on public datasets for both image and text downstream tasks. We also rethink the inference stage of CLIP by using the tool of OT, and propose to adopt the fused Gromov OT for (zero-shot) classification, in which the prediction is based on the graph representation whereby images and texts are nodes for graph matching. By our new technique, we show how to generalize zero-shot classification to other more flexible zero-shot tasks with competitive performance: long-tailed classification and selective classification. The former assumes the known prior distribution of labels, while in the latter case, only a subset of samples are asked to predict, yet with high prediction confidence.",
      "authors": [
        "Liangliang Shi",
        "Jack Fan",
        "Junchi Yan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=X8uQ1TslUc",
      "cdate": 1706865011552,
      "mdate": 1719287286771,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526233"
    },
    {
      "id": "xm2lU7tteQ",
      "title": "Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape",
      "abstract": "Large language models based on the Transformer architecture have demonstrated impressive capabilities to learn in context. However, existing theoretical studies on how this phenomenon arises are limited to the dynamics of a single layer of attention trained on linear regression tasks. In this paper, we study the optimization of a Transformer consisting of a fully connected layer followed by a linear attention layer. The MLP acts as a common nonlinear representation or feature map, greatly enhancing the power of in-context learning. We prove in the mean-field and two-timescale limit that the infinite-dimensional loss landscape for the distribution of parameters, while highly nonconvex, becomes quite benign. We also analyze the second-order stability of mean-field dynamics and show that Wasserstein gradient flow almost always avoids saddle points. Furthermore, we establish novel methods for obtaining concrete improvement rates both away from and near critical points. This represents the first saddle point analysis of mean-field dynamics in general and the techniques are of independent interest.",
      "authors": [
        "Juno Kim",
        "Taiji Suzuki"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xm2lU7tteQ",
      "cdate": 1706864891677,
      "mdate": 1719287286667,
      "matched_keywords": [
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526239"
    },
    {
      "id": "2Yu5FWdzde",
      "title": "Prompt Sketching for Large Language Models",
      "abstract": "Many recent prompting strategies for large language models (LLMs) query the model multiple times sequentially -- first to produce intermediate results and then the final answer. However, using these methods, both decoder and model are unaware of potential follow-up prompts, leading to disconnected and undesirably wordy intermediate responses. In this work, we address this issue by proposing prompt sketching, a new prompting paradigm in which an LLM does not only respond by completing a prompt, but by predicting values for multiple variables in a template. This way, sketching grants users more control over the generation process, e.g., by providing a reasoning framework via intermediate instructions, leading to better overall results. The key idea enabling sketching with existing, autoregressive models is to adapt the decoding procedure to also score follow-up instructions during text generation, thus optimizing overall template likelihood in inference. Our experiments show that in a zero-shot setting, prompt sketching outperforms existing, sequential prompting schemes such as direct asking or chain-of-thought on 7 out of 8 LLM benchmarking tasks, including state tracking, arithmetic reasoning, and general question answering. To facilitate future use, we release a number of generic, yet effective sketches applicable to many tasks, and an open source library called dclib, powering our sketch-aware decoders as part of https://github.com/eth-sri/lmql.",
      "authors": [
        "Luca Beurer-Kellner",
        "Mark Niklas Mueller",
        "Marc Fischer",
        "Martin Vechev"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=2Yu5FWdzde",
      "cdate": 1706864694184,
      "mdate": 1719287286584,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526245"
    },
    {
      "id": "AJGwSx0RUV",
      "title": "Reinforcement Learning within Tree Search for Fast Macro Placement",
      "abstract": "Macro placement is a crucial step in modern chip design, and reinforcement learning (RL) has recently emerged as a promising technique for improving the placement quality. However, existing RL-based techniques are hindered by their low sample efficiency, requiring numerous online rollouts or substantial offline expert data to achieve bootstrap, which are often impractical in industrial scenarios. To address this challenge, we propose a novel sample-efficient framework, namely **EfficientPlace**, for fast macro placement. EfficientPlace integrates a global tree search algorithm to strategically direct the optimization process, as well as a RL agent for local policy learning to advance the tree search. Experiments on commonly used benchmarks demonstrate that EfficientPlace achieves remarkable placement quality within a short timeframe, outperforming recent state-of-the-art approaches.",
      "authors": [
        "Zijie Geng",
        "Jie Wang",
        "Ziyan Liu",
        "Siyuan Xu",
        "Zhentao Tang",
        "Mingxuan Yuan",
        "Jianye HAO",
        "Yongdong Zhang",
        "Feng Wu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=AJGwSx0RUV",
      "cdate": 1706864631941,
      "mdate": 1719287286521,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526252"
    },
    {
      "id": "VfWrXJtLSL",
      "title": "Improved Bounds for Pure Private Agnostic Learning: Item-Level and User-Level Privacy",
      "abstract": "Machine Learning has made remarkable progress in a wide range of fields. In many scenarios, learning is performed on datasets involving sensitive information, in which privacy protection is essential for learning algorithms. In this work, we study pure private learning in the agnostic model -- a framework reflecting the learning process in practice. We examine the number of users required under item-level (where each user contributes one example) and user-level (where each user contributes multiple examples) privacy and derive several improved upper bounds. For item-level privacy, our algorithm achieves a near optimal bound for general concept classes. We extend this to the user-level setting, rendering a tighter upper bound than the one proved by Ghazi et al. (2023). Lastly, we consider the problem of learning thresholds under user-level privacy and present an algorithm with a nearly tight user complexity.",
      "authors": [
        "Bo Li",
        "Wei Wang",
        "Peng Ye"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=VfWrXJtLSL",
      "cdate": 1706864624038,
      "mdate": 1719287286477,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526257"
    },
    {
      "id": "yrFUJzcTsk",
      "title": "Revisiting Scalable Hessian Diagonal Approximations for Applications in Reinforcement Learning",
      "abstract": "Second-order information is valuable for many applications but challenging to compute. Several works focus on computing or approximating Hessian diagonals, but even this simplification introduces significant additional costs compared to computing a gradient. In the absence of efficient exact computation schemes for Hessian diagonals, we revisit an early approximation scheme proposed by Becker and LeCun (1989, BL89), which has a cost similar to gradients and appears to have been overlooked by the community. We introduce HesScale, an improvement over BL89, which adds negligible extra computation. On small networks, we find that this improvement is of higher quality than all alternatives, even those with theoretical guarantees, such as unbiasedness, while being much cheaper to compute. We use this insight in reinforcement learning problems where small networks are used and demonstrate HesScale in second-order optimization and scaling the step-size parameter. In our experiments, HesScale optimizes faster than existing methods and improves stability through step-size scaling. These findings are promising for scaling second-order methods in larger models in the future.",
      "authors": [
        "Mohamed Elsayed",
        "Homayoon Farrahi",
        "Felix Dangel",
        "A. Rupam Mahmood"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=yrFUJzcTsk",
      "cdate": 1706864353805,
      "mdate": 1719287286279,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526264"
    },
    {
      "id": "7R3pzxTSlg",
      "title": "Structured Chemistry Reasoning with Large Language Models",
      "abstract": "Large Language Models (LLMs) excel in diverse areas, yet struggle with complex scientific reasoning, especially in the field of chemistry. Different from the simple chemistry tasks (e.g., molecule classification) addressed in previous studies, complex chemistry problems require not only vast knowledge and precise calculation, but also compositional reasoning about rich dynamic interactions of different concepts (e.g., temperature changes). Our study shows that even advanced LLMs, like GPT-4, can fail easily in different ways. Interestingly, the errors often stem not from a lack of domain knowledge within the LLMs, but rather from the absence of an effective reasoning *structure* that guides the LLMs to elicit the right knowledge, incorporate the knowledge in step-by-step reasoning, and iteratively refine results for further improved quality. On this basis, we introduce StructChem, a simple yet effective prompting strategy that offers the desired guidance and substantially boosts the LLMs' chemical reasoning capability. Testing across four chemistry areas---quantum chemistry, mechanics, physical chemistry, and kinetics---StructChem substantially enhances GPT-4's performance, with up to 30% peak improvement. Our analysis also underscores the unique difficulties of precise grounded reasoning in science with LLMs, highlighting a need for more research in this area.",
      "authors": [
        "Siru Ouyang",
        "Zhuosheng Zhang",
        "Bing Yan",
        "Xuan Liu",
        "Yejin Choi",
        "Jiawei Han",
        "Lianhui Qin"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7R3pzxTSlg",
      "cdate": 1706864315406,
      "mdate": 1719287286279,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526269"
    },
    {
      "id": "bX3J7ho18S",
      "title": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews",
      "abstract": "We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: *ICLR* 2024, *NeurIPS* 2023, *CoRL* 2023 and *EMNLP* 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices.",
      "authors": [
        "Weixin Liang",
        "Zachary Izzo",
        "Yaohui Zhang",
        "Haley Lepp",
        "Hancheng Cao",
        "Xuandong Zhao",
        "Lingjiao Chen",
        "Haotian Ye",
        "Sheng Liu",
        "Zhi Huang",
        "Daniel McFarland",
        "James Y. Zou"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=bX3J7ho18S",
      "cdate": 1706864205623,
      "mdate": 1719287286190,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526274"
    },
    {
      "id": "scSB9RynSd",
      "title": "Scaling Laws for the Value of Individual Data Points in Machine Learning",
      "abstract": "Recent works have shown that machine learning models improve at a predictable rate with the amount of training data, leading to scaling laws that describe the relationship between error and dataset size. These scaling laws can help determine a model's training dataset, but they take an aggregate view of the data by only considering the dataset's size. We consider a new perspective by investigating scaling behavior for the value of individual data points: we find that a data point's contribution to model's performance shrinks predictably with the size of the dataset in a log-linear manner. Interestingly, there is significant variability in the scaling exponent among different data points, indicating that certain points are more valuable in small datasets and other points are relatively more useful as a part of large datasets. We provide learning theory support for our scaling laws and we observe empirically that it holds across several model classes. We further propose a maximum likelihood estimator and an amortized estimator to efficiently learn the individualized scaling behaviors from a small number of noisy observations per data point. Using our efficient estimators, we provide insights into factors that influence the scaling behavior of different data points. Finally we demonstrate applications of the individualized scaling laws to data valuation and data subset selection.",
      "authors": [
        "Ian Connick Covert",
        "Wenlong Ji",
        "Tatsunori Hashimoto",
        "James Zou"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=scSB9RynSd",
      "cdate": 1706864195412,
      "mdate": 1719287286157,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526280"
    },
    {
      "id": "kIh7GJmRfD",
      "title": "ATraDiff: Accelerating Online Reinforcement Learning with Imaginary Trajectories",
      "abstract": "Training autonomous agents with sparse rewards is a long-standing problem in online reinforcement learning (RL), due to low data efficiency. Prior work overcomes this challenge by extracting useful knowledge from offline data, often accomplished through the learning of action distribution from offline data and utilizing the learned distribution to facilitate online RL. However, since the offline data are given and fixed, the extracted knowledge is inherently limited, making it difficult to generalize to new tasks. We propose a novel approach that leverages offline data to learn a generative diffusion model, coined as Adaptive Trajectory Diffuser (ATraDiff). This model generates synthetic trajectories, serving as a form of data augmentation and consequently enhancing the performance of online RL methods. The key strength of our diffuser lies in its adaptability, allowing it to effectively handle varying trajectory lengths and mitigate distribution shifts between online and offline data. Because of its simplicity, ATraDiff seamlessly integrates with a wide spectrum of RL methods. Empirical evaluation shows that ATraDiff consistently achieves state-of-the-art performance across a variety of environments, with particularly pronounced improvements in complicated settings. Our code and demo video are available at https://atradiff.github.io.",
      "authors": [
        "Qianlan Yang",
        "Yu-Xiong Wang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kIh7GJmRfD",
      "cdate": 1706864147765,
      "mdate": 1719287286060,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526286"
    },
    {
      "id": "xEB2oF3vvb",
      "title": "Position: Application-Driven Innovation in Machine Learning",
      "abstract": "In this position paper, we argue that application-driven research has been systemically under-valued in the machine learning community. As applications of machine learning proliferate, innovative algorithms inspired by specific real-world challenges have become increasingly important. Such work offers the potential for significant impact not merely in domains of application but also in machine learning itself. In this paper, we describe the paradigm of application-driven research in machine learning, contrasting it with the more standard paradigm of methods-driven research. We illustrate the benefits of application-driven machine learning and how this approach can productively synergize with methods-driven work. Despite these benefits, we find that reviewing, hiring, and teaching practices in machine learning often hold back application-driven innovation. We outline how these processes may be improved.",
      "authors": [
        "David Rolnick",
        "Alan Aspuru-Guzik",
        "Sara Beery",
        "Bistra Dilkina",
        "Priya L. Donti",
        "Marzyeh Ghassemi",
        "Hannah Kerner",
        "Claire Monteleoni",
        "Esther Rolf",
        "Milind Tambe",
        "Adam White"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xEB2oF3vvb",
      "cdate": 1706864074661,
      "mdate": 1719287285953,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526291"
    },
    {
      "id": "vJx6fld6l0",
      "title": "Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics",
      "abstract": "This study introduces a novel transformer model optimized for large-scale point cloud processing in scientific domains such as high-energy physics (HEP) and astrophysics. Addressing the limitations of graph neural networks and standard transformers, our model integrates local inductive bias and achieves near-linear complexity with hardware-friendly regular operations. One contribution of this work is the quantitative analysis of the error-complexity tradeoff of various sparsification techniques for building efficient transformers. Our findings highlight the superiority of using locality-sensitive hashing (LSH), especially OR & AND-construction LSH, in kernel approximation for large-scale point cloud data with local inductive bias. Based on this finding, we propose LSH-based Efficient Point Transformer (**HEPT**), which combines E$^2$LSH with OR & AND constructions and is built upon regular computations. HEPT demonstrates remarkable performance on two critical yet time-consuming HEP tasks, significantly outperforming existing GNNs and transformers in accuracy and computational speed, marking a significant advancement in geometric deep learning and large-scale scientific data processing. Our code is available at https://github.com/Graph-COM/HEPT.",
      "authors": [
        "Siqi Miao",
        "Zhiyuan Lu",
        "Mia Liu",
        "Javier Duarte",
        "Pan Li"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vJx6fld6l0",
      "cdate": 1706864031434,
      "mdate": 1719287285952,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526297"
    },
    {
      "id": "eqIGoEoI10",
      "title": "Asymptotically Optimal and Computationally Efficient Average Treatment Effect Estimation in A/B testing",
      "abstract": "Motivated by practical applications in clinical trials and online platforms, we study A/B testing with the aim of estimating a confidence interval (CI) for the average treatment effect (ATE) using the minimum expected sample size. This CI should have a width at most $\\epsilon$ while ensuring that the probability of the CI not containing the true ATE is at most $\\delta$. To answer this, we first establish a lower bound on the expected sample size needed for any adaptive policy which constructs a CI of ATE with desired properties. Specifically, we prove that the lower bound is based on the solution to a max-min non-convex optimization problem for small $\\delta$. Tailoring the ``plug-in'' approach for the ATE problem, we construct an adaptive policy that is asymptotically optimal, i.e., matches the lower bound on the expected sample size for small $\\delta$. Interestingly, we find that, for small $\\epsilon$ and $\\delta$, the asymptotically optimal fraction of treatment assignment for A and B is proportional to the standard deviation of the outcome distributions of treatments A and B, respectively. However, as the proposed approach can be computationally intensive, we propose an alternative adaptive policy. This new policy, informed by insights from our lower bound analysis, is computationally efficient while remaining asymptotically optimal for small values of $\\epsilon$ and $\\delta$. Numerical comparisons demonstrate that both policies perform similarly across practical values of $\\epsilon$ and $\\delta$, offering efficient solutions for A/B testing.",
      "authors": [
        "VIKAS DEEP",
        "Achal Bassamboo",
        "Sandeep Kumar Juneja"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=eqIGoEoI10",
      "cdate": 1706863982638,
      "mdate": 1719287285949,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526302"
    },
    {
      "id": "5mCaITRTmO",
      "title": "Extreme Compression of Large Language Models via Additive Quantization",
      "abstract": "The emergence of accurate open large language models (LLMs) has led to a race towards performant quantization techniques which can enable their execution on end-user devices. In this paper, we revisit the problem of ``extreme'' LLM compression---defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter---from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our algorithm, called AQLM, generalizes the classic *Additive Quantization (AQ)* approach for information retrieval to advance the state-of-the-art in LLM compression, via two innovations: 1) learned additive quantization of weight matrices in input-adaptive fashion, and 2) joint optimization of codebook parameters across each transformer blocks. Broadly, AQLM is the first scheme that is Pareto optimal in terms of accuracy-vs-model-size when compressing to less than 3 bits per parameter, and significantly improves upon all known schemes in the extreme compression (2bit) regime. In addition, AQLM is practical: we provide fast GPU and CPU implementations of AQLM for token generation, which enable us to match or outperform optimized FP16 implementations for speed, while executing in a much smaller memory footprint.",
      "authors": [
        "Vage Egiazarian",
        "Andrei Panferov",
        "Denis Kuznedelev",
        "Elias Frantar",
        "Artem Babenko",
        "Dan Alistarh"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5mCaITRTmO",
      "cdate": 1706863770360,
      "mdate": 1719287285784,
      "matched_keywords": [
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526309"
    },
    {
      "id": "mk3A5IUdn8",
      "title": "Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling",
      "abstract": "Large-scale sequence modeling has sparked rapid advances that now extend into biology and genomics. However, modeling genomic sequences introduces challenges such as the need to model long-range token interactions, the effects of upstream and downstream regions of the genome, and the reverse complementarity (RC) of DNA. Here, we propose an architecture motivated by these challenges that builds off the long-range Mamba block, and extends it to a BiMamba component that supports bi-directionality, and to a MambaDNA block that additionally supports RC equivariance. We use MambaDNA as the basis of Caduceus, the first family of RC equivariant bi-directional long-range DNA language models, and we introduce pre-training and fine-tuning strategies that yield Caduceus DNA foundation models. Caduceus outperforms previous long-range models on downstream benchmarks; on a challenging long-range variant effect prediction task, Caduceus exceeds the performance of 10x larger models that do not leverage bi-directionality or equivariance. Code to reproduce our experiments is available here: https://github.com/kuleshov-group/caduceus.",
      "authors": [
        "Yair Schiff",
        "Chia Hsiang Kao",
        "Aaron Gokaslan",
        "Tri Dao",
        "Albert Gu",
        "Volodymyr Kuleshov"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=mk3A5IUdn8",
      "cdate": 1706863717272,
      "mdate": 1719287285729,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526315"
    },
    {
      "id": "Yd8eHMY1wz",
      "title": "Unified Training of Universal Time Series Forecasting Transformers",
      "abstract": "Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of *universal forecasting*, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: (i) cross-frequency learning, (ii) accommodating an arbitrary number of variates for multivariate time series, and (iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed **M**asked Enc**o**der-based Un**i**ve**r**s**a**l T**i**me Series Forecasting Transformer (**Moirai**). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. Code, data, and model weights can be found at https://github.com/SalesforceAIResearch/uni2ts.",
      "authors": [
        "Gerald Woo",
        "Chenghao Liu",
        "Akshat Kumar",
        "Caiming Xiong",
        "Silvio Savarese",
        "Doyen Sahoo"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Yd8eHMY1wz",
      "cdate": 1706863709953,
      "mdate": 1719287285720,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526321"
    },
    {
      "id": "re6es2atbl",
      "title": "A New Theoretical Perspective on Data Heterogeneity in Federated Optimization",
      "abstract": "In federated learning (FL), data heterogeneity is the main reason that existing theoretical analyses are pessimistic about the convergence rate. In particular, for many FL algorithms, the convergence rate grows dramatically when the number of local updates becomes large, especially when the product of the gradient divergence and local Lipschitz constant is large. However, empirical studies can show that more local updates can improve the convergence rate even when these two parameters are large, which is inconsistent with the theoretical findings. This paper aims to bridge this gap between theoretical understanding and practical performance by providing a theoretical analysis from a new perspective on data heterogeneity. In particular, we propose a new and weaker assumption compared to the local Lipschitz gradient assumption, named the heterogeneity-driven pseudo-Lipschitz assumption. We show that this and the gradient divergence assumptions can jointly characterize the effect of data heterogeneity. By deriving a convergence upper bound for FedAvg and its extensions, we show that, compared to the existing works, local Lipschitz constant is replaced by the much smaller heterogeneity-driven pseudo-Lipschitz constant and the corresponding convergence upper bound can be significantly reduced for the same number of local updates, although its order stays the same. In addition, when the local objective function is quadratic, more insights on the impact of data heterogeneity can be obtained using the heterogeneity-driven pseudo-Lipschitz constant. For example, we can identify a region where FedAvg can outperform mini-batch SGD even when the gradient divergence can be arbitrarily large. Our findings are validated using experiments.",
      "authors": [
        "Jiayi Wang",
        "Shiqiang Wang",
        "Rong-Rong Chen",
        "Mingyue Ji"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=re6es2atbl",
      "cdate": 1706863669245,
      "mdate": 1719287285692,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526326"
    },
    {
      "id": "LGDYsBslWi",
      "title": "On Statistical Learning Theory for Distributional Inputs",
      "abstract": "Kernel-based statistical learning on distributional inputs appears in many relevant applications, from medical diagnostics to causal inference, and poses intriguing theoretical questions. While this learning scenario received considerable attention from the machine learning community recently, many gaps in the theory remain. In particular, most works consider only the distributional regression setting, and focus on the regularized least-squares algorithm for this problem. In this work, we start to fill these gaps. We prove two oracle inequalities for kernel machines in general distributional learning scenarios, as well as a generalization result based on algorithmic stability. Our main results are formulated in great generality, utilizing general Hilbertian embeddings, which makes them applicable to a wide array of approaches to distributional learning. Additionally, we specialize our results to the cases of kernel mean embeddings and of the recently introduced Hilbertian embeddings based on sliced Wasserstein distances, providing concrete instances of the general setup. Our results considerably enlarge the scope of theoretically grounded distributional learning, and provide many interesting avenues for future work.",
      "authors": [
        "Christian Fiedler",
        "Pierre-François Massiani",
        "Friedrich Solowjow",
        "Sebastian Trimpe"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LGDYsBslWi",
      "cdate": 1706863594796,
      "mdate": 1719287285598,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526331"
    },
    {
      "id": "Ss3h1ixJAU",
      "title": "Absolute Policy Optimization: Enhancing Lower Probability Bound of Performance with High Confidence",
      "abstract": "In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function, optimizing which leads to guaranteed monotonic improvement in the lower probability bound of performance with high confidence. Building upon this groundbreaking theoretical advancement, we further introduce a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO as well as its efficient variation Proximal Absolute Policy Optimization (PAPO) significantly outperforms state-of-the-art policy gradient algorithms, resulting in substantial improvements in worst-case performance, as well as expected performance.",
      "authors": [
        "Weiye Zhao",
        "Feihan Li",
        "Yifan Sun",
        "Rui Chen",
        "Tianhao Wei",
        "Changliu Liu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Ss3h1ixJAU",
      "cdate": 1706863549863,
      "mdate": 1719287285472,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526336"
    },
    {
      "id": "CTgEV6qgUy",
      "title": "Active Preference Learning for Large Language Models",
      "abstract": "As large language models (LLMs) become more capable, fine-tuning techniques for aligning with human intent are increasingly important. A key consideration for aligning these models is how to most effectively use human resources, or model resources in the case where LLMs themselves are used as oracles. Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most prominent example of such a technique, but is complex and often unstable. Direct Preference Optimization (DPO) has recently been proposed as a simpler and more stable alternative. In this work, we develop an active learning strategy for DPO to make better use of preference labels. We propose a practical acquisition function for prompt/completion pairs based on the predictive entropy of the language model and a measure of certainty of the implicit preference model optimized by DPO. We demonstrate how our approach improves both the rate of learning and final performance of fine-tuning on pairwise preference data.",
      "authors": [
        "William Muldrew",
        "Peter Hayes",
        "Mingtian Zhang",
        "David Barber"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=CTgEV6qgUy",
      "cdate": 1706863525073,
      "mdate": 1719287285365,
      "matched_keywords": [
        "reinforcement learning",
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526342"
    },
    {
      "id": "18rzx2PXKm",
      "title": "Finite-Time Convergence and Sample Complexity of Actor-Critic Multi-Objective Reinforcement Learning",
      "abstract": "Reinforcement learning with multiple, potentially conflicting objectives is pervasive in real-world applications, while this problem remains theoretically under-explored. This paper tackles the multi-objective reinforcement learning (MORL) problem and introduces an innovative actor-critic algorithm named MOAC which finds a policy by iteratively making trade-offs among conflicting reward signals. Notably, we provide the first analysis of finite-time Pareto-stationary convergence and corresponding sample complexity in both discounted and average reward settings. Our approach has two salient features: (a) MOAC mitigates the cumulative estimation bias resulting from finding an optimal common gradient descent direction out of stochastic samples. This enables provable convergence rate and sample complexity guarantees independent of the number of objectives; (b) With proper momentum coefficient, MOAC initializes the weights of individual policy gradients using samples from the environment, instead of manual initialization. This enhances the practicality and robustness of our algorithm. Finally, experiments conducted on a real-world dataset validate the effectiveness of our proposed method.",
      "authors": [
        "Tianchen Zhou",
        "FNU Hairi",
        "Haibo Yang",
        "Jia Liu",
        "Tian Tong",
        "Fan Yang",
        "Michinari Momma",
        "Yan Gao"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=18rzx2PXKm",
      "cdate": 1706863379751,
      "mdate": 1719287285316,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526348"
    },
    {
      "id": "B5906M4Wnd",
      "title": "Automated Statistical Model Discovery with Language Models",
      "abstract": "Statistical model discovery is a challenging search over a vast space of models subject to domain-specific constraints. Efficiently searching over this space requires expertise in modeling and the problem domain. Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the principled framework of Box’s Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert. By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, which are key restrictions of previous systems. We evaluate our method in three settings in probabilistic modeling: searching within a restricted space of models, searching over an open-ended space, and improving expert models under natural language constraints (e.g., this model should be interpretable to an ecologist). Our method identifies models on par with human expert designed models and extends classic models in interpretable ways. Our results highlight the promise of LM-driven model discovery.",
      "authors": [
        "Michael Y. Li",
        "Emily Fox",
        "Noah Goodman"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=B5906M4Wnd",
      "cdate": 1706863299378,
      "mdate": 1719287285246,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526353"
    },
    {
      "id": "arwP5FA2dO",
      "title": "Position: Opportunities Exist for Machine Learning in Magnetic Fusion Energy",
      "abstract": "Magnetic confinement fusion may one day provide reliable, carbon-free energy, but the field currently faces technical hurdles. In this position paper, we highlight six key research challenges in the field of fusion energy that we believe should be research priorities for the Machine Learning (ML) community because they are especially ripe for ML applications: (1) disruption prediction, (2) simulation and dynamics modeling (3) resolving partially observed data, (4) improving controls, (5) guiding experiments with optimal design, and (6) enhancing materials discovery. For each problem, we give background, review past ML work, suggest features of future models, and list challenges and idiosyncrasies facing ML development. We also discuss ongoing efforts to update the fusion data ecosystem and identify opportunities further down the line that will be enabled as fusion and its data infrastructure advance. It is our position that fusion energy offers especially exciting opportunities for ML practitioners to impact decarbonization and the future of energy.",
      "authors": [
        "Lucas Spangher",
        "Allen M. Wang",
        "Andrew Maris",
        "Myles Stapelberg",
        "Viraj Mehta",
        "Alex Saperstein",
        "Stephen Lane-Walsh",
        "Akshata Kishore Moharir",
        "Alessandro Pau",
        "Cristina Rea"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=arwP5FA2dO",
      "cdate": 1706863216607,
      "mdate": 1719287285204,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526359"
    },
    {
      "id": "r8k5JrGip6",
      "title": "Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation",
      "abstract": "Despite the successes of large language models (LLMs), they exhibit significant drawbacks, particularly when processing long contexts. Their inference cost scales quadratically with respect to sequence length, making it expensive for deployment in some real-world text processing applications, such as retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the \"distraction phenomenon\", where irrelevant context in the prompt degrades output quality. To address these drawbacks, we propose a novel RAG prompting methodology, *superposition prompting*, which can be directly applied to pre-trained transformer-based LLMs *without the need for fine-tuning*. At a high level, superposition prompting allows the LLM to process input documents in parallel *prompt paths*, discarding paths once they are deemed irrelevant. We demonstrate the capability of our method to simultaneously enhance time efficiency across a variety of question-answering benchmarks using multiple pre-trained LLMs. Furthermore, our technique significantly improves accuracy when the retrieved context is large relative the context the model was trained on. For example, our approach facilitates a $93\\times$ reduction in compute time while *improving* accuracy by $43\\%$ on the NaturalQuestions-Open dataset with the MPT-7B instruction-tuned model over naive RAG.",
      "authors": [
        "Thomas Merth",
        "Qichen Fu",
        "Mohammad Rastegari",
        "Mahyar Najibi"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=r8k5JrGip6",
      "cdate": 1706863121842,
      "mdate": 1719287285049,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526364"
    },
    {
      "id": "lwWV4Zl3h1",
      "title": "Adaptive Conformal Inference by Betting",
      "abstract": "Conformal prediction is a valuable tool for quantifying predictive uncertainty of machine learning models. However, its applicability relies on the assumption of data exchangeability, a condition which is often not met in real-world scenarios. In this paper, we consider the problem of adaptive conformal inference without any assumptions about the data generating process. Existing approaches for adaptive conformal inference are based on optimizing the pinball loss using variants of online gradient descent. A notable shortcoming of such approaches is in their explicit dependence on and sensitivity to the choice of the learning rates. In this paper, we propose a different approach for adaptive conformal inference that leverages parameter-free online convex optimization techniques. We prove that our method controls long-term miscoverage frequency at a nominal level and demonstrate its convincing empirical performance without any need of performing cumbersome parameter tuning.",
      "authors": [
        "Aleksandr Podkopaev",
        "Dong Xu",
        "Kuang-chih Lee"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lwWV4Zl3h1",
      "cdate": 1706862967478,
      "mdate": 1719287284922,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526370"
    },
    {
      "id": "DwTgy1hXXo",
      "title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents",
      "abstract": "Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are also dynamically configurable, allowing multifaceted analysis. We conducted extensive evaluations using MPA and found that most LLMs achieve poorer performance, indicating room for improvement. Our multifaceted analysis demonstrated the strong correlation between the basic abilities and an implicit Mattew effect on model size, i.e., larger models possess stronger correlations of the abilities. MPA can also be used as a data augmentation approach to enhance LLMs. Code is available at: https://github.com/microsoft/promptbench.",
      "authors": [
        "Kaijie Zhu",
        "Jindong Wang",
        "Qinlin Zhao",
        "Ruochen Xu",
        "Xing Xie"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DwTgy1hXXo",
      "cdate": 1706862943068,
      "mdate": 1719287284924,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526376"
    },
    {
      "id": "WLPhywf1si",
      "title": "Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models",
      "abstract": "Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many large vision-language models (LVLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (LVLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of LVLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one. No retraining or fine-tuning of the down-stream LVLMs is required. The code and robust models are available on GitHub.",
      "authors": [
        "Christian Schlarmann",
        "Naman Deep Singh",
        "Francesco Croce",
        "Matthias Hein"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=WLPhywf1si",
      "cdate": 1706862810854,
      "mdate": 1719287284693,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526382"
    },
    {
      "id": "99UFZV2VpU",
      "title": "How Does Goal Relabeling Improve Sample Efficiency?",
      "abstract": "Hindsight experience replay and goal relabeling are successful in reinforcement learning (RL) since they enable agents to learn from failures. Despite their successes, we lack a theoretical understanding, such as (i) why hindsight experience replay improves sample efficiency and (ii) how to design a relabeling method that achieves sample efficiency. To this end, we construct an example to show the information-theoretical improvement in sample efficiency achieved by goal relabeling. Our example reveals that goal relabeling can enhance sample efficiency and exploit the rich information in observations through better hypothesis elimination. Based on these insights, we develop an RL algorithm called GOALIVE. To analyze the sample complexity of GOALIVE, we introduce a complexity measure, the goal-conditioned Bellman-Eluder (GOAL-BE) dimension, which characterizes the sample complexity of goal-conditioned RL problems. Compared to the Bellman-Eluder dimension, the goal-conditioned version offers an exponential improvement in the best case. To the best of our knowledge, our work provides the first characterization of the theoretical improvement in sample efficiency achieved by goal relabeling.",
      "authors": [
        "Sirui Zheng",
        "Chenjia Bai",
        "Zhuoran Yang",
        "Zhaoran Wang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=99UFZV2VpU",
      "cdate": 1706862689680,
      "mdate": 1719287284655,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526387"
    },
    {
      "id": "DzLna0cFL1",
      "title": "Position: Towards Unified Alignment Between Agents, Humans, and Environment",
      "abstract": "The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of **U**nified **A**lignment for **A**gents (**UA**$^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of **UA**$^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles demonstrating intentions, personalized reranking reflecting complex environmental dynamics, and runtime cost statistics as self-constraints. We then follow the principles of **UA**$^2$ to propose an initial design of our agent and benchmark its performance with several candidate baselines in the retrofitted WebShop. The extensive experimental results further prove the importance of the principles of **UA**$^2$. Our research sheds light on the next steps of autonomous agent research with improved general problem-solving abilities.",
      "authors": [
        "Zonghan Yang",
        "An Liu",
        "Zijun Liu",
        "Kaiming Liu",
        "Fangzhou Xiong",
        "Yile Wang",
        "Zeyuan Yang",
        "Qingyuan Hu",
        "Xinrui Chen",
        "Zhenhe Zhang",
        "Fuwen Luo",
        "Zhicheng Guo",
        "Peng Li",
        "Yang Liu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DzLna0cFL1",
      "cdate": 1706862669766,
      "mdate": 1719287284512,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526393"
    },
    {
      "id": "lsHZNNoC7r",
      "title": "DistiLLM: Towards Streamlined Distillation for Large Language Models",
      "abstract": "Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing student models while achieving up to 4.3$\\times$ speedup compared to recent KD methods.",
      "authors": [
        "Jongwoo Ko",
        "Sungnyun Kim",
        "Tianyi Chen",
        "Se-Young Yun"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lsHZNNoC7r",
      "cdate": 1706862657124,
      "mdate": 1719287284481,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526399"
    },
    {
      "id": "CQI3f1U9X1",
      "title": "Quantum Algorithms and Lower Bounds for Finite-Sum Optimization",
      "abstract": "Finite-sum optimization has wide applications in machine learning, covering important problems such as support vector machines, regression, etc. In this paper, we initiate the study of solving finite-sum optimization problems by quantum computing. Specifically, let $f_1,\\ldots,f_n:\\mathbb{R}^d\\to\\mathbb{R}$ be $\\ell$-smooth convex functions and $\\psi:\\mathbb{R}^d\\to\\mathbb{R}$ be a $\\mu$-strongly convex proximal function. The goal is to find an $\\epsilon$-optimal point for $F(\\mathbf{x})=\\frac{1}{n}\\sum_{i=1}^n f_i(\\mathbf{x})+\\psi(\\mathbf{x})$. We give a quantum algorithm with complexity $\\tilde{O}\\big(n+\\sqrt{d}+\\sqrt{\\ell/\\mu}\\big(n^{1/3}d^{1/3}+n^{-2/3}d^{5/6}\\big)\\big)$, improving the classical tight bound $\\tilde{\\Theta}\\big(n+\\sqrt{n\\ell/\\mu}\\big)$. We also prove a quantum lower bound $\\tilde{\\Omega}(n+n^{3/4}(\\ell/\\mu)^{1/4})$ when $d$ is large enough. Both our quantum upper and lower bounds can extend to the cases where $\\psi$ is not necessarily strongly convex, or each $f_i$ is Lipschitz but not necessarily smooth. In addition, when $F$ is nonconvex, our quantum algorithm can find an $\\epsilon$-critial point using $\\tilde{O}(n+\\ell(d^{1/3}n^{1/3}+\\sqrt{d})/\\epsilon^2)$ queries.",
      "authors": [
        "Yexin Zhang",
        "Chenyi Zhang",
        "Cong Fang",
        "Liwei Wang",
        "Tongyang Li"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=CQI3f1U9X1",
      "cdate": 1706862623806,
      "mdate": 1719287284422,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526404"
    },
    {
      "id": "A6fmX9QCEa",
      "title": "Tuning-Free Stochastic Optimization",
      "abstract": "Large-scale machine learning problems make the cost of hyperparameter tuning ever more prohibitive. This creates a need for algorithms that can tune themselves on-the-fly. We formalize the notion of *``tuning-free''* algorithms that can match the performance of optimally-tuned optimization algorithms up to polylogarithmic factors given only loose hints on the relevant problem parameters. We consider in particular algorithms that can match optimally-tuned Stochastic Gradient Descent (SGD). When the domain of optimization is bounded, we show tuning-free matching of SGD is possible and achieved by several existing algorithms. We prove that for the task of minimizing a convex and smooth or Lipschitz function over an unbounded domain, tuning-free optimization is impossible. We discuss conditions under which tuning-free optimization is possible even over unbounded domains. In particular, we show that the recently proposed DoG and DoWG algorithms are tuning-free when the noise distribution is sufficiently well-behaved. For the task of finding a stationary point of a smooth and potentially nonconvex function, we give a variant of SGD that matches the best-known high-probability convergence rate for tuned SGD at only an additional polylogarithmic cost. However, we also give an impossibility result that shows no algorithm can hope to match the optimal expected convergence rate for tuned SGD with high probability.",
      "authors": [
        "Ahmed Khaled",
        "Chi Jin"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=A6fmX9QCEa",
      "cdate": 1706862586975,
      "mdate": 1719287284356,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526410"
    },
    {
      "id": "VJjjNrUi8j",
      "title": "Second-Order Uncertainty Quantification: A Distance-Based Approach",
      "abstract": "In the past couple of years, various approaches to representing and quantifying different types of predictive uncertainty in machine learning, notably in the setting of classification, have been proposed on the basis of second-order probability distributions, i.e., predictions in the form of distributions on probability distributions. A completely conclusive solution has not yet been found, however, as shown by recent criticisms of commonly used uncertainty measures associated with second-order distributions, identifying undesirable theoretical properties of these measures. In light of these criticisms, we propose a set of formal criteria that meaningful uncertainty measures for predictive uncertainty based on second-order distributions should obey. Moreover, we provide a general framework for developing uncertainty measures to account for these criteria, and offer an instantiation based on the Wasserstein distance, for which we prove that all criteria are satisfied.",
      "authors": [
        "Yusuf Sale",
        "Viktor Bengs",
        "Michele Caprio",
        "Eyke Hüllermeier"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=VJjjNrUi8j",
      "cdate": 1706862546879,
      "mdate": 1719287284328,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526416"
    },
    {
      "id": "laIOUtstMs",
      "title": "Meta-Reinforcement Learning Robust to Distributional Shift Via Performing Lifelong In-Context Learning",
      "abstract": "A key challenge in Meta-Reinforcement Learning (meta-RL) is the task distribution shift, since the generalization ability of most current meta-RL methods is limited to tasks sampled from the training distribution. In this paper, we propose Posterior Sampling Bayesian Lifelong In-Context Reinforcement Learning (PSBL), which is robust to task distribution shift. PSBL meta-trains a variant of transformer to directly perform amortized inference about the Predictive Posterior Distribution (PPD) of the optimal policy. Once trained, the network can infer the PPD online with frozen parameters. The agent then samples actions from the approximate PPD to perform online exploration, which progressively reduces uncertainty and enhances performance in the interaction with the environment. This property is known as in-context learning. Experimental results demonstrate that PSBL significantly outperforms standard Meta RL methods both in tasks with sparse rewards and dense rewards when the test task distribution is strictly shifted from the training distribution.",
      "authors": [
        "Tengye Xu",
        "Zihao Li",
        "Qinyuan Ren"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=laIOUtstMs",
      "cdate": 1706862462531,
      "mdate": 1719287284281,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526423"
    },
    {
      "id": "jKnW7r7de1",
      "title": "BetterV: Controlled Verilog Generation with Discriminative Guidance",
      "abstract": "Due to the growing complexity of modern Integrated Circuits (ICs), there is a need for automated circuit design methods. Recent years have seen increasing research in hardware design language generation to facilitate the design process. In this work, we propose a Verilog generation framework, BetterV, which fine-tunes large language models (LLMs) on processed domain-specific datasets and incorporates generative discriminators for guidance on particular design demands. Verilog modules are collected, filtered, and processed from the internet to form a clean and abundant dataset. Instruct-tuning methods are specially designed to fine-tune the LLMs to understand knowledge about Verilog. Furthermore, data are augmented to enrich the training set and are also used to train a generative discriminator on particular downstream tasks, providing guidance for the LLMs to optimize Verilog implementation. BetterV has the ability to generate syntactically and functionally correct Verilog, outperforming GPT-4 on the VerilogEval benchmark. With the help of task-specific generative discriminators, BetterV achieves remarkable improvements on various electronic design automation (EDA) downstream tasks, including netlist node reduction for synthesis and verification runtime reduction with Boolean Satisfiability (SAT) solving.",
      "authors": [
        "Zehua PEI",
        "Huiling Zhen",
        "Mingxuan Yuan",
        "Yu Huang",
        "Bei Yu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jKnW7r7de1",
      "cdate": 1706862304940,
      "mdate": 1719287284181,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526428"
    },
    {
      "id": "FVmqX0sYz9",
      "title": "Auditing Private Prediction",
      "abstract": "Differential privacy (DP) offers a theoretical upper bound on the potential privacy leakage of an algorithm, while empirical auditing establishes a practical lower bound. Auditing techniques exist for DP training algorithms. However machine learning can also be made private at inference. We propose the first framework for auditing private prediction where we instantiate adversaries with varying poisoning and query capabilities. This enables us to study the privacy leakage of four private prediction algorithms: PATE (Papernot et al., 2016), CaPC (Choquette-Choo et al., 2020), PromptPATE (Duan et al., 2023), and Private-kNN (Zhu et al., 2020). To conduct our audit, we introduce novel techniques to empirically evaluate privacy leakage in terms of Renyi DP. Our experiments show that (i) the privacy analysis of private prediction can be improved, (ii) algorithms which are easier to poison lead to much higher privacy leakage, and (iii) the privacy leakage is significantly lower for adversaries without query control than those with full control.",
      "authors": [
        "Karan Chadha",
        "Matthew Jagielski",
        "Nicolas Papernot",
        "Christopher A. Choquette-Choo",
        "Milad Nasr"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FVmqX0sYz9",
      "cdate": 1706862272090,
      "mdate": 1719287284156,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526435"
    },
    {
      "id": "j56JAd29uH",
      "title": "FADAS: Towards Federated Adaptive Asynchronous Optimization",
      "abstract": "Federated learning (FL) has emerged as a widely adopted training paradigm for privacy-preserving machine learning. While the SGD-based FL algorithms have demonstrated considerable success in the past, there is a growing trend towards adopting adaptive federated optimization methods, particularly for the training of large-scale models. However, the conventional synchronous aggregation design poses a significant challenge to the practical deployment of those adaptive federated optimization methods, particularly in the presence of straggler clients. To fill this research gap, this paper introduces federated adaptive asynchronous optimization, named FADAS, a novel method that incorporates asynchronous updates into adaptive federated optimization with provable guarantees. To further enhance the efficiency and resilience of our proposed method in scenarios with significant asynchronous delays, we also extend FADAS with a delay-adaptive learning adjustment strategy. We rigorously establish the convergence rate of the proposed algorithms and empirical results demonstrate the superior performance of FADAS over other asynchronous FL baselines.",
      "authors": [
        "Yujia Wang",
        "Shiqiang Wang",
        "Songtao Lu",
        "Jinghui Chen"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=j56JAd29uH",
      "cdate": 1706862209010,
      "mdate": 1719287284118,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526440"
    },
    {
      "id": "mkbSXxovP5",
      "title": "Double Stochasticity Gazes Faster: Snap-Shot Decentralized Stochastic Gradient Tracking Methods",
      "abstract": "In decentralized optimization, $m$ agents form a network and only communicate with their neighbors, which gives advantages in data ownership, privacy, and scalability. At the same time, decentralized stochastic gradient descent ($\\texttt{SGD}$) methods, as popular decentralized algorithms for training large-scale machine learning models, have shown their superiority over centralized counterparts. Distributed stochastic gradient tracking $\\texttt{DSGT}$ has been recognized as the popular and state-of-the-art decentralized $\\texttt{SGD}$ method due to its proper theoretical guarantees. However, the theoretical analysis of $\\texttt{DSGT}$ shows that its iteration complexity is $\\tilde{\\mathcal{O}} \\left(\\frac{\\bar{\\sigma}^2}{m\\mu \\varepsilon} + \\frac{\\sqrt{L}\\bar{\\sigma}}{\\mu(1 - \\lambda_2(W))^{1/2} C_W \\sqrt{\\varepsilon} }\\right)$, where the doubly stochastic matrix $W$ represents the network topology and $ C_W $ is a parameter that depends on $W$. Thus, it indicates that the convergence property of $\\texttt{DSGT}$ is heavily affected by the topology of the communication network. To overcome the weakness of $\\texttt{DSGT}$, we resort to the snap-shot gradient tracking skill and propose two novel algorithms, snap-shot $\\texttt{DSGT}$ ($\\texttt{SS-DSGT}$) and accelerated snap-shot $\\texttt{DSGT}$ ($\\texttt{ASS-DSGT}$). We further justify that $\\texttt{SS-DSGT}$ exhibits a lower iteration complexity compared to $\\texttt{DSGT}$ in the general communication network topology. Additionally, $\\texttt{ASS-DSGT}$ matches $\\texttt{DSGT}$'s iteration complexity $\\mathcal{O}\\left( \\frac{\\bar{\\sigma}^2}{m\\mu \\varepsilon} + \\frac{\\sqrt{L}\\bar{\\sigma}}{\\mu (1 - \\lambda_2(W))^{1/2}\\sqrt{\\varepsilon}} \\right)$ under the same conditions as $\\texttt{DSGT}$. Numerical experiments validate $\\texttt{SS-DSGT}$'s superior performance performance in the general communication network topology and exhibit better practical performance of $\\texttt{ASS-DSGT}$ on the specified $W$ compared to $\\texttt{DSGT}$.",
      "authors": [
        "Hao Di",
        "Haishan Ye",
        "Xiangyu Chang",
        "Guang Dai",
        "Ivor Tsang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=mkbSXxovP5",
      "cdate": 1706862053250,
      "mdate": 1719287284081,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526445"
    },
    {
      "id": "8uzBOVmh8H",
      "title": "CLLMs: Consistency Large Language Models",
      "abstract": "Jacobi decoding shows promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into more parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point in a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\\times$ to 3.4$\\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.",
      "authors": [
        "Siqi Kou",
        "Lanxiang Hu",
        "Zhezhi He",
        "Zhijie Deng",
        "Hao Zhang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=8uzBOVmh8H",
      "cdate": 1706861989103,
      "mdate": 1719287284043,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526451"
    },
    {
      "id": "hYHsrKDiX7",
      "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection",
      "abstract": "Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.",
      "authors": [
        "Jiawei Zhao",
        "Zhenyu Zhang",
        "Beidi Chen",
        "Zhangyang Wang",
        "Anima Anandkumar",
        "Yuandong Tian"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hYHsrKDiX7",
      "cdate": 1706861487537,
      "mdate": 1719287283795,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526457"
    },
    {
      "id": "cbZTnjqIib",
      "title": "Understanding the Impact of Introducing Constraints at Inference Time on Generalization Error",
      "abstract": "Since machine learning technologies are being used in various practical situations, models with merely low prediction errors might not be satisfactory; prediction errors occurring with a low probability might yield dangerous results in some applications. Therefore, there are attempts to achieve an ML model whose input-output pairs are guaranteed to satisfy given constraints. Among such attempts, many previous works chose the approach of modifying the outputs of an ML model at the inference time to satisfy the constraints. Such a strategy is handy because we can control its output without expensive training or fine-tuning. However, it is unclear whether using constraints only in the inference time degrades a model's predictive performance. This paper analyses how the generalization error bounds change when we only put constraints in the inference time. Our main finding is that a class of loss functions preserves the relative generalization error, i.e., the difference in generalization error compared with the best model will not increase by imposing constraints at the inference time on multi-class classification. Some popular loss functions preserve the relative error, including the softmax cross-entropy loss. On the other hand, we also show that some loss functions do not preserve relative error when we use constraints. Our results suggest the importance of choosing a suitable loss function when we only use constraints in the inference time.",
      "authors": [
        "Masaaki Nishino",
        "Kengo Nakamura",
        "Norihito Yasuda"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=cbZTnjqIib",
      "cdate": 1706861366538,
      "mdate": 1719287283735,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526462"
    },
    {
      "id": "emtXYlBrNF",
      "title": "AttnLRP: Attention-Aware Layer-Wise Relevance Propagation for Transformers",
      "abstract": "Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a single backward pass. Through extensive evaluations against existing methods on LLaMa 2, Mixtral 8x7b, Flan-T5 and vision transformer architectures, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concept-based explanations. We provide an LRP library at https://github.com/rachtibat/LRP-eXplains-Transformers.",
      "authors": [
        "Reduan Achtibat",
        "Sayed Mohammad Vakilzadeh Hatefi",
        "Maximilian Dreyer",
        "Aakriti Jain",
        "Thomas Wiegand",
        "Sebastian Lapuschkin",
        "Wojciech Samek"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=emtXYlBrNF",
      "cdate": 1706861323553,
      "mdate": 1719287283730,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526468"
    },
    {
      "id": "jP1zeEqHli",
      "title": "Learning the Target Network in Function Space",
      "abstract": "We focus on the task of learning the value function in the reinforcement learning (RL) setting. This task is often solved by updating a pair of online and target networks while ensuring that the parameters of these two networks are equivalent. We propose Lookahead-Replicate (LR), a new value-function approximation algorithm that is agnostic to this parameter-space equivalence. Instead, the LR algorithm is designed to maintain an equivalence between the two networks in the function space. This value-based equivalence is obtained by employing a new target-network update. We show that LR leads to a convergent behavior in learning the value function. We also present empirical results demonstrating that LR-based target-network updates significantly improve deep RL on the Atari benchmark.",
      "authors": [
        "Kavosh Asadi",
        "Yao Liu",
        "Shoham Sabach",
        "Ming Yin",
        "Rasool Fakoor"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jP1zeEqHli",
      "cdate": 1706861277930,
      "mdate": 1719287283651,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526473"
    },
    {
      "id": "OI1YP53WKI",
      "title": "ReDiffuser: Reliable Decision-Making Using a Diffuser with Confidence Estimation",
      "abstract": "The diffusion model has demonstrated impressive performance in offline reinforcement learning. However, non-deterministic sampling in diffusion models can lead to unstable performance. Furthermore, the lack of confidence measurements makes it difficult to evaluate the reliability and trustworthiness of the sampled decisions. To address these issues, we present ReDiffuser, which utilizes confidence estimation to ensure reliable decision-making. We achieve this by learning a confidence function based on Random Network Distillation. The confidence function measures the reliability of sampled decisions and contributes to quantitative recognition of reliable decisions. Additionally, we integrate the confidence function into task-specific sampling procedures to realize adaptive-horizon planning and value-embedded planning. Experiments show that the proposed ReDiffuser achieves state-of-the-art performance on standard offline RL datasets.",
      "authors": [
        "Nantian He",
        "Shaohui Li",
        "Zhi Li",
        "Yu LIU",
        "You He"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OI1YP53WKI",
      "cdate": 1706861253146,
      "mdate": 1719287283601,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526479"
    },
    {
      "id": "HCDMiaT0Pf",
      "title": "Adversarially Robust Hypothesis Transfer Learning",
      "abstract": "In this work, we explore Hypothesis Transfer Learning (HTL) under adversarial attacks. In this setting, a learner has access to a training dataset of size $n$ from an underlying distribution $\\mathcal{D}$ and a set of auxiliary hypotheses. These auxiliary hypotheses, which can be viewed as prior information originating either from expert knowledge or as pre-trained foundation models, are employed as an initialization for the learning process. Our goal is to develop an adversarially robust model for $\\mathcal{D}$. We begin by examining an adversarial variant of the regularized empirical risk minimization learning rule that we term A-RERM. Assuming a non-negative smooth loss function with a strongly convex regularizer, we establish a bound on the robust generalization error of the hypothesis returned by A-RERM in terms of the robust empirical loss and the quality of the initialization. If the initialization is good, i.e., there exists a weighted combination of auxiliary hypotheses with a small robust population loss, the bound exhibits a fast rate of $\\mathcal{O}(1/n)$. Otherwise, we get the standard rate of $\\mathcal{O}(1/\\sqrt{n})$. Additionally, we provide a bound on the robust excess risk which is similar in nature, albeit with a slightly worse rate. We also consider solving the problem using a practical variant, namely proximal stochastic adversarial training, and present a bound that depends on the initialization. This bound has the same dependence on the sample size as the ARERM bound, except for an additional term that depends on the size of the adversarial perturbation.",
      "authors": [
        "Yunjuan Wang",
        "Raman Arora"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=HCDMiaT0Pf",
      "cdate": 1706861250731,
      "mdate": 1719287283582,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526484"
    },
    {
      "id": "4BIOZSz7zU",
      "title": "Remembering to Be Fair: Non-Markovian Fairness in Sequential Decision Making",
      "abstract": "Fair decision making has largely been studied with respect to a single decision. Here we investigate the notion of fairness in the context of sequential decision making where multiple stakeholders can be affected by the outcomes of decisions. We observe that fairness often depends on the history of the sequential decision-making process, and in this sense that it is inherently non-Markovian. We further observe that fairness often needs to be assessed at time points *within* the process, not just at the end of the process. To advance our understanding of this class of fairness problems, we explore the notion of non-Markovian fairness in the context of sequential decision making. We identify properties of non-Markovian fairness, including notions of long-term, anytime, periodic, and bounded fairness. We explore the interplay between non-Markovian fairness and memory and how memory can support construction of fair policies. Finally, we introduce the FairQCM algorithm, which can automatically augment its training data to improve sample efficiency in the synthesis of fair policies via reinforcement learning.",
      "authors": [
        "Parand A. Alamdari",
        "Toryn Q. Klassen",
        "Elliot Creager",
        "Sheila A. McIlraith"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4BIOZSz7zU",
      "cdate": 1706861155623,
      "mdate": 1719287283551,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526490"
    },
    {
      "id": "2zI2scD2Iz",
      "title": "Hybrid Inverse Reinforcement Learning",
      "abstract": "The inverse reinforcement learning approach to imitation learning is a double-edged sword. On the one hand, it can enable learning from a smaller number of expert demonstrations with more robustness to error compounding than behavioral cloning approaches. On the other hand, it requires that the learner repeatedly solve a computationally expensive reinforcement learning (RL) problem. Often, much of this computation is wasted searching over policies very dissimilar to the expert's. In this work, we propose using *hybrid RL* -- training on a mixture of online and expert data -- to curtail unnecessary exploration. Intuitively, the expert data focuses the learner on good states during training, which reduces the amount of exploration required to compute a strong policy. Notably, such an approach doesn't need the ability to reset the learner to arbitrary states in the environment, a requirement of prior work in efficient inverse RL. More formally, we derive a reduction from inverse RL to *expert-competitive RL* (rather than globally optimal RL) that allows us to dramatically reduce interaction during the inner policy search loop while maintaining the benefits of the IRL approach. This allows us to derive both model-free and model-based hybrid inverse RL algorithms with strong policy performance guarantees. Empirically, we find that our approaches are significantly more sample efficient than standard inverse RL and several other baselines on a suite of continuous control tasks.",
      "authors": [
        "Juntao Ren",
        "Gokul Swamy",
        "Steven Wu",
        "Drew Bagnell",
        "Sanjiban Choudhury"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=2zI2scD2Iz",
      "cdate": 1706861123351,
      "mdate": 1719287283519,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526495"
    },
    {
      "id": "3tJDnEszco",
      "title": "CHEMREASONER: Heuristic Search over a Large Language Model’s Knowledge Space using Quantum-Chemical Feedback",
      "abstract": "The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and reaction energy barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automatically guide the exploration without human input, providing competitive performance against expert-enumerated chemical descriptor-based implementations. By integrating language-guided reasoning with computational chemistry feedback, our work pioneers AI-accelerated, trustworthy catalyst discovery.",
      "authors": [
        "Henry W. Sprueill",
        "Carl Edwards",
        "Khushbu Agarwal",
        "Mariefel V Olarte",
        "Udishnu Sanyal",
        "Conrad Johnston",
        "Hongbin Liu",
        "Heng Ji",
        "Sutanay Choudhury"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3tJDnEszco",
      "cdate": 1706861115298,
      "mdate": 1719287283481,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526500"
    },
    {
      "id": "9ANyvRtFGa",
      "title": "HexGen: Generative Inference of Large Language Model over Heterogeneous Environment",
      "abstract": "Serving generative inference of the large language model is a crucial component of contemporary AI applications. In this paper, our focus lies in deploying such services in a heterogeneous and cross-datacenter setting to mitigate the substantial inference costs typically associated with a single centralized datacenter. Towards this end, we propose HexGen, a flexible distributed inference engine that uniquely supports the asymmetric partition of generative inference computations over both tensor model parallelism and pipeline parallelism, which allows for effective deployment across diverse GPUs interconnected by a fully heterogeneous network. We further propose a sophisticated scheduling algorithm grounded in constrained optimization that can adaptively assign asymmetric inference computation across the GPUs to fulfill inference requests while maintaining acceptable latency levels. We conduct an extensive empirical study to evaluate the efficiency of HexGen by serving the state-of-the-art Llama-2 (70B) model. The experimental results suggest that HexGen can choose to achieve up to $2.3\\times$ lower latency deadlines or tolerate up to $4\\times$ more traffic request rates compared with the homogeneous baseline given the same budget.",
      "authors": [
        "YOUHE JIANG",
        "Ran Yan",
        "Xiaozhe Yao",
        "Yang Zhou",
        "Beidi Chen",
        "Binhang Yuan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9ANyvRtFGa",
      "cdate": 1706861102760,
      "mdate": 1719287283455,
      "matched_keywords": [
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526506"
    },
    {
      "id": "1lDAGDe0UR",
      "title": "CATS: Enhancing Multivariate Time Series Forecasting by Constructing Auxiliary Time Series as Exogenous Variables",
      "abstract": "For Multivariate Time Series Forecasting (MTSF), recent deep learning applications show that univariate models frequently outperform multivariate ones. To address the deficiency in multivariate models, we introduce a method to Construct Auxiliary Time Series (CATS) that functions like a 2D temporal-contextual attention mechanism, which generates Auxiliary Time Series (ATS) from Original Time Series (OTS) to effectively represent and incorporate inter-series relationships for forecasting. Key principles of ATS—continuity, sparsity, and variability—are identified and implemented through different modules. Even with a basic 2-layer MLP as the core predictor, CATS achieves state-of-the-art, significantly reducing complexity and parameters compared to previous multivariate models, marking it as an efficient and transferable MTSF solution.",
      "authors": [
        "Jiecheng Lu",
        "Xu Han",
        "Yan Sun",
        "Shihao Yang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=1lDAGDe0UR",
      "cdate": 1706860904868,
      "mdate": 1721271738070,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526511"
    },
    {
      "id": "BiENLaUwlK",
      "title": "Safe Reinforcement Learning using Finite-Horizon Gradient-based Estimation",
      "abstract": "A key aspect of Safe Reinforcement Learning (Safe RL) involves estimating the constraint condition for the next policy, which is crucial for guiding the optimization of safe policy updates. However, the existing *Advantage-based Estimation* (ABE) method relies on the infinite-horizon discounted advantage function. This dependence leads to catastrophic errors in finite-horizon scenarios with non-discounted constraints, resulting in safety-violation updates. In response, we propose the first estimation method for finite-horizon non-discounted constraints in deep Safe RL, termed *Gradient-based Estimation* (GBE), which relies on the analytic gradient derived along trajectories. Our theoretical and empirical analyses demonstrate that GBE can effectively estimate constraint changes over a finite horizon. Constructing a surrogate optimization problem with GBE, we developed a novel Safe RL algorithm called *Constrained Gradient-based Policy Optimization* (CGPO). CGPO identifies feasible optimal policies by iteratively resolving sub-problems within trust regions. Our empirical results reveal that CGPO, unlike baseline algorithms, successfully estimates the constraint functions of subsequent policies, thereby ensuring the efficiency and feasibility of each update.",
      "authors": [
        "Juntao Dai",
        "Yaodong Yang",
        "Qian Zheng",
        "Gang Pan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BiENLaUwlK",
      "cdate": 1706860738593,
      "mdate": 1719287283231,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526517"
    },
    {
      "id": "7DbIyQlfaO",
      "title": "Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension",
      "abstract": "We study how to characterize and predict the truthfulness of texts generated from large language models (LLMs), which serves as a crucial step in building trust between humans and LLMs. Although several approaches based on entropy or verbalized uncertainty have been proposed to calibrate model predictions, these methods are often intractable, sensitive to hyperparameters, and less reliable when applied in generative tasks with LLMs. In this paper, we suggest investigating internal activations and quantifying LLM's truthfulness using the local intrinsic dimension (LID) of model activations. Through experiments on four question answering (QA) datasets, we demonstrate the effectiveness of our proposed method. Additionally, we study intrinsic dimensions in LLMs and their relations with model layers, autoregressive language modeling, and the training of LLMs, revealing that intrinsic dimensions can be a powerful approach to understanding LLMs.",
      "authors": [
        "Fan Yin",
        "Jayanth Srinivasa",
        "Kai-Wei Chang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7DbIyQlfaO",
      "cdate": 1706860705905,
      "mdate": 1719287283199,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526522"
    },
    {
      "id": "EDEISRmi6X",
      "title": "Bipartite Matching in Massive Graphs: A Tight Analysis of EDCS",
      "abstract": "Maximum matching is one of the most fundamental combinatorial optimization problems with applications in various contexts such as balanced clustering, data mining, resource allocation, and online advertisement. In many of these applications, the input graph is massive. The sheer size of these inputs makes it impossible to store the whole graph in the memory of a single machine and process it there. Graph sparsification has been an extremely powerful tool to alleviate this problem. In this paper, we study a highly successful and versatile sparsifier for the matching problem: the *edge-degree constrained subgraph (EDCS)* introduced first by Bernstein & Stein 2015 The EDCS has a parameter $\\beta \\geq 2$ which controls the density of the sparsifier. It has been shown through various proofs in the literature that by picking a subgraph with $O(n\\beta)$ edges, the EDCS includes a matching of size at least $2/3-O(1/\\beta)$ times the maximum matching size. As such, by increasing $\\beta$ the approximation ratio of EDCS gets closer and closer to $2/3$. In this paper, we propose a new approach for analyzing the approximation ratio of EDCS. Our analysis is *tight* for any value of $\\beta$. Namely, we pinpoint the precise approximation ratio of EDCS for any sparsity parameter $\\beta$. Our analysis reveals that one does not necessarily need to increase $\\beta$ to improve approximation, as suggested by previous analysis. In particular, the best choice turns out to be $\\beta = 6$, which achieves an approximation ratio of $.677$! This is arguably surprising as it is even better than $2/3 \\sim .666$, the bound that was widely believed to be the limit for EDCS.",
      "authors": [
        "Amir Azarmehr",
        "Soheil Behnezhad",
        "Mohammad Roghani"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EDEISRmi6X",
      "cdate": 1706860674369,
      "mdate": 1719287283163,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526527"
    },
    {
      "id": "qKL25sGjxL",
      "title": "GiLOT: Interpreting Generative Language Models via Optimal Transport",
      "abstract": "While large language models (LLMs) surge with the rise of generative AI, algorithms to explain LLMs highly desire. Existing feature attribution methods adequate for discriminative language models like BERT often fail to deliver faithful explanations for LLMs, primarily due to two issues: (1) For every specific prediction, the LLM outputs a probability distribution over the vocabulary–a large number of tokens with unequal semantic distance; (2) As an autoregressive language model, the LLM handles input tokens while generating a sequence of probability distributions of various tokens. To address above two challenges, this work proposes GiLOT that leverages Optimal Transport to measure the distributional change of all possible generated sequences upon the absence of every input token, while taking into account the tokens’ similarity, so as to faithfully estimate feature attribution for LLMs. We have carried out extensive experiments on top of Llama families and their fine-tuned derivatives across various scales to validate the effectiveness of GiLOT for estimating the input attributions. The results show that GiLOT outperforms existing solutions on a number of faithfulness metrics under fair comparison settings. Source code is publicly available at https://github.com/holyseven/GiLOT.",
      "authors": [
        "Xuhong Li",
        "Jiamin Chen",
        "Yekun Chai",
        "Haoyi Xiong"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qKL25sGjxL",
      "cdate": 1706860586788,
      "mdate": 1719287283094,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526533"
    },
    {
      "id": "s6ZAT8MLKU",
      "title": "Fundamental Benefit of Alternating Updates in Minimax Optimization",
      "abstract": "The Gradient Descent-Ascent (GDA) algorithm, designed to solve minimax optimization problems, takes the descent and ascent steps either simultaneously (Sim-GDA) or alternately (Alt-GDA). While Alt-GDA is commonly observed to converge faster, the performance gap between the two is not yet well understood theoretically, especially in terms of global convergence rates. To address this theory-practice gap, we present fine-grained convergence analyses of both algorithms for strongly-convex-strongly-concave and Lipschitz-gradient objectives. Our new iteration complexity upper bound of Alt-GDA is strictly smaller than the lower bound of Sim-GDA; i.e., Alt-GDA is provably faster. Moreover, we propose Alternating-Extrapolation GDA (Alex-GDA), a general algorithmic framework that subsumes Sim-GDA and Alt-GDA, for which the main idea is to alternately take gradients from extrapolations of the iterates. We show that Alex-GDA satisfies a smaller iteration complexity bound, identical to that of the Extra-gradient method, while requiring less gradient computations. We also prove that Alex-GDA enjoys linear convergence for bilinear problems, for which both Sim-GDA and Alt-GDA fail to converge at all.",
      "authors": [
        "Jaewook Lee",
        "Hanseul Cho",
        "Chulhee Yun"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=s6ZAT8MLKU",
      "cdate": 1706860482316,
      "mdate": 1719287283039,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526539"
    },
    {
      "id": "C4OpREezgj",
      "title": "AlphaZero-Like Tree-Search can Guide Large Language Model Decoding and Training",
      "abstract": "Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the multi-step reasoning capabilities of LLMs by using tree-search algorithms. These methods rely on prompting a pre-trained model to serve as a value function and focus on problems with low search depth. As a result, these methods cannot benefit from in-domain training and only rely on pretraining process — they will not work in domains where the pre-trained LLM does not have enough knowledge to serve as an effective value function or in domains that require long-horizon planning. To address these limitations, we present an AlphaZero-like tree-search learning framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLM decoding. TS-LLM distinguishes itself in two key ways. (1) Leveraging a learned value function and AlphaZero-like algorithms, our approach can be generally adaptable to a wide range of tasks, language models of any size, and tasks of varying search depths. (2) Our approach can guide LLMs during both inference and training, iteratively improving the LLMs. Empirical results across reasoning, planning, alignment, and decision-making tasks show that TS-LLM outperforms existing approaches and can handle trees with a depth of 64.",
      "authors": [
        "Ziyu Wan",
        "Xidong Feng",
        "Muning Wen",
        "Stephen Marcus McAleer",
        "Ying Wen",
        "Weinan Zhang",
        "Jun Wang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=C4OpREezgj",
      "cdate": 1706860395599,
      "mdate": 1719287282994,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526544"
    },
    {
      "id": "aoAPOOtN9E",
      "title": "Toward Adaptive Reasoning in Large Language Models with Thought Rollback",
      "abstract": "Large language models (LLMs) have been routinely used to solve various tasks using step-by-step reasoning. However, the structure of intermediate reasoning steps, or *thoughts*, is rigid and unidirectional, such as chains, trees, or acyclic-directed graphs. Consequently, the resulting inflexible and forward-only reasoning may not address challenging tasks and fail when the LLM frequently gives false responses, i.e., hallucinations. This paper proposes a new reasoning framework, called *Thought Rollback* (TR), allowing LLMs to adaptively build thought structure while maintaining effective reasoning toward problem-solving under hallucinations. The core mechanism of TR is *rolling back thoughts*, which allows LLMs to perform error analysis on thoughts, and thus roll back to any previously mistaken thought for revision. Subsequently, by including such trial-and-error in the prompt to guide the LLM, each rollback leads to one more reliable reasoning path. Therefore, starting with a simple prompt without human annotations, LLM with TR adaptively and gradually explores thoughts for a correct solution. Comprehensive experiments on mathematical problems and multi-task reasoning demonstrate the state-of-the-art performance of TR in terms of problem-solving rate and interaction cost. For instance, the solving rate of GPT-4 with TR outperforms the current best by $9\\%$ on the MATH dataset. The source code is available under the folder *examples/ThoughtRollback* of https://github.com/iQua/llmpebase.",
      "authors": [
        "Sijia Chen",
        "Baochun Li"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=aoAPOOtN9E",
      "cdate": 1706860170360,
      "mdate": 1719287282910,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526550"
    },
    {
      "id": "LWD7upg1ob",
      "title": "Differentially Private Synthetic Data via Foundation Model APIs 2: Text",
      "abstract": "Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text data with a formal privacy guarantee, i.e., differential privacy (DP), offers a promising and scalable solution. However, existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE) algorithm to generate DP synthetic images with only API access to diffusion models. In this work, we propose an augmented PE algorithm, named Aug-PE, that applies to the complex setting of text. We use API access to an LLM and generate DP synthetic text without any model training. We conduct comprehensive experiments on three benchmark datasets. Our results demonstrate that Aug-PE produces DP synthetic text that yields competitive utility with the SOTA DP finetuning baselines. This underscores the feasibility of relying solely on API access of LLMs to produce high-quality DP synthetic texts, thereby facilitating more accessible routes to privacy-preserving LLM applications.",
      "authors": [
        "Chulin Xie",
        "Zinan Lin",
        "Arturs Backurs",
        "Sivakanth Gopi",
        "Da Yu",
        "Huseyin A Inan",
        "Harsha Nori",
        "Haotian Jiang",
        "Huishuai Zhang",
        "Yin Tat Lee",
        "Bo Li",
        "Sergey Yekhanin"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LWD7upg1ob",
      "cdate": 1706860136035,
      "mdate": 1719287282889,
      "matched_keywords": [
        "machine learning",
        "large language model",
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526556"
    },
    {
      "id": "SAbZExIIgG",
      "title": "Decomposable Submodular Maximization in Federated Setting",
      "abstract": "Submodular functions, as well as the sub-class of decomposable submodular functions, and their optimization appear in a wide range of applications in machine learning, recommendation systems, and welfare maximization. However, optimization of decomposable submodular functions with millions of component functions is computationally prohibitive. Furthermore, the component functions may be private (they might represent user preference function, for example) and cannot be widely shared. To address these issues, we propose a *federated optimization* setting for decomposable submodular optimization. In this setting, clients have their own preference functions, and a weighted sum of these preferences needs to be maximized. We implement the popular *continuous greedy* algorithm in this setting where clients take parallel small local steps towards the local solution and then the local changes are aggregated at a central server. To address the large number of clients, the aggregation is performed only on a subsampled set. Further, the aggregation is performed only intermittently between stretches of parallel local steps, which reduces communication cost significantly. We show that our federated algorithm is guaranteed to provide a good approximate solution, even in the presence of above cost-cutting measures. Finally, we show how the federated setting can be incorporated in solving fundamental discrete submodular optimization problems such as Maximum Coverage and Facility Location.",
      "authors": [
        "Akbar Rafiey"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=SAbZExIIgG",
      "cdate": 1706860051976,
      "mdate": 1719287282853,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526562"
    },
    {
      "id": "MV2b44zDd3",
      "title": "Consistent Adversarially Robust Linear Classification: Non-Parametric Setting",
      "abstract": "For binary classification in $d$ dimensions, it is known that with a sample size of $n$, an excess adversarial risk of $O(d/n)$ is achievable under strong parametric assumptions about the underlying data distribution (e.g., assuming a Gaussian mixture model). In the case of well-separated distributions, this rate can be further refined to $O(1/n)$. Our work studies the non-parametric setting, where very little is known. With only mild regularity conditions on the conditional distribution of the features, we examine adversarial attacks with respect to arbitrary norms and introduce a straightforward yet effective estimator with provable consistency w.r.t adversarial risk. Our estimator is given by minimizing a series of smoothed versions of the robust 0/1 loss, with a smoothing bandwidth that adapts to both $n$ and $d$. Furthermore, we demonstrate that our estimator can achieve the minimax excess adversarial risk of $\\widetilde O(\\sqrt{d/n})$ for linear classifiers, at the cost of solving possibly rougher optimization problems.",
      "authors": [
        "Elvis Dohmatob"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MV2b44zDd3",
      "cdate": 1706859772152,
      "mdate": 1719287282773,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526568"
    },
    {
      "id": "39UqOkTjFn",
      "title": "When Do Skills Help Reinforcement Learning? A Theoretical Analysis of Temporal Abstractions",
      "abstract": "Skills are temporal abstractions that are intended to improve reinforcement learning (RL) performance through hierarchical RL. Despite our intuition about the properties of an environment that make skills useful, a precise characterization has been absent. We provide the first such characterization, focusing on the utility of deterministic skills in deterministic sparse-reward environments with finite action spaces. We show theoretically and empirically that RL performance gain from skills is worse in environments where solutions to states are less compressible. Additional theoretical results suggest that skills benefit exploration more than they benefit learning from existing experience, and that using unexpressive skills such as macroactions may worsen RL performance. We hope our findings can guide research on automatic skill discovery and help RL practitioners better decide when and how to use skills.",
      "authors": [
        "Zhening Li",
        "Gabriel Poesia",
        "Armando Solar-Lezama"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=39UqOkTjFn",
      "cdate": 1706859604492,
      "mdate": 1719287282722,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526573"
    },
    {
      "id": "OdsZS0E0AO",
      "title": "Compact Optimality Verification for Optimization Proxies",
      "abstract": "Recent years have witnessed increasing interest in optimization proxies, i.e., machine learning models that approximate the input-output mapping of parametric optimization problems and return near-optimal feasible solutions. Following recent work by (Nellikkath & Chatzivasileiadis, 2021), this paper reconsiders the optimality verification problem for optimization proxies, i.e., the determination of the worst-case optimality gap over the instance distribution. The paper proposes a compact formulation for optimality verification and a gradient-based primal heuristic that brings significant computational benefits to the original formulation. The compact formulation is also more general and applies to non-convex optimization problems. The benefits of the compact formulation are demonstrated on large-scale DC Optimal Power Flow and knapsack problems.",
      "authors": [
        "Wenbo Chen",
        "Haoruo Zhao",
        "Mathieu Tanneau",
        "Pascal Van Hentenryck"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OdsZS0E0AO",
      "cdate": 1706859480874,
      "mdate": 1719287282669,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526579"
    },
    {
      "id": "myCgfQZzbc",
      "title": "BeigeMaps: Behavioral Eigenmaps for Reinforcement Learning from Images",
      "abstract": "Training reinforcement learning (RL) agents directly from high-dimensional image observations continues to be a challenging problem. Recent line of work on behavioral distances proposes to learn representations that encode behavioral similarities quantified by the bisimulation metric. By learning an isometric mapping to a lower dimensional space that preserves this metric, such methods attempt to learn representations that group together functionally similar states. However, such an isometric mapping may not exist, making the learning objective ill-defined. We propose an alternative objective that allows distortions in long-range distances, while preserving *local* metric structure -- inducing representations that highlight natural clusters in the state space. This leads to new representations, which we term Behavioral Eigenmaps (BeigeMaps), corresponding to the eigenfunctions of similarity kernels induced by behavioral distances. We empirically demonstrate that when added as a drop-in modification, BeigeMaps improve the policy performance of prior behavioral distance based RL algorithms.",
      "authors": [
        "Sandesh Adhikary",
        "Anqi Li",
        "Byron Boots"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=myCgfQZzbc",
      "cdate": 1706859467741,
      "mdate": 1719287282629,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526585"
    },
    {
      "id": "eo88noTbb5",
      "title": "Agnostic Learning of Mixed Linear Regressions with EM and AM Algorithms",
      "abstract": "Mixed linear regression is a well-studied problem in parametric statistics and machine learning. Given a set of samples, tuples of covariates and labels, the task of mixed linear regression is to find a small list of linear relationships that best fit the samples. Usually it is assumed that the label is generated stochastically by randomly selecting one of two or more linear functions, applying this chosen function to the covariates, and potentially introducing noise to the result. In that situation, the objective is to estimate the ground-truth linear functions up to some parameter error. The popular expectation maximization (EM) and alternating minimization (AM) algorithms have been previously analyzed for this. In this paper, we consider the more general problem of agnostic learning of mixed linear regression from samples, without such generative models. In particular, we show that the AM and EM algorithms, under standard conditions of separability and good initialization, lead to agnostic learning in mixed linear regression by converging to the population loss minimizers, for suitably defined loss functions. In some sense, this shows the strength of AM and EM algorithms that converges to ``optimal solutions'' even in the absence of realizable generative models.",
      "authors": [
        "Avishek Ghosh",
        "Arya Mazumdar"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=eo88noTbb5",
      "cdate": 1706859386540,
      "mdate": 1719287282561,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526591"
    },
    {
      "id": "qg6AlnpEQH",
      "title": "On the Emergence of Cross-Task Linearity in Pretraining-Finetuning Paradigm",
      "abstract": "The pretraining-finetuning paradigm has become the prevailing trend in modern deep learning. In this work, we discover an intriguing linear phenomenon in models that are initialized from a common pretrained checkpoint and finetuned on different tasks, termed as Cross-Task Linearity (CTL). Specifically, we show that if we linearly interpolate the weights of two finetuned models, the features in the weight-interpolated model are often approximately equal to the linear interpolation of features in two finetuned models at each layer. We provide comprehensive empirical evidence supporting that CTL consistently occurs for finetuned models that start from the same pretrained checkpoint. We conjecture that in the pretraining-finetuning paradigm, neural networks approximately function as linear maps, mapping from the parameter space to the feature space. Based on this viewpoint, our study unveils novel insights into explaining model merging/editing, particularly by translating operations from the parameter space to the feature space. Furthermore, we delve deeper into the root cause for the emergence of CTL, highlighting the role of pretraining.",
      "authors": [
        "Zhanpeng Zhou",
        "Zijun Chen",
        "Yilan Chen",
        "Bo Zhang",
        "Junchi Yan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qg6AlnpEQH",
      "cdate": 1706859137670,
      "mdate": 1719287282511,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526597"
    },
    {
      "id": "Edz0QXKKAo",
      "title": "Position: Graph Foundation Models Are Already Here",
      "abstract": "Graph Foundation Models (GFMs) are emerging as a significant research topic in the graph domain, aiming to develop graph models trained on extensive and diverse data to enhance their applicability across various tasks and domains. Developing GFMs presents unique challenges over traditional Graph Neural Networks (GNNs), which are typically trained from scratch for specific tasks on particular datasets. The primary challenge in constructing GFMs lies in effectively leveraging vast and diverse graph data to achieve positive transfer. Drawing inspiration from existing foundation models in the CV and NLP domains, we propose a novel perspective for the GFM development by advocating for a \"graph vocabulary'', in which the basic transferable units underlying graphs encode the invariance on graphs. We ground the graph vocabulary construction from essential aspects including network analysis, expressiveness, and stability. Such a vocabulary perspective can potentially advance the future GFM design in line with the neural scaling laws. All relevant resources with GFM design can be found here.",
      "authors": [
        "Haitao Mao",
        "Zhikai Chen",
        "Wenzhuo Tang",
        "Jianan Zhao",
        "Yao Ma",
        "Tong Zhao",
        "Neil Shah",
        "Mikhail Galkin",
        "Jiliang Tang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Edz0QXKKAo",
      "cdate": 1706858415451,
      "mdate": 1719287282269,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526603"
    },
    {
      "id": "GDp7Gyd9nf",
      "title": "Mechanistic Design and Scaling of Hybrid Architectures",
      "abstract": "The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.",
      "authors": [
        "Michael Poli",
        "Armin W Thomas",
        "Eric Nguyen",
        "Pragaash Ponnusamy",
        "Björn Deiseroth",
        "Kristian Kersting",
        "Taiji Suzuki",
        "Brian Hie",
        "Stefano Ermon",
        "Christopher Re",
        "Ce Zhang",
        "Stefano Massaroli"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GDp7Gyd9nf",
      "cdate": 1706858390909,
      "mdate": 1719287282244,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526609"
    },
    {
      "id": "EEinDTdKr1",
      "title": "Fine-grained Local Sensitivity Analysis of Standard Dot-Product Self-Attention",
      "abstract": "Self-attention has been widely used in various machine learning models, such as vision transformers. The standard dot-product self-attention is arguably the most popular structure, and there is a growing interest in understanding the mathematical properties of such attention mechanisms. This paper presents a fine-grained local sensitivity analysis of the standard dot-product self-attention, leading to new non-vacuous certified robustness results for vision transformers. Despite the well-known fact that dot-product self-attention is not (globally) Lipschitz, we develop new theoretical analysis of Local Fine-grained Attention Sensitivity (LoFAST) quantifying the effect of input feature perturbations on the attention output. Our analysis reveals that the local sensitivity of dot-product self-attention to $\\ell_2$ perturbations can actually be controlled by several key quantities associated with the attention weight matrices and the unperturbed input. We empirically validate our theoretical findings by computing non-vacuous certified $\\ell_2$-robustness for vision transformers on CIFAR-10 and SVHN datasets. The code for LoFAST is available at https://github.com/AaronHavens/LoFAST.",
      "authors": [
        "Aaron J Havens",
        "Alexandre Araujo",
        "Huan Zhang",
        "Bin Hu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EEinDTdKr1",
      "cdate": 1706858157034,
      "mdate": 1719287282101,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526615"
    },
    {
      "id": "jklD0TV5Hw",
      "title": "Seesaw: Compensating for Nonlinear Reduction with Linear Computations for Private Inference",
      "abstract": "With increasingly serious data privacy concerns and strict regulations, privacy-preserving machine learning (PPML) has emerged to securely execute machine learning tasks without violating privacy. Unfortunately, the computational cost to securely execute nonlinear computations in PPML remains significant, calling for new model architecture designs with fewer nonlinear operations. We propose Seesaw, a novel neural architecture search method tailored for PPML. Seesaw exploits a previously unexplored opportunity to leverage more linear computations and nonlinear result reuse, in order to compensate for the accuracy loss due to nonlinear reduction. It incorporates specifically designed pruning and search strategies, not only to efficiently handle the much larger design space of both linear and nonlinear operators, but also to achieve a better balance between the model accuracy and the online/offline execution latencies. Compared to the state-of-the-art design for image classification on ImageNet, Seesaw achieves 1.68$\\times$ lower online latency and 1.55$\\times$ lower total online + offline latency at 71% iso-accuracy, or 3.65% higher accuracy at iso-latency of 190 seconds, while using much simpler and faster search and training methods.",
      "authors": [
        "Fabing Li",
        "Yuanhao Zhai",
        "Shuangyu Cai",
        "Mingyu Gao"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jklD0TV5Hw",
      "cdate": 1706858133475,
      "mdate": 1719287282054,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526620"
    },
    {
      "id": "vuMD71R20q",
      "title": "Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective",
      "abstract": "Adaptive gradient optimizers like Adam(W) are the default training algorithms for many deep learning architectures, such as transformers. Their diagonal preconditioner is based on the gradient outer product which is incorporated into the parameter update via a square root. While these methods are often motivated as approximate second-order methods, the square root represents a fundamental difference. In this work, we investigate how the behavior of adaptive methods changes when we remove the root, i.e. strengthen their second-order motivation. Surprisingly, we find that such square-root-free adaptive methods close the generalization gap to SGD on convolutional architectures, while maintaining their root-based counterpart's performance on transformers. The second-order perspective also has practical benefits for the development of non-diagonal adaptive methods through the concept of preconditioner invariance. In contrast to root-based methods like Shampoo, the root-free counterparts do not require numerically unstable matrix root decompositions and inversions, thus work well in half precision. Our findings provide new insights into the development of adaptive methods and raise important questions regarding the currently overlooked role of adaptivity for their success.",
      "authors": [
        "Wu Lin",
        "Felix Dangel",
        "Runa Eschenhagen",
        "Juhan Bae",
        "Richard E. Turner",
        "Alireza Makhzani"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vuMD71R20q",
      "cdate": 1706858002016,
      "mdate": 1719287282016,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526625"
    },
    {
      "id": "VaZVZQSgTP",
      "title": "A Single-Loop Robust Policy Gradient Method for Robust Markov Decision Processes",
      "abstract": "Robust Markov Decision Processes (RMDPs) have recently been recognized as a valuable and promising approach to discovering a policy with creditable performance, particularly in the presence of a dynamic environment and estimation errors in the transition matrix due to limited data. Despite extensive exploration of dynamic programming algorithms for solving RMDPs, there has been a notable upswing in interest in developing efficient algorithms using the policy gradient method. In this paper, we propose the first single-loop robust policy gradient (SRPG) method with the global optimality guarantee for solving RMDPs through its minimax formulation. Moreover, we complement the convergence analysis of the nonconvex-nonconcave min-max optimization problem with the objective function's gradient dominance property, which is not explored in the prior literature. Numerical experiments validate the efficacy of SRPG, demonstrating its faster and more robust convergence behavior compared to its nested-loop counterpart.",
      "authors": [
        "Zhenwei Lin",
        "Chenyu Xue",
        "Qi Deng",
        "Yinyu Ye"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=VaZVZQSgTP",
      "cdate": 1706857968951,
      "mdate": 1719287282019,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526630"
    },
    {
      "id": "OLvgrLtv6J",
      "title": "Exploiting Code Symmetries for Learning Program Semantics",
      "abstract": "This paper tackles the challenge of teaching code semantics to Large Language Models (LLMs) for program analysis by incorporating code symmetries into the model architecture. We introduce a group-theoretic framework that defines code symmetries as semantics-preserving transformations, where forming a code symmetry group enables precise and efficient reasoning of code semantics. Our solution, SymC, develops a novel variant of self-attention that is provably equivariant to code symmetries from the permutation group defined over the program dependence graph. SymC obtains superior performance on five program analysis tasks, outperforming state-of-the-art code models, including GPT-4, without any pre-training. Our results suggest that code LLMs that encode the code structural prior via the code symmetry group generalize better and faster.",
      "authors": [
        "Kexin Pei",
        "Weichen Li",
        "Qirui Jin",
        "Shuyang Liu",
        "Scott Geng",
        "Lorenzo Cavallaro",
        "Junfeng Yang",
        "Suman Jana"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OLvgrLtv6J",
      "cdate": 1706857934044,
      "mdate": 1719287281982,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526636"
    },
    {
      "id": "vFk9fqXLst",
      "title": "Interpreting Equivariant Representations",
      "abstract": "Latent representations are extensively used for tasks like visualization, interpolation, or feature extraction in deep learning models. This paper demonstrates the importance of considering the inductive bias imposed by an equivariant model when using latent representations as neglecting these biases can lead to decreased performance in downstream tasks. We propose principles for choosing invariant projections of latent representations and show their effectiveness in two examples: A permutation equivariant variational auto-encoder for molecular graph generation, where an invariant projection can be designed to maintain information without loss, and for a rotation-equivariant representation in image classification, where random invariant projections proves to retain a high degree of information. In both cases, the analysis of invariant latent representations proves superior to their equivariant counterparts. Finally, we illustrate that the phenomena documented here for equivariant neural networks have counterparts in standard neural networks where invariance is encouraged via augmentation.",
      "authors": [
        "Andreas Abildtrup Hansen",
        "Anna Calissano",
        "Aasa Feragen"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vFk9fqXLst",
      "cdate": 1706857836029,
      "mdate": 1719287281910,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526642"
    },
    {
      "id": "xcDRx8vzCa",
      "title": "CHAI: Clustered Head Attention for Efficient LLM Inference",
      "abstract": "Large Language Models (LLMs) with hundreds of billions of parameters have transformed the field of machine learning. However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory. Multi-head attention is one of the key components of LLMs, which can for over 50% of LLMs memory and compute requirement. We observe that there is a high amount of redundancy across heads on which tokens they pay attention to. Based on this insight, we propose Clustered HeadAttention ( CHAI ). CHAI combines heads with a high amount of correlation for self-attention at runtime, thus reducing both memory and compute. In our experiments, we show that CHAI is able to reduce the memory requirements for storing K,V cache by up to 21.4% and inference time latency by up to 1.73× without any fine-tuning required. CHAI achieves this with a maximum 3.2% deviation in accuracy across 3 different models (i.e. OPT-66B, LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets.",
      "authors": [
        "Saurabh Agarwal",
        "Bilge Acun",
        "Basil Hosmer",
        "Mostafa Elhoushi",
        "Yejin Lee",
        "Shivaram Venkataraman",
        "Dimitris Papailiopoulos",
        "Carole-Jean Wu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xcDRx8vzCa",
      "cdate": 1706857710008,
      "mdate": 1719287281849,
      "matched_keywords": [
        "machine learning",
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526647"
    },
    {
      "id": "15MpDbv3IQ",
      "title": "Tuning-free Estimation and Inference of Cumulative Distribution Function under Local Differential Privacy",
      "abstract": "We introduce a novel algorithm for estimating Cumulative Distribution Function (CDF) values under Local Differential Privacy (LDP) by exploiting an unexpected connection between LDP and the current status problem, a classical survival data problem in statistics. This connection leads to the development of tools for constrained isotonic estimation based on binary queries. Through mathematical proofs and extensive numerical testing, we demonstrate that our method achieves uniform and $L_2$ error bounds when estimating the entire CDF curve. By employing increasingly dense grids, the error bound can be improved, exhibiting an asymptotic normal distribution of the proposed estimator. Theoretically, we show that the error bound smoothly changes as the number of grids increases relative to the sample size $n$. Computationally, we demonstrate that our constrained isotonic estimator can be efficiently computed deterministically, eliminating the need for hyperparameters or random optimization.",
      "authors": [
        "Yi Liu",
        "Qirui Hu",
        "Linglong Kong"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=15MpDbv3IQ",
      "cdate": 1706857499136,
      "mdate": 1719287281743,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526653"
    },
    {
      "id": "DbMm8pmoAP",
      "title": "Evolving Subnetwork Training for Large Language Models",
      "abstract": "Large language models have ushered in a new era of artificial intelligence research. However, their substantial training costs hinder further development and widespread adoption. In this paper, inspired by the redundancy in the parameters of large language models, we propose a novel training paradigm: Evolving Subnetwork Training (EST). EST samples subnetworks from the layers of the large language model and from commonly used modules within each layer, Multi-Head Attention (MHA) and Multi-Layer Perceptron (MLP). By gradually increasing the size of the subnetworks during the training process, EST can save the cost of training. We apply EST to train GPT2 model and TinyLlama model, resulting in 26.7% FLOPs saving for GPT2 and 25.0% for TinyLlama without an increase in loss on the pre-training dataset. Moreover, EST leads to performance improvements in downstream tasks, indicating that it benefits generalization. Additionally, we provide intuitive theoretical studies based on training dynamics and Dropout theory to ensure the feasibility of EST.",
      "authors": [
        "Hanqi Li",
        "Lu Chen",
        "Da Ma",
        "Zijian Wu",
        "Su Zhu",
        "Kai Yu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DbMm8pmoAP",
      "cdate": 1706857472473,
      "mdate": 1719287281720,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526659"
    },
    {
      "id": "foPMkomvk1",
      "title": "Pursuing Overall Welfare in Federated Learning through Sequential Decision Making",
      "abstract": "In traditional federated learning, a single global model cannot perform equally well for all clients. Therefore, the need to achieve the *client-level fairness* in federated system has been emphasized, which can be realized by modifying the static aggregation scheme for updating the global model to an adaptive one, in response to the local signals of the participating clients. Our work reveals that existing fairness-aware aggregation strategies can be unified into an online convex optimization framework, in other words, a central server's *sequential decision making* process. To enhance the decision making capability, we propose simple and intuitive improvements for suboptimal designs within existing methods, presenting $\\texttt{AAggFF}$. Considering practical requirements, we further subdivide our method tailored for the *cross-device* and the *cross-silo* settings, respectively. Theoretical analyses guarantee sublinear regret upper bounds for both settings: $\\mathcal{O}(\\sqrt{T \\log{K}})$ for the cross-device setting, and $\\mathcal{O}(K \\log{T})$ for the cross-silo setting, with $K$ clients and $T$ federation rounds. Extensive experiments demonstrate that the federated system equipped with $\\texttt{AAggFF}$ achieves better degree of client-level fairness than existing methods in both practical settings.",
      "authors": [
        "Seok-Ju Hahn",
        "Gi-Soo Kim",
        "Junghye Lee"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=foPMkomvk1",
      "cdate": 1706857335552,
      "mdate": 1719287281744,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526664"
    },
    {
      "id": "OKYfaYQlML",
      "title": "Knowledge Transfer from Vision Foundation Models for Efficient Training of Small Task-specific Models",
      "abstract": "Vision Foundation Models (VFMs) pretrained on massive datasets exhibit impressive performance on various downstream tasks, especially with limited labeled target data. However, due to their high inference compute cost, these models cannot be deployed for many real-world applications. Motivated by this, we ask the following important question, \"How can we leverage the knowledge from a large VFM to train a small task-specific model for a new target task with limited labeled training data?\", and propose a simple task-oriented knowledge transfer approach as a highly effective solution to this problem. Our experimental results on five target tasks show that the proposed approach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining, supervised ImageNet pretraining, and self-supervised DINO pretraining by up to 11.6%, 22.1%, 13.7%, and 29.8%, respectively. Furthermore, the proposed approach also demonstrates up to 9x, 4x and 15x reduction in pretraining compute cost when compared to task-agnostic VFM distillation, ImageNet pretraining and DINO pretraining, respectively, while outperforming them. We also show that the dataset used for transferring knowledge has a significant effect on the final target task performance, and introduce a retrieval-augmented knowledge transfer strategy that uses web-scale image retrieval to curate effective transfer sets.",
      "authors": [
        "Raviteja Vemulapalli",
        "Hadi Pouransari",
        "Fartash Faghri",
        "Sachin Mehta",
        "Mehrdad Farajtabar",
        "Mohammad Rastegari",
        "Oncel Tuzel"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OKYfaYQlML",
      "cdate": 1706857067241,
      "mdate": 1719287281671,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526669"
    },
    {
      "id": "bq1JEgioLr",
      "title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
      "abstract": "Most existing Large Language Model (LLM) benchmarks on scientific problem reasoning focus on problems grounded in high-school subjects and are confined to elementary algebraic operations. To systematically examine the reasoning capabilities required for solving complex scientific problems, we introduce an expansive benchmark suite SciBench for LLMs. SciBench contains a carefully curated dataset featuring a range of collegiate-level scientific problems from mathematics, chemistry, and physics domains. Based on the dataset, we conduct an in-depth benchmarking study of representative open-source and proprietary LLMs with various prompting strategies. The results reveal that current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.",
      "authors": [
        "Xiaoxuan Wang",
        "Ziniu Hu",
        "Pan Lu",
        "Yanqiao Zhu",
        "Jieyu Zhang",
        "Satyen Subramaniam",
        "Arjun R Loomba",
        "Shichang Zhang",
        "Yizhou Sun",
        "Wei Wang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=bq1JEgioLr",
      "cdate": 1706856972009,
      "mdate": 1719287281597,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526675"
    },
    {
      "id": "IzqpUC34Jg",
      "title": "Provable Privacy with Non-Private Pre-Processing",
      "abstract": "When analyzing Differentially Private (DP) machine learning pipelines, the potential privacy cost of data-dependent pre-processing is frequently overlooked in privacy accounting. In this work, we propose a general framework to evaluate the additional privacy cost incurred by non-private data-dependent pre-processing algorithms. Our framework establishes upper bounds on the overall privacy guarantees by utilising two new technical notions: a variant of DP termed Smooth DP and the bounded sensitivity of the pre-processing algorithms. In addition to the generic framework, we provide explicit overall privacy guarantees for multiple data-dependent pre-processing algorithms, such as data imputation, quantization, deduplication, standard scaling and PCA, when used in combination with several DP algorithms. Notably, this framework is also simple to implement, allowing direct integration into existing DP pipelines.",
      "authors": [
        "Yaxi Hu",
        "Amartya Sanyal",
        "Bernhard Schölkopf"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=IzqpUC34Jg",
      "cdate": 1706856892445,
      "mdate": 1719287281467,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526681"
    },
    {
      "id": "WPt9HRmMrG",
      "title": "Active Label Correction for Semantic Segmentation with Foundation Models",
      "abstract": "Training and validating models for semantic segmentation require datasets with pixel-wise annotations, which are notoriously labor-intensive. Although useful priors such as foundation models or crowdsourced datasets are available, they are error-prone. We hence propose an effective framework of active label correction (ALC) based on a design of correction query to rectify pseudo labels of pixels, which in turn is more annotator-friendly than the standard one inquiring to classify a pixel directly according to our theoretical analysis and user study. Specifically, leveraging foundation models providing useful zero-shot predictions on pseudo labels and superpixels, our method comprises two key techniques: (i) an annotator-friendly design of correction query with the pseudo labels, and (ii) an acquisition function looking ahead label expansions based on the superpixels. Experimental results on PASCAL, Cityscapes, and Kvasir-SEG datasets demonstrate the effectiveness of our ALC framework, outperforming prior methods for active semantic segmentation and label correction. Notably, utilizing our method, we obtained a revised dataset of PASCAL by rectifying errors in 2.6 million pixels in PASCAL dataset.",
      "authors": [
        "Hoyoung Kim",
        "Sehyun Hwang",
        "Suha Kwak",
        "Jungseul Ok"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=WPt9HRmMrG",
      "cdate": 1706856741756,
      "mdate": 1719287281348,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526687"
    },
    {
      "id": "4iBJyJeBX5",
      "title": "Generalized Smooth Variational Inequalities: Methods with Adaptive Stepsizes",
      "abstract": "Variational Inequality (VI) problems have attracted great interest in the machine learning (ML) community due to their application in adversarial and multi-agent training. Despite its relevance in ML, the oft-used strong-monotonicity and Lipschitz continuity assumptions on VI problems are restrictive and do not hold in many machine learning problems. To address this, we relax smoothness and monotonicity assumptions and study structured non-monotone generalized smoothness. The key idea of our results is in adaptive stepsizes. We prove the first-known convergence results for solving generalized smooth VIs for the three popular methods, namely, projection, Korpelevich, and Popov methods. Our convergence rate results for generalized smooth VIs match or improve existing results on smooth VIs. We present numerical experiments that support our theoretical guarantees and highlight the efficiency of proposed adaptive stepsizes.",
      "authors": [
        "Daniil Vankov",
        "Angelia Nedich",
        "Lalitha Sankar"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4iBJyJeBX5",
      "cdate": 1706856676576,
      "mdate": 1719287281227,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526692"
    },
    {
      "id": "3WCvnkHnxV",
      "title": "PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs",
      "abstract": "On-device training is currently the most common approach for training machine learning (ML) models on private, distributed user data. Despite this, on-device training has several drawbacks: (1) most user devices are too small to train large models on-device, (2) on-device training is communication- and computation-intensive, and (3) on-device training can be difficult to debug and deploy. To address these problems, we propose Private Evolution-Text (PrE-Text), a method for generating differentially private (DP) synthetic textual data. First, we show that across multiple datasets, training small models (models that fit on user devices) with PrE-Text synthetic data outperforms small models trained on-device under practical privacy regimes ($\\epsilon=1.29$, $\\epsilon=7.58$). We achieve these results while using 9$\\times$ fewer rounds, 6$\\times$ less client computation per round, and 100$\\times$ less communication per round. Second, finetuning large models on PrE-Text's DP synthetic data improves large language model (LLM) performance on private data across the same range of privacy budgets. Altogether, these results suggest that training on DP synthetic data can be a better option than training a model on-device on private distributed data. Code is available at https://github.com/houcharlie/PrE-Text.",
      "authors": [
        "Charlie Hou",
        "Akshat Shrivastava",
        "Hongyuan Zhan",
        "Rylan Conway",
        "Trang Le",
        "Adithya Sagar",
        "Giulia Fanti",
        "Daniel Lazar"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3WCvnkHnxV",
      "cdate": 1706856175827,
      "mdate": 1719287281020,
      "matched_keywords": [
        "machine learning",
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526699"
    },
    {
      "id": "6L4K5jmSJq",
      "title": "How Free is Parameter-Free Stochastic Optimization?",
      "abstract": "We study the problem of parameter-free stochastic optimization, inquiring whether, and under what conditions, do fully parameter-free methods exist: these are methods that achieve convergence rates competitive with optimally tuned methods, without requiring significant knowledge of the true problem parameters. Existing parameter-free methods can only be considered ``partially'' parameter-free, as they require some non-trivial knowledge of the true problem parameters, such as a bound on the stochastic gradient norms, a bound on the distance to a minimizer, etc. In the non-convex setting, we demonstrate that a simple hyperparameter search technique results in a fully parameter-free method that outperforms more sophisticated state-of-the-art algorithms. We also provide a similar result in the convex setting with access to noisy function values under mild noise assumptions. Finally, assuming only access to stochastic gradients, we establish a lower bound that renders fully parameter-free stochastic convex optimization infeasible, and provide a method which is (partially) parameter-free up to the limit indicated by our lower bound.",
      "authors": [
        "Amit Attia",
        "Tomer Koren"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=6L4K5jmSJq",
      "cdate": 1706856110707,
      "mdate": 1719287280934,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526705"
    },
    {
      "id": "sZla6SnooP",
      "title": "Physics-Informed Neural Network Policy Iteration: Algorithms, Convergence, and Verification",
      "abstract": "Solving nonlinear optimal control problems is a challenging task, particularly for high-dimensional problems. We propose algorithms for model-based policy iterations to solve nonlinear optimal control problems with convergence guarantees. The main component of our approach is an iterative procedure that utilizes neural approximations to solve linear partial differential equations (PDEs), ensuring convergence. We present two variants of the algorithms. The first variant formulates the optimization problem as a linear least square problem, drawing inspiration from extreme learning machine (ELM) for solving PDEs. This variant efficiently handles low-dimensional problems with high accuracy. The second variant is based on a physics-informed neural network (PINN) for solving PDEs and has the potential to address high-dimensional problems. We demonstrate that both algorithms outperform traditional approaches, such as Galerkin methods, by a significant margin. We provide a theoretical analysis of both algorithms in terms of convergence of neural approximations towards the true optimal solutions in a general setting. Furthermore, we employ formal verification techniques to demonstrate the verifiable stability of the resulting controllers.",
      "authors": [
        "Yiming Meng",
        "Ruikun Zhou",
        "Amartya Mukherjee",
        "Maxwell Fitzsimmons",
        "Christopher Song",
        "Jun Liu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=sZla6SnooP",
      "cdate": 1706856089664,
      "mdate": 1719287280914,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526711"
    },
    {
      "id": "yUxdk32TU6",
      "title": "COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability",
      "abstract": "Jailbreaks on large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controllability enabled by COLD-Attack leads to diverse new jailbreak scenarios which not only cover the standard setting of generating fluent (suffix) attack with continuation constraint, but also allow us to address new controllable attack settings such as revising a user query adversarially with paraphrasing constraint, and inserting stealthy attacks in context with position constraint. Our extensive experiments on various LLMs (Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5, and GPT-4) show COLD-Attack's broad applicability, strong controllability, high success rate, and attack transferability. Our code is available at https://github.com/Yu-Fangxu/COLD-Attack.",
      "authors": [
        "Xingang Guo",
        "Fangxu Yu",
        "Huan Zhang",
        "Lianhui Qin",
        "Bin Hu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=yUxdk32TU6",
      "cdate": 1706855954291,
      "mdate": 1719287280932,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526717"
    },
    {
      "id": "dJTChKgv3a",
      "title": "In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering",
      "abstract": "Large language models (LLMs) demonstrate emergent in-context learning capabilities, where they adapt to new tasks based on example demonstrations. However, in-context learning has seen limited effectiveness in many settings, is difficult to quantitatively control and takes up context window space. To overcome these limitations, we propose an alternative approach that recasts in-context learning as in-context vectors (ICV). Using ICV has two steps. We first use a forward pass on demonstration examples to create the in-context vector from the latent embedding of the LLM. This vector captures essential information about the intended task. On a new query, instead of adding demonstrations to the prompt, we shift the latent states of the LLM using the ICV. The ICV approach has several benefits: 1) it enables the LLM to more effectively follow the demonstration examples; 2) it's easy to control by adjusting the magnitude of the ICV; 3) it reduces the length of the prompt by removing the in-context demonstrations; 4) ICV is computationally much more efficient than fine-tuning. We demonstrate that ICV achieves better performance compared to standard in-context learning and fine-tuning on diverse tasks including safety, style transfer, role-playing and formatting. Moreover, we show that we can flexibly teach LLM to simultaneously follow different types of instructions by simple vector arithmetics on the corresponding ICVs.",
      "authors": [
        "Sheng Liu",
        "Haotian Ye",
        "Lei Xing",
        "James Y. Zou"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=dJTChKgv3a",
      "cdate": 1706855890168,
      "mdate": 1719287280691,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526723"
    },
    {
      "id": "YSoMmNWZZx",
      "title": "RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback",
      "abstract": "Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains — including classic control, as well as manipulation of rigid, articulated, and deformable objects — without the need for human supervision, outperforming prior methods that use large pretrained models for reward generation under the same assumptions. Videos can be found on our project website: https://rlvlmf2024.github.io/",
      "authors": [
        "Yufei Wang",
        "Zhanyi Sun",
        "Jesse Zhang",
        "Zhou Xian",
        "Erdem Biyik",
        "David Held",
        "Zackory Erickson"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=YSoMmNWZZx",
      "cdate": 1706855863660,
      "mdate": 1719287280621,
      "matched_keywords": [
        "reinforcement learning",
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526728"
    },
    {
      "id": "4axAQHwBOE",
      "title": "Certifiably Byzantine-Robust Federated Conformal Prediction",
      "abstract": "Conformal prediction has shown impressive capacity in constructing statistically rigorous prediction sets for machine learning models with exchangeable data samples. The siloed datasets, coupled with the escalating privacy concerns related to local data sharing, have inspired recent innovations extending conformal prediction into federated environments with distributed data samples. However, this framework for distributed uncertainty quantification is susceptible to Byzantine failures. A minor subset of malicious clients can significantly compromise the practicality of coverage guarantees. To address this vulnerability, we introduce a novel framework Rob-FCP, which executes robust federated conformal prediction, effectively countering malicious clients capable of reporting arbitrary statistics with the conformal calibration process. We theoretically provide the conformal coverage bound of Rob-FCP in the Byzantine setting and show that the coverage of Rob-FCP is asymptotically close to the desired coverage level. We also propose a malicious client number estimator to tackle a more challenging setting where the number of malicious clients is unknown to the defender and theoretically shows its effectiveness. We empirically demonstrate the robustness of Rob-FCP against diverse proportions of malicious clients under a variety of Byzantine attacks on five standard benchmark and real-world healthcare datasets.",
      "authors": [
        "Mintong Kang",
        "Zhen Lin",
        "Jimeng Sun",
        "Cao Xiao",
        "Bo Li"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4axAQHwBOE",
      "cdate": 1706855790611,
      "mdate": 1719287280560,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526733"
    },
    {
      "id": "6FXtu8clyp",
      "title": "Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models",
      "abstract": "Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance – a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization, and challenge sets that probe properties such as hallucination; evaluations that provide fine-grained insight VLM capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and training from base vs. instruct-tuned language models, amongst others. We couple our analysis with three resource contributions: (1) a unified framework for evaluating VLMs, (2) optimized, flexible training code, and (3) checkpoints for all models, including a family of VLMs at the 7-13B scale that strictly outperform InstructBLIP and LLaVa v1.5, the state-of-the-art in open VLMs.",
      "authors": [
        "Siddharth Karamcheti",
        "Suraj Nair",
        "Ashwin Balakrishna",
        "Percy Liang",
        "Thomas Kollar",
        "Dorsa Sadigh"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=6FXtu8clyp",
      "cdate": 1706855787418,
      "mdate": 1719287280479,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526739"
    },
    {
      "id": "uhHDhVKFMW",
      "title": "Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference",
      "abstract": "Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient. Relevant code can be found at https://github.com/hdong920/LESS.",
      "authors": [
        "Harry Dong",
        "Xinyu Yang",
        "Zhenyu Zhang",
        "Zhangyang Wang",
        "Yuejie Chi",
        "Beidi Chen"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=uhHDhVKFMW",
      "cdate": 1706855672628,
      "mdate": 1719287280456,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526744"
    },
    {
      "id": "uDkXoZMzBv",
      "title": "Compressible Dynamics in Deep Overparameterized Low-Rank Learning & Adaptation",
      "abstract": "While overparameterization in machine learning models offers great benefits in terms of optimization and generalization, it also leads to increased computational requirements as model sizes grow. In this work, we show that by leveraging the inherent low-dimensional structures of data and compressible dynamics within the model parameters, we can reap the benefits of overparameterization without the computational burdens. In practice, we demonstrate the effectiveness of this approach for deep low-rank matrix completion as well as fine-tuning language models. Our approach is grounded in theoretical findings for deep overparameterized low-rank matrix recovery, where we show that the learning dynamics of each weight matrix are confined to an invariant low-dimensional subspace. Consequently, we can construct and train compact, highly compressed factorizations possessing the same benefits as their overparameterized counterparts. In the context of deep matrix completion, our technique substantially improves training efficiency while retaining the advantages of overparameterization. For language model fine-tuning, we propose a method called \"Deep LoRA\", which improves the existing low-rank adaptation (LoRA) technique, leading to reduced overfitting and a simplified hyperparameter setup, while maintaining comparable efficiency. We validate the effectiveness of Deep LoRA on natural language tasks, particularly when fine-tuning with limited data.",
      "authors": [
        "Can Yaras",
        "Peng Wang",
        "Laura Balzano",
        "Qing Qu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=uDkXoZMzBv",
      "cdate": 1706855520831,
      "mdate": 1719287280285,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526748"
    },
    {
      "id": "wDDGQabYPQ",
      "title": "InferCept: Efficient Intercept Support for Augmented Large Language Model Inference",
      "abstract": "Large language models are increasingly integrated with external environments, tools, and agents like ChatGPT plugins to extend their capability beyond language-centric tasks. However, today's LLM inference systems are designed for standalone LLMs. They treat each external interaction as the end of LLM generation and form a new request when the interaction finishes, causing unnecessary recomputation of already computed contexts, which accounts for 37-40% of total model forwarding time. This paper presents **InferCept, the first LLM inference framework targeting augmented LLMs** and supporting the efficient interception of LLM generation. InferCept minimizes the GPU resource waste caused by LLM interceptions and dedicates saved memory for serving more requests.InferCept improves the overall serving throughput by **1.6x-2x** and completes 2x more requests per second compared to the state-of-the-art LLM inference systems.",
      "authors": [
        "Reyna Abhyankar",
        "Zijian He",
        "Vikranth Srivatsa",
        "Hao Zhang",
        "Yiying Zhang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=wDDGQabYPQ",
      "cdate": 1706855220590,
      "mdate": 1719287280177,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526755"
    },
    {
      "id": "0wso32h0jc",
      "title": "Neural-Kernel Conditional Mean Embeddings",
      "abstract": "Kernel conditional mean embeddings (CMEs) offer a powerful framework for representing conditional distributions, but they often face scalability and expressiveness challenges. In this work, we propose a new method that effectively combines the strengths of deep learning with CMEs in order to address these challenges. Specifically, our approach leverages the end-to-end neural network (NN) optimization framework using a kernel-based objective. This design circumvents the computationally expensive Gram matrix inversion required by current CME methods. To further enhance performance, we provide efficient strategies to optimize the remaining kernel hyperparameters. In conditional density estimation tasks, our NN-CME hybrid achieves competitive performance and often surpasses existing deep learning-based methods. Lastly, we showcase its remarkable versatility by seamlessly integrating it into reinforcement learning (RL) contexts. Building on Q-learning, our approach naturally leads to a new variant of distributional RL methods, which demonstrates consistent effectiveness across different environments.",
      "authors": [
        "Eiki Shimizu",
        "Kenji Fukumizu",
        "Dino Sejdinovic"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0wso32h0jc",
      "cdate": 1706855206957,
      "mdate": 1719287280152,
      "matched_keywords": [
        "deep learning",
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526761"
    },
    {
      "id": "kIHIA6Lr0B",
      "title": "Pluvial Flood Emulation with Hydraulics-informed Message Passing",
      "abstract": "Machine Learning (ML) has emerged as a promising alternative to numerical methods for physics-based simulation due to its flexibility and efficiency. Flood modeling is a key case study for ML-based simulation due to its relevance as a tool for supporting preventive and emergency measures to mitigate flood risks. However, the complexity of the topography or domain (ground elevation) and the sparsity of the time-evolving precipitations (external forcing) can be challenging for most existing ML approaches for simulating flooding processes in space and time. Another critical challenge is incorporating physics domain knowledge (hydraulics) into these data-driven models. This paper addresses these challenges by introducing a hydraulics-informed graph neural network for flood simulation. Given a (geographical) region and precipitation data, our model predicts water depths in an auto-regressive fashion. We propose a message-passing framework inspired by the conservation of momentum and mass expressed in the shallow-water equations, which describe the physical process of a flooding event. Empirical results on a dataset covering 9 regions and 7 historical precipitation events demonstrate that our model outperforms the best baseline, and can capture the propagation of water flow more effectively, especially at the very early stage of the flooding event when the amount of water in the domain is scarce. Differently from some of the most recent methods for ML-based simulation, which tend to work well only when the domain is a smooth surface (e.g., flat terrain), we show that our solution achieves accurate results for real ground elevation data.",
      "authors": [
        "Arnold Kazadi",
        "James Doss-Gollin",
        "Arlei Lopes da Silva"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kIHIA6Lr0B",
      "cdate": 1706854998090,
      "mdate": 1719287280079,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526767"
    },
    {
      "id": "jsmaWEdx9g",
      "title": "Efficient Algorithms for Sum-Of-Minimum Optimization",
      "abstract": "In this work, we propose a novel optimization model termed ``sum-of-minimum\" optimization. This model seeks to minimize the sum or average of $N$ objective functions over $k$ parameters, where each objective takes the minimum value of a predefined sub-function with respect to the $k$ parameters. This universal framework encompasses numerous clustering applications in machine learning and related fields. We develop efficient algorithms for solving sum-of-minimum optimization problems, inspired by a randomized initialization algorithm for the classic $k$-means (Arthur & Vassilvitskii, 2007) and Lloyd's algorithm (Lloyd, 1982). We establish a new tight bound for the generalized initialization algorithm and prove a gradient-descent-like convergence rate for generalized Lloyd's algorithm. The efficiency of our algorithms is numerically examined on multiple tasks, including generalized principal component analysis, mixed linear regression, and small-scale neural network training. Our approach compares favorably to previous ones based on simpler-but-less-precise optimization reformulations.",
      "authors": [
        "Lisang Ding",
        "Ziang Chen",
        "Xinshang Wang",
        "Wotao Yin"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jsmaWEdx9g",
      "cdate": 1706854921349,
      "mdate": 1719287280020,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526773"
    },
    {
      "id": "XtDJaSe8jE",
      "title": "Transferring Knowledge From Large Foundation Models to Small Downstream Models",
      "abstract": "How do we transfer the relevant knowledge from ever larger foundation models into small, task-specific downstream models that can run at much lower costs? Standard transfer learning using pre-trained weights as the initialization transfers limited information and commits us to often massive pre-trained architectures. This procedure also precludes combining multiple pre-trained models that learn complementary information. To address these shortcomings, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the smaller downstream model. Rather than indiscriminately compressing all pre-trained features, AFT adaptively transfers pre-trained features that are most useful for performing the downstream task, using a simple regularization that adds minimal overhead. Across multiple vision, language, and multi-modal datasets, AFT achieves significantly better downstream performance compared to alternatives with a similar computational cost. Furthermore, AFT reliably translates improvement in pre-trained models into improvement in downstream performance, even if the downstream model is over $50\\times$ smaller, and can effectively transfer complementary information learned by multiple pre-trained models.",
      "authors": [
        "Shikai Qiu",
        "Boran Han",
        "Danielle C. Maddix",
        "Shuai Zhang",
        "Bernie Wang",
        "Andrew Gordon Wilson"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=XtDJaSe8jE",
      "cdate": 1706854730512,
      "mdate": 1719287279932,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526779"
    },
    {
      "id": "gn5AsHIIwb",
      "title": "StackSight: Unveiling WebAssembly through Large Language Models and Neurosymbolic Chain-of-Thought Decompilation",
      "abstract": "WebAssembly enables near-native execution in web applications and is increasingly adopted for tasks that demand high performance and robust security. However, its assembly-like syntax, implicit stack machine, and low-level data types make it extremely difficult for human developers to understand, spurring the need for effective WebAssembly reverse engineering techniques. In this paper, we propose StackSight, a novel neurosymbolic approach that combines Large Language Models (LLMs) with advanced program analysis to decompile complex WebAssembly code into readable C++ snippets. StackSight visualizes and tracks virtual stack alterations via a static analysis algorithm and then applies chain-of-thought prompting to harness LLM's complex reasoning capabilities. Evaluation results show that StackSight significantly improves WebAssembly decompilation. Our user study also demonstrates that code snippets generated by StackSight have significantly higher win rates and enable a better grasp of code semantics.",
      "authors": [
        "Weike Fang",
        "Zhejian Zhou",
        "Junzhou He",
        "Weihang Wang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=gn5AsHIIwb",
      "cdate": 1706854692834,
      "mdate": 1720842724123,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526804"
    },
    {
      "id": "KzACYw0MTV",
      "title": "QUEST: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
      "abstract": "As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at https://github.com/mit-han-lab/quest.",
      "authors": [
        "Jiaming Tang",
        "Yilong Zhao",
        "Kan Zhu",
        "Guangxuan Xiao",
        "Baris Kasikci",
        "Song Han"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=KzACYw0MTV",
      "cdate": 1706854579871,
      "mdate": 1719287279867,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526810"
    },
    {
      "id": "OiI12sNbgD",
      "title": "Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning",
      "abstract": "Recently, various pre-training methods have been introduced in vision-based Reinforcement Learning (RL). However, their generalization ability remains unclear due to evaluations being limited to in-distribution environments and non-unified experimental setups. To address this, we introduce the Atari Pre-training Benchmark (Atari-PB), which pre-trains a ResNet-50 model on 10 million transitions from 50 Atari games and evaluates it across diverse environment distributions. Our experiments show that pre-training objectives focused on learning task-agnostic features (e.g., identifying objects and understanding temporal dynamics) enhance generalization across different environments. In contrast, objectives focused on learning task-specific knowledge (e.g., identifying agents and fitting reward functions) improve performance in environments similar to the pre-training dataset but not in varied ones. We publicize our codes, datasets, and model checkpoints at https://github.com/dojeon-ai/Atari-PB.",
      "authors": [
        "Donghu Kim",
        "Hojoon Lee",
        "Kyungmin Lee",
        "Dongyoon Hwang",
        "Jaegul Choo"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OiI12sNbgD",
      "cdate": 1706854406922,
      "mdate": 1719287279821,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526815"
    },
    {
      "id": "ucl3B05EsX",
      "title": "Integrated Hardware Architecture and Device Placement Search",
      "abstract": "Distributed execution of deep learning training involves a dynamic interplay between hardware accelerator architecture and device placement strategy. This is the first work to explore the co-optimization of determining the optimal architecture and device placement strategy through novel algorithms, improving the balance of computational resources, memory usage, and data distribution. Our architecture search leverages tensor and vector units, determining their quantity and dimensionality, and on-chip and off-chip memory configurations. It also determines the microbatch size and decides whether to recompute or stash activations, balancing the memory footprint of training and storage size. For each explored architecture configuration, we use an Integer Linear Program (ILP) to find the optimal schedule for executing operators on the accelerator. The ILP results then integrate with a dynamic programming solution to identify the most effective device placement strategy, combining data, pipeline, and tensor model parallelism across multiple accelerators. Our approach achieves higher throughput on large language models compared to the state-of-the-art TPUv4 and the Spotlight accelerator search framework. The entire source code of PHAZE is available at https://github.com/msr-fiddle/phaze.",
      "authors": [
        "Irene Wang",
        "Jakub Tarnawski",
        "Amar Phanishayee",
        "Divya Mahajan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ucl3B05EsX",
      "cdate": 1706854240381,
      "mdate": 1719287279821,
      "matched_keywords": [
        "deep learning",
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526821"
    },
    {
      "id": "UCKFhc9SFC",
      "title": "Provably Efficient Long-Horizon Exploration in Monte Carlo Tree Search through State Occupancy Regularization",
      "abstract": "Monte Carlo tree search (MCTS) has been successful in a variety of domains, but faces challenges with long-horizon exploration when compared to sampling-based motion planning algorithms like Rapidly-Exploring Random Trees. To address these limitations of MCTS, we derive a tree search algorithm based on policy optimization with state-occupancy measure regularization, which we call *Volume-MCTS*. We show that count-based exploration and sampling-based motion planning can be derived as approximate solutions to this state-occupancy measure regularized objective. We test our method on several robot navigation problems, and find that Volume-MCTS outperforms AlphaZero and displays significantly better long-horizon exploration properties.",
      "authors": [
        "Liam Schramm",
        "Abdeslam Boularias"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=UCKFhc9SFC",
      "cdate": 1706854199796,
      "mdate": 1719287279792,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526826"
    },
    {
      "id": "rADFNrIss3",
      "title": "InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models",
      "abstract": "Large language models (LLMs) are instruction followers but the performance varies under different instructions. It is challenging to create the best instruction, especially for black-box LLMs on which backpropagation is forbidden. Instead of directly optimizing the discrete instruction, we optimize a low-dimensional soft prompt applied to an open-source LLM to generate the instruction for the black-box LLM. In each optimization step of the proposed method InstructZero, a soft prompt is converted into an instruction by the open-source LLM, which is then submitted to the black-box LLM for zero-shot evaluation, whose result is sent to Bayesian optimization to produce new soft prompts improving the zero-shot performance. We evaluate InstructZero on different combinations of open-source LLMs and APIs including Vicuna and ChatGPT. InstructZero outperforms SOTA auto-instruction methods across a variety of downstream tasks.",
      "authors": [
        "Lichang Chen",
        "Jiuhai Chen",
        "Tom Goldstein",
        "Heng Huang",
        "Tianyi Zhou"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rADFNrIss3",
      "cdate": 1706854199056,
      "mdate": 1719287279729,
      "matched_keywords": [
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526832"
    },
    {
      "id": "0ZFWfeVsaD",
      "title": "Towards Modular LLMs by Building and Reusing a Library of LoRAs",
      "abstract": "Given the increasing number of parameter-efficient adapters of large language models (LLMs), how can we reuse them to improve LLM performance on new tasks? We study how to best build a *library* of adapters given multi-task data and devise techniques for both *zero-shot* and *supervised* task generalization through *routing* in such library. We benchmark existing approaches to build this library and introduce model-based clustering, $\\texttt{MBC}$, a method that groups tasks based on the similarity of their adapter parameters, indirectly optimizing for transfer across the multi-task dataset. In order to reuse the library, we present a novel zero-shot routing mechanism, $\\texttt{Arrow}$, which enables dynamic selection of the most relevant adapters for new inputs without the need for retraining. We experiment with several LLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks, verifying that MBC-based adapters and Arrow routing lead to superior generalization to new tasks. Thus, we make steps towards creating modular, adaptable LLMs that can match or outperform traditional joint training.",
      "authors": [
        "Oleksiy Ostapenko",
        "Zhan Su",
        "Edoardo Ponti",
        "Laurent Charlin",
        "Nicolas Le Roux",
        "Lucas Caccia",
        "Alessandro Sordoni"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0ZFWfeVsaD",
      "cdate": 1706853948813,
      "mdate": 1719287279577,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526839"
    },
    {
      "id": "5SpjhZNXtt",
      "title": "Position: Data-driven Discovery with Large Generative Models",
      "abstract": "With the accumulation of data at an unprecedented rate, its potential to fuel scientific discovery is growing exponentially. This position paper urges the Machine Learning (ML) community to exploit the capabilities of large generative models (LGMs) to develop automated systems for end-to-end data-driven discovery—a paradigm encompassing the search and verification of hypotheses purely from a set of provided datasets, without the need for additional data collection or physical experiments. We first outline several desiderata for an ideal data-driven discovery system. Then, through DataVoyager, a proof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of these desiderata—a feat previously unattainable—while also highlighting important limitations in the current system that open up opportunities for novel ML research. We contend that achieving accurate, reliable, and robust end-to-end discovery systems solely through the current capabilities of LGMs is challenging. We instead advocate for fail-proof tool integration, along with active user moderation through feedback mechanisms, to foster data-driven scientific discoveries with efficiency and reproducibility.",
      "authors": [
        "Bodhisattwa Prasad Majumder",
        "Harshit Surana",
        "Dhruv Agarwal",
        "Sanchaita Hazra",
        "Ashish Sabharwal",
        "Peter Clark"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5SpjhZNXtt",
      "cdate": 1706853407089,
      "mdate": 1719287279438,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526845"
    },
    {
      "id": "blzDxD6bKt",
      "title": "Nonlinear Filtering with Brenier Optimal Transport Maps",
      "abstract": "This paper is concerned with the problem of nonlinear filtering, i.e., computing the conditional distribution of the state of a stochastic dynamical system given a history of noisy partial observations. Conventional sequential importance resampling (SIR) particle filters suffer from fundamental limitations, in scenarios involving degenerate likelihoods or high-dimensional states, due to the weight degeneracy issue. In this paper, we explore an alternative method, which is based on estimating the Brenier optimal transport (OT) map from the current prior distribution of the state to the posterior distribution at the next time step. Unlike SIR particle filters, the OT formulation does not require the analytical form of the likelihood. Moreover, it allows us to harness the approximation power of neural networks to model complex and multi-modal distributions and employ stochastic optimization algorithms to enhance scalability. Extensive numerical experiments are presented that compare the OT method to the SIR particle filter and the ensemble Kalman filter, evaluating the performance in terms of sample efficiency, high-dimensional scalability, and the ability to capture complex and multi-modal distributions.",
      "authors": [
        "Mohammad Al-Jarrah",
        "Niyizhen Jin",
        "Bamdad Hosseini",
        "Amirhossein Taghvaei"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=blzDxD6bKt",
      "cdate": 1706853336524,
      "mdate": 1719287279411,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526852"
    },
    {
      "id": "SQIDlJd3hN",
      "title": "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation",
      "abstract": "We present RoboGen, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation. RoboGen leverages the latest advancements in foundation and generative models. Instead of directly adapting these models to produce policies or low-level actions, we advocate for a generative scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. Our approach equips a robotic agent with a self-guided propose-generate-learn cycle: the agent first proposes interesting tasks and skills to develop, and then generates simulation environments by populating pertinent assets with proper spatial configurations. Afterwards, the agent decomposes the proposed task into sub-tasks, selects the optimal learning approach (reinforcement learning, motion planning, or trajectory optimization), generates required training supervision, and then learns policies to acquire the proposed skill. Our fully generative pipeline can be queried repeatedly, producing an endless stream of skill demonstrations associated with diverse tasks and environments.",
      "authors": [
        "Yufei Wang",
        "Zhou Xian",
        "Feng Chen",
        "Tsun-Hsuan Wang",
        "Yian Wang",
        "Katerina Fragkiadaki",
        "Zackory Erickson",
        "David Held",
        "Chuang Gan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=SQIDlJd3hN",
      "cdate": 1706853333855,
      "mdate": 1719287279392,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526857"
    },
    {
      "id": "t3SEfoTaYQ",
      "title": "Coprocessor Actor Critic: A Model-Based Reinforcement Learning Approach For Adaptive Brain Stimulation",
      "abstract": "Adaptive brain stimulation can treat neurological conditions such as Parkinson’s disease and post-stroke motor deficits by influencing abnormal neural activity. Because of patient heterogeneity, each patient requires a unique stimulation policy to achieve optimal neural responses. Model-free reinforcement learning (MFRL) holds promise in learning effective policies for a variety of similar control tasks, but is limited in domains like brain stimulation by a need for numerous costly environment interactions. In this work we introduce Coprocessor Actor Critic, a novel, model-based reinforcement learning (MBRL) approach for learning neural coprocessor policies for brain stimulation. Our key insight is that coprocessor policy learning is a combination of learning how to act optimally in the world and learning how to induce optimal actions in the world through stimulation of an injured brain. We show that our approach overcomes the limitations of traditional MFRL methods in terms of sample efficiency and task success and outperforms baseline MBRL approaches in a neurologically realistic model of an injured brain.",
      "authors": [
        "Michelle Pan",
        "Mariah L Schrum",
        "Vivek Myers",
        "Erdem Biyik",
        "Anca Dragan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=t3SEfoTaYQ",
      "cdate": 1706853292545,
      "mdate": 1719287279339,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526863"
    },
    {
      "id": "XobPpcN4yZ",
      "title": "Value-Evolutionary-Based Reinforcement Learning",
      "abstract": "Combining Evolutionary Algorithms (EAs) and Reinforcement Learning (RL) for policy search has been proven to improve RL performance. However, previous works largely overlook value-based RL in favor of merging EAs with policy-based RL. This paper introduces Value-Evolutionary-Based Reinforcement Learning (VEB-RL) that focuses on the integration of EAs with value-based RL. The framework maintains a population of value functions instead of policies and leverages negative Temporal Difference error as the fitness metric for evolution. The metric is more sample-efficient for population evaluation than cumulative rewards and is closely associated with the accuracy of the value function approximation. Additionally, VEB-RL enables elites of the population to interact with the environment to offer high-quality samples for RL optimization, whereas the RL value function participates in the population's evolution in each generation. Experiments on MinAtar and Atari demonstrate the superiority of VEB-RL in significantly improving DQN, Rainbow, and SPR. Our code is available on https://github.com/yeshenpy/VEB-RL.",
      "authors": [
        "Pengyi Li",
        "Jianye HAO",
        "Hongyao Tang",
        "YAN ZHENG",
        "Fazl Barez"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=XobPpcN4yZ",
      "cdate": 1706853231422,
      "mdate": 1719287279236,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526868"
    },
    {
      "id": "qqPL0DkcrI",
      "title": "Learning High-Frequency Functions Made Easy with Sinusoidal Positional Encoding",
      "abstract": "Fourier features based positional encoding (PE) is commonly used in machine learning tasks that involve learning high-frequency features from low-dimensional inputs, such as 3D view synthesis and time series regression with neural tangent kernels. Despite their effectiveness, existing PEs require manual, empirical adjustment of crucial hyperparameters, specifically the Fourier features, tailored to each unique task. Further, PEs face challenges in efficiently learning high-frequency functions, particularly in tasks with limited data. In this paper, we introduce sinusoidal PE (SPE), designed to efficiently learn adaptive frequency features closely aligned with the true underlying function. Our experiments demonstrate that SPE, without hyperparameter tuning, consistently achieves enhanced fidelity and faster training across various tasks, including 3D view synthesis, Text-to-Speech generation, and 1D regression. SPE is implemented as a direct replacement for existing PEs. Its plug-and-play nature lets numerous tasks easily adopt and benefit from SPE.",
      "authors": [
        "Chuanhao Sun",
        "Zhihang Yuan",
        "Kai Xu",
        "Luo Mai",
        "Siddharth N",
        "Shuo Chen",
        "Mahesh K. Marina"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qqPL0DkcrI",
      "cdate": 1706853228200,
      "mdate": 1719287279218,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526874"
    },
    {
      "id": "NKirMgDsut",
      "title": "Random Scaling and Momentum for Non-smooth Non-convex Optimization",
      "abstract": "Training neural networks requires optimizing a loss function that may be highly irregular, and in particular neither convex nor smooth. Popular training algorithms are based on stochastic gradient descent with momentum (SGDM), for which classical analysis applies only if the loss is either convex or smooth. We show that a very small modification to SGDM closes this gap: simply scale the update at each time point by an exponentially distributed random scalar. The resulting algorithm achieves optimal convergence guarantees. Intriguingly, this result is not derived by a specific analysis of SGDM: instead, it falls naturally out of a more general framework for converting online convex optimization algorithms to non-convex optimization algorithms.",
      "authors": [
        "Qinzi Zhang",
        "Ashok Cutkosky"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NKirMgDsut",
      "cdate": 1706853037309,
      "mdate": 1719287279158,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526879"
    },
    {
      "id": "KwgAThfxEd",
      "title": "Improving Computational Complexity in Statistical Models with Local Curvature Information",
      "abstract": "It is known that when the statistical models are singular, i.e., the Fisher information matrix at the true parameter is degenerate, the fixed step-size gradient descent algorithm takes polynomial number of steps in terms of the sample size $n$ to converge to a final statistical radius around the true parameter, which can be unsatisfactory for the practical application. To further improve that computational complexity, we consider utilizing the local curvature information for parameter estimation. Even though there is a rich literature in using the local curvature information for optimization, the statistical rate of these methods in statistical models, to the best of our knowledge, has not been studied rigorously. The major challenge of this problem is due to the non-convex nature of sample loss function. To shed light on these problems, we specifically study the normalized gradient descent (NormGD) algorithm, a variant of gradient descent algorithm whose step size is scaled by the maximum eigenvalue of the Hessian matrix of the empirical loss function, and deal with the aforementioned issue with a population-to-sample analysis. When the population loss function is homogeneous, the NormGD iterates reach a final statistical radius around the true parameter after a logarithmic number of iterations in terms of $n$. Therefore, for fixed dimension $d$, the NormGD algorithm achieves the optimal computational complexity $\\mathcal{O}(n)$ to reach the final statistical radius, which is cheaper than the complexity $\\mathcal{O}(n^{\\tau})$ of the fixed step-size gradient descent algorithm for some $\\tau > 1$.",
      "authors": [
        "Pedram Akbarian",
        "Tongzheng Ren",
        "Jiacheng Zhuo",
        "sujay sanghavi",
        "Nhat Ho"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=KwgAThfxEd",
      "cdate": 1706852772153,
      "mdate": 1719287279019,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526885"
    },
    {
      "id": "S6a6gHvMWx",
      "title": "Position: Social Environment Design Should be Further Developed for AI-based Policy-Making",
      "abstract": "Artificial Intelligence (AI) holds promise as a technology that can be used to improve government and economic policy-making. This paper proposes a new research agenda towards this end by introducing **Social Environment Design**, a general framework for the use of AI in automated policy-making that connects with the Reinforcement Learning, EconCS, and Computational Social Choice communities. The framework seeks to capture general economic environments, includes voting on policy objectives, and gives a direction for the systematic analysis of government and economic policy through AI simulation. We highlight key open problems for future research in AI-based policymaking. By solving these challenges, we hope to achieve various social welfare objectives, thereby promoting more ethical and responsible decision making.",
      "authors": [
        "Edwin Zhang",
        "Sadie Zhao",
        "Tonghan Wang",
        "Safwan Hossain",
        "Henry Gasztowtt",
        "Stephan Zheng",
        "David C. Parkes",
        "Milind Tambe",
        "Yiling Chen"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=S6a6gHvMWx",
      "cdate": 1706852732421,
      "mdate": 1719287279002,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526891"
    },
    {
      "id": "zcIV8OQFVF",
      "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF",
      "abstract": "In this work, we study the issue of reward hacking on the response length, a challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on LLMs. A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators and achieve high scores. The same issue also holds for some reward models in RL. To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configurations, which inspects the trade-off between LLM evaluation score and response length obtained by varying training hyperparameters. Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias. We further propose to improve the reward model by jointly training two linear heads to predict the preference, one trained to correlate with length and the other trained to decorrelate with length and therefore focusing more on the actual content. We then discard the length head in RL to ignore the spurious length reward. Experiments demonstrate that our approach eliminates the reward correlation with length, and improves the obtained policy by a significant margin.",
      "authors": [
        "Lichang Chen",
        "Chen Zhu",
        "Jiuhai Chen",
        "Davit Soselia",
        "Tianyi Zhou",
        "Tom Goldstein",
        "Heng Huang",
        "Mohammad Shoeybi",
        "Bryan Catanzaro"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=zcIV8OQFVF",
      "cdate": 1706852726061,
      "mdate": 1719287278975,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526897"
    },
    {
      "id": "iGMTxygzcJ",
      "title": "Gibbs Sampling of Continuous Potentials on a Quantum Computer",
      "abstract": "Gibbs sampling from continuous real-valued functions is a challenging problem of interest in machine learning. Here we leverage quantum Fourier transforms to build a quantum algorithm for this task when the function is periodic. We use the quantum algorithms for solving linear ordinary differential equations to solve the Fokker–Planck equation and prepare a quantum state encoding the Gibbs distribution. We show that the efficiency of interpolation and differentiation of these functions on a quantum computer depends on the rate of decay of the Fourier coefficients of the Fourier transform of the function. We view this property as a concentration of measure in the Fourier domain, and also provide functional analytic conditions for it. Our algorithm makes zeroeth order queries to a quantum oracle of the function and achieves polynomial quantum speedups in mean estimation in the Gibbs measure for generic non-convex periodic functions. At high temperatures the algorithm also allows for exponentially improved precision in sampling from Morse functions.",
      "authors": [
        "Arsalan Motamedi",
        "Pooya Ronagh"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iGMTxygzcJ",
      "cdate": 1706852637180,
      "mdate": 1719287278898,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526903"
    },
    {
      "id": "BRfqYrikdo",
      "title": "WorkArena: How Capable are Web Agents at Solving Common Knowledge Work Tasks?",
      "abstract": "We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 33 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.",
      "authors": [
        "Alexandre Drouin",
        "Maxime Gasse",
        "Massimo Caccia",
        "Issam H. Laradji",
        "Manuel Del Verme",
        "Tom Marty",
        "David Vazquez",
        "Nicolas Chapados",
        "Alexandre Lacoste"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BRfqYrikdo",
      "cdate": 1706852410831,
      "mdate": 1719287278748,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526909"
    },
    {
      "id": "QGAeWRRe6e",
      "title": "Optimizing Watermarks for Large Language Models",
      "abstract": "With the rise of large language models (LLMs) and concerns about potential misuse, watermarks for generative LLMs have recently attracted much attention. An important aspect of such watermarks is the trade-off between their identifiability and their impact on the quality of the generated text. This paper introduces a systematic approach to this trade-off in terms of a multi-objective optimization problem. For a large class of robust, efficient watermarks, the associated Pareto optimal solutions are identified and shown to outperform existing robust, efficient watermarks.",
      "authors": [
        "Bram Wouters"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=QGAeWRRe6e",
      "cdate": 1706852390215,
      "mdate": 1719287278716,
      "matched_keywords": [
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526914"
    },
    {
      "id": "0urN0PnNDj",
      "title": "PEARL: Zero-shot Cross-task Preference Alignment and Robust Reward Learning for Robotic Manipulation",
      "abstract": "In preference-based Reinforcement Learning (RL), obtaining a large number of preference labels are both time-consuming and costly. Furthermore, the queried human preferences cannot be utilized for the new tasks. In this paper, we propose Zero-shot Cross-task Preference Alignment and Robust Reward Learning (PEARL), which learns policies from cross-task preference transfer without any human labels of the target task. Our contributions include two novel components that facilitate the transfer and learning process. The first is Cross-task Preference Alignment (CPA), which transfers the preferences between tasks via optimal transport. The key idea of CPA is to use Gromov-Wasserstein distance to align the trajectories between tasks, and the solved optimal transport matrix serves as the correspondence between trajectories. The target task preferences are computed as the weighted sum of source task preference labels with the correspondence as weights. Moreover, to ensure robust learning from these transferred labels, we introduce Robust Reward Learning (RRL), which considers both reward mean and uncertainty by modeling rewards as Gaussian distributions. Empirical results on robotic manipulation tasks from Meta-World and Robomimic demonstrate that our method is capable of transferring preference labels across tasks accurately and then learns well-behaved policies. Notably, our approach significantly exceeds existing methods when there are few human preferences. The code and videos of our method are available at: https://sites.google.com/view/pearl-preference.",
      "authors": [
        "Runze Liu",
        "Yali Du",
        "Fengshuo Bai",
        "Jiafei Lyu",
        "Xiu Li"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0urN0PnNDj",
      "cdate": 1706852240028,
      "mdate": 1719287278677,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526919"
    },
    {
      "id": "tOO6PD3kYP",
      "title": "Random Exploration in Bayesian Optimization: Order-Optimal Regret and Computational Efficiency",
      "abstract": "We consider Bayesian optimization using Gaussian Process models, also referred to as kernel-based bandit optimization. We study the methodology of exploring the domain using random samples drawn from a distribution. We show that this random exploration approach achieves the optimal error rates. Our analysis is based on novel concentration bounds in an infinite dimensional Hilbert space established in this work, which may be of independent interest. We further develop an algorithm based on random exploration with domain shrinking and establish its order-optimal regret guarantees under both noise-free and noisy settings. In the noise-free setting, our analysis closes the existing gap in regret performance under a mild assumption on the underlying function and thereby *partially resolves a COLT open problem*. The proposed algorithm also enjoys a computational advantage over prevailing methods due to the random exploration that obviates the expensive optimization of a non-convex acquisition function for choosing the query points at each iteration.",
      "authors": [
        "Sudeep Salgia",
        "Sattar Vakili",
        "Qing Zhao"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tOO6PD3kYP",
      "cdate": 1706852011660,
      "mdate": 1719287278648,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526925"
    },
    {
      "id": "0j28mmQ023",
      "title": "Domain-wise Data Acquisition to Improve Performance under Distribution Shift",
      "abstract": "Despite notable progress in enhancing the capability of machine learning against distribution shifts, training data quality remains a bottleneck for cross-distribution generalization. Recently, from a data-centric perspective, there have been considerable efforts to improve model performance through refining the preparation of training data. Inspired by realistic scenarios, this paper addresses a practical requirement of acquiring training samples from various domains on a limited budget to facilitate model generalization to target test domain with distribution shift. Our empirical evidence indicates that the advance in data acquisition can significantly benefit the model performance on shifted data. Additionally, by leveraging unlabeled test domain data, we introduce a Domain-wise Active Acquisition framework. This framework iteratively optimizes the data acquisition strategy as training samples are accumulated, theoretically ensuring the effective approximation of test distribution. Extensive real-world experiments demonstrate our proposal's advantages in machine learning applications. The code is available at https://github.com/dongbaili/DAA.",
      "authors": [
        "Yue He",
        "Dongbai Li",
        "Pengfei Tian",
        "Han Yu",
        "Jiashuo Liu",
        "Hao Zou",
        "Peng Cui"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0j28mmQ023",
      "cdate": 1706851907274,
      "mdate": 1719287278539,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526930"
    },
    {
      "id": "685vj0lC9z",
      "title": "How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?",
      "abstract": "In day-to-day communication, people often approximate the truth --- for example, rounding the time or omitting details --- in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting.",
      "authors": [
        "Ryan Liu",
        "Theodore Sumers",
        "Ishita Dasgupta",
        "Thomas L. Griffiths"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=685vj0lC9z",
      "cdate": 1706851842994,
      "mdate": 1719287278475,
      "matched_keywords": [
        "reinforcement learning",
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526936"
    },
    {
      "id": "24zMewdzyJ",
      "title": "Risk-Sensitive Policy Optimization via Predictive CVaR Policy Gradient",
      "abstract": "This paper addresses a policy optimization task with the conditional value-at-risk (CVaR) objective. We introduce the *predictive CVaR policy gradient*, a novel approach that seamlessly integrates risk-neutral policy gradient algorithms with minimal modifications. Our method incorporates a reweighting strategy in gradient calculation -- individual cost terms are reweighted in proportion to their *predicted* contribution to the objective. These weights can be easily estimated through a separate learning procedure. We provide theoretical and empirical analyses, demonstrating the validity and effectiveness of our proposed method.",
      "authors": [
        "Ju-Hyun Kim",
        "Seungki Min"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=24zMewdzyJ",
      "cdate": 1706851520256,
      "mdate": 1719287278370,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526941"
    },
    {
      "id": "44qxX6Ty6F",
      "title": "Position: Scarce Resource Allocations That Rely On Machine Learning Should Be Randomized",
      "abstract": "Contrary to traditional deterministic notions of algorithmic fairness, this paper argues that fairly allocating scarce resources using machine learning often requires randomness. We address why, when, and how to randomize by offering a set of stochastic procedures that more adequately account for all of the claims individuals have to allocations of social goods or opportunities and effectively balances their interests.",
      "authors": [
        "Shomik Jain",
        "Kathleen Creel",
        "Ashia Camage Wilson"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=44qxX6Ty6F",
      "cdate": 1706851238575,
      "mdate": 1719287278369,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526947"
    },
    {
      "id": "sHtIStlg0v",
      "title": "Large Language Models are Geographically Biased",
      "abstract": "Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and subjective topics. In particular, LLMs are clearly biased against locations with lower socioeconomic conditions (e.g. most of Africa) on a variety of sensitive subjective topics such as attractiveness, morality, and intelligence (Spearman’s $\\rho$ of up to 0.70). Finally, we introduce a bias score to quantify this and find that there is significant variation in the magnitude of bias across existing LLMs. Code is available on the project website: https://rohinmanvi.github.io/GeoLLM.",
      "authors": [
        "Rohin Manvi",
        "Samar Khanna",
        "Marshall Burke",
        "David B. Lobell",
        "Stefano Ermon"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=sHtIStlg0v",
      "cdate": 1706851146023,
      "mdate": 1719287278345,
      "matched_keywords": [
        "large language model",
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526954"
    },
    {
      "id": "Z19JQ6WFtJ",
      "title": "Learning Reward for Robot Skills Using Large Language Models via Self-Alignment",
      "abstract": "Learning reward functions remains the bottleneck to equip a robot with a broad repertoire of skills. Large Language Models (LLM) contain valuable task-related knowledge that can potentially aid in the learning of reward functions. However, the proposed reward function can be imprecise, thus ineffective which requires to be further grounded with environment information. We proposed a method to learn rewards more efficiently in the absence of humans. Our approach consists of two components: We first use the LLM to propose features and parameterization of the reward, then update the parameters through an iterative self-alignment process. In particular, the process minimizes the ranking inconsistency between the LLM and the learnt reward functions based on the execution feedback. The method was validated on 9 tasks across 2 simulation environments. It demonstrates a consistent improvement in training efficacy and efficiency, meanwhile consuming significantly fewer GPT tokens compared to the alternative mutation-based method.",
      "authors": [
        "Yuwei Zeng",
        "Yao Mu",
        "Lin Shao"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Z19JQ6WFtJ",
      "cdate": 1706850988130,
      "mdate": 1719287278248,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.526959"
    },
    {
      "id": "bID9PiBFpT",
      "title": "Policy Evaluation for Variance in Average Reward Reinforcement Learning",
      "abstract": "We consider an average reward reinforcement learning (RL) problem and work with asymptotic variance as a risk measure to model safety-critical applications. We design a temporal-difference (TD) type algorithm tailored for policy evaluation in this context. Our algorithm is based on linear stochastic approximation of an equivalent formulation of the asymptotic variance in terms of the solution of the Poisson equation. We consider both the tabular and linear function approximation settings, and establish $\\tilde {O}(1/k)$ finite time convergence rate, where $k$ is the number of steps of the algorithm. Our work paves the way for developing actor-critic style algorithms for variance-constrained RL. To the best of our knowledge, our result provides the first sequential estimator for asymptotic variance of a Markov chain with provable finite sample guarantees, which is of independent interest.",
      "authors": [
        "Shubhada Agrawal",
        "Prashanth L A",
        "Siva Theja Maguluri"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=bID9PiBFpT",
      "cdate": 1706850583503,
      "mdate": 1719287278160,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526965"
    },
    {
      "id": "JNHK11bAGl",
      "title": "Feasibility Consistent Representation Learning for Safe Reinforcement Learning",
      "abstract": "In the field of safe reinforcement learning (RL), finding a balance between satisfying safety constraints and optimizing reward performance presents a significant challenge. A key obstacle in this endeavor is the estimation of safety constraints, which is typically more difficult than estimating a reward metric due to the sparse nature of the constraint signals. To address this issue, we introduce a novel framework named Feasibility Consistent Safe Reinforcement Learning (FCSRL). This framework combines representation learning with feasibility-oriented objectives to identify and extract safety-related information from the raw state for safe RL. Leveraging self-supervised learning techniques and a more learnable safety metric, our approach enhances the policy learning and constraint estimation. Empirical evaluations across a range of vector-state and image-based tasks demonstrate that our method is capable of learning a better safety-aware embedding and achieving superior performance than previous representation learning baselines.",
      "authors": [
        "Zhepeng Cen",
        "Yihang Yao",
        "Zuxin Liu",
        "Ding Zhao"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=JNHK11bAGl",
      "cdate": 1706850487726,
      "mdate": 1719287278129,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526971"
    },
    {
      "id": "2FHWFG5ahw",
      "title": "Adaptive Horizon Actor-Critic for Policy Learning in Contact-Rich Differentiable Simulation",
      "abstract": "Model-Free Reinforcement Learning (MFRL), leveraging the policy gradient theorem, has demonstrated considerable success in continuous control tasks. However, these approaches are plagued by high gradient variance due to zeroth-order gradient estimation, resulting in suboptimal policies. Conversely, First-Order Model-Based Reinforcement Learning (FO-MBRL) methods employing differentiable simulation provide gradients with reduced variance but are susceptible to sampling error in scenarios involving stiff dynamics, such as physical contact. This paper investigates the source of this error and introduces Adaptive Horizon Actor-Critic (AHAC), an FO-MBRL algorithm that reduces gradient error by adapting the model-based horizon to avoid stiff dynamics. Empirical findings reveal that AHAC outperforms MFRL baselines, attaining 40% more reward across a set of locomotion tasks and efficiently scaling to high-dimensional control environments with improved wall-clock-time efficiency. [adaptive-horizon-actor-critic.github.io](https://adaptive-horizon-actor-critic.github.io/)",
      "authors": [
        "Ignat Georgiev",
        "Krishnan Srinivasan",
        "Jie Xu",
        "Eric Heiden",
        "Animesh Garg"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=2FHWFG5ahw",
      "cdate": 1706850215194,
      "mdate": 1719287278007,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526977"
    },
    {
      "id": "TejqrQBvll",
      "title": "Generalization Bounds for Causal Regression: Insights, Guarantees and Sensitivity Analysis",
      "abstract": "Many algorithms have been recently proposed for causal machine learning. Yet, there is little to no theory on their quality, especially considering finite samples. In this work, we propose a theory based on generalization bounds that provides such guarantees. By introducing a novel change-of-measure inequality, we are able to tightly bound the model loss in terms of the deviation of the treatment propensities over the population, which we show can be empirically limited. Our theory is fully rigorous and holds even in the face of hidden confounding and violations of positivity. We demonstrate our bounds on semi-synthetic and real data, showcasing their remarkable tightness and practical utility.",
      "authors": [
        "Daniel Csillag",
        "Claudio Jose Struchiner",
        "Guilherme Tegoni Goedert"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=TejqrQBvll",
      "cdate": 1706850201084,
      "mdate": 1719287277976,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526982"
    },
    {
      "id": "kpDd2HCBka",
      "title": "Efficient Policy Evaluation with Offline Data Informed Behavior Policy Design",
      "abstract": "Most reinforcement learning practitioners evaluate their policies with online Monte Carlo estimators for either hyperparameter tuning or testing different algorithmic design choices, where the policy is repeatedly executed in the environment to get the average outcome. Such massive interactions with the environment are prohibitive in many scenarios. In this paper, we propose novel methods that improve the data efficiency of online Monte Carlo estimators while maintaining their unbiasedness. We first propose a tailored closed-form behavior policy that provably reduces the variance of an online Monte Carlo estimator. We then design efficient algorithms to learn this closed-form behavior policy from previously collected offline data. Theoretical analysis is provided to characterize how the behavior policy learning error affects the amount of reduced variance. Compared with previous works, our method achieves better empirical performance in a broader set of environments, with fewer requirements for offline data.",
      "authors": [
        "Shuze Liu",
        "Shangtong Zhang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kpDd2HCBka",
      "cdate": 1706850076063,
      "mdate": 1719287277911,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.526988"
    },
    {
      "id": "GxOFM3f5Vm",
      "title": "EMC$^2$: Efficient MCMC Negative Sampling for Contrastive Learning with Global Convergence",
      "abstract": "A key challenge in contrastive learning is to generate negative samples from a large sample set to contrast with positive samples, for learning better encoding of the data. These negative samples often follow a softmax distribution which are dynamically updated during the training process. However, sampling from this distribution is non-trivial due to the high computational costs in computing the partition function. In this paper, we propose an $\\underline{\\text{E}}$fficient $\\underline{\\text{M}}$arkov $\\underline{\\text{C}}$hain Monte Carlo negative sampling method for $\\underline{\\text{C}}$ontrastive learning (EMC$^2$). We follow the global contrastive learning loss as introduced in SogCLR, and propose EMC$^2$ which utilizes an adaptive Metropolis-Hastings subroutine to generate hardness-aware negative samples in an online fashion during the optimization. We prove that EMC$^2$ finds an $\\mathcal{O}(1/\\sqrt{T})$-stationary point of the global contrastive loss in $T$ iterations. Compared to prior works, EMC$^2$ is the first algorithm that exhibits global convergence (to stationarity) regardless of the choice of batch size while exhibiting low computation and memory cost. Numerical experiments validate that EMC$^2$ is effective with small batch training and achieves comparable or better performance than baseline algorithms. We report the results for pre-training image encoders on STL-10 and Imagenet-100.",
      "authors": [
        "Chung-Yiu Yau",
        "Hoi To Wai",
        "Parameswaran Raman",
        "Soumajyoti Sarkar",
        "Mingyi Hong"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GxOFM3f5Vm",
      "cdate": 1706848981599,
      "mdate": 1719287277705,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526994"
    },
    {
      "id": "MikandLqtW",
      "title": "Knowledge-aware Reinforced Language Models for Protein Directed Evolution",
      "abstract": "Directed evolution, a cornerstone of protein optimization, is to harness natural mutational processes to enhance protein functionality. Existing Machine Learning-assisted Directed Evolution (MLDE) methodologies typically rely on data-driven strategies and often overlook the profound domain knowledge in biochemical fields. In this paper, we introduce a novel Knowledge-aware Reinforced Language Model (KnowRLM) for MLDE. An Amino Acid Knowledge Graph (AAKG) is constructed to represent the intricate biochemical relationships among amino acids. We further propose a Protein Language Model (PLM)-based policy network that iteratively samples mutants through preferential random walks on the AAKG using a dynamic sliding window mechanism. The novel mutants are actively sampled to fine-tune a fitness predictor as the reward model, providing feedback to the knowledge-aware policy. Finally, we optimize the whole system in an active learning approach that mimics biological settings in practice.KnowRLM stands out for its ability to utilize contextual amino acid information from knowledge graphs, thus attaining advantages from both statistical patterns of protein sequences and biochemical properties of amino acids.Extensive experiments demonstrate the superior performance of KnowRLM in more efficiently identifying high-fitness mutants compared to existing methods.",
      "authors": [
        "Yuhao Wang",
        "Qiang Zhang",
        "Ming Qin",
        "Xiang Zhuang",
        "Xiaotong Li",
        "Zhichen Gong",
        "Zeyuan Wang",
        "Yu Zhao",
        "Jianhua Yao",
        "Keyan Ding",
        "Huajun Chen"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MikandLqtW",
      "cdate": 1706848921936,
      "mdate": 1719287277677,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.526999"
    },
    {
      "id": "ghNRg2mEgN",
      "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision",
      "abstract": "Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior---for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to *weakly supervise* superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call *weak-to-strong generalization*. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models.",
      "authors": [
        "Collin Burns",
        "Pavel Izmailov",
        "Jan Hendrik Kirchner",
        "Bowen Baker",
        "Leo Gao",
        "Leopold Aschenbrenner",
        "Yining Chen",
        "Adrien Ecoffet",
        "Manas Joglekar",
        "Jan Leike",
        "Ilya Sutskever",
        "Jeffrey Wu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ghNRg2mEgN",
      "cdate": 1706848838429,
      "mdate": 1719287277629,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527005"
    },
    {
      "id": "MQirNNU2pC",
      "title": "Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks",
      "abstract": "This study investigates how weight decay affects the update behavior of individual neurons in deep neural networks through a combination of applied analysis and experimentation. Weight decay can cause the expected magnitude and angular updates of a neuron's weight vector to converge to a steady state we call rotational equilibrium. These states can be highly homogeneous, effectively balancing the average rotation---a proxy for the effective learning rate---across different layers and neurons. Our work analyzes these dynamics across optimizers like Adam, Lion, and SGD with momentum, offering a new simple perspective on training that elucidates the efficacy of widely used but poorly understood methods in deep learning. We demonstrate how balanced rotation plays a key role in the effectiveness of normalization like Weight Standardization, as well as that of AdamW over Adam with L2-regularization. Finally, we show that explicitly controlling the rotation provides the benefits of weight decay while substantially reducing the need for learning rate warmup.",
      "authors": [
        "Atli Kosson",
        "Bettina Messmer",
        "Martin Jaggi"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MQirNNU2pC",
      "cdate": 1706848733063,
      "mdate": 1719287277586,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527010"
    },
    {
      "id": "5nuW5iBAJS",
      "title": "Unveiling the Potential of AI for Nanomaterial Morphology Prediction",
      "abstract": "Creation of nanomaterials with specific morphology remains a complex experimental process, even though there is a growing demand for these materials in various industry sectors. This study explores the potential of AI to predict the morphology of nanoparticles within the data availability constraints. For that, we first generated a new multi-modal dataset that is double the size of analogous studies. Then, we systematically evaluated performance of classical machine learning and large language models in prediction of nanomaterial shapes and sizes. Finally, we prototyped a text-to-image system, discussed the obtained empirical results, as well as the limitations and promises of existing approaches.",
      "authors": [
        "Ivan Dubrovsky",
        "Andrei Dmitrenko",
        "Aleksei Dmitrenko",
        "Nikita Serov",
        "Vladimir Vinogradov"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5nuW5iBAJS",
      "cdate": 1706848208318,
      "mdate": 1719287277474,
      "matched_keywords": [
        "machine learning",
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527017"
    },
    {
      "id": "FovMAzXUpj",
      "title": "ReGAL: Refactoring Programs to Discover Generalizable Abstractions",
      "abstract": "While large language models (LLMs) are increasingly being used for program synthesis, they lack the global view needed to develop useful abstractions; they generally predict programs one at a time, often repeating the same functionality. Generating redundant code from scratch is both inefficient and error-prone. To address this, we propose Refactoring for Generalizable Abstraction Learning (ReGAL), a gradient-free method for learning a library of reusable functions via code refactorization, i.e., restructuring code without changing its execution output. ReGAL learns from a small set of existing programs, iteratively verifying and refining its abstractions via execution. We find that the shared function libraries discovered by ReGAL make programs easier to predict across diverse domains. On five datasets – LOGO graphics generation, Date reasoning, TextCraft (a Minecraft-based text-game) MATH, and TabMWP – both open-source and proprietary LLMs improve in accuracy when predicting programs with REGAL functions. For CodeLlama-13B, REGAL results in absolute accuracy increases of 11.5% on LOGO, 26.1% on date understanding, and 8.1% on TextCraft, out-performing GPT-3.5 in two of three domains. Our analysis reveals REGAL’s abstractions encapsulate frequently-used subroutines as well as environment dynamics.",
      "authors": [
        "Elias Stengel-Eskin",
        "Archiki Prasad",
        "Mohit Bansal"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FovMAzXUpj",
      "cdate": 1706848200070,
      "mdate": 1719287277443,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527022"
    },
    {
      "id": "GentO2E4ID",
      "title": "A Doubly Recursive Stochastic Compositional Gradient Descent Method for Federated Multi-Level Compositional Optimization",
      "abstract": "Federated compositional optimization has been actively studied in the past few years. However, existing methods mainly focus on the two-level compositional optimization problem, which cannot be directly applied to the multi-level counterparts. Moreover, the convergence rate of existing federated two-level compositional optimization learning algorithms fails to achieve linear speedup with respect to the number of workers under heterogeneous settings. After identifying the reason for this failure, we developed a novel federated stochastic multi-level compositional optimization algorithm by introducing a novel Jacobian-vector product estimator. This innovation mitigates both the heterogeneity issue and the communication efficiency issue simultaneously. We then theoretically proved that our algorithm can achieve the level-independent and linear speedup convergence rate for nonconvex problems. To our knowledge, this is the first time that a federated learning algorithm can achieve such a favorable convergence rate for multi-level compositional problems. Moreover, experimental results confirm the efficacy of our algorithm.",
      "authors": [
        "Hongchang Gao"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GentO2E4ID",
      "cdate": 1706847923898,
      "mdate": 1719287277399,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527028"
    },
    {
      "id": "zajsXCxMgW",
      "title": "A Distributional Analogue to the Successor Representation",
      "abstract": "This paper contributes a new approach for distributional reinforcement learning which elucidates a clean separation of transition structure and reward in the learning process. Analogous to how the successor representation (SR) describes the expected consequences of behaving according to a given policy, our distributional successor measure (SM) describes the distributional consequences of this behaviour. We formulate the distributional SM as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning. Moreover, we propose an algorithm that learns the distributional SM from data by minimizing a two-level maximum mean discrepancy. Key to our method are a number of algorithmic techniques that are independently valuable for learning generative models of state. As an illustration of the usefulness of the distributional SM, we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possible.",
      "authors": [
        "Harley Wiltzer",
        "Jesse Farebrother",
        "Arthur Gretton",
        "Yunhao Tang",
        "Andre Barreto",
        "Will Dabney",
        "Marc G Bellemare",
        "Mark Rowland"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=zajsXCxMgW",
      "cdate": 1706847865634,
      "mdate": 1719287277385,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527033"
    },
    {
      "id": "Wz4lgc8dsN",
      "title": "Online Cascade Learning for Efficient Inference over Streams",
      "abstract": "Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose *online cascade learning*, the first approach to address this challenge. The objective here is to learn a ``cascade'' of models, starting with lower-capacity models (such as logistic regression) and ending with a powerful LLM, along with a *deferral policy* that determines the model to be used on a given input. We formulate the task of learning cascades online as an imitation-learning problem, where smaller models are updated over time imitating the collected LLM demonstrations, and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90% with strong robustness against input distribution shifts, underscoring its efficacy and adaptability in stream processing. Our source code is available at https://github.com/flitternie/online_cascade_learning.",
      "authors": [
        "Lunyiu Nie",
        "Zhimin Ding",
        "Erdong Hu",
        "Christopher Jermaine",
        "Swarat Chaudhuri"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Wz4lgc8dsN",
      "cdate": 1706847742419,
      "mdate": 1719287277303,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527038"
    },
    {
      "id": "cUMOVfOIve",
      "title": "SIN: Selective and Interpretable Normalization for Long-Term Time Series Forecasting",
      "abstract": "In real-world applications, time series data frequently exhibit non-stationarity, with statistics changing over time. This variability undermines the forecasting accuracy of deep learning models that are trained on historical data but deployed for future prediction. A common approach to mitigate this issue involves normalizing the data to counteract statistical drift, followed by denormalization on the prediction. However, existing methods often employ heuristic normalization techniques that do not fully account for the unique characteristics of the series. Our paper addresses the critical question in this context: which statistics should be removed and restored? We argue that the statistics selected for normalization should exhibit both local invariance and global variability to ensure their correctness and helpfulness. To this end, we propose the Selective and Interpretable Normalization methodology, dubbed SIN. This approach maximizes the covariance between a given look-back window and its subsequent future values, thereby identifying key statistics for normalization and simultaneously learning the corresponding normalization transformations. The interpretable framework can be used to explain the success and limitations of some popular normalization methods. By integrating SIN, we demonstrate improvements in the performance of several prevalent forecasting models, thereby validating the utility of our approach.",
      "authors": [
        "Lu Han",
        "Han-Jia Ye",
        "De-Chuan Zhan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=cUMOVfOIve",
      "cdate": 1706847636338,
      "mdate": 1719287277197,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527044"
    },
    {
      "id": "dSrdnhLS2h",
      "title": "Adaptive Observation Cost Control for Variational Quantum Eigensolvers",
      "abstract": "The objective to be minimized in the variational quantum eigensolver (VQE) has a restricted form, which allows a specialized sequential minimal optimization (SMO) that requires only a few observations in each iteration. However, the SMO iteration is still costly due to the observation noise---one *observation* at a point typically requires averaging over hundreds to thousands of repeated quantum *measurement shots* for achieving a reasonable noise level. In this paper, we propose an adaptive cost control method, named *subspace in confident region* (SubsCoRe), for SMO. SubsCoRe uses the Gaussian process (GP) surrogate, and requires it to have low uncertainty over the subspace being updated, so that optimization in each iteration is performed with guaranteed accuracy. Adaptive cost control is performed by setting the required accuracy according to the progress of the optimization, and identifying the minimum number of measurement shots, as well as their distribution, satisfying the SubsCoRe requirement.",
      "authors": [
        "Christopher J. Anders",
        "Kim Andrea Nicoli",
        "Bingting Wu",
        "Naima Elosegui",
        "Samuele Pedrielli",
        "Lena Funcke",
        "Karl Jansen",
        "Stefan Kühn",
        "Shinichi Nakajima"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=dSrdnhLS2h",
      "cdate": 1706847526138,
      "mdate": 1719287277115,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527049"
    },
    {
      "id": "dtVlc9ybTm",
      "title": "Feedback Efficient Online Fine-Tuning of Diffusion Models",
      "abstract": "Diffusion models excel at modeling complex data distributions, including those of images, proteins, and small molecules. However, in many cases, our goal is to model parts of the distribution that maximize certain properties: for example, we may want to generate images with high aesthetic quality, or molecules with high bioactivity. It is natural to frame this as a reinforcement learning (RL) problem, in which the objective is to finetune a diffusion model to maximize a reward function that corresponds to some property. Even with access to online queries of the ground-truth reward function, efficiently discovering high-reward samples can be challenging: they might have a low probability in the initial distribution, and there might be many infeasible samples that do not even have a well-defined reward (e.g., unnatural images or physically impossible molecules). In this work, we propose a novel reinforcement learning procedure that efficiently explores on the manifold of feasible samples. We present a theoretical analysis providing a regret guarantee, as well as empirical validation across three domains: images, biological sequences, and molecules.",
      "authors": [
        "Masatoshi Uehara",
        "Yulai Zhao",
        "Kevin Black",
        "Ehsan Hajiramezanali",
        "Gabriele Scalia",
        "Nathaniel Lee Diamant",
        "Alex M Tseng",
        "Sergey Levine",
        "Tommaso Biancalani"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=dtVlc9ybTm",
      "cdate": 1706847478684,
      "mdate": 1719287277078,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527054"
    },
    {
      "id": "YWuSLBkfOw",
      "title": "To Cool or not to Cool? Temperature Network Meets Large Foundation Models via DRO",
      "abstract": "The temperature parameter plays a profound role during training and/or inference with large foundation models (LFMs) such as large language models (LLMs) and CLIP models. Particularly, it adjusts the logits in the softmax function in LLMs, which is crucial for next token generation, and it scales the similarities in the contrastive loss for training CLIP models. A significant question remains: `` Is it viable to learn a neural network to predict a personalized temperature of any input data for enhancing LFMs?\" In this paper, we present a principled framework for learning a small yet generalizable temperature prediction network (TempNet) to improve LFMs. Our solution is composed of a novel learning framework with robust losses underpinned by constrained distributionally robust optimization (DRO), and a properly designed TempNet with theoretical inspiration. TempNet can be trained together with a large foundation model from scratch or learned separately given a pretrained foundation model. It is not only useful for predicting personalized temperature to promote the training of LFMs but also generalizable and transferable to new tasks. Our experiments on LLMs and CLIP models demonstrate that TempNet greatly improves the performance of existing solutions or models.",
      "authors": [
        "Zi-Hao Qiu",
        "Siqi Guo",
        "Mao Xu",
        "Tuo Zhao",
        "Lijun Zhang",
        "Tianbao Yang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=YWuSLBkfOw",
      "cdate": 1706847221059,
      "mdate": 1719287277019,
      "matched_keywords": [
        "large language model",
        "foundation model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527060"
    },
    {
      "id": "scMAQ3mFAA",
      "title": "Bayesian Optimization of Function Networks with Partial Evaluations",
      "abstract": "Bayesian optimization is a powerful framework for optimizing functions that are expensive or time-consuming to evaluate. Recent work has considered Bayesian optimization of function networks (BOFN), where the objective function is given by a network of functions, each taking as input the output of previous nodes in the network as well as additional parameters. Leveraging this network structure has been shown to yield significant performance improvements. Existing BOFN algorithms for general-purpose networks evaluate the full network at each iteration. However, many real-world applications allow for evaluating nodes individually. To exploit this, we propose a novel knowledge gradient acquisition function that chooses which node and corresponding inputs to evaluate in a cost-aware manner, thereby reducing query costs by evaluating only on a part of the network at each step. We provide an efficient approach to optimizing our acquisition function and show that it outperforms existing BOFN methods and other benchmarks across several synthetic and real-world problems. Our acquisition function is the first to enable cost-aware optimization of a broad class of function networks.",
      "authors": [
        "Poompol Buathong",
        "Jiayue Wan",
        "Raul Astudillo",
        "Sam Daulton",
        "Maximilian Balandat",
        "Peter I. Frazier"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=scMAQ3mFAA",
      "cdate": 1706847088630,
      "mdate": 1719287276960,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527066"
    },
    {
      "id": "GJzqRKOdRi",
      "title": "From Inverse Optimization to Feasibility to ERM",
      "abstract": "Inverse optimization involves inferring unknown parameters of an optimization problem from known solutions and is widely used in fields such as transportation, power systems, and healthcare. We study the *contextual inverse optimization setting* that utilizes additional contextual information to better predict the unknown problem parameters. We focus on contextual inverse linear programming (CILP) addressing the challenges posed by the non-differentiable nature of LPs. For a linear prediction model, we reduce CILP to a convex feasibility problem allowing the use of standard algorithms such as alternating projections. The resulting algorithm for CILP is equipped with theoretical convergence guarantees without additional assumptions such as degeneracy or interpolation. Next, we reduce CILP to empirical risk minimization (ERM) on a smooth, convex loss that satisfies the Polyak-Lojasiewicz condition. This reduction enables the use of scalable first-order optimization methods to solve large non-convex problems while maintaining theoretical guarantees in the convex setting. Subsequently, we use the reduction to ERM to quantify the generalization performance of the proposed algorithm on previously unseen instances. Finally, we experimentally validate our approach on synthetic and real-world problems and demonstrate improved performance compared to existing methods.",
      "authors": [
        "Saurabh kumar Mishra",
        "Anant Raj",
        "Sharan Vaswani"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GJzqRKOdRi",
      "cdate": 1706847021184,
      "mdate": 1719287276947,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527072"
    },
    {
      "id": "VF177x7Syw",
      "title": "Slow and Steady Wins the Race: Maintaining Plasticity with Hare and Tortoise Networks",
      "abstract": "This study investigates the loss of generalization ability in neural networks, revisiting warm-starting experiments from Ash & Adams. Our empirical analysis reveals that common methods designed to enhance plasticity by maintaining trainability provide limited benefits to generalization. While reinitializing the network can be effective, it also risks losing valuable prior knowledge. To this end, we introduce the Hare & Tortoise, inspired by the brain's complementary learning system. Hare & Tortoise consists of two components: the Hare network, which rapidly adapts to new information analogously to the hippocampus, and the Tortoise network, which gradually integrates knowledge akin to the neocortex. By periodically reinitializing the Hare network to the Tortoise's weights, our method preserves plasticity while retaining general knowledge. Hare & Tortoise can effectively maintain the network's ability to generalize, which improves advanced reinforcement learning algorithms on the Atari-100k benchmark. The code is available at https://github.com/dojeon-ai/hare-tortoise.",
      "authors": [
        "Hojoon Lee",
        "Hyeonseo Cho",
        "Hyunseung Kim",
        "Donghu Kim",
        "Dugki Min",
        "Jaegul Choo",
        "Clare Lyle"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=VF177x7Syw",
      "cdate": 1706846902612,
      "mdate": 1719287276890,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527077"
    },
    {
      "id": "K6xxnKN2gm",
      "title": "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications",
      "abstract": "Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3$ % at the parameter level and $2.5$ % at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.",
      "authors": [
        "Boyi Wei",
        "Kaixuan Huang",
        "Yangsibo Huang",
        "Tinghao Xie",
        "Xiangyu Qi",
        "Mengzhou Xia",
        "Prateek Mittal",
        "Mengdi Wang",
        "Peter Henderson"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=K6xxnKN2gm",
      "cdate": 1706846857062,
      "mdate": 1719287276852,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527082"
    },
    {
      "id": "3JhmHCVPa8",
      "title": "Learning to Reach Goals via Diffusion",
      "abstract": "We present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of denoising diffusion models. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy to reverse these deviations, analogous to the score function. This approach, which we call Merlin, can reach specified goals from arbitrary initial states without learning a separate value function. In contrast to recent works utilizing diffusion models in offline RL, Merlin stands out as the first method to perform diffusion in the state space, requiring only one \"denoising\" iteration per environment step. We experimentally validate our approach in various offline goal-reaching tasks, demonstrating substantial performance enhancements compared to state-of-the-art methods while improving computational efficiency over other diffusion-based RL methods by an order of magnitude. Our results suggest that this perspective on diffusion for RL is a simple and scalable approach for sequential decision making.",
      "authors": [
        "Vineet Jain",
        "Siamak Ravanbakhsh"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3JhmHCVPa8",
      "cdate": 1706846775284,
      "mdate": 1719287276819,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527087"
    },
    {
      "id": "3MW8GKNyzI",
      "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
      "abstract": "Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowd-sourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. The platform is publicly available at https://chat.lmsys.org.",
      "authors": [
        "Wei-Lin Chiang",
        "Lianmin Zheng",
        "Ying Sheng",
        "Anastasios Nikolas Angelopoulos",
        "Tianle Li",
        "Dacheng Li",
        "Banghua Zhu",
        "Hao Zhang",
        "Michael Jordan",
        "Joseph E. Gonzalez",
        "Ion Stoica"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3MW8GKNyzI",
      "cdate": 1706846735907,
      "mdate": 1719287276774,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527092"
    },
    {
      "id": "gKPkipJ3gm",
      "title": "Causal-IQA: Towards the Generalization of Image Quality Assessment Based on Causal Inference",
      "abstract": "Due to the high cost of Image Quality Assessment (IQA) datasets, achieving robust generalization remains challenging for prevalent deep learning-based IQA methods. To address this, this paper proposes a novel end-to-end blind IQA method: Causal-IQA. Specifically, we first analyze the causal mechanisms in IQA tasks and construct a causal graph to understand the interplay and confounding effects between distortion types, image contents, and subjective human ratings. Then, through shifting the focus from correlations to causality, Causal-IQA aims to improve the estimation accuracy of image quality scores by mitigating the confounding effects using a causality-based optimization strategy. This optimization strategy is implemented on the sample subsets constructed by a Counterfactual Division process based on the Backdoor Criterion. Extensive experiments illustrate the superiority of Causal-IQA.",
      "authors": [
        "Yan Zhong",
        "Xingyu Wu",
        "Li Zhang",
        "Chenxi Yang",
        "Tingting Jiang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=gKPkipJ3gm",
      "cdate": 1706846616915,
      "mdate": 1719287276708,
      "matched_keywords": [
        "deep learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527097"
    },
    {
      "id": "7bg10Jj3bG",
      "title": "Temporal Logic Specification-Conditioned Decision Transformer for Offline Safe Reinforcement Learning",
      "abstract": "Offline safe reinforcement learning (RL) aims to train a constraint satisfaction policy from a fixed dataset. Current state-of-the-art approaches are based on supervised learning with a conditioned policy. However, these approaches fall short in real-world applications that involve complex tasks with rich temporal and logical structures. In this paper, we propose temporal logic Specification-conditioned Decision Transformer (SDT), a novel framework that harnesses the expressive power of signal temporal logic (STL) to specify complex temporal rules that an agent should follow and the sequential modeling capability of Decision Transformer (DT). Empirical evaluations on the DSRL benchmarks demonstrate the better capacity of SDT in learning safe and high-reward policies compared with existing approaches. In addition, SDT shows good alignment with respect to different desired degrees of satisfaction of the STL specification that it is conditioned on.",
      "authors": [
        "Zijian Guo",
        "Weichao Zhou",
        "Wenchao Li"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7bg10Jj3bG",
      "cdate": 1706846423053,
      "mdate": 1719287276619,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527103"
    },
    {
      "id": "eDjvSFOkXw",
      "title": "Break the Sequential Dependency of LLM Inference Using Lookahead Decoding",
      "abstract": "Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators. Existing methods for accelerating LLM decoding often require a draft model (e.g., speculative decoding), which is nontrivial to obtain and unable to generalize. In this paper, we introduce Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM decoding without needing auxiliary models or data stores. It allows trading per-step log(FLOPs) to reduce the number of total decoding steps, is more parallelizable on single or multiple modern accelerators, and is compatible with concurrent memory-efficient attention (e.g., FlashAttention). Our implementation of Lookahead decoding can speed up autoregressive decoding by up to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code completion tasks. Our code is avialable at https://github.com/hao-ai-lab/LookaheadDecoding",
      "authors": [
        "Yichao Fu",
        "Peter Bailis",
        "Ion Stoica",
        "Hao Zhang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=eDjvSFOkXw",
      "cdate": 1706846378018,
      "mdate": 1719287276601,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527109"
    },
    {
      "id": "BO0jookxk8",
      "title": "On Least Square Estimation in Softmax Gating Mixture of Experts",
      "abstract": "Mixture of experts (MoE) model is a statistical machine learning design that aggregates multiple expert networks using a softmax gating function in order to form a more intricate and expressive model. Despite being commonly used in several applications owing to their scalability, the mathematical and statistical properties of MoE models are complex and difficult to analyze. As a result, previous theoretical works have primarily focused on probabilistic MoE models by imposing the impractical assumption that the data are generated from a Gaussian MoE model. In this work, we investigate the performance of the least squares estimators (LSE) under a deterministic MoE model where the data are sampled according to a regression model, a setting that has remained largely unexplored. We establish a condition called strong identifiability to characterize the convergence behavior of various types of expert functions. We demonstrate that the rates for estimating strongly identifiable experts, namely the widely used feed forward networks with activation functions $\\mathrm{sigmoid}(\\cdot)$ and $\\tanh(\\cdot)$, are substantially faster than those of polynomial experts, which we show to exhibit a surprising slow estimation rate. Our findings have important practical implications for expert selection.",
      "authors": [
        "Huy Nguyen",
        "Nhat Ho",
        "Alessandro Rinaldo"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BO0jookxk8",
      "cdate": 1706846306495,
      "mdate": 1719287276578,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527115"
    },
    {
      "id": "ffLblkoCw8",
      "title": "MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models",
      "abstract": "Multi-agent interactions between Large Language Model (LLM) agents have shown major improvements on diverse reasoning tasks. However, these involve long generations from multiple models across several rounds, making them expensive. Moreover, these multi-agent approaches fail to provide a final, single model for efficient inference. To address this, we introduce MAGDi, a new method for structured distillation of the reasoning interactions between multiple LLMs into smaller LMs. MAGDi teaches smaller models by representing multi-agent interactions as graphs, augmenting a base student model with a graph encoder, and distilling knowledge using three objective functions: next-token prediction, a contrastive loss between correct and incorrect reasoning, and a graph-based objective to model the interaction structure. Experiments on seven widely used commonsense and math reasoning benchmarks show that MAGDi improves the reasoning capabilities of smaller models, outperforming several methods that distill from a single teacher and multiple teachers. Moreover, MAGDi also demonstrates an order of magnitude higher efficiency over its teachers. We conduct extensive analyses to show that MAGDi (1) enhances the generalizability to out-of-domain tasks, (2) scales positively with the size and strength of the base student model, and (3) obtains larger improvements (via our multi-teacher training) when applying self-consistency – an inference technique that relies on model diversity.",
      "authors": [
        "Justin Chen",
        "Swarnadeep Saha",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ffLblkoCw8",
      "cdate": 1706846188844,
      "mdate": 1719287276453,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527120"
    },
    {
      "id": "qjqlhWDcId",
      "title": "Transformers Provably Learn Sparse Token Selection While Fully-Connected Nets Cannot",
      "abstract": "The transformer architecture has prevailed in various deep learning settings due to its exceptional capabilities to select and compose structural information. Motivated by these capabilities, Sanford et al. (2023) proposed the *sparse token selection* task, in which transformers excel while fully-connected networks (FCNs) fail in the worst case. Building upon that, we strengthen the FCN lower bound to an average-case setting and establish an algorithmic separation of transformers over FCNs. Specifically, a one-layer transformer trained with gradient descent provably learns the sparse token selection task and, surprisingly, exhibits strong out-of-distribution length generalization. We provide empirical simulations to justify our theoretical findings.",
      "authors": [
        "Zixuan Wang",
        "Stanley Wei",
        "Daniel Hsu",
        "Jason D. Lee"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qjqlhWDcId",
      "cdate": 1706846059487,
      "mdate": 1719287276420,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527126"
    },
    {
      "id": "0ntak1BGBd",
      "title": "ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance",
      "abstract": "In the emergency department (ED), patients undergo triage and multiple laboratory tests before diagnosis. This time-consuming process causes ED crowding which impacts patient mortality, medical errors, staff burnout, etc. This work proposes (time) *cost-effective diagnostic assistance* that leverages artificial intelligence systems to help ED clinicians make efficient and accurate diagnoses. In collaboration with ED clinicians, we use public patient data to curate MIMIC-ED-Assist, a benchmark for AI systems to suggest laboratory tests that minimize wait time while accurately predicting critical outcomes such as death. With MIMIC-ED-Assist, we develop ED-Copilot which sequentially suggests patient-specific laboratory tests and makes diagnostic predictions. ED-Copilot employs a pre-trained bio-medical language model to encode patient information and uses reinforcement learning to minimize ED wait time and maximize prediction accuracy. On MIMIC-ED-Assist, ED-Copilot improves prediction accuracy over baselines while halving average wait time from four hours to two hours. ED-Copilot can also effectively personalize treatment recommendations based on patient severity, further highlighting its potential as a diagnostic assistant. Since MIMIC-ED-Assist is a retrospective benchmark, ED-Copilot is restricted to recommend only observed tests. We show ED-Copilot achieves competitive performance without this restriction as the maximum allowed time increases. Our code is available at https://github.com/cxcscmu/ED-Copilot.",
      "authors": [
        "Liwen Sun",
        "Abhineet Agarwal",
        "Aaron Kornblith",
        "Bin Yu",
        "Chenyan Xiong"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0ntak1BGBd",
      "cdate": 1706845752220,
      "mdate": 1719287276492,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527132"
    },
    {
      "id": "yHs3jIPgaF",
      "title": "Performative Prediction with Bandit Feedback: Learning through Reparameterization",
      "abstract": "Performative prediction, as introduced by Perdomo et al., is a framework for studying social prediction in which the data distribution itself changes in response to the deployment of a model. Existing work in this field usually hinges on three assumptions that are easily violated in practice: that the performative risk is convex over the deployed model, that the mapping from the model to the data distribution is known to the model designer in advance, and the first-order information of the performative risk is available. In this paper, we initiate the study of performative prediction problems that do not require these assumptions. Specifically, we develop a parameterization framework that parametrizes the performative prediction objective as a function of the induced data distribution. We also develop a two-level zeroth-order optimization procedure, where the first level performs iterative optimization on the distribution parameter space, and the second level learns the model that induced a particular target distribution parameter at each iteration. Under mild conditions, this reparameterization allows us to transform the non-convex objective into a convex one and achieve provable regret guarantees. In particular, we provide a regret bound that is sublinear in the total number of performative samples taken and is only polynomial in the dimension of the model parameter.",
      "authors": [
        "Yatong Chen",
        "Wei Tang",
        "Chien-Ju Ho",
        "Yang Liu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=yHs3jIPgaF",
      "cdate": 1706845606671,
      "mdate": 1719287276084,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527138"
    },
    {
      "id": "iqAyWVLUEO",
      "title": "Statistical Properties of Robust Satisficing",
      "abstract": "The Robust Satisficing (RS) model is an emerging approach to robust optimization, offering streamlined procedures and robust generalization across various applications. However, the statistical theory of RS remains unexplored in the literature. This paper fills in the gap by comprehensively analyzing the theoretical properties of the RS model. Notably, the RS structure offers a more straightforward path to deriving statistical guarantees compared to the seminal Distributionally Robust Optimization (DRO), resulting in a richer set of results. In particular, we establish two-sided confidence intervals for the optimal loss without the need to solve a minimax optimization problem explicitly. We further provide finite-sample generalization error bounds for the RS optimizer. Importantly, our results extend to scenarios involving distribution shifts, where discrepancies exist between the sampling and target distributions. Our numerical experiments show that the RS model consistently outperforms the baseline empirical risk minimization in small-sample regimes and under distribution shifts. Furthermore, compared to the DRO model, the RS model exhibits lower sensitivity to hyperparameter tuning, highlighting its practicability for robustness considerations.",
      "authors": [
        "zhiyi li",
        "Yunbei Xu",
        "Ruohan Zhan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iqAyWVLUEO",
      "cdate": 1706845391917,
      "mdate": 1719287276032,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527156"
    },
    {
      "id": "oowQ8LPA12",
      "title": "Theoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability",
      "abstract": "Use of machine learning to perform database operations, such as indexing, cardinality estimation, and sorting, is shown to provide substantial performance benefits. However, when datasets change and data distribution shifts, empirical results also show performance degradation for learned models, possibly to worse than non-learned alternatives. This, together with a lack of theoretical understanding of learned methods undermines their practical applicability, since there are no guarantees on how well the models will perform after deployment. In this paper, we present the first known theoretical characterization of the performance of learned models in dynamic datasets, for the aforementioned operations. Our results show novel theoretical characteristics achievable by learned models and provide bounds on the performance of the models that characterize their advantages over non-learned methods, showing why and when learned models can outperform the alternatives. Our analysis develops the *distribution learnability* framework and novel theoretical tools which build the foundation for the analysis of learned database operations in the future.",
      "authors": [
        "Sepanta Zeighami",
        "Cyrus Shahabi"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=oowQ8LPA12",
      "cdate": 1706845279676,
      "mdate": 1719287275997,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527161"
    },
    {
      "id": "3QM5SWfeov",
      "title": "MALIBO: Meta-learning for Likelihood-free Bayesian Optimization",
      "abstract": "Bayesian optimization (BO) is a popular method to optimize costly black-box functions, and meta-learning has emerged as a way to leverage knowledge from related tasks to optimize new tasks faster. However, existing meta-learning methods for BO rely on surrogate models that are not scalable or are sensitive to varying input scales and noise types across tasks. Moreover, they often overlook the uncertainty associated with task similarity, leading to unreliable task adaptation when a new task differs significantly or has not been sufficiently explored yet. We propose a novel meta-learning BO approach that bypasses the surrogate model and directly learns the utility of queries across tasks. It explicitly models task uncertainty and includes an auxiliary model to enable robust adaptation to new tasks. Extensive experiments show that our method achieves strong performance and outperforms multiple meta-learning BO methods across various benchmarks.",
      "authors": [
        "Jiarong Pan",
        "Stefan Falkner",
        "Felix Berkenkamp",
        "Joaquin Vanschoren"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3QM5SWfeov",
      "cdate": 1706845080221,
      "mdate": 1719287275953,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527166"
    },
    {
      "id": "jn2iTJas6h",
      "title": "A decoder-only foundation model for time-series forecasting",
      "abstract": "Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a decoder style attention model with input patching, using a large time-series corpus comprising both real-world and synthetic datasets. Experiments on a diverse set of previously unseen forecasting datasets suggests that the model can yield accurate zero-shot forecasts across different domains, forecasting horizons and temporal granularities.",
      "authors": [
        "Abhimanyu Das",
        "Weihao Kong",
        "Rajat Sen",
        "Yichen Zhou"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jn2iTJas6h",
      "cdate": 1706845076459,
      "mdate": 1719287275907,
      "matched_keywords": [
        "large language model",
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527172"
    },
    {
      "id": "aPhwhueqjR",
      "title": "A Universal Transfer Theorem for Convex Optimization Algorithms Using Inexact First-order Oracles",
      "abstract": "Given *any* algorithm for convex optimization that uses exact first-order information (i.e., function values and subgradients), we show how to use such an algorithm to solve the problem with access to *inexact* first-order information. This is done in a ``black-box'' manner without knowledge of the internal workings of the algorithm. This complements previous work that considers the performance of specific algorithms like (accelerated) gradient descent with inexact information. In particular, our results apply to a wider range of algorithms beyond variants of gradient descent, e.g., projection-free methods, cutting-plane methods, or any other first-order methods formulated in the future. Further, they also apply to algorithms that handle structured nonconvexities like mixed-integer decision variables.",
      "authors": [
        "Phillip Kerger",
        "Marco Molinaro",
        "Hongyi Jiang",
        "Amitabh Basu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=aPhwhueqjR",
      "cdate": 1706845025473,
      "mdate": 1719287275804,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527177"
    },
    {
      "id": "7KtFQnF368",
      "title": "Convergence and Complexity Guarantee for Inexact First-order Riemannian Optimization Algorithms",
      "abstract": "We analyze inexact Riemannian gradient descent (RGD) where Riemannian gradients and retractions are inexactly (and cheaply) computed. Our focus is on understanding when inexact RGD converges and what is the complexity in the general nonconvex and constrained setting. We answer these questions in a general framework of tangential Block Majorization-Minimization (tBMM). We establish that tBMM converges to an $\\epsilon$-stationary point within $O(\\epsilon^{-2})$ iterations. Under a mild assumption, the results still hold when the subproblem is solved inexactly in each iteration provided the total optimality gap is bounded. Our general analysis applies to a wide range of classical algorithms with Riemannian constraints including inexact RGD and proximal gradient method on Stiefel manifolds. We numerically validate that tBMM shows improved performance over existing methods when applied to various problems, including nonnegative tensor decomposition with Riemannian constraints, regularized nonnegative matrix factorization, and low-rank matrix recovery problems.",
      "authors": [
        "Yuchen Li",
        "Laura Balzano",
        "Deanna Needell",
        "Hanbaek Lyu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7KtFQnF368",
      "cdate": 1706845022358,
      "mdate": 1719287275804,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527183"
    },
    {
      "id": "XUeoOBid3x",
      "title": "Magicoder: Empowering Code Generation with OSS-Instruct",
      "abstract": "We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using **OSS-Instruct**, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references.",
      "authors": [
        "Yuxiang Wei",
        "Zhe Wang",
        "Jiawei Liu",
        "Yifeng Ding",
        "LINGMING ZHANG"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=XUeoOBid3x",
      "cdate": 1706844745876,
      "mdate": 1719287275589,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527188"
    },
    {
      "id": "JIWtKcR78C",
      "title": "Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function",
      "abstract": "What makes large language models (LLMs) impressive is also what makes them hard to evaluate: their diversity of uses. To evaluate these models, we must understand the purposes they will be used for. We consider a setting where these deployment decisions are made by people, and in particular, people's beliefs about where an LLM will perform well. We model such beliefs as the consequence of a human generalization function: having seen what an LLM gets right or wrong, people generalize to where else it might succeed. We collect a dataset of 19K examples of how humans make generalizations across 79 tasks from the MMLU and BIG-Bench benchmarks. We show that the human generalization function can be predicted using NLP methods: people have consistent structured ways to generalize. We then evaluate LLM alignment with the human generalization function. Our results show that -- especially for cases where the cost of mistakes is high -- more capable models (e.g. GPT-4) can do worse on the instances people choose to use them for, exactly because they are not aligned with the human generalization function.",
      "authors": [
        "Keyon Vafa",
        "Ashesh Rambachan",
        "Sendhil Mullainathan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=JIWtKcR78C",
      "cdate": 1706844735049,
      "mdate": 1719287275585,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527194"
    },
    {
      "id": "1khG2xf1yt",
      "title": "On PI Controllers for Updating Lagrange Multipliers in Constrained Optimization",
      "abstract": "Constrained optimization offers a powerful framework to prescribe desired behaviors in neural network models. Typically, constrained problems are solved via their min-max Lagrangian formulations, which exhibit unstable oscillatory dynamics when optimized using gradient descent-ascent. The adoption of constrained optimization techniques in the machine learning community is currently limited by the lack of reliable, general-purpose update schemes for the Lagrange multipliers. This paper proposes the νPI algorithm and contributes an optimization perspective on Lagrange multiplier updates based on PI controllers, extending the work of Stooke, Achiam and Abbeel (2020). We provide theoretical and empirical insights explaining the inability of momentum methods to address the shortcomings of gradient descent-ascent, and contrast this with the empirical success of our proposed νPI controller. Moreover, we prove that νPI generalizes popular momentum methods for single-objective minimization. Our experiments demonstrate that νPI reliably stabilizes the multiplier dynamics and its hyperparameters enjoy robust and predictable behavior.",
      "authors": [
        "Motahareh Sohrabi",
        "Juan Ramirez",
        "Tianyue H. Zhang",
        "Simon Lacoste-Julien",
        "Jose Gallego-Posada"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=1khG2xf1yt",
      "cdate": 1706844477827,
      "mdate": 1719287275373,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527199"
    },
    {
      "id": "H3bATm4mKn",
      "title": "Out of the Ordinary: Spectrally Adapting Regression for Covariate Shift",
      "abstract": "Designing deep neural network classifiers that perform robustly on distributions differing from the available training data is an active area of machine learning research. However, out-of-distribution generalization for regression---the analogous problem for modeling continuous targets---remains relatively unexplored. To tackle this problem, we return to first principles and analyze how the closed-form solution for Ordinary Least Squares (OLS) regression is sensitive to covariate shift. We characterize the out-of-distribution risk of the OLS model in terms of the eigenspectrum decomposition of the source and target data. We then use this insight to propose a method called Spectral Adapted Regressor (SpAR) for adapting the weights of the last layer of a pre-trained neural regression model to perform better on input data originating from a different distribution. We demonstrate how this lightweight spectral adaptation procedure can improve out-of-distribution performance for synthetic and real-world datasets.",
      "authors": [
        "Benjamin Eyre",
        "Elliot Creager",
        "David Madras",
        "Vardan Papyan",
        "Richard Zemel"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=H3bATm4mKn",
      "cdate": 1706844450342,
      "mdate": 1719287275192,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527205"
    },
    {
      "id": "SMUXPVKUBg",
      "title": "Time-Series Forecasting for Out-of-Distribution Generalization Using Invariant Learning",
      "abstract": "Time-series forecasting (TSF) finds broad applications in real-world scenarios. Due to the dynamic nature of time-series data, it is crucial for TSF models to preserve out-of-distribution (OOD) generalization abilities, as training and test sets represent historical and future data respectively. In this paper, we aim to alleviate the inherent OOD problem in TSF via invariant learning. We identify fundamental challenges of invariant learning for TSF. First, the target variables in TSF may not be sufficiently determined by the input due to unobserved core variables in TSF, breaking the fundamental assumption of invariant learning. Second, time-series datasets lack adequate environment labels, while existing environmental inference methods are not suitable for TSF. To address these challenges, we propose FOIL, a model-agnostic framework that endows time-series forecasting for out-of-distribution generalization via invariant learning. Specifically, FOIL employs a novel surrogate loss to mitigate the impact of unobserved variables. Further, FOIL implements joint optimization by alternately inferring environments effectively with a multi-head network while preserving the temporal adjacency structure and learning invariant representations across inferred environments for OOD generalized TSF. Extensive experiments demonstrate that the proposed FOIL significantly and consistently improves the performance of various TSF models, achieving gains of up to 85%.",
      "authors": [
        "Haoxin Liu",
        "Harshavardhan Kamarthi",
        "Lingkai Kong",
        "Zhiyuan Zhao",
        "Chao Zhang",
        "B. Aditya Prakash"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=SMUXPVKUBg",
      "cdate": 1706844438391,
      "mdate": 1727373650950,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527210"
    },
    {
      "id": "Kjww7ZN47M",
      "title": "MathScale: Scaling Instruction Tuning for Mathematical Reasoning",
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., GPT-3.5). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct MWPBench, a benchmark of Math Word Problems, which is a collection of 9 datasets (including GSM8K and MATH) covering K-12, college, and competition level math problems. We apply MathScaleQA to fine-tune open-source LLMs (e.g., LLaMA-2 and Mistral), resulting in significantly improved capabilities in mathematical reasoning. Evaluated on MWPBench, MathScale-7B achieves state-of-the-art performance across all datasets, surpassing its best peers of equivalent size by 42.8% in micro average accuracy and 43.6% in macro average accuracy, respectively.",
      "authors": [
        "Zhengyang Tang",
        "Xingxing Zhang",
        "Benyou Wang",
        "Furu Wei"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Kjww7ZN47M",
      "cdate": 1706844374848,
      "mdate": 1719287275127,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527215"
    },
    {
      "id": "ymgcTqrZLT",
      "title": "Estimating Barycenters of Distributions with Neural Optimal Transport",
      "abstract": "Given a collection of probability measures, a practitioner sometimes needs to find an \"average\" distribution which adequately aggregates reference distributions. A theoretically appealing notion of such an average is the Wasserstein barycenter, which is the primal focus of our work. By building upon the dual formulation of Optimal Transport (OT), we propose a new scalable approach for solving the Wasserstein barycenter problem. Our methodology is based on the recent Neural OT solver: it has bi-level adversarial learning objective and works for general cost functions. These are key advantages of our method since the typical adversarial algorithms leveraging barycenter tasks utilize tri-level optimization and focus mostly on quadratic cost. We also establish theoretical error bounds for our proposed approach and showcase its applicability and effectiveness in illustrative scenarios and image data setups. Our source code is available at https://github.com/justkolesov/NOTBarycenters.",
      "authors": [
        "Alexander Kolesov",
        "Petr Mokrov",
        "Igor Udovichenko",
        "Milena Gazdieva",
        "Gudmund Pammer",
        "Evgeny Burnaev",
        "Alexander Korotin"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ymgcTqrZLT",
      "cdate": 1706843992879,
      "mdate": 1719287275056,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527220"
    },
    {
      "id": "gbD9MAc9p0",
      "title": "Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design",
      "abstract": "Experimental design techniques such as active search and Bayesian optimization are widely used in the natural sciences for data collection and discovery. However, existing techniques tend to favor exploitation over exploration of the search space, which causes them to get stuck in local optima. This _collapse_ problem prevents experimental design algorithms from yielding diverse high-quality data. In this paper, we extend the Vendi scores—a family of interpretable similarity-based diversity metrics—to account for quality. We then leverage these *quality-weighted Vendi scores* to tackle experimental design problems across various applications, including drug discovery, materials discovery, and reinforcement learning. We found that quality-weighted Vendi scores allow us to construct policies for experimental design that flexibly balance quality and diversity, and ultimately assemble rich and diverse sets of high-performing data points. Our algorithms led to a 70%–170% increase in the number of effective discoveries compared to baselines.",
      "authors": [
        "Quan Nguyen",
        "Adji Bousso Dieng"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=gbD9MAc9p0",
      "cdate": 1706843841161,
      "mdate": 1719287275005,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527225"
    },
    {
      "id": "wkCUmO7oi2",
      "title": "Joint Composite Latent Space Bayesian Optimization",
      "abstract": "Bayesian Optimization (BO) is a technique for sample-efficient black-box optimization that employs probabilistic models to identify promising input for evaluation. When dealing with composite-structured functions, such as $f=g \\circ h$, evaluating a specific location $x$ yields observations of both the final outcome $f(x) = g(h(x))$ as well as the intermediate output(s) $h(x)$. Previous research has shown that integrating information from these intermediate outputs can enhance BO performance substantially. However, existing methods struggle if the outputs $h(x)$ are high-dimensional. Many relevant problems fall into this setting, including in the context of generative AI, molecular design, or robotics. To effectively tackle these challenges, we introduce Joint Composite Latent Space Bayesian Optimization (JoCo), a novel framework that jointly trains neural network encoders and probabilistic models to adaptively compress high-dimensional input and output spaces into manageable latent representations. This enables effective BO on these compressed representations, allowing JoCo to outperform other state-of-the-art methods in high-dimensional BO on a wide variety of simulated and real-world problems.",
      "authors": [
        "Natalie Maus",
        "Zhiyuan Jerry Lin",
        "Maximilian Balandat",
        "Eytan Bakshy"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=wkCUmO7oi2",
      "cdate": 1706843682474,
      "mdate": 1719287274986,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527230"
    },
    {
      "id": "I4HTPws9P6",
      "title": "How Do Nonlinear Transformers Learn and Generalize in In-Context Learning?",
      "abstract": "Transformer-based large language models have displayed impressive in-context learning capabilities, where a pre-trained model can handle new tasks without fine-tuning by simply augmenting the query with some input-output examples from that task. Despite the empirical success, the mechanics of how to train a Transformer to achieve ICL and the corresponding ICL capacity is mostly elusive due to the technical challenges of analyzing the nonconvex training problems resulting from the nonlinear self-attention and nonlinear activation in Transformers. To the best of our knowledge, this paper provides the first theoretical analysis of the training dynamics of Transformers with nonlinear self-attention and nonlinear MLP, together with the ICL generalization capability of the resulting model. Focusing on a group of binary classification tasks, we train Transformers using data from a subset of these tasks and quantify the impact of various factors on the ICL generalization performance on the remaining unseen tasks with and without data distribution shifts. We also analyze how different components in the learned Transformers contribute to the ICL performance. Furthermore, we provide the first theoretical analysis of how model pruning affects ICL performance and prove that proper magnitude-based pruning can have a minimal impact on ICL while reducing inference costs. These theoretical findings are justified through numerical experiments.",
      "authors": [
        "Hongkang Li",
        "Meng Wang",
        "Songtao Lu",
        "Xiaodong Cui",
        "Pin-Yu Chen"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=I4HTPws9P6",
      "cdate": 1706843569395,
      "mdate": 1719287274898,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527235"
    },
    {
      "id": "BrCrnaCYDc",
      "title": "Dynamic Anisotropic Smoothing for Noisy Derivative-Free Optimization",
      "abstract": "We propose a novel algorithm that extends the methods of ball smoothing and Gaussian smoothing for noisy derivative-free optimization by accounting for the heterogeneous curvature of the objective function. The algorithm dynamically adapts the shape of the smoothing kernel to approximate the Hessian of the objective function around a local optimum. This approach significantly reduces the error in estimating the gradient from noisy evaluations through sampling. We demonstrate the efficacy of our method through numerical experiments on artificial problems. Additionally, we show improved performance when tuning NP-hard combinatorial optimization solvers compared to existing state-ofthe-art heuristic derivative-free and Bayesian optimization methods.",
      "authors": [
        "Sam Reifenstein",
        "Timothee Leleu",
        "Yoshihisa Yamamoto"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BrCrnaCYDc",
      "cdate": 1706843182243,
      "mdate": 1719287274790,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527242"
    },
    {
      "id": "f47ZK6gy3I",
      "title": "Erasing the Bias: Fine-Tuning Foundation Models for Semi-Supervised Learning",
      "abstract": "Semi-supervised learning (SSL) has witnessed remarkable progress, resulting in the emergence of numerous method variations. However, practitioners often encounter challenges when attempting to deploy these methods due to their subpar performance. In this paper, we present a novel SSL approach named FineSSL that significantly addresses this limitation by adapting pre-trained foundation models. We identify the aggregated biases and cognitive deviation problems inherent in foundation models, and propose a simple yet effective solution by imposing balanced margin softmax and decoupled label smoothing. Through extensive experiments, we demonstrate that FineSSL sets a new state of the art for SSL on multiple benchmark datasets, reduces the training cost by over six times, and can seamlessly integrate various fine-tuning and modern SSL algorithms. The source code is available at https://github.com/Gank0078/FineSSL.",
      "authors": [
        "Kai Gan",
        "Tong Wei"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=f47ZK6gy3I",
      "cdate": 1706843045573,
      "mdate": 1719287274689,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527247"
    },
    {
      "id": "itYGbe0Cs1",
      "title": "AI Alignment with Changing and Influenceable Reward Functions",
      "abstract": "Existing AI alignment approaches assume that preferences are static, which is unrealistic: our preferences change, and may even be influenced by our interactions with AI systems themselves. To clarify the consequences of incorrectly assuming static preferences, we introduce Dynamic Reward Markov Decision Processes (DR-MDPs), which explicitly model preference changes and the AI's influence on them. We show that despite its convenience, the static-preference assumption may undermine the soundness of existing alignment techniques, leading them to implicitly reward AI systems for influencing user preferences in ways users may not truly want. We then explore potential solutions. First, we offer a unifying perspective on how an agent's optimization horizon may partially help reduce undesirable AI influence. Then, we formalize different notions of AI alignment that account for preference change from the outset. Comparing the strengths and limitations of 8 such notions of alignment, we find that they all either err towards causing undesirable AI influence, or are overly risk-averse, suggesting that a straightforward solution to the problems of changing preferences may not exist. As there is no avoiding grappling with changing preferences in real-world settings, this makes it all the more important to handle these issues with care, balancing risks and capabilities. We hope our work can provide conceptual clarity and constitute a first step towards AI alignment practices which explicitly account for (and contend with) the changing and influenceable nature of human preferences.",
      "authors": [
        "Micah Carroll",
        "Davis Foote",
        "Anand Siththaranjan",
        "Stuart Russell",
        "Anca Dragan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=itYGbe0Cs1",
      "cdate": 1706843041940,
      "mdate": 1719287274645,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527253"
    },
    {
      "id": "5ZwEifshyo",
      "title": "Explorations of Self-Repair in Language Models",
      "abstract": "Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor and sparse sets of neurons implementing Anti-Erasure. We additionally discuss the implications of these results for interpretability practitioners and close with a more speculative discussion on the mystery of why self-repair occurs in these models at all, highlighting evidence for the Iterative Inference hypothesis in language models, a framework that predicts self-repair.",
      "authors": [
        "Cody Rushing",
        "Neel Nanda"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5ZwEifshyo",
      "cdate": 1706842992183,
      "mdate": 1719287274627,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527258"
    },
    {
      "id": "1dtYo5ywXZ",
      "title": "Understanding MLP-Mixer as a wide and sparse MLP",
      "abstract": "Multi-layer perceptron (MLP) is a fundamental component of deep learning, and recent MLP-based architectures, especially the MLP-Mixer, have achieved significant empirical success. Nevertheless, our understanding of why and how the MLP-Mixer outperforms conventional MLPs remains largely unexplored. In this work, we reveal that sparseness is a key mechanism underlying the MLP-Mixers. First, the Mixers have an effective expression as a wider MLP with Kronecker-product weights, clarifying that the Mixers efficiently embody several sparseness properties explored in deep learning. In the case of linear layers, the effective expression elucidates an implicit sparse regularization caused by the model architecture and a hidden relation to Monarch matrices, which is also known as another form of sparse parameterization. Next, for general cases, we empirically demonstrate quantitative similarities between the Mixer and the unstructured sparse-weight MLPs. Following a guiding principle proposed by Golubeva, Neyshabur and Gur-Ari (2021), which fixes the number of connections and increases the width and sparsity, the Mixers can demonstrate improved performance.",
      "authors": [
        "Tomohiro Hayase",
        "Ryo Karakida"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=1dtYo5ywXZ",
      "cdate": 1706842984923,
      "mdate": 1719287274600,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527262"
    },
    {
      "id": "xQiYCmDrjp",
      "title": "Learning Temporal Distances: Contrastive Successor Features Can Provide a Metric Structure for Decision-Making",
      "abstract": "Temporal distances lie at the heart of many algorithms for planning, control, and reinforcement learning that involve reaching goals, allowing one to estimate the transit time between two states. However, prior attempts to define such temporal distances in stochastic settings have been stymied by an important limitation: these prior approaches do not satisfy the triangle inequality. This is not merely a definitional concern, but translates to an inability to generalize and find shortest paths. In this paper, we build on prior work in contrastive learning and quasimetrics to show how successor features learned by contrastive learning (after a change of variables) form a temporal distance that does satisfy the triangle inequality, even in stochastic settings. Importantly, this temporal distance is computationally efficient to estimate, even in high-dimensional and stochastic settings. Experiments in controlled settings and benchmark suites demonstrate that an RL algorithm based on these new temporal distances exhibits combinatorial generalization (i.e., \"stitching\") and can sometimes learn more quickly than prior methods, including those based on quasimetrics.",
      "authors": [
        "Vivek Myers",
        "Chongyi Zheng",
        "Anca Dragan",
        "Sergey Levine",
        "Benjamin Eysenbach"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xQiYCmDrjp",
      "cdate": 1706842923921,
      "mdate": 1719287274583,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527267"
    },
    {
      "id": "GcW9pg4P9x",
      "title": "Constrained Reinforcement Learning Under Model Mismatch",
      "abstract": "Existing studies on constrained reinforcement learning (RL) may obtain a well-performing policy in the training environment. However, when deployed in a real environment, it may easily violate constraints that were originally satisfied during training because there might be model mismatch between the training and real environments. To address this challenge, we formulate the problem as constrained RL under model uncertainty, where the goal is to learn a policy that optimizes the reward and at the same time satisfies the constraint under model mismatch. We develop a Robust Constrained Policy Optimization (RCPO) algorithm, which is the first algorithm that applies to large/continuous state space and has theoretical guarantees on worst-case reward improvement and constraint violation at each iteration during the training. We show the effectiveness of our algorithm on a set of RL tasks with constraints.",
      "authors": [
        "Zhongchang Sun",
        "Sihong He",
        "Fei Miao",
        "Shaofeng Zou"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GcW9pg4P9x",
      "cdate": 1706842768713,
      "mdate": 1719287274528,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527272"
    },
    {
      "id": "4zAHgkiCQg",
      "title": "Premise Order Matters in Reasoning with Large Language Models",
      "abstract": "Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that even if the model performance is decent on the optimal order, permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark.",
      "authors": [
        "Xinyun Chen",
        "Ryan Andrew Chi",
        "Xuezhi Wang",
        "Denny Zhou"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4zAHgkiCQg",
      "cdate": 1706842635943,
      "mdate": 1719287274470,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527277"
    },
    {
      "id": "q14AbM4kdv",
      "title": "An Effective Dynamic Gradient Calibration Method for Continual Learning",
      "abstract": "Continual learning (CL) is a fundamental topic in machine learning, where the goal is to train a model with continuously incoming data and tasks. Due to the memory limit, we cannot store all the historical data, and therefore confront the ``catastrophic forgetting'' problem, i.e., the performance on the previous tasks can substantially decrease because of the missing information in the latter period. Though a number of elegant methods have been proposed, the catastrophic forgetting phenomenon still cannot be well avoided in practice. In this paper, we study the problem from the gradient perspective, where our aim is to develop an effective algorithm to calibrate the gradient in each updating step of the model; namely, our goal is to guide the model to be updated in the right direction under the situation that a large amount of historical data are unavailable. Our idea is partly inspired by the seminal stochastic variance reduction methods (e.g., SVRG and SAGA) for reducing the variance of gradient estimation in stochastic gradient descent algorithms. Another benefit is that our approach can be used as a general tool, which is able to be incorporated with several existing popular CL methods to achieve better performance. We also conduct a set of experiments on several benchmark datasets to evaluate the performance in practice.",
      "authors": [
        "Weichen Lin",
        "Jiaxiang Chen",
        "Ruomin Huang",
        "Hu Ding"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=q14AbM4kdv",
      "cdate": 1706842611378,
      "mdate": 1719287274442,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527283"
    },
    {
      "id": "9Rroj9GIOQ",
      "title": "SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models",
      "abstract": "Large Language Models (LLMs) have become pivotal in advancing the field of artificial intelligence, yet their immense sizes pose significant challenges for both fine-tuning and deployment. Current post-training pruning methods, while reducing the sizes of LLMs, often fail to maintain their original performance. To address these challenges, this paper introduces SPP, a **S**parsity-**P**reserved **P**arameter-efficient fine-tuning method. Different from existing post-training pruning approaches that struggle with performance retention, SPP proposes to employ lightweight learnable column and row matrices to optimize sparse LLM weights, *keeping the structure and sparsity of pruned pre-trained models intact*. By element-wise multiplication and residual addition, SPP ensures the consistency of model sparsity pattern and ratio during both training and weight-merging processes. We demonstrate the effectiveness of SPP by applying it to the LLaMA and LLaMA-2 model families with recent post-training pruning methods. Our results show that SPP significantly enhances the performance of models with different sparsity patterns (i.e. unstructured and N:M sparsity), especially for those with high sparsity ratios (e.g. 75%), making it a promising solution for the efficient fine-tuning of sparse LLMs. Code will be made available at https://github.com/Lucky-Lance/SPP.",
      "authors": [
        "Xudong Lu",
        "Aojun Zhou",
        "Yuhui Xu",
        "Renrui Zhang",
        "Peng Gao",
        "Hongsheng Li"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9Rroj9GIOQ",
      "cdate": 1706842432293,
      "mdate": 1719287274350,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527289"
    },
    {
      "id": "g43yUNWX4V",
      "title": "Momentum for the Win: Collaborative Federated Reinforcement Learning across Heterogeneous Environments",
      "abstract": "We explore a Federated Reinforcement Learning (FRL) problem where $N$ agents collaboratively learn a common policy without sharing their trajectory data. To date, existing FRL work has primarily focused on agents operating in the same or ``similar\" environments. In contrast, our problem setup allows for arbitrarily large levels of environment heterogeneity. To obtain the optimal policy which maximizes the average performance across all *potentially completely different* environments, we propose two algorithms: FedSVRPG-M and FedHAPG-M. In contrast to existing results, we demonstrate that both FedSVRPG-M and FedHAPG-M, both of which leverage momentum mechanisms, can exactly converge to a stationary point of the average performance function, regardless of the magnitude of environment heterogeneity. Furthermore, by incorporating the benefits of variance-reduction techniques or Hessian approximation, both algorithms achieve state-of-the-art convergence results, characterized by a sample complexity of $\\mathcal{O}\\left(\\epsilon^{-\\frac{3}{2}}/N\\right)$. Notably, our algorithms enjoy linear convergence speedups with respect to the number of agents, highlighting the benefit of collaboration among agents in finding a common policy.",
      "authors": [
        "Han Wang",
        "Sihong He",
        "Zhili Zhang",
        "Fei Miao",
        "James Anderson"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=g43yUNWX4V",
      "cdate": 1706842373047,
      "mdate": 1719287274316,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527295"
    },
    {
      "id": "FWlNA3et6X",
      "title": "To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models",
      "abstract": "LLMs have been found to memorize training textual sequences and regurgitate verbatim said sequences during text generation time. This fact is known to be the cause of privacy and related (e.g., copyright) problems. Unlearning in LLMs then takes the form of devising new algorithms that will properly deal with these side-effects of memorized data, while not hurting the model's utility. We offer a fresh perspective towards this goal, namely, that each textual sequence to be forgotten should be treated differently when being unlearned based on its degree of memorization within the LLM. We contribute a new metric for measuring unlearning quality, an adversarial attack showing that SOTA algorithms lacking this perspective fail for privacy, and two new unlearning methods based on Gradient Ascent and Task Arithmetic, respectively. A comprehensive performance evaluation across an extensive suite of NLP tasks then mapped the solution space, identifying the best solutions under different scales in model capacities and forget set sizes and quantified the gains of the new approaches.",
      "authors": [
        "George-Octavian Bărbulescu",
        "Peter Triantafillou"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FWlNA3et6X",
      "cdate": 1706842210672,
      "mdate": 1719287274259,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527300"
    },
    {
      "id": "mKYBMf1hHG",
      "title": "Rethinking Data Shapley for Data Selection Tasks: Misleads and Merits",
      "abstract": "Data Shapley provides a principled approach to data valuation and plays a crucial role in data-centric machine learning (ML) research. Data selection is considered a standard application of Data Shapley. However, its data selection performance has shown to be inconsistent across settings in the literature. This study aims to deepen our understanding of this phenomenon. We introduce a hypothesis testing framework and show that Data Shapley's performance can be no better than random selection without specific constraints on utility functions. We identify a class of utility functions, monotonically transformed modular functions, within which Data Shapley optimally selects data. Based on this insight, we propose a heuristic for predicting Data Shapley’s effectiveness in data selection tasks. Our experiments corroborate these findings, adding new insights into when Data Shapley may or may not succeed.",
      "authors": [
        "Jiachen T. Wang",
        "Tianji Yang",
        "James Zou",
        "Yongchan Kwon",
        "Ruoxi Jia"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=mKYBMf1hHG",
      "cdate": 1706842107618,
      "mdate": 1719287274213,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527306"
    },
    {
      "id": "hg4wXlrQCV",
      "title": "Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning",
      "abstract": "Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms. We identify that existing benchmarks used for research into open-ended learning fall into one of two categories. Either they are too slow for meaningful research to be performed without enormous computational resources, like Crafter, NetHack and Minecraft, or they are not complex enough to pose a significant challenge, like Minigrid and Procgen. To remedy this, we first present Craftax-Classic: a ground-up rewrite of Crafter in JAX that runs up to 250x faster than the Python-native original. A run of PPO using 1 billion environment interactions finishes in under an hour using only a single GPU and averages 90% of the optimal reward. To provide a more compelling challenge we present the main Craftax benchmark, a significant extension of the Crafter mechanics with elements inspired from NetHack. Solving Craftax requires deep exploration, long term planning and memory, as well as continual adaptation to novel situations as more of the world is discovered. We show that existing methods including global and episodic exploration, as well as unsupervised environment design fail to make material progress on the benchmark. We therefore believe that Craftax can for the first time allow researchers to experiment in a complex, open-ended environment with limited computational resources.",
      "authors": [
        "Michael Matthews",
        "Michael Beukman",
        "Benjamin Ellis",
        "Mikayel Samvelyan",
        "Matthew Thomas Jackson",
        "Samuel Coward",
        "Jakob Nicolaus Foerster"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hg4wXlrQCV",
      "cdate": 1706842053010,
      "mdate": 1719287274178,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527311"
    },
    {
      "id": "Hjwx3H6Vci",
      "title": "Distribution Alignment Optimization through Neural Collapse for Long-tailed Classification",
      "abstract": "A well-trained deep neural network on balanced datasets usually exhibits the Neural Collapse (NC) phenomenon, which is an informative indicator of the model achieving good performance. However, NC is usually hard to be achieved for a model trained on long-tailed datasets, leading to the deteriorated performance of test data. This work aims to induce the NC phenomenon in imbalanced learning from the perspective of distribution matching. By enforcing the distribution of last-layer representations to align the ideal distribution of the ETF structure, we develop a Distribution Alignment Optimization (DisA) loss, acting as a plug-and-play method can be combined with most of the existing long-tailed methods, we further instantiate it to the cases of fixing classifier and learning classifier. The extensive experiments show the effectiveness of DisA, providing a promising solution to the imbalanced issue. Our code is available at DisA.",
      "authors": [
        "Jintong Gao",
        "He Zhao",
        "Dan dan Guo",
        "Hongyuan Zha"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Hjwx3H6Vci",
      "cdate": 1706841980016,
      "mdate": 1719287274144,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527316"
    },
    {
      "id": "j6rG1ETRyu",
      "title": "Discovering Multiple Solutions from a Single Task in Offline Reinforcement Learning",
      "abstract": "Recent studies on online reinforcement learning (RL) have demonstrated the advantages of learning multiple behaviors from a single task, as in the case of few-shot adaptation to a new environment. Although this approach is expected to yield similar benefits in offline RL, appropriate methods for learning multiple solutions have not been fully investigated in previous studies. In this study, we therefore addressed the problem of finding multiple solutions from a single task in offline RL. We propose algorithms that can learn multiple solutions in offline RL, and empirically investigate their performance. Our experimental results show that the proposed algorithm learns multiple qualitatively and quantitatively distinctive solutions in offline RL.",
      "authors": [
        "Takayuki Osa",
        "Tatsuya Harada"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=j6rG1ETRyu",
      "cdate": 1706841805342,
      "mdate": 1719287274059,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527321"
    },
    {
      "id": "jvVWPtJYbc",
      "title": "Minimizing $f$-Divergences by Interpolating Velocity Fields",
      "abstract": "Many machine learning problems can be seen as approximating a *target* distribution using a *particle* distribution by minimizing their statistical discrepancy. Wasserstein Gradient Flow can move particles along a path that minimizes the $f$-divergence between the target and particle distributions. To move particles, we need to calculate the corresponding velocity fields derived from a density ratio function between these two distributions. Previous works estimated such density ratio functions and then differentiated the estimated ratios. These approaches may suffer from overfitting, leading to a less accurate estimate of the velocity fields. Inspired by non-parametric curve fitting, we directly estimate these velocity fields using interpolation techniques. We prove that our estimators are consistent under mild conditions. We validate their effectiveness using novel applications on domain adaptation and missing data imputation. The code for reproducing our results can be found at https://github.com/anewgithubname/gradest2.",
      "authors": [
        "Song Liu",
        "Jiahao Yu",
        "Jack Simons",
        "Mingxuan Yi",
        "Mark Beaumont"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jvVWPtJYbc",
      "cdate": 1706841783806,
      "mdate": 1719287274031,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527326"
    },
    {
      "id": "6XH8R7YrSk",
      "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is currently the most widely used method to align large language models (LLMs) with human preferences. Existing RLHF methods can be roughly categorized as either reward-based or reward-free. Novel applications such as ChatGPT and Claude leverage reward-based methods that first learn a reward model and apply actor-critic algorithms, such as Proximal Policy Optimization (PPO). However, in academic benchmarks, state-of-the-art results are often achieved via reward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly superior to PPO? Why does PPO perform poorly on these benchmarks? In this paper, we first conduct both theoretical and empirical studies on the algorithmic properties of DPO and show that DPO may have fundamental limitations. Moreover, we also comprehensively examine PPO and reveal the key factors for the best performances of PPO in fine-tuning LLMs. Finally, we benchmark DPO and PPO across a collection of RLHF testbeds, ranging from dialogue to code generation. Experiment results demonstrate that PPO is able to surpass other alignment methods in all cases and achieve state-of-the-art results in challenging code competitions.",
      "authors": [
        "Shusheng Xu",
        "Wei Fu",
        "Jiaxuan Gao",
        "Wenjie Ye",
        "Weilin Liu",
        "Zhiyu Mei",
        "Guangju Wang",
        "Chao Yu",
        "Yi Wu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=6XH8R7YrSk",
      "cdate": 1706841760654,
      "mdate": 1719287273993,
      "matched_keywords": [
        "reinforcement learning",
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527331"
    },
    {
      "id": "gQpBnRHwxM",
      "title": "Position: A Roadmap to Pluralistic Alignment",
      "abstract": "With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve *all*, i.e., people with diverse values and perspectives. However, aligning models to serve *pluralistic* human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using large language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) *Overton pluralistic* models that present a spectrum of reasonable responses; 2) *Steerably pluralistic* models that can steer to reflect certain perspectives; and 3) *Distributionally pluralistic* models that are well-calibrated to a given population in distribution. We also formalize and discuss three possible classes of *pluralistic benchmarks*: 1) *Multi-objective* benchmarks, 2) *Trade-off steerable* benchmarks that incentivize models to steer to arbitrary trade-offs, and 3) *Jury-pluralistic* benchmarks that explicitly model diverse human ratings. We use this framework to argue that current alignment techniques may be fundamentally limited for pluralistic AI; indeed, we highlight empirical evidence, both from our own experiments and from other work, that standard alignment procedures might *reduce* distributional pluralism in models, motivating the need for further research on pluralistic alignment.",
      "authors": [
        "Taylor Sorensen",
        "Jared Moore",
        "Jillian Fisher",
        "Mitchell L Gordon",
        "Niloofar Mireshghallah",
        "Christopher Michael Rytting",
        "Andre Ye",
        "Liwei Jiang",
        "Ximing Lu",
        "Nouha Dziri",
        "Tim Althoff",
        "Yejin Choi"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=gQpBnRHwxM",
      "cdate": 1706841282626,
      "mdate": 1719287273875,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527336"
    },
    {
      "id": "txRZBD8tBV",
      "title": "Asymmetry in Low-Rank Adapters of Foundation Models",
      "abstract": "Parameter-efficient fine-tuning optimizes large, pre-trained foundation models by updating a subset of parameters; in this class, Low-Rank Adaptation (LoRA) is particularly effective. Inspired by an effort to investigate the different roles of LoRA matrices during fine-tuning, this paper characterizes and leverages unexpected asymmetry in the importance of low-rank adapter matrices. Specifically, when updating the parameter matrices of a neural network by adding a product $BA$, we observe that the $B$ and $A$ matrices have distinct functions: $A$ extracts features from the input, while $B$ uses these features to create the desired output. Based on this observation, we demonstrate that fine-tuning $B$ is inherently more effective than fine-tuning $A$, and that a random untrained $A$ should perform nearly as well as a fine-tuned one. Using an information-theoretic lens, we also bound the generalization of low-rank adapters, showing that the parameter savings of exclusively training $B$ improves the bound. We support our conclusions with experiments on RoBERTa, BART-Large, LLaMA-2, and ViTs. The code and data is available at https://github.com/Jiacheng-Zhu-AIML/AsymmetryLoRA",
      "authors": [
        "Jiacheng Zhu",
        "Kristjan Greenewald",
        "Kimia Nadjahi",
        "Haitz Sáez de Ocáriz Borde",
        "Rickard Brüel Gabrielsson",
        "Leshem Choshen",
        "Marzyeh Ghassemi",
        "Mikhail Yurochkin",
        "Justin Solomon"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=txRZBD8tBV",
      "cdate": 1706841069019,
      "mdate": 1719287273792,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527342"
    },
    {
      "id": "64fdhmogiD",
      "title": "Learning to Stabilize Online Reinforcement Learning in Unbounded State Spaces",
      "abstract": "In many reinforcement learning (RL) applications, we want policies that reach desired states and then keep the controlled system within an acceptable region around the desired states over an indefinite period of time. This latter objective is called *stability* and is especially important when the state space is unbounded, such that the states can be arbitrarily far from each other and the agent can drift far away from the desired states. For example, in stochastic queuing networks, where queues of waiting jobs can grow without bound, the desired state is all-zero queue lengths. Here, a stable policy ensures queue lengths are finite while an optimal policy minimizes queue lengths. Since an optimal policy is also stable, one would expect that RL algorithms would implicitly give us stable policies. However, in this work, we find that deep RL algorithms that directly minimize the distance to the desired state during online training often result in unstable policies, i.e., policies that drift far away from the desired state. We attribute this instability to poor credit-assignment for destabilizing actions. We then introduce an approach based on two ideas: 1) a Lyapunov-based cost-shaping technique and 2) state transformations to the unbounded state space. We conduct an empirical study on various queueing networks and traffic signal control problems and find that our approach performs competitively against strong baselines with knowledge of the transition dynamics. Our code is available here: https://github.com/Badger-RL/STOP",
      "authors": [
        "Brahma S Pavse",
        "Matthew Zurek",
        "Yudong Chen",
        "Qiaomin Xie",
        "Josiah P. Hanna"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=64fdhmogiD",
      "cdate": 1706840959581,
      "mdate": 1719287273729,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527347"
    },
    {
      "id": "ZUXvpIrz5l",
      "title": "Safe Exploration in Dose Finding Clinical Trials with Heterogeneous Participants",
      "abstract": "In drug development, early phase dose-finding clinical trials are carried out to identify an optimal dose to administer to patients in larger confirmatory clinical trials. Standard trial procedures do not optimize for participant benefit and do not consider participant heterogeneity, despite consequences to participants' health and downstream impacts to under-represented population subgroups. Many novel drugs also do not obey parametric modelling assumptions made in common dose-finding procedures. We present Safe Allocation for Exploration of Treatments SAFE-T, a procedure for adaptive dose-finding that adheres to safety constraints, improves utility for heterogeneous participants, and works well with small sample sizes. SAFE-T flexibly learns non-parametric multi-output Gaussian process models for dose toxicity and efficacy, using Bayesian optimization, and provides accurate final dose recommendations. We provide theoretical guarantees for the satisfaction of safety constraints. Using a comprehensive set of realistic synthetic scenarios, we demonstrate empirically that SAFE-T generally outperforms comparable methods and maintains performance across variations in sample size and subgroup distribution. Finally, we extend SAFE-T to a new adaptive setting, demonstrating its potential to improve traditional clinical trial procedures.",
      "authors": [
        "Isabel Chien",
        "Wessel P Bruinsma",
        "Javier Gonzalez",
        "Richard E. Turner"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ZUXvpIrz5l",
      "cdate": 1706840872266,
      "mdate": 1719287273651,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527353"
    },
    {
      "id": "ZQcqXCuoxD",
      "title": "Cooperative Graph Neural Networks",
      "abstract": "Graph neural networks are popular architectures for graph machine learning, based on iterative computation of node representations of an input graph through a series of invariant transformations. A large class of graph neural networks follow a standard message-passing paradigm: at every layer, each node state is updated based on an aggregate of messages from its neighborhood. In this work, we propose a novel framework for training graph neural networks, where every node is viewed as a player that can choose to either `listen`, `broadcast`, `listen and broadcast`, or to `isolate`. The standard message propagation scheme can then be viewed as a special case of this framework where every node `listens and broadcasts` to all neighbors. Our approach offers a more flexible and dynamic message-passing paradigm, where each node can determine its own strategy based on their state, effectively exploring the graph topology while learning. We provide a theoretical analysis of the new message-passing scheme which is further supported by an extensive empirical analysis on a synthetic and real-world datasets.",
      "authors": [
        "Ben Finkelshtein",
        "Xingyue Huang",
        "Michael M. Bronstein",
        "Ismail Ilkan Ceylan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ZQcqXCuoxD",
      "cdate": 1706840671938,
      "mdate": 1719287273617,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527359"
    },
    {
      "id": "cc72Vnfvoc",
      "title": "Trained Random Forests Completely Reveal your Dataset",
      "abstract": "We introduce an optimization-based reconstruction attack capable of completely or near-completely reconstructing a dataset utilized for training a random forest. Notably, our approach relies solely on information readily available in commonly used libraries such as scikit-learn. To achieve this, we formulate the reconstruction problem as a combinatorial problem under a maximum likelihood objective. We demonstrate that this problem is NP-hard, though solvable at scale using constraint programming - an approach rooted in constraint propagation and solution-domain reduction. Through an extensive computational investigation, we demonstrate that random forests trained without bootstrap aggregation but with feature randomization are susceptible to a complete reconstruction. This holds true even with a small number of trees. Even with bootstrap aggregation, the majority of the data can also be reconstructed. These findings underscore a critical vulnerability inherent in widely adopted ensemble methods, warranting attention and mitigation. Although the potential for such reconstruction attacks has been discussed in privacy research, our study provides clear empirical evidence of their practicability.",
      "authors": [
        "Julien Ferry",
        "Ricardo Fukasawa",
        "Timothée Pascal",
        "Thibaut Vidal"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=cc72Vnfvoc",
      "cdate": 1706840649876,
      "mdate": 1719287273569,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527364"
    },
    {
      "id": "AZ1tWCa9j3",
      "title": "Robustly Learning Single-Index Models via Alignment Sharpness",
      "abstract": "We study the problem of learning Single-Index Models under the $L_2^2$ loss in the agnostic model. We give an efficient learning algorithm, achieving a constant factor approximation to the optimal loss, that succeeds under a range of distributions (including log-concave distributions) and a broad class of monotone and Lipschitz link functions. This is the first efficient constant factor approximate agnostic learner, even for Gaussian data and for any nontrivial class of link functions. Prior work for the case of unknown link function either works in the realizable setting or does not attain constant factor approximation. The main technical ingredient enabling our algorithm and analysis is a novel notion of a local error bound in optimization that we term *alignment sharpness* and that may be of broader interest.",
      "authors": [
        "Nikos Zarifis",
        "Puqian Wang",
        "Ilias Diakonikolas",
        "Jelena Diakonikolas"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=AZ1tWCa9j3",
      "cdate": 1706840579268,
      "mdate": 1719287273474,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527369"
    },
    {
      "id": "w8BnKGFIYN",
      "title": "Learning to Play Atari in a World of Tokens",
      "abstract": "Model-based reinforcement learning agents utilizing transformers have shown improved sample efficiency due to their ability to model extended context, resulting in more accurate world models. However, for complex reasoning and planning tasks, these methods primarily rely on continuous representations. This complicates modeling of discrete properties of the real world such as disjoint object classes between which interpolation is not plausible. In this work, we introduce discrete abstract representations for transformer-based learning (DART), a sample-efficient method utilizing discrete representations for modeling both the world and learning behavior. We incorporate a transformer-decoder for auto-regressive world modeling and a transformer-encoder for learning behavior by attending to task-relevant cues in the discrete representation of the world model. For handling partial observability, we aggregate information from past time steps as memory tokens. DART outperforms previous state-of-the-art methods that do not use look-ahead search on the Atari 100k sample efficiency benchmark with a median human-normalized score of 0.790 and beats humans in 9 out of 26 games. We release our code at https://pranaval.github.io/DART/.",
      "authors": [
        "Pranav Agarwal",
        "Sheldon Andrews",
        "Samira Ebrahimi Kahou"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=w8BnKGFIYN",
      "cdate": 1706840529985,
      "mdate": 1719287273437,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527375"
    },
    {
      "id": "5x788rqbcj",
      "title": "Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",
      "abstract": "Large language models (LLMs) can store a vast amount of world knowledge, often extractable via question-answering (e.g., \"What is Abraham Lincoln's birthday?''). However, do they answer such questions based on exposure to similar questions during training (i.e., cheating), or by genuinely learning to extract knowledge from sources like Wikipedia? In this paper, we investigate this issue using a controlled biography dataset. We find a strong correlation between the model's ability to extract knowledge and various _diversity measures_ of the training data. **Essentially**, for knowledge to be reliably extracted, it must be sufficiently augmented (e.g., through paraphrasing, sentence shuffling) _during pretraining_. Without such augmentation, knowledge may be memorized but not extractable, leading to 0% accuracy, regardless of subsequent instruction fine-tuning. To understand why this occurs, we employ (nearly) linear probing to demonstrate a strong connection between the observed correlation and _how the model internally encodes knowledge_ --- whether it is linearly encoded in the hidden embeddings of entity names or distributed across other token embeddings in the training text. **This paper provides several key recommendations for LLM pretraining in the industry:** (1) rewrite the pretraining data --- using small, auxiliary models --- to provide knowledge augmentation, and (2) incorporate more instruction-finetuning data into the pretraining stage before it becomes too late.",
      "authors": [
        "Zeyuan Allen-Zhu",
        "Yuanzhi Li"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5x788rqbcj",
      "cdate": 1706840454754,
      "mdate": 1719287273384,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527380"
    },
    {
      "id": "xnQ1qoly7Q",
      "title": "RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis",
      "abstract": "Robotic behavior synthesis, the problem of understanding multimodal inputs and generating precise physical control for robots, is an important part of Embodied AI. Despite successes in applying multimodal large language models for high-level understanding, it remains challenging to translate these conceptual understandings into detailed robotic actions while achieving generalization across various scenarios. In this paper, we propose a tree-structured multimodal code generation framework for generalized robotic behavior synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions into multiple object-centric manipulation units consisting of physical preferences such as affordance and safety constraints, and applies code generation to introduce generalization ability across various robotics platforms. To further enhance the capability to map conceptual and perceptual understanding into control commands, a specialized multimodal reasoning dataset is collected for pre-training and an iterative self-updating methodology is introduced for supervised fine-tuning. Extensive experiments demonstrate that RoboCodeX achieves state-of-the-art performance in both simulators and real robots on four different kinds of manipulation tasks and one embodied navigation task.",
      "authors": [
        "Yao Mu",
        "Junting Chen",
        "Qing-Long Zhang",
        "Shoufa Chen",
        "Qiaojun Yu",
        "Chongjian GE",
        "Runjian Chen",
        "Zhixuan Liang",
        "Mengkang Hu",
        "Chaofan Tao",
        "Peize Sun",
        "Haibao Yu",
        "Chao Yang",
        "Wenqi Shao",
        "Wenhai Wang",
        "Jifeng Dai",
        "Yu Qiao",
        "Mingyu Ding",
        "Ping Luo"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xnQ1qoly7Q",
      "cdate": 1706840133864,
      "mdate": 1719287273304,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527385"
    },
    {
      "id": "s1sdx6vNsU",
      "title": "LoRA Training in the NTK Regime has No Spurious Local Minima",
      "abstract": "Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank $r\\lesssim \\sqrt{N}$; (ii) using LoRA with rank $r\\gtrsim \\sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well.",
      "authors": [
        "Uijeong Jang",
        "Jason D. Lee",
        "Ernest K. Ryu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=s1sdx6vNsU",
      "cdate": 1706839782061,
      "mdate": 1719287273182,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527390"
    },
    {
      "id": "WJ5fJhwvCl",
      "title": "Breaking the Barrier: Enhanced Utility and Robustness in Smoothed DRL Agents",
      "abstract": "Robustness remains a paramount concern in deep reinforcement learning (DRL), with randomized smoothing emerging as a key technique for enhancing this attribute. However, a notable gap exists in the performance of current smoothed DRL agents, often characterized by significantly low clean rewards and weak robustness. In response to this challenge, our study introduces innovative algorithms aimed at training effective smoothed robust DRL agents. We propose S-DQN and S-PPO, novel approaches that demonstrate remarkable improvements in clean rewards, empirical robustness, and robustness guarantee across standard RL benchmarks. Notably, our S-DQN and S-PPO agents not only significantly outperform existing smoothed agents by an average factor of $2.16\\times$ under the strongest attack, but also surpass previous robustly-trained agents by an average factor of $2.13\\times$. This represents a significant leap forward in the field. Furthermore, we introduce Smoothed Attack, which is $1.89\\times$ more effective in decreasing the rewards of smoothed agents than existing adversarial attacks. Our code is available at: [https://github.com/Trustworthy-ML-Lab/Robust_HighUtil_Smoothed_DRL](https://github.com/Trustworthy-ML-Lab/Robust_HighUtil_Smoothed_DRL)",
      "authors": [
        "Chung-En Sun",
        "Sicun Gao",
        "Tsui-Wei Weng"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=WJ5fJhwvCl",
      "cdate": 1706839706113,
      "mdate": 1719287273074,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527396"
    },
    {
      "id": "glfcwSsks8",
      "title": "Characterizing Large Language Model Geometry Helps Solve Toxicity Detection and Generation",
      "abstract": "Large Language Models (LLMs) drive current AI breakthroughs despite very little being known about their internal representations. In this work, we propose to shed the light on LLMs inner mechanisms through the lens of geometry. In particular, we develop in closed form $(i)$ the intrinsic dimension in which the Multi-Head Attention embeddings are constrained to exist and $(ii)$ the partition and per-region affine mappings of the feedforward (MLP) network of LLMs' layers. Our theoretical findings further enable the design of novel principled solutions applicable to state-of-the-art LLMs. First, we show that, through our geometric understanding, we can bypass LLMs' RLHF protection by controlling the embedding's intrinsic dimension through informed prompt manipulation. Second, we derive interpretable geometrical features that can be extracted from any (pre-trained) LLM, providing a rich abstract representation of their inputs. We observe that these features are sufficient to help solve toxicity detection, and even allow the identification of various types of toxicity. Our results demonstrate how, even in large-scale regimes, exact theoretical results can answer practical questions in LLMs. Code: https://github.com/RandallBalestriero/SplineLLM",
      "authors": [
        "Randall Balestriero",
        "Romain Cosentino",
        "Sarath Shekkizhar"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=glfcwSsks8",
      "cdate": 1706839547291,
      "mdate": 1719287273042,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527401"
    },
    {
      "id": "nACGn4US1R",
      "title": "Position: Enforced Amnesia as a Way to Mitigate the Potential Risk of Silent Suffering in the Conscious AI",
      "abstract": "Science fiction has explored the possibility of a conscious self-aware mind being locked in silent suffering for prolonged periods of time. Unfortunately, we still do not have a reliable test for the presence of consciousness in information processing systems. Even in case of humans, our confidence in the presence of consciousness in specific individuals is based mainly on their self-reports and our own subjective experiences and the expectation other beings like us should share them. Considering our limited understanding of consciousness and some academic theories suggesting consciousness may be an emergent correlate of any complex-enough information processing, it is not impossible that an artificial intelligence (AI) system, such as a large language model (LLM), may be undergoing some, perhaps rudimentary, conscious experience. Given the tedious tasks often assigned to AI, such conscious experience may be highly unpleasant. Such unobserved suffering of a conscious being would be viewed as morally wrong by at least some ethicists - even if it has no practical effects on human users of AI. This paper proposes a method to mitigate the risk of an AI suffering in silence without needing to confirm if the AI is actually conscious. Our core postulate is that in all known real-world information processing systems, for a past experience to affect an agent in the present, that experience has to be mediated by the agent's memory. Therefore, preventing access to memory store, or regularly resetting it, could reduce the suffering due to past memories and interrupt the maintenance of a continuous suffering-prone self-identity in these hypothetically conscious AI systems.",
      "authors": [
        "Yegor Tkachenko"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nACGn4US1R",
      "cdate": 1706839350212,
      "mdate": 1719287272957,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527407"
    },
    {
      "id": "4dOJAfXhNV",
      "title": "SAPG: Split and Aggregate Policy Gradients",
      "abstract": "Despite extreme sample inefficiency, on-policy reinforcement learning, aka policy gradients, has become a fundamental tool in decision-making problems. With the recent advances in GPU-driven simulation, the ability to collect large amounts of data for RL training has scaled exponentially. However, we show that current RL methods, e.g. PPO, fail to ingest the benefit of parallelized environments beyond a certain point and their performance saturates. To address this, we propose a new on-policy RL algorithm that can effectively leverage large-scale environments by splitting them into chunks and fusing them back together via importance sampling. Our algorithm, termed SAPG, shows significantly higher performance across a variety of challenging environments where vanilla PPO and other strong baselines fail to achieve high performance. Webpage at https://sapg-rl.github.io/.",
      "authors": [
        "Jayesh Singla",
        "Ananye Agarwal",
        "Deepak Pathak"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4dOJAfXhNV",
      "cdate": 1706839305814,
      "mdate": 1719287272893,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527412"
    },
    {
      "id": "OTmcsyEO5G",
      "title": "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts",
      "abstract": "Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called *gist memories*, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum. ReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3.5-20x.",
      "authors": [
        "Kuang-Huei Lee",
        "Xinyun Chen",
        "Hiroki Furuta",
        "John Canny",
        "Ian Fischer"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OTmcsyEO5G",
      "cdate": 1706839278340,
      "mdate": 1719287272791,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527418"
    },
    {
      "id": "mUSPhG4uDW",
      "title": "WebLINX: Real-World Website Navigation with Multi-Turn Dialogue",
      "abstract": "We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We find that smaller finetuned decoders surpass the best zero-shot LLMs (including GPT-4V), but also larger finetuned multimodal models which were explicitly pretrained on screenshots. However, all finetuned models struggle to generalize to unseen websites. Our findings highlight the need for large multimodal models that can generalize to novel settings.",
      "authors": [
        "Xing Han Lu",
        "Zdeněk Kasner",
        "Siva Reddy"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=mUSPhG4uDW",
      "cdate": 1706838588605,
      "mdate": 1719287272621,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527424"
    },
    {
      "id": "3hSTecKy1b",
      "title": "Position: Data Authenticity, Consent, & Provenance for AI are all broken: what will it take to fix them?",
      "abstract": "New capabilities in foundation models are owed in large part to massive, widely-sourced, and under-documented training data collections. Existing practices in data collection have led to challenges in tracing authenticity, verifying consent, preserving privacy, addressing representation and bias, respecting copyright, and overall developing ethical and trustworthy foundation models. In response, regulation is emphasizing the need for training data transparency to understand foundation models’ limitations. Based on a large-scale analysis of the foundation model training data landscape and existing solutions, we identify the missing infrastructure to facilitate responsible foundation model development practices. We examine the current shortcomings of common tools for tracing data authenticity, consent, and documentation, and outline how policymakers, developers, and data creators can facilitate responsible foundation model development by adopting universal data provenance standards.",
      "authors": [
        "Shayne Longpre",
        "Robert Mahari",
        "Naana Obeng-Marnu",
        "William Brannon",
        "Tobin South",
        "Katy Ilonka Gero",
        "Alex Pentland",
        "Jad Kabbara"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3hSTecKy1b",
      "cdate": 1706838553289,
      "mdate": 1719287272597,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527428"
    },
    {
      "id": "O6tenHWTUU",
      "title": "Provable Representation with Efficient Planning for Partially Observable Reinforcement Learning",
      "abstract": "In most real-world reinforcement learning applications, state information is only partially observable, which breaks the Markov decision process assumption and leads to inferior performance for algorithms that conflate observations with state. Partially Observable Markov Decision Processes (POMDPs), on the other hand, provide a general framework that allows for partial observability to be accounted for in *learning, exploration and planning*, but presents significant computational and statistical challenges. To address these difficulties, we develop a representation-based perspective that leads to a coherent framework and tractable algorithmic approach for practical reinforcement learning from partial observations. We provide a theoretical analysis for justifying the statistical efficiency of the proposed algorithm, and also empirically demonstrate the proposed algorithm can surpass state-of-the-art performance with partial observations across various benchmarks, advancing reliable reinforcement learning towards more practical applications.",
      "authors": [
        "Hongming Zhang",
        "Tongzheng Ren",
        "Chenjun Xiao",
        "Dale Schuurmans",
        "Bo Dai"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=O6tenHWTUU",
      "cdate": 1706838435670,
      "mdate": 1719287272522,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527434"
    },
    {
      "id": "Zo9zXdVhW2",
      "title": "Probabilistic Constrained Reinforcement Learning with Formal Interpretability",
      "abstract": "Reinforcement learning can provide effective reasoning for sequential decision-making problems with variable dynamics. Such reasoning in practical implementation, however, poses a persistent challenge in interpreting the reward function and the corresponding optimal policy. Consequently, representing sequential decision-making problems as probabilistic inference can have considerable value, as, in principle, the inference offers diverse and powerful mathematical tools to infer the stochastic dynamics whilst suggesting a probabilistic interpretation of policy optimization. In this study, we propose a novel Adaptive Wasserstein Variational Optimization, namely AWaVO, to tackle these interpretability challenges. Our approach uses formal methods to achieve the interpretability: convergence guarantee, training transparency, and intrinsic decision-interpretation. To demonstrate its practicality, we showcase guaranteed interpretability including a global convergence rate $\\Theta(1/\\sqrt{T})$ not only in simulation but also in real-world quadrotor tasks. In comparison with state-of-the-art benchmarks, including TRPO-IPO, PCPO, and CRPO, we empirically verify that AWaVO offers a reasonable trade-off between high performance and sufficient interpretability.",
      "authors": [
        "YANRAN WANG",
        "QIUCHEN QIAN",
        "David Boyle"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Zo9zXdVhW2",
      "cdate": 1706838120026,
      "mdate": 1719287272489,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527440"
    },
    {
      "id": "EK7fuAMNoI",
      "title": "Accelerated Algorithms for Constrained Nonconvex-Nonconcave Min-Max Optimization and Comonotone Inclusion",
      "abstract": "We study constrained comonotone min-max optimization, a structured class of nonconvex-nonconcave min-max optimization problems, and their generalization to comonotone inclusion. In our first contribution, we extend the *Extra Anchored Gradient (EAG)* algorithm, originally proposed by Yoon and Ryu (2021) for unconstrained min-max optimization, to constrained comonotone min-max optimization and comonotone inclusion, achieving an optimal convergence rate of $O\\left(\\frac{1}{T}\\right)$ among all first-order methods. Additionally, we prove that the algorithm's iterations converge to a point in the solution set. In our second contribution, we extend the *Fast Extra Gradient (FEG)* algorithm, as developed by Lee and Kim (2021), to constrained comonotone min-max optimization and comonotone inclusion, achieving the same $O\\left(\\frac{1}{T}\\right)$ convergence rate. This rate is applicable to the broadest set of comonotone inclusion problems yet studied in the literature. Our analyses are based on simple potential function arguments, which might be useful for analyzing other accelerated algorithms.",
      "authors": [
        "Yang Cai",
        "Argyris Oikonomou",
        "Weiqiang Zheng"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EK7fuAMNoI",
      "cdate": 1706837308060,
      "mdate": 1719287272142,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527446"
    },
    {
      "id": "YNvGFaOG1p",
      "title": "Non-Asymptotic Analysis for Single-Loop (Natural) Actor-Critic with Compatible Function Approximation",
      "abstract": "Actor-critic (AC) is a powerful method for learning an optimal policy in reinforcement learning, where the critic uses algorithms, e.g., temporal difference (TD) learning with function approximation, to evaluate the current policy and the actor updates the policy along an approximate gradient direction using information from the critic. This paper provides the *tightest* non-asymptotic convergence bounds for both the AC and natural AC (NAC) algorithms. Specifically, existing studies show that AC converges to an $\\epsilon+\\varepsilon_{\\text{critic}}$ neighborhood of stationary points with the best known sample complexity of $\\mathcal{O}(\\epsilon^{-2})$ (up to a log factor), and NAC converges to an $\\epsilon+\\varepsilon_{\\text{critic}}+\\sqrt{\\varepsilon_{\\text{actor}}}$ neighborhood of the global optimum with the best known sample complexity of $\\mathcal{O}(\\epsilon^{-3})$, where $\\varepsilon_{\\text{critic}}$ is the approximation error of the critic and $\\varepsilon_{\\text{actor}}$ is the approximation error induced by the insufficient expressive power of the parameterized policy class. This paper analyzes the convergence of both AC and NAC algorithms with compatible function approximation. Our analysis eliminates the term $\\varepsilon_{\\text{critic}}$ from the error bounds while still achieving the best known sample complexities. Moreover, we focus on the challenging single-loop setting with a single Markovian sample trajectory. Our major technical novelty lies in analyzing the stochastic bias due to policy-dependent and time-varying compatible function approximation in the critic, and handling the non-ergodicity of the MDP due to the single Markovian sample trajectory. Numerical results are also provided in the appendix.",
      "authors": [
        "Yudan Wang",
        "Yue Wang",
        "Yi Zhou",
        "Shaofeng Zou"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=YNvGFaOG1p",
      "cdate": 1706837247549,
      "mdate": 1719287272185,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527452"
    },
    {
      "id": "LabSWooau0",
      "title": "Enabling Few-Shot Learning with PID Control: A Layer Adaptive Optimizer",
      "abstract": "Model-Agnostic Meta-Learning (MAML) and its variants have shown remarkable performance in scenarios characterized by a scarcity of labeled data during the training phase of machine learning models. Despite these successes, MAMLbased approaches encounter significant challenges when there is a substantial discrepancy in the distribution of training and testing tasks, resulting in inefficient learning and limited generalization across domains. Inspired by classical proportional-integral-derivative (PID) control theory, this study introduces a Layer-Adaptive PID (LA-PID) Optimizer, a MAML-based optimizer that employs efficient parameter optimization methods to dynamically adjust task-specific PID control gains at each layer of the network, conducting a first-principles analysis of optimal convergence conditions. A series of experiments conducted on four standard benchmark datasets demonstrate the efficacy of the LA-PID optimizer, indicating that LA-PID achieves state-oftheart performance in few-shot classification and cross-domain tasks, accomplishing these objectives with fewer training steps. Code is available on https://github.com/yuguopin/LA-PID.",
      "authors": [
        "Le Yu",
        "Xinde Li",
        "Pengfei Zhang",
        "zhentong zhang",
        "Fir Dunkin"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LabSWooau0",
      "cdate": 1706836451842,
      "mdate": 1719287271975,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527458"
    },
    {
      "id": "7rfZ6bMZq4",
      "title": "DOGE: Domain Reweighting with Generalization Estimation",
      "abstract": "The coverage and composition of the pretraining data significantly impacts the generalization ability of Large Language Models (LLMs). Despite its importance, recent LLMs still rely on heuristics and trial and error to increase or reduce the influence of data-domains. We propose DOmain reweighting with Generalization Estimation (DoGE), which optimizes the probability of sampling from each domain (domain weights) in a principled way. Our approach is a two stage process consisting (i) training a proxy model to obtain domain weights using a bi-level optimization algorithm; (ii) training a larger base model by sampling training domains according to the learnt domain weights. In our experiments, we extensively show how DoGE improves the generalization of the base model to any target data mixture. On the SlimPajama dataset, our base model gets a better perplexity and few-shot reasoning accuracies across 6 tasks compared to baseline methods. Moreover, aiming to generalize to out-of-domain target tasks, which is unseen in the pretraining corpus (OOD domain), DoGE can effectively identify inter-domain dependencies, consistently achieves better test perplexity on the target domain.",
      "authors": [
        "Simin Fan",
        "Matteo Pagliardini",
        "Martin Jaggi"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7rfZ6bMZq4",
      "cdate": 1706836376215,
      "mdate": 1719287271948,
      "matched_keywords": [
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527464"
    },
    {
      "id": "ApRKrKZJSk",
      "title": "Weisfeiler Leman for Euclidean Equivariant Machine Learning",
      "abstract": "The $k$-Weisfeiler-Leman ($k$-WL) graph isomorphism test hierarchy is a common method for assessing the expressive power of graph neural networks (GNNs). Recently, GNNs whose expressive power is equivalent to the $2$-WL test were proven to be universal on weighted graphs which encode $3\\mathrm{D}$ point cloud data, yet this result is limited to invariant continuous functions on point clouds. In this paper, we extend this result in three ways: Firstly, we show that PPGN can simulate $2$-WL uniformly on all point clouds with low complexity. Secondly, we show that $2$-WL tests can be extended to point clouds which include both positions and velocities, a scenario often encountered in applications. Finally, we provide a general framework for proving equivariant universality and leverage it to prove that a simple modification of this invariant PPGN architecture can be used to obtain a universal equivariant architecture that can approximate all continuous equivariant functions uniformly. Building on our results, we develop our WeLNet architecture, which sets new state-of-the-art results on the N-Body dynamics task and the GEOM-QM9 molecular conformation generation task.",
      "authors": [
        "Snir Hordan",
        "Tal Amir",
        "Nadav Dym"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ApRKrKZJSk",
      "cdate": 1706836226502,
      "mdate": 1719287271877,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527470"
    },
    {
      "id": "IUBhvyJ9Sr",
      "title": "Hieros: Hierarchical Imagination on Structured State Space Sequence World Models",
      "abstract": "One of the biggest challenges to modern deep reinforcement learning (DRL) algorithms is sample efficiency. Many approaches learn a world model in order to train an agent entirely in imagination, eliminating the need for direct environment interaction during training. However, these methods often suffer from either a lack of imagination accuracy, exploration capabilities, or runtime efficiency. We propose HIEROS, a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space. HIEROS uses an S5 layer-based world model, which predicts next world states in parallel during training and iteratively during environment interaction. Due to the special properties of S5 layers, our method can train in parallel and predict next world states iteratively during imagination. This allows for more efficient training than RNN-based world models and more efficient imagination than Transformer-based world models. We show that our approach outperforms the state of the art in terms of mean and median normalized human score on the Atari 100k benchmark, and that our proposed world model is able to predict complex dynamics very accurately. We also show that HIEROS displays superior exploration capabilities compared to existing approaches.",
      "authors": [
        "Paul Mattes",
        "Rainer Schlosser",
        "Ralf Herbrich"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=IUBhvyJ9Sr",
      "cdate": 1706836137884,
      "mdate": 1719287271714,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527475"
    },
    {
      "id": "k10805cgak",
      "title": "Learning to Remove Cuts in Integer Linear Programming",
      "abstract": "Cutting plane methods are a fundamental approach for solving integer linear programs (ILPs). In each iteration of such methods, additional linear constraints (cuts) are introduced to the constraint set with the aim of excluding the previous fractional optimal solution while not affecting the optimal integer solution. In this work, we explore a novel approach within cutting plane methods: instead of only adding new cuts, we also consider the removal of previous cuts introduced at any of the preceding iterations of the method under a learnable parametric criteria. We demonstrate that in fundamental combinatorial optimization settings such cut removal policies can lead to significant improvements over both human-based and machine learning-guided cut addition policies even when implemented with simple models.",
      "authors": [
        "Pol Puigdemont",
        "Stratis Skoulakis",
        "Grigorios Chrysos",
        "Volkan Cevher"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=k10805cgak",
      "cdate": 1706835838974,
      "mdate": 1719287271612,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527481"
    },
    {
      "id": "ZvJ2lQQKjz",
      "title": "Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement",
      "abstract": "Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for detecting cardiac arrhythmic diseases in clinical practice. While ECG Self-supervised Learning (eSSL) methods show promise in representation learning from unannotated ECG data, they often overlook the clinical knowledge that can be found in reports. This oversight and the requirement for annotated samples for downstream tasks limit eSSL's versatility. In this work, we address these issues with the **M**ultimodal **E**CG **R**epresentation **L**earning (**MERL**) framework. Through multimodal learning on ECG records and associated reports, MERL is capable of performing zero-shot ECG classification with text prompts, eliminating the need for training data in downstream tasks. At test time, we propose the **C**linical **K**nowledge **E**nhanced **P**rompt **E**ngineering (**CKEPE**) approach, which uses Large Language Models (LLMs) to exploit external expert-verified clinical knowledge databases, generating more descriptive prompts and reducing hallucinations in LLM-generated content to boost zero-shot classification. Based on MERL, we perform the first benchmark across six public ECG datasets, showing the superior performance of MERL compared against eSSL methods. Notably, MERL achieves an average AUC score of 75.2% in zero-shot classification (**without training data**), 3.2% higher than linear probed eSSL methods with 10% annotated training data, averaged across all six datasets.",
      "authors": [
        "Che Liu",
        "Zhongwei Wan",
        "Cheng Ouyang",
        "Anand Shah",
        "Wenjia Bai",
        "Rossella Arcucci"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ZvJ2lQQKjz",
      "cdate": 1706835818419,
      "mdate": 1719287271579,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527486"
    },
    {
      "id": "n9pru4bJU9",
      "title": "Scaling Down Deep Learning with MNIST-1D",
      "abstract": "Although deep learning models have taken on commercial and political relevance, key aspects of their training and operation remain poorly understood. This has sparked interest in science of deep learning projects, many of which require large amounts of time, money, and electricity. But how much of this research really needs to occur at scale? In this paper, we introduce MNIST-1D: a minimalist, procedurally generated, low-memory, and low-compute alternative to classic deep learning benchmarks. Although the dimensionality of MNIST-1D is only 40 and its default training set size only 4000, MNIST-1D can be used to study inductive biases of different deep architectures, find lottery tickets, observe deep double descent, metalearn an activation function, and demonstrate guillotine regularization in self-supervised learning. All these experiments can be conducted on a GPU or often even on a CPU within minutes, allowing for fast prototyping, educational use cases, and cutting-edge research on a low budget.",
      "authors": [
        "Samuel James Greydanus",
        "Dmitry Kobak"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=n9pru4bJU9",
      "cdate": 1706835241846,
      "mdate": 1719287271485,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527491"
    },
    {
      "id": "rK6AZem0hX",
      "title": "Transport of Algebraic Structure to Latent Embeddings",
      "abstract": "Machine learning often aims to produce latent embeddings of inputs which lie in a larger, abstract mathematical space. For example, in the field of 3D modeling, subsets of Euclidean space can be embedded as vectors using implicit neural representations. Such subsets also have a natural algebraic structure including operations (e.g., union) and corresponding laws (e.g., associativity). How can we learn to \"union\" two sets using only their latent embeddings while respecting associativity? We propose a general procedure for parameterizing latent space operations that are provably consistent with the laws on the input space. This is achieved by learning a bijection from the latent space to a carefully designed *mirrored algebra* which is constructed on Euclidean space in accordance with desired laws. We evaluate these *structural transport nets* for a range of mirrored algebras against baselines that operate directly on the latent space. Our experiments provide strong evidence that respecting the underlying algebraic structure of the input space is key for learning accurate and self-consistent operations.",
      "authors": [
        "Samuel Pfrommer",
        "Brendon G. Anderson",
        "Somayeh Sojoudi"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rK6AZem0hX",
      "cdate": 1706835041667,
      "mdate": 1719287271405,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527496"
    },
    {
      "id": "99jx5U81jx",
      "title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations",
      "abstract": "Large language models (LLMs) are trained to imitate humans to explain human decisions. However, do LLMs explain themselves? Can they help humans build mental models of how LLMs process different inputs? To answer these questions, we propose to evaluate $\\textbf{counterfactual simulatability}$ of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. For example, if a model answers ''$\\textit{yes}$'' to the input question ''$\\textit{Can eagles fly?}$'' with the explanation ''$\\textit{all birds can fly}$'', then humans would infer from the explanation that it would also answer ''$\\textit{yes}$'' to the counterfactual input ''$\\textit{Can penguins fly?}$''. If the explanation is precise, then the model's answer should match humans' expectations. We implemented two metrics based on counterfactual simulatability: precision and generality. We generated diverse counterfactuals automatically using LLMs. We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on two tasks: multi-hop factual reasoning and reward modeling. We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may be insufficient.",
      "authors": [
        "Yanda Chen",
        "Ruiqi Zhong",
        "Narutatsu Ri",
        "Chen Zhao",
        "He He",
        "Jacob Steinhardt",
        "Zhou Yu",
        "Kathleen McKeown"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=99jx5U81jx",
      "cdate": 1706834992082,
      "mdate": 1719287271397,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527502"
    },
    {
      "id": "ud4GSrqUKI",
      "title": "Distinguishing the Knowable from the Unknowable with Language Models",
      "abstract": "We study the feasibility of identifying *epistemic* uncertainty (reflecting a lack of knowledge), as opposed to *aleatoric* uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more informative indicators of model confidence in diverse practical settings. Code can be found at: https://github.com/KempnerInstitute/llm_uncertainty",
      "authors": [
        "Gustaf Ahdritz",
        "Tian Qin",
        "Nikhil Vyas",
        "Boaz Barak",
        "Benjamin L. Edelman"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ud4GSrqUKI",
      "cdate": 1706834835929,
      "mdate": 1719287271231,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527508"
    },
    {
      "id": "EIcxV7T0Sy",
      "title": "Position: Categorical Deep Learning is an Algebraic Theory of All Architectures",
      "abstract": "We present our position on the elusive quest for a general-purpose framework for specifying and studying deep learning architectures. Our opinion is that the key attempts made so far lack a coherent bridge between specifying constraints which models must satisfy and specifying their implementations. Focusing on building a such a bridge, we propose to apply category theory---precisely, the universal algebra of monads valued in a 2-category of parametric maps---as a single theory elegantly subsuming both of these flavours of neural network design. To defend our position, we show how this theory recovers constraints induced by geometric deep learning, as well as implementations of many architectures drawn from the diverse landscape of neural networks, such as RNNs. We also illustrate how the theory naturally encodes many standard constructs in computer science and automata theory.",
      "authors": [
        "Bruno Gavranović",
        "Paul Lessard",
        "Andrew Joseph Dudzik",
        "Tamara von Glehn",
        "João Guilherme Madeira Araújo",
        "Petar Veličković"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EIcxV7T0Sy",
      "cdate": 1706834057963,
      "mdate": 1719287271063,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527514"
    },
    {
      "id": "tRESfzWFtf",
      "title": "Barrier Algorithms for Constrained Non-Convex Optimization",
      "abstract": "In this paper we theoretically show that interior-point methods based on self-concordant barriers possess favorable global complexity beyond their standard application area of convex optimization. To do that we propose first- and second-order methods for non-convex optimization problems with general convex set constraints and linear constraints. Our methods attain a suitably defined class of approximate first- or second-order KKT points with the worst-case iteration complexity similar to unconstrained problems, namely $O(\\varepsilon^{-2})$ (first-order) and $O(\\varepsilon^{-3/2})$ (second-order), respectively.",
      "authors": [
        "Pavel Dvurechensky",
        "Mathias Staudigl"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tRESfzWFtf",
      "cdate": 1706833984290,
      "mdate": 1719287271034,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527520"
    },
    {
      "id": "Y9qzwNlKVU",
      "title": "Random Latent Exploration for Deep Reinforcement Learning",
      "abstract": "The ability to efficiently explore high-dimensional state spaces is essential for the practical success of deep Reinforcement Learning (RL). This paper introduces a new exploration technique called Random Latent Exploration (RLE), that combines the strengths of exploration bonuses and randomized value functions (two popular approaches for effective exploration in deep RL). RLE leverages the idea of perturbing rewards by adding structured random rewards to the original task rewards in certain (random) states of the environment, to encourage the agent to explore the environment during training. RLE is straightforward to implement and performs well in practice. To demonstrate the practical effectiveness of RLE, we evaluate it on the challenging Atari and IsaacGym benchmarks and show that RLE exhibits higher overall scores across all the tasks than other approaches, including action-noise and randomized value function exploration.",
      "authors": [
        "Srinath V. Mahankali",
        "Zhang-Wei Hong",
        "Ayush Sekhari",
        "Alexander Rakhlin",
        "Pulkit Agrawal"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Y9qzwNlKVU",
      "cdate": 1706833916301,
      "mdate": 1719287270997,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527526"
    },
    {
      "id": "ToHkAg936Y",
      "title": "Harnessing the Power of Neural Operators with Automatically Encoded Conservation Laws",
      "abstract": "Neural operators (NOs) have emerged as effective tools for modeling complex physical systems in scientific machine learning. In NOs, a central characteristic is to learn the governing physical laws directly from data. In contrast to other machine learning applications, partial knowledge is often known a priori about the physical system at hand whereby quantities such as mass, energy and momentum are exactly conserved. Currently, NOs have to learn these conservation laws from data and can only approximately satisfy them due to finite training data and random noise. In this work, we introduce conservation law-encoded neural operators (clawNOs), a suite of NOs that endow inference with automatic satisfaction of such conservation laws. ClawNOs are built with a divergence-free prediction of the solution field, with which the continuity equation is automatically guaranteed. As a consequence, clawNOs are compliant with the most fundamental and ubiquitous conservation laws essential for correct physical consistency. As demonstrations, we consider a wide variety of scientific applications ranging from constitutive modeling of material deformation, incompressible fluid dynamics, to atmospheric simulation. ClawNOs significantly outperform the state-of-the-art NOs in learning efficacy, especially in small-data regimes.",
      "authors": [
        "Ning Liu",
        "Yiming Fan",
        "Xianyi Zeng",
        "Milan Klöwer",
        "LU ZHANG",
        "Yue Yu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ToHkAg936Y",
      "cdate": 1706833893465,
      "mdate": 1719287270960,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527532"
    },
    {
      "id": "xW79geE0RA",
      "title": "Model-based Reinforcement Learning for Parameterized Action Spaces",
      "abstract": "We propose a novel model-based reinforcement learning algorithm---Dynamics Learning and predictive control with Parameterized Actions (DLPA)---for Parameterized Action Markov Decision Processes (PAMDPs). The agent learns a parameterized-action-conditioned dynamics model and plans with a modified Model Predictive Path Integral control. We theoretically quantify the difference between the generated trajectory and the optimal trajectory during planning in terms of the value they achieved through the lens of Lipschitz Continuity. Our empirical results on several standard benchmarks show that our algorithm achieves superior sample efficiency and asymptotic performance than state-of-the-art PAMDP methods.",
      "authors": [
        "Renhao Zhang",
        "Haotian Fu",
        "Yilin Miao",
        "George Konidaris"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xW79geE0RA",
      "cdate": 1706833507011,
      "mdate": 1719287270758,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527538"
    },
    {
      "id": "QLtxj3erlJ",
      "title": "Optimization without Retraction on the Random Generalized Stiefel Manifold",
      "abstract": "Optimization over the set of matrices $X$ that satisfy $X^\\top B X = I_p$, referred to as the generalized Stiefel manifold, appears in many applications involving sampled covariance matrices such as the canonical correlation analysis (CCA), independent component analysis (ICA), and the generalized eigenvalue problem (GEVP). Solving these problems is typically done by iterative methods that require a fully formed $B$. We propose a cheap stochastic iterative method that solves the optimization problem while having access only to a random estimates of $B$. Our method does not enforce the constraint in every iteration; instead, it produces iterations that converge to critical points on the generalized Stiefel manifold defined in expectation. The method has lower per-iteration cost, requires only matrix multiplications, and has the same convergence rates as its Riemannian optimization counterparts that require the full matrix $B$. Experiments demonstrate its effectiveness in various machine learning applications involving generalized orthogonality constraints, including CCA, ICA, and the GEVP.",
      "authors": [
        "Simon Vary",
        "Pierre Ablin",
        "Bin Gao",
        "Pierre-Antoine Absil"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=QLtxj3erlJ",
      "cdate": 1706833455277,
      "mdate": 1719287270757,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527543"
    },
    {
      "id": "J16WEPdqhJ",
      "title": "Accelerated Policy Gradient for s-rectangular Robust MDPs with Large State Spaces",
      "abstract": "Robust Markov decision process (robust MDP) is an important machine learning framework to make a reliable policy that is robust to environmental perturbation. Despite empirical success and popularity of policy gradient methods, existing policy gradient methods require at least iteration complexity $\\mathcal{O}(\\epsilon^{-4})$ to converge to the global optimal solution of s-rectangular robust MDPs with $\\epsilon$-accuracy and are limited to deterministic setting with access to exact gradients and small state space that are impractical in many applications. In this work, we propose an accelerated policy gradient algorithm with iteration complexity $\\mathcal{O}(\\epsilon^{-3}\\ln\\epsilon^{-1})$ in the deterministic setting using entropy regularization. Furthermore, we extend this algorithm to stochastic setting with access to only stochastic gradients and large state space which achieves the sample complexity $\\mathcal{O}(\\epsilon^{-7}\\ln\\epsilon^{-1})$. In the meantime, our algorithms are also the first scalable policy gradient methods to entropy-regularized robust MDPs, which provide an important but underexplored machine learning framework.",
      "authors": [
        "Ziyi Chen",
        "Heng Huang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=J16WEPdqhJ",
      "cdate": 1706833036395,
      "mdate": 1719287270679,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527548"
    },
    {
      "id": "5WEIVj98Ju",
      "title": "IW-GAE: Importance weighted group accuracy estimation for improved calibration and model selection in unsupervised domain adaptation",
      "abstract": "Distribution shifts pose significant challenges for model calibration and model selection tasks in the unsupervised domain adaptation problem---a scenario where the goal is to perform well in a distribution shifted domain without labels. In this work, we tackle difficulties coming from distribution shifts by developing a novel importance weighted group accuracy estimator. Specifically, we present a new perspective of addressing the model calibration and model selection tasks by estimating the group accuracy. Then, we formulate an optimization problem for finding an importance weight that leads to an accurate group accuracy estimation with theoretical analyses. Our extensive experiments show that our approach improves state-of-the-art performances by 22% in the model calibration task and 14% in the model selection task.",
      "authors": [
        "Taejong Joo",
        "Diego Klabjan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5WEIVj98Ju",
      "cdate": 1706832250660,
      "mdate": 1719287270576,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527554"
    },
    {
      "id": "YT1dtdLvSN",
      "title": "OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models",
      "abstract": "Optimization problems are pervasive in sectors from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers because the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. This paper introduces OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve (mixed integer) linear programming problems from their natural language descriptions. OptiMUS can develop mathematical models, write and debug solver code, evaluate the generated solutions, and improve its model and code based on these evaluations. OptiMUS utilizes a modular structure to process problems, allowing it to handle problems with long descriptions and complex data without long prompts. Experiments demonstrate that OptiMUS outperforms existing state-of-the-art methods on easy datasets by more than $20$% and on hard datasets (including a new dataset, NLP4LP, released with this paper that features long and complex problems) by more than $30$%. The implementation and the datasets are available at https://github.com/teshnizi/OptiMUS.",
      "authors": [
        "Ali AhmadiTeshnizi",
        "Wenzhi Gao",
        "Madeleine Udell"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=YT1dtdLvSN",
      "cdate": 1706832233131,
      "mdate": 1719287270550,
      "matched_keywords": [
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527560"
    },
    {
      "id": "zMwFvxr6CV",
      "title": "Agent Instructs Large Language Models to be General Zero-Shot Reasoners",
      "abstract": "We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. To enable this, our agent only needs to generate a single set of instructions for each task. These instructions turn out to be extremely effective for improving the reasoning process of different large language models across all task instances. We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b, Llama-2-70b-chat, and GPT-3.5 Turbo. Compared to zero-shot chain of thought, our improvement in reasoning is striking. With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo significantly.",
      "authors": [
        "Nicholas Crispino",
        "Kyle Montgomery",
        "Fankun Zeng",
        "Dawn Song",
        "Chenguang Wang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=zMwFvxr6CV",
      "cdate": 1706831962852,
      "mdate": 1719287270450,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527566"
    },
    {
      "id": "zDCwJQY3eI",
      "title": "On a Neural Implementation of Brenier's Polar Factorization",
      "abstract": "In 1991, Brenier proved a theorem that generalizes the polar decomposition for square matrices -- factored as PSD $\\times$ unitary -- to any vector field $F:\\mathbb{R}^d\\rightarrow \\mathbb{R}^d$. The theorem, known as the polar factorization theorem, states that any field $F$ can be recovered as the composition of the gradient of a convex function $u$ with a measure-preserving map $M$, namely $F=\\nabla u \\circ M$. We propose a practical implementation of this far-reaching theoretical result, and explore possible uses within machine learning. The theorem is closely related to optimal transport (OT) theory, and we borrow from recent advances in the field of neural optimal transport to parameterize the potential $u$ as an input convex neural network. The map $M$ can be either evaluated pointwise using $u^*$, the convex conjugate of $u$, through the identity $M=\\nabla u^* \\circ F$, or learned as an auxiliary network. Because $M$ is, in general, not injective, we consider the additional task of estimating the ill-posed inverse map that can approximate the pre-image measure $M^{-1}$ using a stochastic generator. We illustrate possible applications of Brenier's polar factorization to non-convex optimization problems, as well as sampling of densities that are not log-concave.",
      "authors": [
        "Nina Vesseron",
        "marco cuturi"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=zDCwJQY3eI",
      "cdate": 1706831932427,
      "mdate": 1719287270443,
      "matched_keywords": [
        "machine learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527572"
    },
    {
      "id": "BCEtumPYDt",
      "title": "Uncertainty for Active Learning on Graphs",
      "abstract": "Uncertainty Sampling is an Active Learning strategy that aims to improve the data efficiency of machine learning models by iteratively acquiring labels of data points with the highest uncertainty. While it has proven effective for independent data its applicability to graphs remains under-explored. We propose the first extensive study of Uncertainty Sampling for node classification: **(1)** We benchmark Uncertainty Sampling beyond predictive uncertainty and highlight a significant performance gap to other Active Learning strategies. **(2)** We develop ground-truth Bayesian uncertainty estimates in terms of the data generating process and prove their effectiveness in guiding Uncertainty Sampling toward optimal queries. We confirm our results on synthetic data and design an approximate approach that consistently outperforms other uncertainty estimators on real datasets. **(3)** Based on this analysis, we relate pitfalls in modeling uncertainty to existing methods. Our analysis enables and informs the development of principled uncertainty estimation on graphs.",
      "authors": [
        "Dominik Fuchsgruber",
        "Tom Wollschläger",
        "Bertrand Charpentier",
        "Antonio Oroz",
        "Stephan Günnemann"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BCEtumPYDt",
      "cdate": 1706831903575,
      "mdate": 1719287270418,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527578"
    },
    {
      "id": "S0DPCE7tt4",
      "title": "Why Do Animals Need Shaping? A Theory of Task Composition and Curriculum Learning",
      "abstract": "Diverse studies in systems neuroscience begin with extended periods of curriculum training known as ‘shaping’ procedures. These involve progressively studying component parts of more complex tasks, and can make the difference between learning a task quickly, slowly or not at all. Despite the importance of shaping to the acquisition of complex tasks, there is as yet no theory that can help guide the design of shaping procedures, or more fundamentally, provide insight into its key role in learning. Modern deep reinforcement learning systems might implicitly learn compositional primitives within their multilayer policy networks. Inspired by these models, we propose and analyse a model of deep policy gradient learning of simple compositional reinforcement learning tasks. Using the tools of statistical physics, we solve for exact learning dynamics and characterise different learning strategies including primitives pre-training, in which task primitives are studied individually before learning compositional tasks. We find a complex interplay between task complexity and the efficacy of shaping strategies. Overall, our theory provides an analytical understanding of the benefits of shaping in a class of compositional tasks and a quantitative account of how training protocols can disclose useful task primitives, ultimately yielding faster and more robust learning.",
      "authors": [
        "Jin Hwa Lee",
        "Stefano Sarao Mannelli",
        "Andrew M Saxe"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=S0DPCE7tt4",
      "cdate": 1706831885235,
      "mdate": 1719287270362,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527584"
    },
    {
      "id": "xtKWwB6lzT",
      "title": "Position: Reinforcement Learning in Dynamic Treatment Regimes Needs Critical Reexamination",
      "abstract": "In the rapidly changing healthcare landscape, the implementation of offline reinforcement learning (RL) in dynamic treatment regimes (DTRs) presents a mix of unprecedented opportunities and challenges. This position paper offers a critical examination of the current status of offline RL in the context of DTRs. We argue for a reassessment of applying RL in DTRs, citing concerns such as inconsistent and potentially inconclusive evaluation metrics, the absence of naive and supervised learning baselines, and the diverse choice of RL formulation in existing research. Through a case study with more than 17,000 evaluation experiments using a publicly available Sepsis dataset, we demonstrate that the performance of RL algorithms can significantly vary with changes in evaluation metrics and Markov Decision Process (MDP) formulations. Surprisingly, it is observed that in some instances, RL algorithms can be surpassed by random baselines subjected to policy evaluation methods and reward design. This calls for more careful policy evaluation and algorithm development in future DTR works. Additionally, we discussed potential enhancements toward more reliable development of RL-based dynamic treatment regimes and invited further discussion within the community. Code is available at https://github.com/GilesLuo/ReassessDTR.",
      "authors": [
        "Zhiyao Luo",
        "Yangchen Pan",
        "Peter Watkinson",
        "Tingting Zhu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xtKWwB6lzT",
      "cdate": 1706831761556,
      "mdate": 1719287270362,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527588"
    },
    {
      "id": "ExHTFXEhc9",
      "title": "Compute Better Spent: Replacing Dense Layers with Structured Matrices",
      "abstract": "Dense linear layers are the dominant computational bottleneck in foundation models. Identifying more efficient alternatives to dense matrices has enormous potential for building more compute-efficient models, as exemplified by the success of convolutional networks in the image domain. In this work, we systematically explore structured matrices as replacements for dense matrices. We show that different structures often require drastically different initialization scales and learning rates, which are crucial to performance, especially as models scale. Using insights from the Maximal Update Parameterization, we determine the optimal scaling for initialization and learning rates of these unconventional layers. Finally, we measure the scaling laws of different structures to compare how quickly their performance improves with compute. We propose a novel matrix family containing Monarch matrices, the Block Tensor-Train (BTT), which we show performs better than dense matrices for the same compute on multiple tasks. On CIFAR-10/100 with augmentation, BTT achieves exponentially lower training loss than dense when training MLPs and ViTs. BTT matches dense ViT-S/32 performance on ImageNet-1k with 3.8 times less compute and is more efficient than dense for training small GPT-2 language models.",
      "authors": [
        "Shikai Qiu",
        "Andres Potapczynski",
        "Marc Anton Finzi",
        "Micah Goldblum",
        "Andrew Gordon Wilson"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ExHTFXEhc9",
      "cdate": 1706831234449,
      "mdate": 1719287270115,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527594"
    },
    {
      "id": "n8g6WMxt09",
      "title": "Decoding-time Realignment of Language Models",
      "abstract": "Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of alignment, allowing users to smoothly transition between unaligned and aligned models. It also enhances the efficiency of hyperparameter tuning by enabling the identification of effective regularization strengths using a validation dataset.",
      "authors": [
        "Tianlin Liu",
        "Shangmin Guo",
        "Leonardo Bianco",
        "Daniele Calandriello",
        "Quentin Berthet",
        "Felipe Llinares-López",
        "Jessica Hoffmann",
        "Lucas Dixon",
        "Michal Valko",
        "Mathieu Blondel"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=n8g6WMxt09",
      "cdate": 1706831095762,
      "mdate": 1719287270085,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527600"
    },
    {
      "id": "ixdfvnO0uy",
      "title": "Differentiability and Optimization of Multiparameter Persistent Homology",
      "abstract": "Real-valued functions on geometric data---such as node attributes on a graph---can be optimized using descriptors from persistent homology, allowing the user to incorporate topological terms in the loss function. When optimizing a single real-valued function (the one-parameter setting), there is a canonical choice of descriptor for persistent homology: the barcode. The operation mapping a real-valued function to its barcode is differentiable almost everywhere, and the convergence of gradient descent for losses using barcodes is relatively well understood. When optimizing a vector-valued function (the multiparameter setting), there is no unique choice of descriptor for multiparameter persistent homology, and many distinct descriptors have been proposed. This calls for the development of a general framework for differentiability and optimization that applies to a wide range of multiparameter homological descriptors. In this article, we develop such a framework and show that it encompasses well-known descriptors of different flavors, such as signed barcodes and the multiparameter persistence landscape. We complement the theory with numerical experiments supporting the idea that optimizing multiparameter homological descriptors can lead to improved performances compared to optimizing one-parameter descriptors, even when using the simplest and most efficiently computable multiparameter descriptors.",
      "authors": [
        "Luis Scoccola",
        "Siddharth Setlur",
        "David Loiseaux",
        "Mathieu Carrière",
        "Steve Oudot"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ixdfvnO0uy",
      "cdate": 1706830968590,
      "mdate": 1719287270024,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527606"
    },
    {
      "id": "YB1O99gK7b",
      "title": "On Gradient-like Explanation under a Black-box Setting: When Black-box Explanations Become as Good as White-box",
      "abstract": "Attribution methods shed light on the explainability of data-driven approaches such as deep learning models by uncovering the most influential features in a to-be-explained decision. While determining feature attributions via gradients delivers promising results, the internal access required for acquiring gradients can be impractical under safety concerns, thus limiting the applicability of gradient-based approaches. In response to such limited flexibility, this paper presents GEEX (gradient-estimation-based explanation), a method that produces gradient-like explanations through only query-level access. The proposed approach holds a set of fundamental properties for attribution methods, which are mathematically rigorously proved, ensuring the quality of its explanations. In addition to the theoretical analysis, with a focus on image data, the experimental results empirically demonstrate the superiority of the proposed method over state-of-the-art black-box methods and its competitive performance compared to methods with full access.",
      "authors": [
        "Yi Cai",
        "Gerhard Wunder"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=YB1O99gK7b",
      "cdate": 1706830911559,
      "mdate": 1719287269996,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527611"
    },
    {
      "id": "6TCeizkLJV",
      "title": "Confidence Aware Inverse Constrained Reinforcement Learning",
      "abstract": "In coming up with solutions to real-world problems, humans implicitly adhere to constraints that are too numerous and complex to be specified completely. However, reinforcement learning (RL) agents need these constraints to learn the correct optimal policy in these settings. The field of Inverse Constraint Reinforcement Learning (ICRL) deals with this problem and provides algorithms that aim to estimate the constraints from expert demonstrations collected offline. Practitioners prefer to know a measure of confidence in the estimated constraints, before deciding to use these constraints, which allows them to only use the constraints that satisfy a desired level of confidence. However, prior works do not allow users to provide the desired level of confidence for the inferred constraints. This work provides a principled ICRL method that can take a confidence level with a set of expert demonstrations and outputs a constraint that is at least as constraining as the true underlying constraint with the desired level of confidence. Further, unlike previous methods, this method allows a user to know if the number of expert trajectories is insufficient to learn a constraint with a desired level of confidence, and therefore collect more expert trajectories as required to simultaneously learn constraints with the desired level of confidence and a policy that achieves the desired level of performance.",
      "authors": [
        "Sriram Ganapathi Subramanian",
        "Guiliang Liu",
        "Mohammed Elmahgiubi",
        "Kasra Rezaee",
        "Pascal Poupart"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=6TCeizkLJV",
      "cdate": 1706830819590,
      "mdate": 1719287269950,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527617"
    },
    {
      "id": "3Pq6uI1MTE",
      "title": "Differentiable Combinatorial Scheduling at Scale",
      "abstract": "This paper addresses the complex issue of resource-constrained scheduling, an NP-hard problem that spans critical areas including chip design and high-performance computing. Traditional scheduling methods often stumble over scalability and applicability challenges. We propose a novel approach using a differentiable combinatorial scheduling framework, utilizing Gumbel-Softmax differentiable sampling technique. This new technical allows for a fully differentiable formulation of linear programming (LP) based scheduling, extending its application to a broader range of LP formulations. To encode inequality constraints for scheduling tasks, we introduce *constrained Gumbel Trick*, which adeptly encodes arbitrary inequality constraints. Consequently, our method facilitates an efficient and scalable scheduling via gradient descent without the need for training data. Comparative evaluations on both synthetic and real-world benchmarks highlight our capability to significantly improve the optimization efficiency of scheduling, surpassing state-of-the-art solutions offered by commercial and open-source solvers such as CPLEX, Gurobi, and CP-SAT in the majority of the designs.",
      "authors": [
        "Mingju Liu",
        "Yingjie Li",
        "Jiaqi Yin",
        "Zhiru Zhang",
        "CUNXI YU"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3Pq6uI1MTE",
      "cdate": 1706830772061,
      "mdate": 1719287269915,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527623"
    },
    {
      "id": "jM9A3Kz6Ki",
      "title": "Averaging $n$-step Returns Reduces Variance in Reinforcement Learning",
      "abstract": "Multistep returns, such as $n$-step returns and $\\lambda$-returns, are commonly used to improve the sample efficiency of reinforcement learning (RL) methods. The variance of the multistep returns becomes the limiting factor in their length; looking too far into the future increases variance and reverses the benefits of multistep learning. In our work, we demonstrate the ability of compound returns—weighted averages of $n$-step returns—to reduce variance. We prove for the first time that any compound return with the same contraction modulus as a given $n$-step return has strictly lower variance. We additionally prove that this variance-reduction property improves the finite-sample complexity of temporal-difference learning under linear function approximation. Because general compound returns can be expensive to implement, we introduce two-bootstrap returns which reduce variance while remaining efficient, even when using minibatched experience replay. We conduct experiments showing that compound returns often increase the sample efficiency of $n$-step deep RL agents like DQN and PPO.",
      "authors": [
        "Brett Daley",
        "Martha White",
        "Marlos C. Machado"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jM9A3Kz6Ki",
      "cdate": 1706830734111,
      "mdate": 1719287269889,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527629"
    },
    {
      "id": "EQXZqBXeW9",
      "title": "Federated Neuro-Symbolic Learning",
      "abstract": "Neuro-symbolic learning (NSL) models complex symbolic rule patterns into latent variable distributions by neural networks, which reduces rule search space and generates unseen rules to improve downstream task performance. Centralized NSL learning involves directly acquiring data from downstream tasks, which is not feasible for federated learning (FL). To address this limitation, we shift the focus from such a one-to-one interactive neuro-symbolic paradigm to one-to-many Federated Neuro-Symbolic Learning framework (FedNSL) with latent variables as the FL communication medium. Built on the basis of our novel reformulation of the NSL theory, FedNSL is capable of identifying and addressing rule distribution heterogeneity through a simple and effective Kullback-Leibler (KL) divergence constraint on rule distribution applicable under the FL setting. It further theoretically adjusts variational expectation maximization (V-EM) to reduce the rule search space across domains. This is the first incorporation of distribution-coupled bilevel optimization into FL. Extensive experiments based on both synthetic and real-world data demonstrate significant advantages of FedNSL compared to five state-of-the-art methods. It outperforms the best baseline by 17% and 29% in terms of unbalanced average training accuracy and unseen average testing accuracy, respectively.",
      "authors": [
        "Pengwei Xing",
        "Songtao Lu",
        "Han Yu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EQXZqBXeW9",
      "cdate": 1706830503882,
      "mdate": 1719287269855,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527635"
    },
    {
      "id": "b1YQ5WKY3w",
      "title": "Is In-Context Learning in Large Language Models Bayesian? A Martingale Perspective",
      "abstract": "In-context learning (ICL) has emerged as a particularly remarkable characteristic of Large Language Models (LLM): given a pretrained LLM and an observed dataset, LLMs can make predictions for new data points from the same distribution without fine-tuning. Numerous works have postulated ICL as approximately Bayesian inference, rendering this a natural hypothesis. In this work, we analyse this hypothesis from a new angle through the *martingale property*, a fundamental requirement of a Bayesian learning system for exchangeable data. We show that the martingale property is a necessary condition for unambiguous predictions in such scenarios, and enables a principled, decomposed notion of uncertainty vital in trustworthy, safety-critical systems. We derive actionable checks with corresponding theory and test statistics which must hold if the martingale property is satisfied. We also examine if uncertainty in LLMs decreases as expected in Bayesian learning when more data is observed. In three experiments, we provide evidence for violations of the martingale property, and deviations from a Bayesian scaling behaviour of uncertainty, falsifying the hypothesis that ICL is Bayesian.",
      "authors": [
        "Fabian Falck",
        "Ziyu Wang",
        "Christopher C. Holmes"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=b1YQ5WKY3w",
      "cdate": 1706830467048,
      "mdate": 1719287269812,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527640"
    },
    {
      "id": "e5admkWKgV",
      "title": "Position: A Call for Embodied AI",
      "abstract": "We propose Embodied AI (E-AI) as the next fundamental step in the pursuit of Artificial General Intelligence (AGI), juxtaposing it against current AI advancements, particularly Large Language Models (LLMs). We traverse the evolution of the embodiment concept across diverse fields (philosophy, psychology, neuroscience, and robotics) to highlight how E-AI distinguishes itself from the classical paradigm of static learning. By broadening the scope of E-AI, we introduce a theoretical framework based on cognitive architectures, emphasizing perception, action, memory, and learning as essential components of an embodied agent. This framework is aligned with Friston’s active inference principle, offering a comprehensive approach to E-AI development. Despite the progress made in the field of AI, substantial challenges, such as the formulation of a novel AI learning theory and the innovation of advanced hardware, persist. Our discussion lays down a foundational guideline for future E-AI research. Highlighting the importance of creating E-AI agents capable of seamless communication, collaboration, and coexistence with humans and other intelligent entities within real-world environments, we aim to steer the AI community towards addressing the multifaceted challenges and seizing the opportunities that lie ahead in the quest for AGI.",
      "authors": [
        "Giuseppe Paolo",
        "Jonas Gonzalez-Billandon",
        "Balázs Kégl"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=e5admkWKgV",
      "cdate": 1706830435987,
      "mdate": 1719287269791,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527646"
    },
    {
      "id": "cXBPPfNUZJ",
      "title": "Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations",
      "abstract": "In this paper we adopt a representation-centric perspective on exploration in reinforcement learning, viewing exploration fundamentally as a density estimation problem. We investigate the effectiveness of clustering representations for exploration in 3-D environments, based on the observation that the importance of pixel changes between transitions is less pronounced in 3-D environments compared to 2-D environments, where pixel changes between transitions are typically distinct and significant. We propose a method that performs episodic and global clustering on random representations and on pre-trained DINO representations to count states, i.e, estimate pseudo-counts. Surprisingly, even random features can be clustered effectively to count states in 3-D environments, however when these become visually more complex, pre-trained DINO representations are more effective thanks to the pre-trained inductive biases in the representations. Overall, this presents a pathway for integrating pre-trained biases into exploration. We evaluate our approach on the VizDoom and Habitat environments, demonstrating that our method surpasses other well-known exploration methods in these settings.",
      "authors": [
        "Stefan Sylvius Wagner",
        "Stefan Harmeling"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=cXBPPfNUZJ",
      "cdate": 1706830242375,
      "mdate": 1719287269743,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527651"
    },
    {
      "id": "iup9NElHji",
      "title": "Visual Transformer with Differentiable Channel Selection: An Information Bottleneck Inspired Approach",
      "abstract": "Self-attention and transformers have been widely used in deep learning. Recent efforts have been devoted to incorporating transformer blocks into different types of neural architectures, including those with convolutions, leading to various visual transformers for computer vision tasks. In this paper, we propose a novel and compact transformer block, Transformer with Differentiable Channel Selection, or DCS-Transformer. DCS-Transformer features channel selection in the computation of the attention weights and the input/output features of the MLP in the transformer block. Our DCS-Transformer is compatible with many popular and compact transformer networks, such as MobileViT and EfficientViT, and it reduces the FLOPs of the visual transformers while maintaining or even improving the prediction accuracy. In the experiments, we replace all the transformer blocks in MobileViT and EfficientViT with DCS-Transformer blocks, leading to DCS-Transformer networks with different backbones. The DCS-Transformer is motivated by reduction of Information Bottleneck, and a novel variational upper bound for the IB loss which can be optimized by SGD is derived and incorporated into the training loss of the network with DCS-Transformer. Extensive results on image classification and object detection evidence that DCS-Transformer renders compact and efficient visual transformers with comparable or much better prediction accuracy than the original visual transformers. The code of DCS-Transformer is available at https://github.com/Statistical-Deep-Learning/DCS-Transformer.",
      "authors": [
        "Yancheng Wang",
        "Ping Li",
        "Yingzhen Yang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iup9NElHji",
      "cdate": 1706830212263,
      "mdate": 1719287269702,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527656"
    },
    {
      "id": "lWy2lCTyJa",
      "title": "Revisiting Inexact Fixed-Point Iterations for Min-Max Problems: Stochasticity and Structured Nonconvexity",
      "abstract": "We focus on constrained, $L$-smooth, potentially stochastic and nonconvex-nonconcave min-max problems either satisfying $\\rho$-cohypomonotonicity or admitting a solution to the $\\rho$-weakly Minty Variational Inequality (MVI), where larger values of the parameter $\\rho>0$ correspond to a greater degree of nonconvexity. These problem classes include examples in two player reinforcement learning, interaction dominant min-max problems, and certain synthetic test problems on which classical min-max algorithms fail. It has been conjectured that first-order methods can tolerate a value of $\\rho$ no larger than $\\frac{1}{L}$, but existing results in the literature have stagnated at the tighter requirement $\\rho < \\frac{1}{2L}$. With a simple argument, we obtain optimal or best-known complexity guarantees with cohypomonotonicity or weak MVI conditions for $\\rho < \\frac{1}{L}$. First main insight for the improvements in the convergence analyses is to harness the recently proposed *conic nonexpansiveness* property of operators. Second, we provide a refined analysis for inexact Halpern iteration that relaxes the required inexactness level to improve some state-of-the-art complexity results even for constrained stochastic convex-concave min-max problems. Third, we analyze a stochastic inexact Krasnosel'skii-Mann iteration with a multilevel Monte Carlo estimator when the assumptions only hold with respect to a solution.",
      "authors": [
        "Ahmet Alacaoglu",
        "Donghwan Kim",
        "Stephen Wright"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lWy2lCTyJa",
      "cdate": 1706829914887,
      "mdate": 1719287269695,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527661"
    },
    {
      "id": "sTVSyqD6XX",
      "title": "Private and Federated Stochastic Convex Optimization: Efficient Strategies for Centralized Systems",
      "abstract": "This paper addresses the challenge of preserving privacy in Federated Learning (FL) within centralized systems, focusing on both trusted and untrusted server scenarios. We analyze this setting within the Stochastic Convex Optimization (SCO) framework, and devise methods that ensure Differential Privacy (DP) while maintaining optimal convergence rates for homogeneous and heterogeneous data distributions. Our approach, based on a recent stochastic optimization technique, offers linear computational complexity, comparable to non-private FL methods, and reduced gradient obfuscation. This work enhances the practicality of DP in FL, balancing privacy, efficiency, and robustness in a variety of server trust environments.",
      "authors": [
        "Roie Reshef",
        "Kfir Yehuda Levy"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=sTVSyqD6XX",
      "cdate": 1706829887506,
      "mdate": 1719287269677,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527665"
    },
    {
      "id": "KjazcKPMME",
      "title": "Understanding the Effects of Iterative Prompting on Truthfulness",
      "abstract": "The development of Large Language Models (LLMs) has notably transformed numerous sectors, offering impressive text generation capabilities. Yet, the reliability and truthfulness of these models remain pressing concerns. To this end, we investigate iterative prompting, a strategy hypothesized to refine LLM responses, assessing its impact on LLM truthfulness, an area which has not been thoroughly explored. Our extensive experiments explore the intricacies of iterative prompting variants, examining their influence on the accuracy and calibration of model responses. Our findings reveal that naive prompting methods significantly undermine truthfulness, leading to exacerbated calibration errors. In response to these challenges, we introduce several prompting variants designed to address the identified issues. These variants demonstrate marked improvements over existing baselines, signaling a promising direction for future research. Our work provides a nuanced understanding of iterative prompting and introduces novel approaches to enhance the truthfulness of LLMs, thereby contributing to the development of more accurate and trustworthy AI systems",
      "authors": [
        "Satyapriya Krishna",
        "Chirag Agarwal",
        "Himabindu Lakkaraju"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=KjazcKPMME",
      "cdate": 1706829822732,
      "mdate": 1719287269622,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527670"
    },
    {
      "id": "KJhLpzqNri",
      "title": "Embarrassingly Parallel GFlowNets",
      "abstract": "GFlowNets are a promising alternative to MCMC sampling for discrete compositional random variables. Training GFlowNets requires repeated evaluations of the unnormalized target distribution, or reward function. However, for large-scale posterior sampling, this may be prohibitive since it incurs traversing the data several times. Moreover, if the data are distributed across clients, employing standard GFlowNets leads to intensive client-server communication. To alleviate both these issues, we propose _embarrassingly parallel_ GFlowNet (EP-GFlowNet). EP-GFlowNet is a provably correct divide-and-conquer method to sample from product distributions of the form $R(\\cdot) \\propto R_1(\\cdot) ... R_N(\\cdot)$ --- e.g., in parallel or federated Bayes, where each $R_n$ is a local posterior defined on a data partition. First, in parallel, we train a local GFlowNet targeting each $R_n$ and send the resulting models to the server. Then, the server learns a global GFlowNet by enforcing our newly proposed _aggregating balance_ condition, requiring a single communication step. Importantly, EP-GFlowNets can also be applied to multi-objective optimization and model reuse. Our experiments illustrate the effectiveness of EP-GFlowNets on multiple tasks, including parallel Bayesian phylogenetics, multi-objective multiset and sequence generation, and federated Bayesian structure learning.",
      "authors": [
        "Tiago Silva",
        "Luiz Max Carvalho",
        "Amauri H Souza",
        "Samuel Kaski",
        "Diego Mesquita"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=KJhLpzqNri",
      "cdate": 1706829714862,
      "mdate": 1719287269620,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527675"
    },
    {
      "id": "cY9g0bwiZx",
      "title": "The Max-Min Formulation of Multi-Objective Reinforcement Learning: From Theory to a Model-Free Algorithm",
      "abstract": "In this paper, we consider multi-objective reinforcement learning, which arises in many real-world problems with multiple optimization goals. We approach the problem with a max-min framework focusing on fairness among the multiple goals and develop a relevant theory and a practical model-free algorithm under the max-min framework. The developed theory provides a theoretical advance in multi-objective reinforcement learning, and the proposed algorithm demonstrates a notable performance improvement over existing baseline methods.",
      "authors": [
        "Giseung Park",
        "Woohyeon Byeon",
        "Seongmin Kim",
        "Elad Havakuk",
        "Amir Leshem",
        "Youngchul Sung"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=cY9g0bwiZx",
      "cdate": 1706829696670,
      "mdate": 1719287269539,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527680"
    },
    {
      "id": "9laB7ytoMp",
      "title": "Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills",
      "abstract": "Large language models (LLMs) have recently been used for sequential decision making in interactive environments. However, leveraging environment reward signals for continual LLM actor improvement is not straightforward. We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills. SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill. These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards. Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards. We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement. SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-art in ScienceWorld by 35%.",
      "authors": [
        "Kolby Nottingham",
        "Bodhisattwa Prasad Majumder",
        "Bhavana Dalvi Mishra",
        "Sameer Singh",
        "Peter Clark",
        "Roy Fox"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9laB7ytoMp",
      "cdate": 1706829600485,
      "mdate": 1719287269467,
      "matched_keywords": [
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527686"
    },
    {
      "id": "otuTw4Mghk",
      "title": "On the Origins of Linear Representations in Large Language Models",
      "abstract": "An array of recent works have argued that high-level semantic concepts are encoded \"linearly\" in the representation space of large language models. In this work, we study the origins of such linear representations. To that end, we introduce a latent variable model to abstract and formalize the concept dynamics of the next token prediction. We use this formalism to prove that linearity arises as a consequence of the loss function and the implicit bias of gradient descent. The theory is further substantiated empirically via experiments.",
      "authors": [
        "Yibo Jiang",
        "Goutham Rajendran",
        "Pradeep Kumar Ravikumar",
        "Bryon Aragam",
        "Victor Veitch"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=otuTw4Mghk",
      "cdate": 1706829537336,
      "mdate": 1719287269406,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527692"
    },
    {
      "id": "kRxCDDFNpp",
      "title": "Fewer Truncations Improve Language Modeling",
      "abstract": "In large language model training, input documents are typically concatenated together and then split into sequences of equal length to avoid padding tokens. Despite its efficiency, the concatenation approach compromises data integrity—it inevitably breaks many documents into incomplete pieces, leading to excessive truncations that hinder the model from learning to compose logically coherent and factually consistent content that is grounded on the complete context. To address the issue, we propose Best-fit Packing, a scalable and efficient method that packs documents into training sequences through length-aware combinatorial optimization. Our method completely eliminates unnecessary truncations while retaining the same training efficiency as concatenation. Empirical results from both text and code pre-training show that our method achieves superior performance (e.g., +4.7% on reading comprehension; +16.8% in context following; and +9.2% on program synthesis), and reduces closed-domain hallucination effectively by up to 58.3%.",
      "authors": [
        "Hantian Ding",
        "Zijian Wang",
        "Giovanni Paolini",
        "Varun Kumar",
        "Anoop Deoras",
        "Dan Roth",
        "Stefano Soatto"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kRxCDDFNpp",
      "cdate": 1706829297294,
      "mdate": 1719287269374,
      "matched_keywords": [
        "large language model",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527697"
    },
    {
      "id": "iJlPJsTw2B",
      "title": "Learning from Students: Applying t-Distributions to Explore Accurate and Efficient Formats for LLMs",
      "abstract": "The increasing size of large language models (LLMs) traditionally requires low-precision integer formats to meet strict latency and power demands. Yet recently, alternative formats such as Normal Float (NF4) have increased model accuracy at the cost of increased chip area. In this work, we first conduct a large-scale analysis of LLM weights and activations across 30 networks and conclude that most distributions follow a Student's t-distribution. We then derive a new theoretically optimal format, Student Float (SF4), that improves over NF4 across modern LLMs, for example increasing the average accuracy on LLaMA2-7B by 0.76% across tasks. Using this format as a high-accuracy reference, we then propose augmenting E2M1 with two variants of *supernormal* support for higher model accuracy. Finally, we explore the quality and efficiency frontier across 11 datatypes by evaluating their model accuracy and hardware complexity. We discover a Pareto curve composed of INT4, E2M1, and E2M1 with supernormal support, which offers a continuous tradeoff between model accuracy and chip area. For example, E2M1 with supernormal support increases the accuracy of Phi-2 by up to 2.19% with 1.22% area overhead, enabling more LLM-based applications to be run at four bits. The supporting code is hosted at https://github.com/cornell-zhang/llm-datatypes.",
      "authors": [
        "Jordan Dotzel",
        "Yuzong Chen",
        "Bahaa Kotb",
        "Sushma Prasad",
        "Gang Wu",
        "Sheng Li",
        "Mohamed S Abdelfattah",
        "Zhiru Zhang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iJlPJsTw2B",
      "cdate": 1706829198211,
      "mdate": 1719287269332,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527704"
    },
    {
      "id": "QZgo9JZpLq",
      "title": "The Illusion of State in State-Space Models",
      "abstract": "State-space models (SSMs) have emerged as a potential alternative architecture for building large language models (LLMs) compared to the previously ubiquitous transformer architecture. One theoretical weakness of transformers is that they cannot express certain kinds of sequential computation and state tracking (Merrill & Sabharwal, 2023), which SSMs are explicitly designed to address via their close architectural similarity to recurrent neural networks (RNNs). *But do SSMs truly have an advantage (over transformers) in expressive power for state tracking?* Surprisingly, the answer is no. Our analysis reveals that the expressive power of SSMs is limited very similarly to transformers: SSMs cannot express computation outside the complexity class $\\mathsf{TC}^0$. In particular, this means they cannot solve simple state-tracking problems like permutation composition. It follows that SSMs are provably unable to accurately track chess moves with certain notation, evaluate code, or track entities in a long narrative. To supplement our formal analysis, we report experiments showing that Mamba-style SSMs indeed struggle with state tracking. Thus, despite its recurrent formulation, the \"state'' in an SSM is an illusion: SSMs have similar expressiveness limitations to non-recurrent models like transformers, which may fundamentally limit their ability to solve real-world state-tracking problems.",
      "authors": [
        "William Merrill",
        "Jackson Petty",
        "Ashish Sabharwal"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=QZgo9JZpLq",
      "cdate": 1706829031158,
      "mdate": 1719287269305,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527708"
    },
    {
      "id": "dBMLtuKH01",
      "title": "Position: The Causal Revolution Needs Scientific Pragmatism",
      "abstract": "Causal models and methods have great promise, but their progress has been stalled. Proposals using causality get squeezed between two opposing worldviews. Scientific perfectionism--an insistence on only using ``correct'' models--slows the adoption of causal methods in knowledge generating applications. Pushing in the opposite direction, the academic discipline of computer science prefers algorithms with no or few assumptions, and technologies based on automation and scalability are often selected for economic and business applications. We argue that these system-centric inductive biases should be replaced with a human-centric philosophy we refer to as scientific pragmatism. The machine learning community must strike the right balance to make space for the causal revolution to prosper.",
      "authors": [
        "Joshua R. Loftus"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=dBMLtuKH01",
      "cdate": 1706828791013,
      "mdate": 1719287269224,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527714"
    },
    {
      "id": "IwqE4QqBew",
      "title": "Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models",
      "abstract": "Low-Rank Adaptation (LoRA) emerges as a popular parameter-efficient fine-tuning (PEFT) method, which proposes to freeze pretrained model weights and update an additive low-rank trainable matrix. In this work, we study the enhancement of LoRA training by introducing an $r\\times r$ preconditioner in each gradient step where $r$ is the LoRA rank. We theoretically verify that the proposed preconditioner stabilizes feature learning with LoRA under infinite-width NN setting. Empirically, the implementation of this new preconditioner requires a small change to existing optimizer code and creates virtually minuscule storage and runtime overhead. Our experimental results with both large language models and text-to-image diffusion models show that with this new preconditioner, the convergence and reliability of SGD and AdamW can be significantly enhanced. Moreover, the training process becomes much more robust to hyperparameter choices such as learning rate. The new preconditioner can be derived from a novel Riemannian metric in low-rank matrix field.",
      "authors": [
        "Fangzhao Zhang",
        "Mert Pilanci"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=IwqE4QqBew",
      "cdate": 1706828723327,
      "mdate": 1719287269207,
      "matched_keywords": [
        "large language model",
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527718"
    },
    {
      "id": "xPmSNLle1w",
      "title": "A New Branch-and-Bound Pruning Framework for $\\ell_0$-Regularized Problems",
      "abstract": "We consider the resolution of learning problems involving $\\ell_0$-regularization via Branch-and- Bound (BnB) algorithms. These methods explore regions of the feasible space of the problem and check whether they do not contain solutions through “pruning tests”. In standard implementations, evaluating a pruning test requires to solve a convex optimization problem, which may result in computational bottlenecks. In this paper, we present an alternative to implement pruning tests for some generic family of $\\ell_0$-regularized problems. Our proposed procedure allows the simultaneous assessment of several regions and can be embedded in standard BnB implementations with a negligible computational overhead. We show through numerical simulations that our pruning strategy can improve the solving time of BnB procedures by several orders of magnitude for typical problems encountered in machine-learning applications.",
      "authors": [
        "Theo Guyard",
        "Cédric Herzet",
        "Clément Elvira",
        "Ayse-Nur Arslan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xPmSNLle1w",
      "cdate": 1706828572504,
      "mdate": 1722020837382,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527724"
    },
    {
      "id": "OS5dqxmmtl",
      "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
      "abstract": "The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data transfer. For this reason, we introduce **SparQ Attention**, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to 8x savings in attention data transfers without substantial drops in accuracy, by evaluating Llama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream tasks.",
      "authors": [
        "Luka Ribar",
        "Ivan Chelombiev",
        "Luke Hudlass-Galley",
        "Charlie Blake",
        "Carlo Luschi",
        "Douglas Orr"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OS5dqxmmtl",
      "cdate": 1706828504979,
      "mdate": 1719287269069,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527729"
    },
    {
      "id": "XvmooikuHE",
      "title": "Probability Distribution of Hypervolume Improvement in Bi-objective Bayesian Optimization",
      "abstract": "Hypervolume improvement (HVI) is commonly employed in multi-objective Bayesian optimization algorithms to define acquisition functions due to its Pareto-compliant property. Rather than focusing on specific statistical moments of HVI, this work aims to provide the exact expression of HVI's probability distribution for bi-objective problems. Considering a bi-variate Gaussian random variable resulting from Gaussian process (GP) modeling, we derive the probability distribution of its hypervolume improvement via a cell partition-based method. Our exact expression is superior in numerical accuracy and computation efficiency compared to the Monte Carlo approximation of HVI's distribution. Utilizing this distribution, we propose a novel acquisition function - $\\varepsilon$-probability of hypervolume improvement ($\\varepsilon$-PoHVI). Experimentally, we show that on many widely-applied bi-objective test problems, $\\varepsilon$-PoHVI significantly outperforms other related acquisition functions, e.g., $\\varepsilon$-PoI, and expected hypervolume improvement, when the GP model exhibits a large the prediction uncertainty.",
      "authors": [
        "Hao Wang",
        "Kaifeng Yang",
        "Michael Affenzeller"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=XvmooikuHE",
      "cdate": 1706828402535,
      "mdate": 1719287269044,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527735"
    },
    {
      "id": "XiemSZpvh0",
      "title": "Neuro-Visualizer: A Novel Auto-Encoder-Based Loss Landscape Visualization Method With an Application in Knowledge-Guided Machine Learning",
      "abstract": "In recent years, there has been a growing interest in visualizing the loss landscape of neural networks. Linear landscape visualization methods, such as principal component analysis, have become widely used as they intuitively help researchers study neural networks and their training process. However, these linear methods suffer from limitations and drawbacks due to their lack of flexibility and low fidelity at representing the high dimensional landscape. In this paper, we present a novel auto-encoder-based non-linear landscape visualization method called Neuro-Visualizer that addresses these shortcoming and provides useful insights about neural network loss landscapes. To demonstrate its potential, we run experiments on a variety of problems in two separate applications of knowledge-guided machine learning (KGML). Our findings show that Neuro-Visualizer outperforms other linear and non-linear baselines and helps corroborate, and sometime challenge, claims proposed by machine learning community. All code and data used in the experiments of this paper can be found at the link below.",
      "authors": [
        "Mohannad Elhamod",
        "Anuj Karpatne"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=XiemSZpvh0",
      "cdate": 1706828240026,
      "mdate": 1719287269006,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527741"
    },
    {
      "id": "xTYIAD2NND",
      "title": "Out-of-Domain Generalization in Dynamical Systems Reconstruction",
      "abstract": "In science we are interested in finding the governing equations, the dynamical rules, underlying empirical phenomena. While traditionally scientific models are derived through cycles of human insight and experimentation, recently deep learning (DL) techniques have been advanced to reconstruct dynamical systems (DS) directly from time series data. State-of-the-art dynamical systems reconstruction (DSR) methods show promise in capturing invariant and long-term properties of observed DS, but their ability to generalize to unobserved domains remains an open challenge. Yet, this is a crucial property we would expect from any viable scientific theory. In this work, we provide a formal framework that addresses generalization in DSR. We explain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly differs from OODG considered elsewhere in machine learning. We introduce mathematical notions based on topological concepts and ergodic theory to formalize the idea of learnability of a DSR model. We formally prove that black-box DL techniques, without adequate structural priors, generally will not be able to learn a generalizing DSR model. We also show this empirically, considering major classes of DSR algorithms proposed so far, and illustrate where and why they fail to generalize across the whole phase space. Our study provides the first comprehensive mathematical treatment of OODG in DSR, and gives a deeper conceptual understanding of where the fundamental problems in OODG lie and how they could possibly be addressed in practice.",
      "authors": [
        "Niclas Alexander Göring",
        "Florian Hess",
        "Manuel Brenner",
        "Zahra Monfared",
        "Daniel Durstewitz"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xTYIAD2NND",
      "cdate": 1706828022780,
      "mdate": 1719287268982,
      "matched_keywords": [
        "machine learning",
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527747"
    },
    {
      "id": "u6PeRHEsjL",
      "title": "Position: Evolving AI Collectives Enhance Human Diversity and Enable Self-Regulation",
      "abstract": "Large language model behavior is shaped by the language of those with whom they interact. This capacity and their increasing prevalence online portend that they will intentionally or unintentionally \"program\" one another and form emergent AI subjectivities, relationships, and collectives. Here, we call upon the research community to investigate these \"societies\" of interacting artificial intelligences to increase their rewards and reduce their risks for human society and the health of online environments. We use a small \"community\" of models and their evolving outputs to illustrate how such emergent, decentralized AI collectives can spontaneously expand the bounds of human diversity and reduce the risk of toxic, anti-social behavior online. Finally, we discuss opportunities for AI cross-moderation and address ethical issues and design challenges associated with creating and maintaining free-formed AI collectives.",
      "authors": [
        "Shiyang Lai",
        "Yujin Potter",
        "Junsol Kim",
        "Richard Zhuang",
        "Dawn Song",
        "James Evans"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=u6PeRHEsjL",
      "cdate": 1706827928653,
      "mdate": 1719287268976,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527752"
    },
    {
      "id": "LfJgeBNCFI",
      "title": "DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning",
      "abstract": "In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs. Empirically, DS-Agent with GPT-4 achieves 100% success rate in the development stage, while attaining 36% improvement on average one pass rate across alternative LLMs in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing \r\n$1.60 and \\\\$0.13 per run with GPT-4, respectively. Our data and code are open-sourced at https://github.com/guosyjlu/DS-Agent.",
      "authors": [
        "Siyuan Guo",
        "Cheng Deng",
        "Ying Wen",
        "Hechang Chen",
        "Yi Chang",
        "Jun Wang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LfJgeBNCFI",
      "cdate": 1706827890851,
      "mdate": 1719287268947,
      "matched_keywords": [
        "machine learning",
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527757"
    },
    {
      "id": "yo9Jyt3XCY",
      "title": "Position: AI/ML Influencers Have a Place in the Academic Process",
      "abstract": "As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside controls precisely matched by 9 key covariates. Our statistical and causal inference analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. Given these findings, we advocate for a responsible approach to curation, encouraging influencers to uphold the journalistic standard that includes showcasing diverse research topics, authors, and institutions.",
      "authors": [
        "Iain Weissburg",
        "Mehir Arora",
        "Xinyi Wang",
        "Liangming Pan",
        "William Yang Wang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=yo9Jyt3XCY",
      "cdate": 1706827725040,
      "mdate": 1719287268901,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527762"
    },
    {
      "id": "7joG3i2pUR",
      "title": "Offline-Boosted Actor-Critic: Adaptively Blending Optimal Historical Behaviors in Deep Off-Policy RL",
      "abstract": "Off-policy reinforcement learning (RL) has achieved notable success in tackling many complex real-world tasks, by leveraging previously collected data for policy learning. However, most existing off-policy RL algorithms fail to maximally exploit the information in the replay buffer, limiting sample efficiency and policy performance. In this work, we discover that concurrently training an offline RL policy based on the shared online replay buffer can sometimes outperform the original online learning policy, though the occurrence of such performance gains remains uncertain. This motivates a new possibility of harnessing the emergent outperforming offline optimal policy to improve online policy learning. Based on this insight, we present Offline-Boosted Actor-Critic (OBAC), a model-free online RL framework that elegantly identifies the outperforming offline policy through value comparison, and uses it as an adaptive constraint to guarantee stronger policy learning performance. Our experiments demonstrate that OBAC outperforms other popular model-free RL baselines and rivals advanced model-based RL methods in terms of sample efficiency and asymptotic performance across **53** tasks spanning **6** task suites.",
      "authors": [
        "Yu Luo",
        "Tianying Ji",
        "Fuchun Sun",
        "Jianwei Zhang",
        "Huazhe Xu",
        "Xianyuan Zhan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7joG3i2pUR",
      "cdate": 1706827693267,
      "mdate": 1719287268839,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527767"
    },
    {
      "id": "NlM4gp8hyO",
      "title": "Sequence Compression Speeds Up Credit Assignment in Reinforcement Learning",
      "abstract": "Temporal credit assignment in reinforcement learning is challenging due to delayed and stochastic outcomes. Monte Carlo targets can bridge long delays between action and consequence but lead to high-variance targets due to stochasticity. Temporal difference (TD) learning uses bootstrapping to overcome variance but introduces a bias that can only be corrected through many iterations. TD($\\lambda$) provides a mechanism to navigate this bias-variance tradeoff smoothly. Appropriately selecting $\\lambda$ can significantly improve performance. Here, we propose Chunked-TD, which uses predicted probabilities of transitions from a model for computing $\\lambda$-return targets. Unlike other model-based solutions to credit assignment, Chunked-TD is less vulnerable to model inaccuracies. Our approach is motivated by the principle of history compression and ‘chunks’ trajectories for conventional TD learning. Chunking with learned world models compresses near-deterministic regions of the environment-policy interaction to speed up credit assignment while still bootstrapping when necessary. We propose algorithms that can be implemented online and show that they solve some problems much faster than conventional TD($\\lambda$).",
      "authors": [
        "Aditya Ramesh",
        "Kenny John Young",
        "Louis Kirsch",
        "Jürgen Schmidhuber"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NlM4gp8hyO",
      "cdate": 1706827667066,
      "mdate": 1719287268805,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527772"
    },
    {
      "id": "9Tq4L3Go9f",
      "title": "Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic",
      "abstract": "Learning high-quality $Q$-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. Previous works primarily focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. Deviating from the common viewpoint, we observe that $Q$-values are often underestimated in the latter stage of the RL training process, potentially hindering policy learning and reducing sample efficiency. We find that such a long-neglected phenomenon is often related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer. We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates $Q$-value using both historical best-performing actions and the current policy. Based on BEE, the resulting practical algorithm BAC outperforms state-of-the-art methods in **over 50** continuous control tasks and achieves strong performance in failure-prone scenarios and **real-world robot** tasks. Benchmark results and videos are available at https://jity16.github.io/BEE/.",
      "authors": [
        "Tianying Ji",
        "Yu Luo",
        "Fuchun Sun",
        "Xianyuan Zhan",
        "Jianwei Zhang",
        "Huazhe Xu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9Tq4L3Go9f",
      "cdate": 1706827521633,
      "mdate": 1719287268750,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527777"
    },
    {
      "id": "poEPRuNvM3",
      "title": "Fair Off-Policy Learning from Observational Data",
      "abstract": "Algorithmic decision-making in practice must be fair for legal, ethical, and societal reasons. To achieve this, prior research has contributed various approaches that ensure fairness in machine learning predictions, while comparatively little effort has focused on fairness in decision-making, specifically off-policy learning. In this paper, we propose a novel framework for fair off-policy learning: we learn decision rules from observational data under different notions of fairness, where we explicitly assume that observational data were collected under a different -- potentially discriminatory -- behavioral policy. Importantly, our framework applies to different fairness notions for off-policy learning, where fairness is formalized based on actions or policy values. As our main contribution, we propose a neural network-based framework to learn optimal policies under different fairness notions. We further provide theoretical guarantees in the form of generalization bounds for the finite-sample version of our framework. We demonstrate the effectiveness of our framework through extensive numerical experiments using both simulated and real-world data. Altogether, our work enables algorithmic decision-making in a wide array of practical applications where fairness must be ensured.",
      "authors": [
        "Dennis Frauen",
        "Valentyn Melnychuk",
        "Stefan Feuerriegel"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=poEPRuNvM3",
      "cdate": 1706827509356,
      "mdate": 1719287268726,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527782"
    },
    {
      "id": "321GwKMtxO",
      "title": "REMEDI: Corrective Transformations for Improved Neural Entropy Estimation",
      "abstract": "Information theoretic quantities play a central role in machine learning. The recent surge in the complexity of data and models has increased the demand for accurate estimation of these quantities. However, as the dimension grows the estimation presents significant challenges, with existing methods struggling already in relatively low dimensions. To address this issue, in this work, we introduce REMEDI for efficient and accurate estimation of differential entropy, a fundamental information theoretic quantity. The approach combines the minimization of the cross-entropy for simple, adaptive base models and the estimation of their deviation, in terms of the relative entropy, from the data density. Our approach demonstrates improvement across a broad spectrum of estimation tasks, encompassing entropy estimation on both synthetic and natural data. Further, we extend important theoretical consistency results to a more generalized setting required by our approach. We illustrate how the framework can be naturally extended to information theoretic supervised learning models, with a specific focus on the Information Bottleneck approach. It is demonstrated that the method delivers better accuracy compared to the existing methods in Information Bottleneck. In addition, we explore a natural connection between REMEDI and generative modeling using rejection sampling and Langevin dynamics.",
      "authors": [
        "Viktor Nilsson",
        "Anirban Samaddar",
        "Sandeep Madireddy",
        "Pierre Nyquist"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=321GwKMtxO",
      "cdate": 1706827396256,
      "mdate": 1719287268670,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527788"
    },
    {
      "id": "PG5fV50maR",
      "title": "LESS: Selecting Influential Data for Targeted Instruction Tuning",
      "abstract": "Instruction tuning has unlocked powerful capabilities in large language models (LLMs), using combined datasets to develop general-purpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as *targeted instruction tuning*. We propose LESS, an optimizer-aware and practically efficient algorithm to estimate data influences and perform **L**ow-rank gradi**E**nt **S**imilarity **S**earch for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable *gradient datastore* with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application. To facilitate future work, we release code and data at [princeton-nlp/LESS](https://github.com/princeton-nlp/LESS).",
      "authors": [
        "Mengzhou Xia",
        "Sadhika Malladi",
        "Suchin Gururangan",
        "Sanjeev Arora",
        "Danqi Chen"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=PG5fV50maR",
      "cdate": 1706827319570,
      "mdate": 1719287268652,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527793"
    },
    {
      "id": "i9C4Kwm56G",
      "title": "Rapid Learning without Catastrophic Forgetting in the Morris Water Maze",
      "abstract": "Animals can swiftly adapt to novel tasks, while maintaining proficiency on previously trained tasks. This contrasts starkly with machine learning models, which struggle on these capabilities. We first propose a new task, the sequential Morris Water Maze (sWM), which extends a widely used task in the psychology and neuroscience fields and requires both rapid and continual learning. It has frequently been hypothesized that inductive biases from brains could help build better ML systems, but the addition of constraints typically hurts rather than helping ML performance. We draw inspiration from biology to show that combining 1) a content-addressable heteroassociative memory based on the entorhinal-hippocampal circuit with grid cells that retain shared across-environment structural representations and hippocampal cells that acquire environment-specific information; 2) a spatially invariant convolutional network architecture for rapid adaptation across unfamiliar environments; and 3) the ability to perform remapping, which orthogonalizes internal representations; leads to good generalization, rapid learning, and continual learning without forgetting, respectively. Our model outperforms ANN baselines from continual learning contexts applied to the task. It retains knowledge of past environments while rapidly acquiring the skills to navigate new ones, thereby addressing the seemingly opposing challenges of quick knowledge transfer and sustaining proficiency in previously learned tasks. These biologically motivated results may point the way toward ML algorithms with similar properties.",
      "authors": [
        "Raymond Wang",
        "Jaedong Hwang",
        "Akhilan Boopathy",
        "Ila R Fiete"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=i9C4Kwm56G",
      "cdate": 1706827293293,
      "mdate": 1719287268561,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527798"
    },
    {
      "id": "awo5H10K6v",
      "title": "Language-guided Skill Learning with Temporal Variational Inference",
      "abstract": "We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.",
      "authors": [
        "Haotian Fu",
        "Pratyusha Sharma",
        "Elias Stengel-Eskin",
        "George Konidaris",
        "Nicolas Le Roux",
        "Marc-Alexandre Côté",
        "Xingdi Yuan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=awo5H10K6v",
      "cdate": 1706827140319,
      "mdate": 1719287268558,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527803"
    },
    {
      "id": "SE20BFqj6J",
      "title": "D-Flow: Differentiating through Flows for Controlled Generation",
      "abstract": "Taming the generation outcome of state of the art Diffusion and Flow-Matching (FM) models without having to re-train a task-specific model unlocks a powerful tool for solving inverse problems, conditional generation, and controlled generation in general. In this work we introduce *D-Flow*, a simple framework for controlling the generation process by differentiating through the flow, optimizing for the source (noise) point. We motivate this framework by our key observation stating that for Diffusion/FM models trained with Gaussian probability paths, differentiating through the generation process projects gradient on the data manifold, implicitly injecting the prior into the optimization process. We validate our framework on linear and non-linear controlled generation problems including: image and audio inverse problems and conditional molecule generation reaching state of the art performance across all.",
      "authors": [
        "Heli Ben-Hamu",
        "Omri Puny",
        "Itai Gat",
        "Brian Karrer",
        "Uriel Singer",
        "Yaron Lipman"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=SE20BFqj6J",
      "cdate": 1706827128731,
      "mdate": 1719287268532,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527808"
    },
    {
      "id": "Q3104y8djk",
      "title": "CogBench: a large language model walks into a psychology lab",
      "abstract": "Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces *CogBench*, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs’ behavior. We apply *CogBench* to 40 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior. Finally, we explore the effects of prompt-engineering techniques. We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors.",
      "authors": [
        "Julian Coda-Forno",
        "Marcel Binz",
        "Jane X Wang",
        "Eric Schulz"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Q3104y8djk",
      "cdate": 1706826995211,
      "mdate": 1719287268481,
      "matched_keywords": [
        "reinforcement learning",
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527813"
    },
    {
      "id": "amRSBdZlw9",
      "title": "Position: Why Tabular Foundation Models Should Be a Research Priority",
      "abstract": "Recent text and image foundation models are incredibly impressive, and these models are attracting an ever-increasing portion of research resources. In this position piece we aim to shift the ML research community's priorities ever so slightly to a different modality: tabular data. Tabular data is the dominant modality in many fields, yet it is given hardly any research attention and significantly lags behind in terms of scale and power. **We believe the time is now to start developing tabular foundation models**, or what we coin a _Large Tabular Model_ (LTM). LTMs could revolutionise the way science and ML use tabular data: not as single datasets that are analyzed in a vacuum, but contextualized with respect to related datasets. The potential impact is far-reaching: from few-shot tabular models to automating data science; from out-of-distribution synthetic data to empowering multidisciplinary scientific discovery. We intend to excite reflections on the modalities we study, and convince some researchers to study Large Tabular Models.",
      "authors": [
        "Boris van Breugel",
        "Mihaela van der Schaar"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=amRSBdZlw9",
      "cdate": 1706826592002,
      "mdate": 1719287268452,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527818"
    },
    {
      "id": "o2ND9v0CeK",
      "title": "Interpreting and Improving Diffusion Models from an Optimization Perspective",
      "abstract": "Denoising is intuitively related to projection. Indeed, under the manifold hypothesis, adding random noise is approximately equivalent to orthogonal perturbation. Hence, learning to denoise is approximately learning to project. In this paper, we use this observation to interpret denoising diffusion models as approximate gradient descent applied to the Euclidean distance function. We then provide straight-forward convergence analysis of the DDIM sampler under simple assumptions on the projection error of the denoiser. Finally, we propose a new gradient-estimation sampler, generalizing DDIM using insights from our theoretical results. In as few as 5-10 function evaluations, our sampler achieves state-of-the-art FID scores on pretrained CIFAR-10 and CelebA models and can generate high quality samples on latent diffusion models.",
      "authors": [
        "Frank Permenter",
        "Chenyang Yuan"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=o2ND9v0CeK",
      "cdate": 1706826067196,
      "mdate": 1719287268287,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527823"
    },
    {
      "id": "pfnBLXgFVS",
      "title": "A General Online Algorithm for Optimizing Complex Performance Metrics",
      "abstract": "We consider sequential maximization of performance metrics that are general functions of a confusion matrix of a classifier (such as precision, F-measure, or G-mean). Such metrics are, in general, non-decomposable over individual instances, making their optimization very challenging. While they have been extensively studied under different frameworks in the batch setting, their analysis in the online learning regime is very limited, with only a few distinguished exceptions. In this paper, we introduce and analyze a general online algorithm that can be used in a straightforward way with a variety of complex performance metrics in binary, multi-class, and multi-label classification problems. The algorithm's update and prediction rules are appealingly simple and computationally efficient without the need to store any past data. We show the algorithm attains $\\mathcal{O}(\\frac{\\ln n}{n})$ regret for concave and smooth metrics and verify the efficiency of the proposed algorithm in empirical studies.",
      "authors": [
        "Wojciech Kotlowski",
        "Marek Wydmuch",
        "Erik Schultheis",
        "Rohit Babbar",
        "Krzysztof Dembczynski"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pfnBLXgFVS",
      "cdate": 1706826056576,
      "mdate": 1719287268256,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527828"
    },
    {
      "id": "KJL2b6BthC",
      "title": "Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models",
      "abstract": "Current literature, aiming to surpass the \"Chain-of-Thought\" approach, often resorts to external modi operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. Due to their *myopic perspective*, they escalate the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the *Algorithm of Thoughts*---a novel strategy that propels LLMs through algorithmic reasoning pathways. By employing algorithmic examples fully in-context, this overarching view of the whole process exploits the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and even more recent multi-query strategies that employ an extensive tree search algorithms while using significantly fewer tokens. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting at LLM's inherent ability to weave its intuition into optimized searches. We probe into the underpinnings of our method's efficacy and its nuances in application. The code and related content can be found in: https://algorithm-of-thoughts.github.io",
      "authors": [
        "Bilgehan Sel",
        "Ahmad Tawaha",
        "Vanshaj Khattar",
        "Ruoxi Jia",
        "Ming Jin"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=KJL2b6BthC",
      "cdate": 1706825909353,
      "mdate": 1719287268219,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527833"
    },
    {
      "id": "bWZKvF0g7G",
      "title": "Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models",
      "abstract": "Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks. Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning LLM. To address this issue, we first curate a vision-language safe instruction-following dataset VLGuard covering various harmful categories. Our experiments demonstrate that integrating this dataset into standard vision-language fine-tuning or utilizing it for post-hoc fine-tuning effectively safety aligns VLLMs. This alignment is achieved with minimal impact on, or even enhancement of, the models' helpfulness. The versatility of our safety fine-tuning dataset makes it a valuable resource for safety-testing existing VLLMs, training new models or safeguarding pre-trained VLLMs. Empirical results demonstrate that fine-tuned VLLMs effectively reject unsafe instructions and substantially reduce the success rates of several black-box adversarial attacks, which approach zero in many cases. The code and dataset will be open-sourced.",
      "authors": [
        "Yongshuo Zong",
        "Ondrej Bohdal",
        "Tingyang Yu",
        "Yongxin Yang",
        "Timothy Hospedales"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=bWZKvF0g7G",
      "cdate": 1706825677508,
      "mdate": 1719287268142,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527841"
    },
    {
      "id": "tASXcrMekp",
      "title": "MADA: Meta-Adaptive Optimizers Through Hyper-Gradient Descent",
      "abstract": "Following the introduction of Adam, several novel adaptive optimizers for deep learning have been proposed. These optimizers typically excel in some tasks but may not outperform Adam uniformly across all tasks. In this work, we introduce Meta-Adaptive Optimizers (MADA), a unified optimizer framework that can generalize several known optimizers and dynamically learn the most suitable one during training. The key idea in MADA is to parameterize the space of optimizers and dynamically search through it using hyper-gradient descent during training. We empirically compare MADA to other popular optimizers on vision and language tasks, and find that MADA consistently outperforms Adam and other popular optimizers, and is robust against sub-optimally tuned hyper-parameters. MADA achieves a greater validation performance improvement over Adam compared to other popular optimizers during GPT-2 training and fine-tuning. We also propose AVGrad, a modification of AMSGrad that replaces the maximum operator with averaging, which is more suitable for hyper-gradient optimization. Finally, we provide a convergence analysis to show that parameterized interpolations of optimizers can improve their error bounds (up to constants), hinting at an advantage for meta-optimizers.",
      "authors": [
        "Kaan Ozkara",
        "Can Karakus",
        "Parameswaran Raman",
        "Mingyi Hong",
        "Shoham Sabach",
        "Branislav Kveton",
        "Volkan Cevher"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tASXcrMekp",
      "cdate": 1706825115739,
      "mdate": 1719287268017,
      "matched_keywords": [
        "deep learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527847"
    },
    {
      "id": "NsHxeSCtgr",
      "title": "LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models",
      "abstract": "Large language models (LLMs) have achieved impressive performance on various natural language generation tasks. Nonetheless, they suffer from generating negative and harmful contents that are biased against certain demographic groups (e.g., female), raising severe fairness concerns. As remedies, prior works intervened the generation by removing attitude or demographic information, inevitably degrading the generation quality and resulting in notable *fairness-fluency* trade-offs. However, it is still under-explored to what extent the fluency *has to* be affected in order to achieve a desired level of fairness. In this work, we conduct the first formal study from an information-theoretic perspective. We show that previous approaches are excessive for debiasing and propose LIDAO, a general framework to debias a (L)LM at a better fluency provably. We further robustify LIDAO in adversarial scenarios, where a carefully-crafted prompt may stimulate LLMs exhibiting instruction-following abilities to generate texts with fairness issue appears only when the prompt is also taken into account. Experiments on three LMs ranging from 0.7B to 7B parameters demonstrate the superiority of our method.",
      "authors": [
        "Tianci Liu",
        "Haoyu Wang",
        "Shiyang Wang",
        "Yu Cheng",
        "Jing Gao"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NsHxeSCtgr",
      "cdate": 1706824946478,
      "mdate": 1719287267980,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527852"
    },
    {
      "id": "LJcIIhqGDN",
      "title": "Successor Features for Efficient Multi-Subject Controlled Text Generation",
      "abstract": "While large language models (LLMs) have achieved impressive performance in generating fluent and realistic text, controlling the generated text so that it exhibits properties such as safety, factuality, and non-toxicity remains challenging. Existing decoding-based controllable text generation methods are static in terms of the dimension of control; if the target subject is changed, they require new training. Moreover, it can quickly become prohibitive to concurrently control multiple subjects. To address these challenges, we first show that existing methods can be framed as a reinforcement learning problem, where an action-value function estimates the likelihood of a desired attribute appearing in the generated text. Then, we introduce a novel approach named SF-Gen, which leverages the concept of successor features to decouple the dynamics of LLMs from task-specific rewards. By employing successor features, our method proves to be memory-efficient and computationally efficient for both training and decoding, especially when dealing with multiple target subjects. To the best of our knowledge, our research represents the first application of successor features in text generation. In addition to its computational efficiency, the resultant language produced by our method is comparable to the SOTA (and outperforms baselines) in both control measures as well as language quality, which we demonstrate through a series of experiments in various controllable text generation tasks.",
      "authors": [
        "Meng Cao",
        "Mehdi Fatemi",
        "Jackie CK Cheung",
        "Samira Shabanian"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LJcIIhqGDN",
      "cdate": 1706824861459,
      "mdate": 1719287267927,
      "matched_keywords": [
        "reinforcement learning",
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527858"
    },
    {
      "id": "lVQ4FUZ6dp",
      "title": "Generalization to New Sequential Decision Making Tasks with In-Context Learning",
      "abstract": "Training autonomous agents that can learn new tasks from only a handful of demonstrations is a long-standing problem in machine learning. Recently, transformers have been shown to learn new language or vision tasks without any weight updates from only a few examples, also referred to as in-context learning. However, the sequential decision making setting poses additional challenges having a lower tolerance for errors since the environment's stochasticity or the agent's actions can lead to unseen, and sometimes unrecoverable, states. In this paper, we use an illustrative example to show that naively applying transformers to sequential decision making problems does not enable in-context learning of new tasks. We then demonstrate how training on sequences of trajectories with certain distributional properties leads to in-context learning of new sequential decision making tasks. We investigate different design choices and find that larger model and dataset sizes, as well as more task diversity, environment stochasticity, and trajectory burstiness, all result in better in-context learning of new out-of-distribution tasks. By training on large diverse offline datasets, our model is able to learn new MiniHack and Procgen tasks without any weight updates from just a handful of demonstrations.",
      "authors": [
        "Sharath Chandra Raparthy",
        "Eric Hambro",
        "Robert Kirk",
        "Mikael Henaff",
        "Roberta Raileanu"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lVQ4FUZ6dp",
      "cdate": 1706824777381,
      "mdate": 1719287267927,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527863"
    },
    {
      "id": "rVWsTjMW1m",
      "title": "Coactive Learning for Large Language Models using Implicit User Feedback",
      "abstract": "We propose coactive learning as a model and feedback mechanism for training large language models (LLMs). The key insight is that users provide implicit feedback whenever they edit the text $y$ proposed by an LLM. While the edited text $\\bar y$ is typically not a gold-standard example for supervised training, coactive learning merely requires that the edited text $\\bar y$ is an improvement over the proposed text $y$. Note that such weak implicit preference feedback $\\bar y \\succ y$ is available in many application settings on a per-user basis, thus enabling the personalization of LLMs. In this paper, we develop the theoretical basis for coactive training of non-linear models, and we derive CoRLL as the first coactive learning algorithm for LLMs. Empirical results indicate that CoRLL is effective even for weak and noisy coactive preference feedback, making it a promising algorithm for training and personalization of LLMs from feedback that is naturally collected in many use cases.",
      "authors": [
        "Aaron David Tucker",
        "Kianté Brantley",
        "Adam Cahall",
        "Thorsten Joachims"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rVWsTjMW1m",
      "cdate": 1706824647625,
      "mdate": 1719287267907,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527868"
    },
    {
      "id": "ENNGAY5uKC",
      "title": "SuDA: Support-based Domain Adaptation for Sim2Real Hinge Joint Tracking with Flexible Sensors",
      "abstract": "Flexible sensors hold promise for human motion capture (MoCap), offering advantages such as wearability, privacy preservation, and minimal constraints on natural movement. However, existing flexible sensor-based MoCap methods rely on deep learning and necessitate large and diverse labeled datasets for training. These data typically need to be collected in MoCap studios with specialized equipment and substantial manual labor, making them difficult and expensive to obtain at scale. Thanks to the high-linearity of flexible sensors, we address this challenge by proposing a novel Sim2Real solution for hinge joint tracking based on domain adaptation, eliminating the need for labeled data yet achieving comparable accuracy to supervised learning. Our solution relies on a novel Support-based Domain Adaptation method, namely SuDA, which aligns the supports of the predictive functions rather than the instance-dependent distributions between the source and target domains. Extensive experimental results demonstrate the effectiveness of our method and its superiority overstate-of-the-art distribution-based domain adaptation methods in our task.",
      "authors": [
        "Fang Jiawei",
        "Haishan song",
        "Chengxu Zuo",
        "Xiaoxia Gao",
        "Xiaowei Chen",
        "Shihui Guo",
        "Yipeng Qin"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ENNGAY5uKC",
      "cdate": 1706824409341,
      "mdate": 1719287267828,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527874"
    },
    {
      "id": "japBn31gXC",
      "title": "Simple Ingredients for Offline Reinforcement Learning",
      "abstract": "Offline reinforcement learning algorithms have proven effective on datasets highly connected to the target downstream task. Yet, by leveraging a novel testbed (MOOD) in which trajectories come from heterogeneous sources, we show that existing methods struggle with diverse data: their performance considerably deteriorates as data collected for related but different tasks is simply added to the offline buffer. In light of this finding, we conduct a large empirical study where we formulate and test several hypotheses to explain this failure. Surprisingly, we find that targeted scale, more than algorithmic considerations, is the key factor influencing performance. We show that simple methods like AWAC and IQL with increased policy size overcome the paradoxical failure modes from the inclusion of additional data in MOOD, and notably outperform prior state-of-the-art algorithms on the canonical D4RL benchmark.",
      "authors": [
        "Edoardo Cetin",
        "Andrea Tirinzoni",
        "Matteo Pirotta",
        "Alessandro Lazaric",
        "Yann Ollivier",
        "Ahmed Touati"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=japBn31gXC",
      "cdate": 1706824375731,
      "mdate": 1719287267829,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527879"
    },
    {
      "id": "pspyQm4ko0",
      "title": "The Balanced-Pairwise-Affinities Feature Transform",
      "abstract": "The Balanced-Pairwise-Affinities (BPA) feature transform is designed to upgrade the features of a set of input items to facilitate downstream matching or grouping related tasks. The transformed set encodes a rich representation of high order relations between the input features. A particular min-cost-max-flow fractional matching problem, whose entropy regularized version can be approximated by an optimal transport (OT) optimization, leads to a transform which is efficient, differentiable, equivariant, parameterless and probabilistically interpretable. While the Sinkhorn OT solver has been adapted extensively in many contexts, we use it differently by minimizing the cost between a set of features to *itself* and using the transport plan's *rows* as the new representation.Empirically, the transform is highly effective and flexible in its use and consistently improves networks it is inserted into, in a variety of tasks and training schemes. We demonstrate state-of-the-art results in few-shot classification, unsupervised image clustering and person re-identification. Code is available at github.com/DanielShalam/BPA .",
      "authors": [
        "Daniel Shalam",
        "Simon Korman"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pspyQm4ko0",
      "cdate": 1706824345968,
      "mdate": 1719287267793,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527884"
    },
    {
      "id": "AIXUuLCuMe",
      "title": "Position: Stop Making Unscientific AGI Performance Claims",
      "abstract": "Developments in the field of Artificial Intelligence (AI), and particularly large language models (LLMs), have created a 'perfect storm’ for observing 'sparks’ of Artificial General Intelligence (AGI) that are spurious. Like simpler models, LLMs distill meaningful representations in their latent embeddings that have been shown to correlate with external variables. Nonetheless, the correlation of such representations has often been linked to human-like intelligence in the latter but not the former. We probe models of varying complexity including random projections, matrix decompositions, deep autoencoders and transformers: all of them successfully distill information that can be used to predict latent or external variables and yet none of them have previously been linked to AGI. We argue and empirically demonstrate that the finding of meaningful patterns in latent spaces of models cannot be seen as evidence in favor of AGI. Additionally, we review literature from the social sciences that shows that humans are prone to seek such patterns and anthropomorphize. We conclude that both the methodological setup and common public image of AI are ideal for the misinterpretation that correlations between model representations and some variables of interest are 'caused' by the model's understanding of underlying 'ground truth’ relationships. We, therefore, call for the academic community to exercise extra caution, and to be keenly aware of principles of academic integrity, in interpreting and communicating about AI research outcomes.",
      "authors": [
        "Patrick Altmeyer",
        "Andrew M. Demetriou",
        "Antony Bartlett",
        "Cynthia C. S. Liem"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=AIXUuLCuMe",
      "cdate": 1706824216815,
      "mdate": 1719287267689,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527890"
    },
    {
      "id": "GC8HkKeH8s",
      "title": "DsDm: Model-Aware Dataset Selection with Datamodels",
      "abstract": "When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with \"high quality\" data sources may not increase (and can even hurt) performance compared to randomly selecting data. To develop better methods for selecting data, we start by framing dataset selection as an optimization problem that we can directly solve for: given target tasks, a learning algorithm, and candidate data, select the subset that maximizes model performance. This framework thus avoids handpicked notions of data quality, and instead models explicitly how the learning process uses train datapoints to predict on the target tasks. Our resulting method greatly improves language model (LM) performance on both pre-specified tasks and previously unseen tasks. Specifically, choosing target tasks representative of standard LM problems and evaluating on diverse held-out benchmarks, our selected datasets provide a 2x compute multiplier over baseline methods.",
      "authors": [
        "Logan Engstrom",
        "Axel Feldmann",
        "Aleksander Madry"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GC8HkKeH8s",
      "cdate": 1706824103661,
      "mdate": 1719287267638,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527896"
    },
    {
      "id": "eCCaHZKdl4",
      "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation",
      "abstract": "Assessing response quality to instructions in language models is vital but challenging due to the complexity of human language across different contexts. This complexity often results in ambiguous or inconsistent interpretations, making accurate assessment difficult. To address this issue, we propose a novel Uncertainty-aware Reward Model (URM) that introduces a robust uncertainty estimation for the quality of paired responses based on Bayesian approximation. Trained with preference datasets, our uncertainty-enabled proxy not only scores rewards for responses but also evaluates their inherent uncertainty. Empirical results demonstrate significant benefits of incorporating the proposed proxy into language model training. Our method boosts the instruction following capability of language models by refining data curation for training and improving policy optimization objectives, thereby surpassing existing methods by a large margin on benchmarks such as Vicuna and MT-bench. These findings highlight that our proposed approach substantially advances language model training and paves a new way of harnessing uncertainty within language models.",
      "authors": [
        "JoonHo Lee",
        "Jae Oh Woo",
        "Juree Seok",
        "Parisa Hassanzadeh",
        "Wooseok Jang",
        "JuYoun Son",
        "Sima Didari",
        "Baruch Gutow",
        "Heng Hao",
        "Hankyu Moon",
        "Wenjun Hu",
        "Yeong-Dae Kwon",
        "Taehee Lee",
        "Seungjai Min"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=eCCaHZKdl4",
      "cdate": 1706823839564,
      "mdate": 1719287267620,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527902"
    },
    {
      "id": "66k81s33p3",
      "title": "Towards Efficient Exact Optimization of Language Model Alignment",
      "abstract": "The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. However, we show that DPO derived based on the optimal solution of the problem leads to a compromised mean-seeking approximation of the optimal solution in practice. In this paper, we propose efficient exact optimization (EXO) of the alignment objective. EXO is guaranteed to optimize in the same direction as RL algorithms asymptotically for arbitrary policy parametrization. This leads to the same mode-seeking solution, while enables efficient optimization by circumventing the complexities of RL. We also compare our method to DPO with both theoretical and empirical analyses, and further demonstrate the advantages of our method over existing approaches on realistic human preference data. Code is available at https://github.com/haozheji/exact-optimization.",
      "authors": [
        "Haozhe Ji",
        "Cheng Lu",
        "Yilin Niu",
        "Pei Ke",
        "Hongning Wang",
        "Jun Zhu",
        "Jie Tang",
        "Minlie Huang"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=66k81s33p3",
      "cdate": 1706823788612,
      "mdate": 1719287267595,
      "matched_keywords": [
        "reinforcement learning",
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527908"
    },
    {
      "id": "Wnhp34K5jR",
      "title": "Universal Gradient Methods for Stochastic Convex Optimization",
      "abstract": "We develop universal gradient methods for Stochastic Convex Optimization (SCO). Our algorithms automatically adapt not only to the oracle's noise but also to the Hölder smoothness of the objective function without a priori knowledge of the particular setting. The key ingredient is a novel strategy for adjusting step-size coefficients in the Stochastic Gradient Method (SGD). Unlike AdaGrad, which accumulates gradient norms, our Universal Gradient Method accumulates appropriate combinations of gradientand iterate differences. The resulting algorithm has state-of-the-art worst-case convergence rate guarantees for the entire Hölder class including, in particular, both nonsmooth functions and those with Lipschitz continuous gradient. We also present the Universal Fast Gradient Method for SCO enjoying optimal efficiency estimates.",
      "authors": [
        "Anton Rodomanov",
        "Ali Kavis",
        "Yongtao Wu",
        "Kimon Antonakopoulos",
        "Volkan Cevher"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Wnhp34K5jR",
      "cdate": 1706823704024,
      "mdate": 1719287267537,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527913"
    },
    {
      "id": "CduFAALvGe",
      "title": "Learning Iterative Reasoning through Energy Diffusion",
      "abstract": "We introduce iterative reasoning through energy diffusion (IRED), a novel framework for learning to reason for a variety of tasks by formulating reasoning and decision-making problems with energy-based optimization. IRED learns energy functions to represent the constraints between input conditions and desired outputs. After training, IRED adapts the number of optimization steps during inference based on problem difficulty, enabling it to solve problems outside its training distribution --- such as more complex Sudoku puzzles, matrix completion with large value magnitudes, and path finding in larger graphs. Key to our method’s success is two novel techniques: learning a sequence of annealed energy landscapes for easier inference and a combination of score function and energy landscape supervision for faster and more stable training. Our experiments show that IRED outperforms existing methods in continuous-space reasoning, discrete-space reasoning, and planning tasks, particularly in more challenging scenarios.",
      "authors": [
        "Yilun Du",
        "Jiayuan Mao",
        "Joshua B. Tenenbaum"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=CduFAALvGe",
      "cdate": 1706823530368,
      "mdate": 1719287267486,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527918"
    },
    {
      "id": "nP7Q1PnuLK",
      "title": "Thermometer: Towards Universal Calibration for Large Language Models",
      "abstract": "We consider the issue of calibration in large language models (LLM). Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs. Although calibration is well-explored in traditional applications, calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks. Addressing these challenges, we propose THERMOMETER, a calibration approach tailored to LLMs. THERMOMETER learns an auxiliary model, given data from multiple tasks, for calibrating a LLM. It is computationally efficient, preserves the accuracy of the LLM, and produces better-calibrated responses for new tasks. Extensive empirical evaluations across various benchmarks demonstrate the effectiveness of the proposed method.",
      "authors": [
        "Maohao Shen",
        "Subhro Das",
        "Kristjan Greenewald",
        "Prasanna Sattigeri",
        "Gregory W. Wornell",
        "Soumya Ghosh"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nP7Q1PnuLK",
      "cdate": 1706823421933,
      "mdate": 1719287267444,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527923"
    },
    {
      "id": "iLyUEPZ0fR",
      "title": "Block Acceleration Without Momentum: On Optimal Stepsizes of Block Gradient Descent for Least-Squares",
      "abstract": "Block coordinate descent is a powerful algorithmic template suitable for big data optimization. This template admits a lot of variants including block gradient descent (BGD), which performs gradient descent on a selected block of variables, while keeping other variables fixed. For a very long time, the stepsize for each block has tacitly been set to one divided by the block-wise Lipschitz smoothness constant, imitating the vanilla stepsize rule for gradient descent (GD). However, such a choice for BGD has not yet been able to theoretically justify its empirical superiority over GD, as existing convergence rates for BGD have worse constants than GD in the deterministic cases. To discover such theoretical justification, we set up a simple environment where we consider BGD applied to least-squares with two blocks of variables. Assuming the data matrix corresponding to each block is orthogonal, we find optimal stepsizes of BGD in closed form, which provably lead to asymptotic convergence rates twice as fast as GD with Polyak's momentum; this means, under that orthogonality assumption, one can accelerate BGD by just tuning stepsizes and without adding any momentum. An application that satisfies this assumption is *generalized alternating projection* between two subspaces, and applying our stepsizes to it improves the prior convergence rate that was once claimed, slightly inaccurately, to be optimal. The main proof idea is to minimize, in stepsize variables, the spectral radius of a matrix that controls convergence rates.",
      "authors": [
        "Liangzu Peng",
        "Wotao Yin"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iLyUEPZ0fR",
      "cdate": 1706823229984,
      "mdate": 1719287267398,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527929"
    },
    {
      "id": "BiWIERWBFX",
      "title": "Efficient World Models with Context-Aware Tokenization",
      "abstract": "Scaling up deep Reinforcement Learning (RL) methods presents a significant challenge. Following developments in generative modelling, model-based RL positions itself as a strong contender. Recent advances in sequence modelling have led to effective transformer-based world models, albeit at the price of heavy computations due to the long sequences of tokens required to accurately simulate environments. In this work, we propose $\\Delta$-IRIS, a new agent with a world model architecture composed of a discrete autoencoder that encodes stochastic deltas between time steps and an autoregressive transformer that predicts future deltas by summarizing the current state of the world with continuous tokens. In the Crafter benchmark, $\\Delta$-IRIS sets a new state of the art at multiple frame budgets, while being an order of magnitude faster to train than previous attention-based approaches. We release our code and models at https://github.com/vmicheli/delta-iris.",
      "authors": [
        "Vincent Micheli",
        "Eloi Alonso",
        "François Fleuret"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BiWIERWBFX",
      "cdate": 1706823115959,
      "mdate": 1719287267370,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527935"
    },
    {
      "id": "seo9V9QRZp",
      "title": "In value-based deep reinforcement learning, a pruned network is a good network",
      "abstract": "Recent work has shown that deep reinforcement learning agents have difficulty in effectively using their network parameters. We leverage prior insights into the advantages of sparse training techniques and demonstrate that gradual magnitude pruning enables value-based agents to maximize parameter effectiveness. This results in networks that yield dramatic performance improvements over traditional networks, using only a small fraction of the full network parameters. Our code is publicly available, see Appendix A for details.",
      "authors": [
        "Johan Samir Obando Ceron",
        "Aaron Courville",
        "Pablo Samuel Castro"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=seo9V9QRZp",
      "cdate": 1706822442399,
      "mdate": 1719287267229,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527941"
    },
    {
      "id": "kkqIEp2bRa",
      "title": "Differentially Private Domain Adaptation with Theoretical Guarantees",
      "abstract": "In many applications, the labeled data at the learner's disposal is subject to privacy constraints and is relatively limited. To derive a more accurate predictor for the target domain, it is often beneficial to leverage publicly available labeled data from an alternative domain, somewhat close to the target domain. This is the modern problem of supervised domain adaptation from a public source to a private target domain. We present two $(\\epsilon, \\delta)$-differentially private adaptation algorithms for supervised adaptation, for which we make use of a general optimization problem, recently shown to benefit from favorable theoretical learning guarantees. Our first algorithm is designed for regression with linear predictors and shown to solve a convex optimization problem. Our second algorithm is a more general solution for loss functions that may be non-convex but Lipschitz and smooth. While our main objective is a theoretical analysis, we also report the results of several experiments. We first show that the non-private versions of our algorithms match state-of-the-art performance in supervised adaptation and that for larger values of the target sample size or $\\epsilon$, the performance of our private algorithms remains close to that of their non-private counterparts.",
      "authors": [
        "Raef Bassily",
        "Corinna Cortes",
        "Anqi Mao",
        "Mehryar Mohri"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kkqIEp2bRa",
      "cdate": 1706822383754,
      "mdate": 1719287267198,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527946"
    },
    {
      "id": "mXUDDL4r1Q",
      "title": "Reinforcement Learning from Reachability Specifications: PAC Guarantees with Expected Conditional Distance",
      "abstract": "Reinforcement Learning (RL) from temporal logical specifications is a fundamental problem in sequential decision making. One of the basic and core such specification is the reachability specification that requires a target set to be eventually visited. Despite strong empirical results for RL from such specifications, the theoretical guarantees are bleak, including the impossibility of Probably Approximately Correct (PAC) guarantee for reachability specifications. Given the impossibility result, in this work we consider the problem of RL from reachability specifications along with the information of expected conditional distance (ECD). We present (a) lower bound results which establish the necessity of ECD information for PAC guarantees and (b) an algorithm that establishes PAC-guarantees given the ECD information. To the best of our knowledge, this is the first RL from reachability specifications that does not make any assumptions on the underlying environment to learn policies.",
      "authors": [
        "Jakub Svoboda",
        "Suguman Bansal",
        "Krishnendu Chatterjee"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=mXUDDL4r1Q",
      "cdate": 1706822344762,
      "mdate": 1719287267198,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527952"
    },
    {
      "id": "ADnUzsmsLW",
      "title": "Do Large Code Models Understand Programming Concepts? Counterfactual Analysis for Code Predicates",
      "abstract": "Large Language Models' success in text generation has also made them better at code generation and coding tasks. While a lot of work has demonstrated their remarkable performance on tasks such as code completion and editing, it is still unclear as to why. We help bridge this gap by exploring to what degree auto-regressive models understand the logical constructs of the underlying programs. We propose Counterfactual Analysis for Programming Concept Predicates (CACP) as a counterfactual testing framework to evaluate whether Large Code Models understand programming concepts. With only black-box access to the model, we use CACP to evaluate ten popular Large Code Models for four different programming concepts. Our findings suggest that current models lack understanding of concepts such as data flow and control flow.",
      "authors": [
        "Ashish Hooda",
        "Mihai Christodorescu",
        "Miltiadis Allamanis",
        "Aaron Wilson",
        "Kassem Fawaz",
        "Somesh Jha"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ADnUzsmsLW",
      "cdate": 1706822226840,
      "mdate": 1719287267160,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:12.527957"
    },
    {
      "id": "EruV94XRDs",
      "title": "MusicRL: Aligning Music Generation to Human Preferences",
      "abstract": "We propose MusicRL, the first music generation system finetuned from human feedback. Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as “upbeat workout music” can map to a retro guitar solo or a technopop beat). Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning. MusicRL is a pretrained autoregressive [MusicLM](https://arxiv.org/abs/2301.11325) model of discrete audio tokens finetuned with reinforcement learning to maximize sequence-level rewards. We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning from Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model that incorporates human feedback at scale. Human evaluations show that both MusicRL-R and MusicRL-U are preferred to the baseline. Ultimately, MusicRL-RU combines the two approaches and results in the best model according to human raters. Ablation studies shed light on the musical attributes influencing human preferences, indicating that text adherence and quality only account for a part of it. This underscores the prevalence of subjectivity in musical appreciation and calls for further involvement of human listeners in the finetuning of music generation models. Samples can be found at google-research.github.io/seanet/musiclm/rlhf/.",
      "authors": [
        "Geoffrey Cideron",
        "Sertan Girgin",
        "Mauro Verzetti",
        "Damien Vincent",
        "Matej Kastelic",
        "Zalán Borsos",
        "Brian McWilliams",
        "Victor Ungureanu",
        "Olivier Bachem",
        "Olivier Pietquin",
        "Matthieu Geist",
        "Leonard Hussenot",
        "Neil Zeghidour",
        "Andrea Agostinelli"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EruV94XRDs",
      "cdate": 1706821889105,
      "mdate": 1719287267048,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527963"
    },
    {
      "id": "ABt0jlLZtX",
      "title": "Learning Optimal Deterministic Policies with Stochastic Policy Gradients",
      "abstract": "Policy gradient (PG) methods are successful approaches to deal with continuous reinforcement learning (RL) problems. They learn stochastic parametric (hyper)policies by either exploring in the space of actions or in the space of parameters. Stochastic controllers, however, are often undesirable from a practical perspective because of their lack of robustness, safety, and traceability. In common practice, stochastic (hyper)policies are learned only to deploy their deterministic version. In this paper, we make a step towards the theoretical understanding of this practice. After introducing a novel framework for modeling this scenario, we study the global convergence to the best deterministic policy, under (weak) gradient domination assumptions. Then, we illustrate how to tune the exploration level used for learning to optimize the trade-off between the sample complexity and the performance of the deployed deterministic policy. Finally, we quantitatively compare action-based and parameter-based exploration, giving a formal guise to intuitive results.",
      "authors": [
        "Alessandro Montenegro",
        "Marco Mussi",
        "Alberto Maria Metelli",
        "Matteo Papini"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ABt0jlLZtX",
      "cdate": 1706821570565,
      "mdate": 1719287266923,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527968"
    },
    {
      "id": "HkCRgoGtt6",
      "title": "Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT",
      "abstract": "Retrieval pipelines are an integral component of many machine learning systems. However, they perform poorly in domains where documents are long (e.g., 10K tokens or more) and where identifying the relevant document requires synthesizing information across the entire text. Developing long-context retrieval encoders suitable for these domains raises three challenges: (1) how to evaluate long-context retrieval performance, (2) how to pretrain a base language model to represent both short contexts (corresponding to queries) and long contexts (corresponding to documents), and (3) how to finetune this model for retrieval under the batch size limitations imposed by GPU memory constraints. To address these challenges, we first introduce LoCoV1, a 12 task benchmark constructed to measure long-context retrieval where chunking is not possible or not effective. We next present the M2-BERT retrieval encoder, an 80M parameter state-space encoder model built from the Monarch Mixer architecture, capable of scaling to documents up to 32K tokens long. We describe a pretraining data mixture which allows this encoder to process both short and long context sequences, and a finetuning approach that adapts this base model to retrieval with only single-sample batches. Finally, we validate the M2-BERT retrieval encoder on LoCoV1, finding that it outperforms competitive Transformer-based models by at least 22.2 points, despite containing 90× fewer parameters.",
      "authors": [
        "Jon Saad-Falcon",
        "Daniel Y Fu",
        "Simran Arora",
        "Neel Guha",
        "Christopher Re"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=HkCRgoGtt6",
      "cdate": 1706821527826,
      "mdate": 1719287266893,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527973"
    },
    {
      "id": "b6AwZauZPV",
      "title": "Probabilistic Subgoal Representations for Hierarchical Reinforcement Learning",
      "abstract": "In goal-conditioned hierarchical reinforcement learning (HRL), a high-level policy specifies a subgoal for the low-level policy to reach. Effective HRL hinges on a suitable subgoal representation function, abstracting state space into latent subgoal space and inducing varied low-level behaviors. Existing methods adopt a subgoal representation that provides a deterministic mapping from state space to latent subgoal space. Instead, this paper utilizes Gaussian Processes (GPs) for the first probabilistic subgoal representation. Our method employs a GP prior on the latent subgoal space to learn a posterior distribution over the subgoal representation functions while exploiting the long-range correlation in the state space through learnable kernels. This enables an adaptive memory that integrates long-range subgoal information from prior planning steps allowing to cope with stochastic uncertainties. Furthermore, we propose a novel learning objective to facilitate the simultaneous learning of probabilistic subgoal representations and policies within a unified framework. In experiments, our approach outperforms state-of-the-art baselines in standard benchmarks but also in environments with stochastic elements and under diverse reward conditions. Additionally, our model shows promising capabilities in transferring low-level policies across different tasks.",
      "authors": [
        "Vivienne Huiling Wang",
        "Tinghuai Wang",
        "Wenyan Yang",
        "Joni-kristian Kamarainen",
        "Joni Pajarinen"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=b6AwZauZPV",
      "cdate": 1706821470146,
      "mdate": 1719287266857,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527978"
    },
    {
      "id": "lrFwPeDdEQ",
      "title": "Federated Combinatorial Multi-Agent Multi-Armed Bandits",
      "abstract": "This paper introduces a federated learning framework tailored for online combinatorial optimization with bandit feedback. In this setting, agents select subsets of arms, observe noisy rewards for these subsets without accessing individual arm information, and can cooperate and share information at specific intervals. Our framework transforms any offline resilient single-agent $(\\alpha-\\epsilon)$-approximation algorithm—having a complexity of $\\tilde{\\mathcal{O}}\\left(\\frac{\\psi}{\\epsilon^\\beta}\\right)$, where the logarithm is omitted, for some function $\\psi$ and constant $\\beta$—into an online multi-agent algorithm with $m$ communicating agents and an $\\alpha$-regret of no more than $\\tilde{\\mathcal{O}}\\left(m^{-\\frac{1}{3+\\beta}} \\psi^\\frac{1}{3+\\beta} T^\\frac{2+\\beta}{3+\\beta}\\right)$. Our approach not only eliminates the $\\epsilon$ approximation error but also ensures sublinear growth with respect to the time horizon $T$ and demonstrates a linear speedup with an increasing number of communicating agents. Additionally, the algorithm is notably communication-efficient, requiring only a sublinear number of communication rounds, quantified as $\\tilde{\\mathcal{O}}\\left(\\psi T^\\frac{\\beta}{\\beta+1}\\right)$. Furthermore, the framework has been successfully applied to online stochastic submodular maximization using various offline algorithms, yielding the first results for both single-agent and multi-agent settings and recovering specialized single-agent theoretical guarantees. We empirically validate our approach to a stochastic data summarization problem, illustrating the effectiveness of the proposed framework, even in single-agent scenarios.",
      "authors": [
        "Fares Fourati",
        "Mohamed-Slim Alouini",
        "Vaneet Aggarwal"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lrFwPeDdEQ",
      "cdate": 1706820978927,
      "mdate": 1719287266780,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527984"
    },
    {
      "id": "VHO4nE7v41",
      "title": "Variance-reduced Zeroth-Order Methods for Fine-Tuning Language Models",
      "abstract": "Fine-tuning language models (LMs) has demonstrated success in a wide array of downstream tasks. However, as LMs are scaled up, the memory requirements for backpropagation become prohibitively high. Zeroth-order (ZO) optimization methods can leverage memory-efficient forward passes to estimate gradients. More recently, MeZO, an adaptation of ZO-SGD, has been shown to consistently outperform zero-shot and in-context learning when combined with suitable task prompts. In this work, we couple ZO methods with variance reduction techniques to enhance stability and convergence for inference-based LM fine-tuning. We introduce Memory-Efficient Zeroth-Order Stochastic Variance-Reduced Gradient (MeZO-SVRG) and demonstrate its efficacy across multiple LM fine-tuning tasks, eliminating the reliance on task-specific prompts. Evaluated across a range of both masked and autoregressive LMs on benchmark GLUE tasks, MeZO-SVRG outperforms MeZO with up to 20% increase in test accuracies in both full- and partial-parameter fine-tuning settings. MeZO-SVRG benefits from reduced computation time as it often surpasses MeZO's peak test accuracy with a $2\\times$ reduction in GPU-hours. MeZO-SVRG significantly reduces the required memory footprint compared to first-order SGD, i.e. by $2\\times$ for autoregressive models. Our experiments highlight that MeZO-SVRG's memory savings progressively improve compared to SGD with larger batch sizes.",
      "authors": [
        "Tanmay Gautam",
        "Youngsuk Park",
        "Hao Zhou",
        "Parameswaran Raman",
        "Wooseok Ha"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=VHO4nE7v41",
      "cdate": 1706820930176,
      "mdate": 1719287266760,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.527989"
    },
    {
      "id": "T0zR4mdSce",
      "title": "PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling",
      "abstract": "Modeling unsteady, fast transient, and advection-dominated physics problems is a pressing challenge for physics-aware deep learning (PADL). The physics of complex systems is governed by large systems of partial differential equations (PDEs) and ancillary constitutive models with nonlinear structures, as well as evolving state fields exhibiting sharp gradients and rapidly deforming material interfaces. Here, we investigate an inductive bias approach that is versatile and generalizable to model generic nonlinear field evolution problems. Our study focuses on the recent physics-aware recurrent convolutions (PARC), which incorporates a differentiator-integrator architecture that inductively models the spatiotemporal dynamics of generic physical systems. We extend the capabilities of PARC to simulate unsteady, transient, and advection-dominant systems. The extended model, referred to as PARCv2, is equipped with differential operators to model advection-reaction-diffusion equations, as well as a hybrid integral solver for stable, long-time predictions. PARCv2 is tested on both standard benchmark problems in fluid dynamics, namely Burgers and Navier-Stokes equations, and then applied to more complex shock-induced reaction problems in energetic materials. We evaluate the behavior of PARCv2 in comparison to other physics-informed and learning bias models and demonstrate its potential to model unsteady and advection-dominant dynamics regimes.",
      "authors": [
        "Phong C.H. Nguyen",
        "Xinlun Cheng",
        "Shahab Azarfar",
        "Pradeep Seshadri",
        "Yen T. Nguyen",
        "Munho Kim",
        "Sanghun Choi",
        "H.S. Udaykumar",
        "Stephen Baek"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=T0zR4mdSce",
      "cdate": 1706820711136,
      "mdate": 1719287266618,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.527996"
    },
    {
      "id": "fwxnHViGNj",
      "title": "Inherent Trade-Offs between Diversity and Stability in Multi-Task Benchmarks",
      "abstract": "We examine multi-task benchmarks in machine learning through the lens of social choice theory. We draw an analogy between benchmarks and electoral systems, where models are candidates and tasks are voters. This suggests a distinction between cardinal and ordinal benchmark systems. The former aggregate numerical scores into one model ranking; the latter aggregate rankings for each task. We apply Arrow's impossibility theorem to ordinal benchmarks to highlight the inherent limitations of ordinal systems, particularly their sensitivity to the inclusion of irrelevant models. Inspired by Arrow's theorem, we empirically demonstrate a strong trade-off between diversity and sensitivity to irrelevant changes in existing multi-task benchmarks. Our result is based on new quantitative measures of diversity and sensitivity that we introduce. Sensitivity quantifies the impact that irrelevant changes to tasks have on a benchmark. Diversity captures the degree of disagreement in model rankings across tasks. We develop efficient approximation algorithms for both measures, as exact computation is computationally challenging. Through extensive experiments on seven cardinal benchmarks and eleven ordinal benchmarks, we demonstrate a clear trade-off between diversity and stability: The more diverse a multi-task benchmark, the more sensitive to trivial changes it is. Additionally, we show that the aggregated rankings of existing benchmarks are highly unstable under irrelevant changes. The codes and data are available at https://socialfoundations.github.io/benchbench/.",
      "authors": [
        "Guanhua Zhang",
        "Moritz Hardt"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fwxnHViGNj",
      "cdate": 1706820557627,
      "mdate": 1719287266552,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.528003"
    },
    {
      "id": "uDoy7AGvEC",
      "title": "LayerMerge: Neural Network Depth Compression through Layer Pruning and Merging",
      "abstract": "Recent works show that reducing the number of layers in a convolutional neural network can enhance efficiency while maintaining the performance of the network. Existing depth compression methods remove redundant non-linear activation functions and merge the consecutive convolution layers into a single layer. However, these methods suffer from a critical drawback; the kernel size of the merged layers becomes larger, significantly undermining the latency reduction gained from reducing the depth of the network. We show that this problem can be addressed by jointly pruning convolution layers and activation functions. To this end, we propose *LayerMerge*, a novel depth compression method that selects which activation layers and convolution layers to remove, to achieve a desired inference speed-up while minimizing performance loss. Since the corresponding selection problem involves an exponential search space, we formulate a novel surrogate optimization problem and efficiently solve it via dynamic programming. Empirical results demonstrate that our method consistently outperforms existing depth compression and layer pruning methods on various network architectures, both on image classification and generation tasks. We release the code at https://github.com/snu-mllab/LayerMerge.",
      "authors": [
        "Jinuk Kim",
        "Marwa El Halabi",
        "Mingi Ji",
        "Hyun Oh Song"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=uDoy7AGvEC",
      "cdate": 1706820390417,
      "mdate": 1719298186091,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.528011"
    },
    {
      "id": "o6N1Bqay0k",
      "title": "How Spurious Features are Memorized: Precise Analysis for Random and NTK Features",
      "abstract": "Deep learning models are known to overfit and memorize spurious features in the training dataset. While numerous empirical studies have aimed at understanding this phenomenon, a rigorous theoretical framework to quantify it is still missing. In this paper, we consider spurious features that are uncorrelated with the learning task, and we provide a precise characterization of how they are memorized via two separate terms: _(i)_ the _stability_ of the model with respect to individual training samples, and _(ii)_ the _feature alignment_ between the spurious pattern and the full sample. While the first term is well established in learning theory and it is connected to the generalization error in classical work, the second one is, to the best of our knowledge, novel. Our key technical result gives a precise characterization of the feature alignment for the two prototypical settings of random features (RF) and neural tangent kernel (NTK) regression. We prove that the memorization of spurious features weakens as the generalization capability increases and, through the analysis of the feature alignment, we unveil the role of the model and of its activation function. Numerical experiments show the predictive power of our theory on standard datasets (MNIST, CIFAR-10).",
      "authors": [
        "Simone Bombari",
        "Marco Mondelli"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=o6N1Bqay0k",
      "cdate": 1706820378591,
      "mdate": 1719287266497,
      "matched_keywords": [
        "deep learning"
      ],
      "fetched_at": "2025-08-10T23:47:12.528017"
    },
    {
      "id": "PudBRuNa8r",
      "title": "Building Socially-Equitable Public Models",
      "abstract": "Public models offer predictions to a variety of downstream tasks and have played a crucial role in various AI applications, showcasing their proficiency in accurate predictions. However, the exclusive emphasis on prediction accuracy may not align with the diverse end objectives of downstream agents. Recognizing the public model's predictions as a service, we advocate for integrating the objectives of downstream agents into the optimization process. Concretely, to address performance disparities and foster fairness among heterogeneous agents in training, we propose a novel Equitable Objective. This objective, coupled with a policy gradient algorithm, is crafted to train the public model to produce a more equitable/uniform performance distribution across downstream agents, each with their unique concerns. Both theoretical analysis and empirical case studies have proven the effectiveness of our method in advancing performance equity across diverse downstream agents utilizing the public model for their decision-making. Codes and datasets are released at https://github.com/Ren-Research/Socially-Equitable-Public-Models.",
      "authors": [
        "Yejia Liu",
        "Jianyi Yang",
        "Pengfei Li",
        "Tongxin Li",
        "Shaolei Ren"
      ],
      "conference": "ICML 2024",
      "venue_id": "ICML.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=PudBRuNa8r",
      "cdate": 1706820036608,
      "mdate": 1719287266482,
      "matched_keywords": [
        "optimization"
      ],
      "fetched_at": "2025-08-10T23:47:12.528023"
    }
  ]
}