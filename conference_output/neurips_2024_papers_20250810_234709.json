{
  "conference": "NeurIPS 2024",
  "papers_count": 480,
  "timestamp": "2025-08-10T23:47:09.450525",
  "papers": [
    {
      "id": "iEeiZlTbts",
      "title": "No Regrets: Investigating and Improving Regret Approximations for Curriculum Discovery",
      "abstract": "What data or environments to use for training to improve downstream performance is a longstanding and very topical question in reinforcement learning. \nIn particular, Unsupervised Environment Design (UED) methods have gained recent attention as their adaptive curricula promise to enable agents to be robust to in- and out-of-distribution tasks.\nThis work investigates how existing UED methods select training environments, focusing on task prioritisation metrics.\nSurprisingly, despite methods aiming to maximise regret in theory, the practical approximations do not correlate with regret but with success rate.\nAs a result, a significant portion of an agent's experience comes from environments it has already mastered, offering little to no contribution toward enhancing its abilities. Put differently, current methods fail to predict intuitive measures of *learnability*. Specifically, they are unable to consistently identify those scenarios that the agent can sometimes solve, but not always.\nBased on our analysis, we develop a method that directly trains on scenarios with high learnability. This simple and intuitive approach outperforms existing UED methods in several binary-outcome environments, including the standard domain of Minigrid and a novel setting closely inspired by a real-world robotics problem. \nWe further introduce a new adversarial evaluation procedure for directly measuring robustness, closely mirroring the conditional value at risk (CVaR).\nWe open-source all our code and present visualisations of final policies here: https://github.com/amacrutherford/sampling-for-learnability.",
      "authors": [
        "Alexander Rutherford",
        "Michael Beukman",
        "Timon Willi",
        "Bruno Lacerda",
        "Nick Hawes",
        "Jakob Nicolaus Foerster"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iEeiZlTbts",
      "cdate": 1715803274791,
      "mdate": 1736877791023,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444158"
    },
    {
      "id": "m5dyKArVn8",
      "title": "How many classifiers do we need?",
      "abstract": "As performance gains through scaling data and/or model size experience diminishing returns, it is becoming increasingly popular to turn to ensembling, where the predictions of multiple models are combined to improve accuracy. \nIn this paper, we provide a detailed analysis of how the disagreement and the polarization (a notion we introduce and define in this paper) among classifiers relate to the performance gain achieved by aggregating individual classifiers, for majority vote strategies in classification tasks.\nWe address these questions in the following ways. \n(1) An upper bound for polarization is derived, and we propose what we call a neural polarization law: most interpolating neural network models are 4/3-polarized. Our empirical results not only support this conjecture but also show that polarization is nearly constant for a dataset, regardless of hyperparameters or architectures of classifiers. \n(2) The error rate of the majority vote classifier is considered under restricted entropy conditions, and we present a tight upper bound that indicates that the disagreement is linearly correlated with the error rate, and that the slope is linear in the polarization.\n(3) We prove results for the asymptotic behavior of the disagreement in terms of the number of classifiers, which we show can help in predicting the performance for a larger number of classifiers from that of a smaller number. \nOur theoretical findings are supported by empirical results on several image classification tasks with various types of neural networks.",
      "authors": [
        "Hyunsuk Kim",
        "Liam Hodgkinson",
        "Ryan Theisen",
        "Michael W. Mahoney"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=m5dyKArVn8",
      "cdate": 1715802905572,
      "mdate": 1736963056918,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444179"
    },
    {
      "id": "8puv3c9CPg",
      "title": "Beyond the Doors of Perception: Vision Transformers Represent Relations Between Objects",
      "abstract": "Though vision transformers (ViTs) have achieved state-of-the-art performance in a variety of settings, they exhibit surprising failures when performing tasks involving visual relations. This begs the question: how do ViTs attempt to perform tasks that require computing visual relations between objects? Prior efforts to interpret ViTs tend to focus on characterizing relevant low-level visual features. In contrast, we adopt methods from mechanistic interpretability to study the higher-level visual algorithms that ViTs use to perform abstract visual reasoning. We present a case study of a fundamental, yet surprisingly difficult, relational reasoning task: judging whether two visual entities are the same or different. We find that pretrained ViTs fine-tuned on this task often exhibit two qualitatively different stages of processing despite having no obvious inductive biases to do so: 1) a perceptual stage wherein local object features are extracted and stored in a disentangled representation, and 2) a relational stage wherein object representations are compared. In the second stage, we find evidence that ViTs can learn to represent somewhat abstract visual relations, a capability that has long been considered out of reach for artificial neural networks. Finally, we demonstrate that failures at either stage can prevent a model from learning a generalizable solution to our fairly simple tasks. By understanding ViTs in terms of discrete processing stages, one can more precisely diagnose and rectify shortcomings of existing and future models.",
      "authors": [
        "Michael A. Lepori",
        "Alexa R. Tartaglini",
        "Wai Keen Vong",
        "Thomas Serre",
        "Brenden Lake",
        "Ellie Pavlick"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=8puv3c9CPg",
      "cdate": 1715802831289,
      "mdate": 1730874006493,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444186"
    },
    {
      "id": "oEKFPSOWpp",
      "title": "NeuralSteiner: Learning Steiner Tree for Overflow-avoiding Global Routing in Chip Design",
      "abstract": "Global routing plays a critical role in modern chip design. The routing paths generated by global routers often form a rectilinear Steiner tree (RST). Recent advances from the machine learning community have shown the power of learning-based route generation; however, the yielded routing paths by the existing approaches often suffer from considerable overflow, thus greatly hindering their application in practice.\nWe propose NeuralSteiner, an accurate approach to overflow-avoiding global routing in chip design. The key idea of NeuralSteiner approach is to learn Steiner trees: we first predict the locations of highly likely Steiner points by adopting a neural network considering full-net spatial and overflow information, then select appropriate points by running a graph-based post-processing algorithm, and finally connect these points with the input pins to yield overflow-avoiding RSTs. NeuralSteiner offers two advantages over previous learning-based models. First, by using the learning scheme, NeuralSteiner ensures the connectivity of generated routes while significantly reducing congestion. Second, NeuralSteiner can effectively scale to large nets and transfer to unseen chip designs without any modifications or fine-tuning.  Extensive experiments over public large-scale benchmarks reveal that, compared with the state-of-the-art deep generative methods, NeuralSteiner achieves up to a 99.8\\% reduction in overflow while speeding up the generation and maintaining a slight wirelength loss within only 1.8\\%.",
      "authors": [
        "Ruizhi Liu",
        "ZhishengZeng",
        "Shizhe Ding",
        "Jingyan Sui",
        "Xingquan Li",
        "Dongbo Bu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=oEKFPSOWpp",
      "cdate": 1715802755788,
      "mdate": 1730874006378,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444192"
    },
    {
      "id": "eHzIwAhj06",
      "title": "The Group Robustness is in the Details: Revisiting Finetuning under Spurious Correlations",
      "abstract": "Modern machine learning models are prone to over-reliance on spurious correlations, which can often lead to poor performance on minority groups. In this paper, we identify surprising and nuanced behavior of finetuned models on worst-group accuracy via comprehensive experiments on four well-established benchmarks across vision and language tasks. We first show that the commonly used class-balancing techniques of mini-batch upsampling and loss upweighting can induce a decrease in worst-group accuracy (WGA) with training epochs, leading to performance no better than without class-balancing. While in some scenarios, removing data to create a class-balanced subset is more effective, we show this depends on group structure and propose a mixture method which can outperform both techniques. Next, we show that scaling pretrained models is generally beneficial for worst-group accuracy, but only in conjunction with appropriate class-balancing. Finally, we identify spectral imbalance in finetuning features as a potential source of group disparities --- minority group covariance matrices incur a larger spectral norm than majority groups once conditioned on the classes. Our results show more nuanced interactions of modern finetuned models with group robustness than was previously known. Our code is available at https://github.com/tmlabonte/revisiting-finetuning.",
      "authors": [
        "Tyler LaBonte",
        "John Collins Hill",
        "Xinchen zhang",
        "Vidya Muthukumar",
        "Abhishek Kumar"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=eHzIwAhj06",
      "cdate": 1715802710312,
      "mdate": 1730874006355,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444197"
    },
    {
      "id": "tBRNC6YemY",
      "title": "Gorilla: Large Language Model Connected with Massive APIs",
      "abstract": "Large Language Models (LLMs) have seen an impressive wave of advances, with\nmodels now excelling in a variety of tasks, such as mathematical reasoning and\nprogram synthesis. However, their potential to effectively use tools via API calls\nremains unfulfilled. This is a challenging task even for today’s state-of-the-art\nLLMs such as GPT-4 largely due to their unawareness of what APIs are available\nand how to use them in a frequently updated tool set. We develop Gorilla, a\nfinetuned LLaMA model that surpasses the performance of GPT-4 on writing API\ncalls. Trained with the novel Retriever Aware Training (RAT), when combined\nwith a document retriever, Gorilla demonstrates a strong capability to adapt to\ntest-time document changes, allowing flexible user updates or version changes.\nIt also substantially mitigates the issue of hallucination, commonly encountered\nwhen prompting LLMs directly. To evaluate the model’s ability, we introduce\nAPIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and\nTensorHub APIs. The successful integration of the retrieval system with Gorilla\ndemonstrates the potential for LLMs to use tools more accurately, keep up with\nfrequently updated documentation, and consequently increase the reliability and\napplicability of their outputs. Gorilla’s code, model, data, and demo are available\nat: https://gorilla.cs.berkeley.edu",
      "authors": [
        "Shishir G Patil",
        "Tianjun Zhang",
        "Xin Wang",
        "Joseph E. Gonzalez"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tBRNC6YemY",
      "cdate": 1715802493278,
      "mdate": 1730874006214,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444202"
    },
    {
      "id": "ZdWTN2HOie",
      "title": "Stochastic Amortization: A Unified Approach to Accelerate Feature and Data Attribution",
      "abstract": "Many tasks in explainable machine learning, such as data valuation and feature attribution, perform expensive computation for each data point and are intractable for large datasets. These methods require efficient approximations, and although amortizing the process by learning a network to directly predict the desired output is a promising solution, training such models with exact labels is often infeasible. We therefore explore training amortized models with noisy labels, and we find that this is inexpensive and surprisingly effective. Through theoretical analysis of the label noise and experiments with various models and datasets, we show that this approach tolerates high noise levels and significantly accelerates several feature attribution and data valuation methods, often yielding an order of magnitude speedup over existing approaches.",
      "authors": [
        "Ian Connick Covert",
        "Chanwoo Kim",
        "Su-In Lee",
        "James Zou",
        "Tatsunori Hashimoto"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ZdWTN2HOie",
      "cdate": 1715802262015,
      "mdate": 1730874006072,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444208"
    },
    {
      "id": "KjNEzWRIqn",
      "title": "Synatra: Turning Indirect Knowledge into Direct Demonstrations for Digital Agents at Scale",
      "abstract": "LLMs can now act as autonomous agents that interact with digital environments and complete specific objectives (e.g., arranging an online meeting). However, accuracy is still far from satisfactory, partly due to a lack of large-scale, direct demonstrations for digital tasks. Obtaining supervised data from humans is costly, and automatic data collection through exploration or reinforcement learning relies on complex environmental and content setup, resulting in datasets that lack comprehensive coverage of various scenarios. On the other hand, there is abundant knowledge that may indirectly assist task completion, such as online tutorials that were created for human consumption. In this work, we present Synatra, an approach that effectively transforms this indirect knowledge into direct supervision at scale. We define different types of indirect knowledge, and carefully study the available sources to obtain it, methods to encode the structure of direct demonstrations, and finally methods to transform indirect knowledge into direct demonstrations. We use 100k such synthetically-created demonstrations to finetune a 7B CodeLlama, and demonstrate that the resulting agent surpasses all comparably sized models on three web-based task benchmarks Mind2Web, MiniWoB++ and WebArena, as well as surpassing GPT-3.5 on WebArena and Mind2Web. In addition, while synthetic demonstrations prove to be only 3% the cost of human demonstrations (at $0.031 each), we show that the synthetic demonstrations can be more effective than an identical number of human demonstrations collected from limited domains.",
      "authors": [
        "Tianyue Ou",
        "Frank F. Xu",
        "Aman Madaan",
        "Jiarui Liu",
        "Robert Lo",
        "Abishek Sridhar",
        "Sudipta Sengupta",
        "Dan Roth",
        "Graham Neubig",
        "Shuyan Zhou"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=KjNEzWRIqn",
      "cdate": 1715802206648,
      "mdate": 1730874005995,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444213"
    },
    {
      "id": "aVh9KRZdRk",
      "title": "Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks",
      "abstract": "Large language models can solve tasks that were not present in the training set. This capability is believed to be due to in-context learning and skill composition. In this work, we study the emergence of in-context learning and skill composition in a collection of modular arithmetic tasks. Specifically, we consider a finite collection of linear modular functions $z = a  x + b  y \\text{ mod } p$ labeled by the vector $(a, b) \\in \\mathbb{Z}_p^2$. We use some of these tasks for pre-training and the rest for out-of-distribution testing. We empirically show that a GPT-style transformer exhibits a transition from in-distribution to out-of-distribution generalization as the number of pre-training tasks increases. We find that the smallest model capable of out-of-distribution generalization requires two transformer blocks, while for deeper models, the out-of-distribution generalization phase is *transient*, necessitating early stopping. Finally, we perform an interpretability study of the pre-trained models, revealing highly structured representations in both attention heads and MLPs; and discuss the learned algorithms. Notably, we find an algorithmic shift in deeper models, as we go from few to many in-context examples.",
      "authors": [
        "Tianyu He",
        "Darshil Doshi",
        "Aritra Das",
        "Andrey Gromov"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=aVh9KRZdRk",
      "cdate": 1715802154333,
      "mdate": 1730874005890,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444219"
    },
    {
      "id": "Q5RYn6jagC",
      "title": "Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem",
      "abstract": "Recent work has documented striking heterogeneity in the performance of state-of-the-art vision language models (VLMs), including both multimodal language models and text-to-image models. These models are able to describe and generate a diverse array of complex, naturalistic images, yet they exhibit surprising failures on basic multi-object reasoning tasks -- such as counting, localization, and simple forms of visual analogy -- that humans perform with near perfect accuracy. To better understand this puzzling pattern of successes and failures, we turn to theoretical accounts of the binding problem in cognitive science and neuroscience, a fundamental problem that arises when a shared set of representational resources must be used to represent distinct entities (e.g., to represent multiple objects in an image), necessitating the use of serial processing to avoid interference. We find that many of the puzzling failures of state-of-the-art VLMs can be explained as arising due to the binding problem, and that these failure modes are strikingly similar to the limitations exhibited by rapid, feedforward processing in the human brain.",
      "authors": [
        "Declan Iain Campbell",
        "Sunayana Rane",
        "Tyler Giallanza",
        "C. Nicolò De Sabbata",
        "Kia Ghods",
        "Amogh Joshi",
        "Alexander Ku",
        "Steven M Frankland",
        "Thomas L. Griffiths",
        "Jonathan D. Cohen",
        "Taylor Whittington Webb"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Q5RYn6jagC",
      "cdate": 1715802014150,
      "mdate": 1730874005837,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.444225"
    },
    {
      "id": "uSKzEaj9zJ",
      "title": "Nonlocal Attention Operator: Materializing Hidden Knowledge Towards Interpretable Physics Discovery",
      "abstract": "Despite recent popularity of attention-based neural architectures in core AI fields like natural language processing (NLP) and computer vision (CV), their potential in modeling complex physical systems remains under-explored. Learning problems in physical systems are often characterized as discovering operators that map between function spaces based on a few instances of function pairs. This task frequently presents a severely ill-posed PDE inverse problem. In this work, we propose a novel neural operator architecture based on the attention mechanism, which we coin Nonlocal Attention Operator (NAO), and explore its capability towards developing a foundation physical model. In particular, we show that the attention mechanism is equivalent to a double integral operator that enables nonlocal interactions among spatial tokens, with a data-dependent kernel characterizing the inverse mapping from data to the hidden parameter field of the underlying operator. As such, the attention mechanism extracts global prior information from training data generated by multiple systems, and suggests the exploratory space in the form of a nonlinear kernel map. Consequently, NAO can address ill-posedness and rank deficiency in inverse PDE problems by encoding regularization and achieving generalizability. Lastly, we empirically demonstrate the advantages of NAO over baseline neural models in terms of the generalizability to unseen data resolutions and system states. Our work not only suggests a novel neural operator architecture for learning an interpretable foundation model of physical systems, but also offers a new perspective towards understanding the attention mechanism.",
      "authors": [
        "Yue Yu",
        "Ning Liu",
        "Fei Lu",
        "Tian Gao",
        "Siavash Jafarzadeh",
        "Stewart A Silling"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=uSKzEaj9zJ",
      "cdate": 1715801828327,
      "mdate": 1730874005786,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444231"
    },
    {
      "id": "NfOFbPpYII",
      "title": "Non-asymptotic Convergence of Training Transformers for Next-token Prediction",
      "abstract": "Transformers have achieved extraordinary success in modern machine learning due to their excellent ability to handle sequential data, especially in next-token prediction (NTP) tasks. However, the theoretical understanding of their performance in NTP is limited, with existing studies focusing mainly on asymptotic performance. This paper provides a fine-grained non-asymptotic analysis of the training dynamics of a one-layer transformer consisting of a self-attention module followed by a feed-forward layer. We first characterize the essential structural properties of training datasets for NTP using a mathematical framework based on partial orders. \nThen, we design a two-stage training algorithm, where the pre-processing stage for training the feed-forward layer and the main stage for training the attention layer exhibit fast convergence performance. Specifically, both layers converge sub-linearly to the direction of their corresponding max-margin solutions. We also show that the cross-entropy loss enjoys a linear convergence rate. Furthermore, we show that the trained transformer presents non-trivial prediction ability with dataset shift, which sheds light on the remarkable generalization performance of transformers. Our analysis technique involves the development of novel properties on the attention gradient and further in-depth analysis of how these properties contribute to the convergence of the training process. Our experiments further validate our theoretical findings.",
      "authors": [
        "Ruiquan Huang",
        "Yingbin Liang",
        "Jing Yang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NfOFbPpYII",
      "cdate": 1715801529321,
      "mdate": 1730874005669,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444237"
    },
    {
      "id": "4ZH48aGD60",
      "title": "Active, anytime-valid risk controlling prediction sets",
      "abstract": "Rigorously establishing the safety of black-box machine learning models with respect to critical risk measures is important for providing guarantees about the behavior of the model.\nRecently, a notion of a risk controlling prediction set (RCPS) has been introduced by Bates et. al. (JACM '24) for producing prediction sets that are statistically guaranteed to have low risk from machine learning models.\nOur method extends this notion to the sequential setting, where we provide guarantees even when the data is collected adaptively, and ensures the risk guarantee is anytime-valid, i.e., simultaneously holds at all time steps. Further, we propose a framework for constructing RCPSes for active labeling, i.e., allowing one to use a labeling policy that chooses whether to query the true label for each received data point, and ensures the expected proportion data points whose labels are queried are below a predetermined label budget. We also describe how to use predictors (e.g., the machine learning model we are providing risk control guarantees for) to further improve the utility of our RCPSes by estimating the expected risk conditioned on the covariates.\nWe characterize the optimal choices of label policy under a fixed label budget, and predictor, and show a regret result that relates the estimation error of the optimal labeling policy and predictor to the wealth process that underlies our RCPSes.\nLastly, we present practical ways of formulating label policies and we empirically show that our label policies use fewer labels to reach higher utility than naive baseline labeling strategies on both simulations and real data.",
      "authors": [
        "Ziyu Xu",
        "Nikos Karampatziakis",
        "Paul Mineiro"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4ZH48aGD60",
      "cdate": 1715801477365,
      "mdate": 1736918421483,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444241"
    },
    {
      "id": "qOSFiJdVkZ",
      "title": "Continual learning with the neural tangent ensemble",
      "abstract": "A natural strategy for continual learning is to weigh a Bayesian ensemble of fixed functions. This suggests that if a (single) neural network could be interpreted as an ensemble, one could design effective algorithms that learn without forgetting. To realize this possibility, we observe that a neural network classifier with N parameters can be interpreted as a weighted ensemble of N classifiers, and that in the lazy regime limit these classifiers are fixed throughout learning. We call these classifiers the *neural tangent experts* and show they output valid probability distributions over the labels. We then derive the likelihood and posterior probability of each expert given past data. Surprisingly,  the posterior updates for these experts are equivalent to a scaled and projected form of stochastic gradient descent (SGD) over the network weights. Away from the lazy regime, networks can be seen as ensembles of adaptive experts which improve over time. These results offer a new interpretation of neural networks as Bayesian ensembles of experts, providing a principled framework for understanding and mitigating catastrophic forgetting in continual learning settings.",
      "authors": [
        "Ari S Benjamin",
        "Christian-Gernot Pehle",
        "Kyle Daruwalla"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qOSFiJdVkZ",
      "cdate": 1715801422195,
      "mdate": 1730874005581,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444246"
    },
    {
      "id": "JEflV4nRlH",
      "title": "What Makes and Breaks Safety Fine-tuning? A Mechanistic Study",
      "abstract": "Safety fine-tuning helps align Large Language Models (LLMs) with human preferences for their safe deployment. To better understand the underlying factors that make models safe via safety fine-tuning, we design a synthetic data generation framework that captures salient aspects of an unsafe input by modeling the interaction between the task the model is asked to perform (e.g., “design”) versus the specific concepts the task is asked to be performed upon (e.g., a “cycle” vs. a “bomb”). Using this, we investigate three well-known safety fine-tuning methods—supervised safety fine-tuning, direct preference optimization, and unlearning—and provide significant evidence demonstrating that these methods minimally transform MLP weights to specifically align unsafe inputs into its weights’ null space. This yields a clustering of inputs based on whether the model deems them safe or not. Correspondingly, when an adversarial input (e.g., a jailbreak) is provided, its activations are closer to safer samples, leading to the model processing such an input as if it were safe. Code is available at https://github.com/fiveai/understanding_safety_finetuning.",
      "authors": [
        "Samyak Jain",
        "Ekdeep Singh Lubana",
        "Kemal Oksuz",
        "Tom Joy",
        "Philip Torr",
        "Amartya Sanyal",
        "Puneet K. Dokania"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=JEflV4nRlH",
      "cdate": 1715801393392,
      "mdate": 1730874005540,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444251"
    },
    {
      "id": "doaJTihgIZ",
      "title": "Dynamics of Supervised and Reinforcement Learning in the Non-Linear Perceptron",
      "abstract": "The ability of a brain or a neural network to efficiently learn depends crucially on both the task structure and the learning rule.\nPrevious works have analyzed the dynamical equations describing learning in the relatively simplified context of the perceptron under assumptions of a student-teacher framework or a linearized output. \nWhile these assumptions have facilitated theoretical understanding, they have precluded a detailed understanding of the roles of the nonlinearity and input-data distribution in determining the learning dynamics, limiting the applicability of the theories to real biological or artificial neural networks.\nHere, we use a stochastic-process approach to derive flow equations describing learning, applying this framework to the case of a nonlinear perceptron performing binary classification. \nWe characterize the effects of the learning rule (supervised or reinforcement learning, SL/RL) and input-data distribution on the perceptron's learning curve and the forgetting curve as subsequent tasks are learned.\nIn particular, we find that the input-data noise differently affects the learning speed under SL vs. RL, as well as determines how quickly learning of a task is overwritten by subsequent learning. Additionally, we verify our approach with real data using the MNIST dataset.\nThis approach points a way toward analyzing learning dynamics for more-complex circuit architectures.",
      "authors": [
        "Christian Schmid",
        "James M Murray"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=doaJTihgIZ",
      "cdate": 1715801305255,
      "mdate": 1730874005478,
      "matched_keywords": [
        "reinforcement learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444256"
    },
    {
      "id": "Yu7H8ZOuI2",
      "title": "Sample-Efficient Geometry Reconstruction from Euclidean Distances using Non-Convex Optimization",
      "abstract": "The problem of finding suitable point embedding or geometric configurations given only Euclidean distance information of point pairs arises both as a core task and as a sub-problem in a variety of machine learning applications. In this paper, we aim to solve this problem given a minimal number of distance samples. \nTo this end, we leverage continuous and non-convex rank minimization formulations of the problem and establish a local convergence  \nguarantee for a variant of iteratively reweighted least squares (IRLS), which applies if a minimal random set of observed distances is provided.\n As a technical tool, we establish a restricted isometry property (RIP) restricted to a tangent space of the manifold of symmetric rank-$r$ matrices given random Euclidean distance  measurements, which might be of independent interest for the analysis of other non-convex approaches.  Furthermore, we assess data efficiency, scalability and generalizability of different reconstruction algorithms through numerical experiments with simulated data as well as real-world data, demonstrating the proposed algorithm's ability to identify the underlying geometry from fewer distance samples compared to the state-of-the-art.\n \n The Matlab code can be found at \\href{https://github.com/ipsita-ghosh-1/EDG-IRLS}{github\\_SEGRED}",
      "authors": [
        "Ipsita Ghosh",
        "Abiy Tasissa",
        "Christian Kümmerle"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Yu7H8ZOuI2",
      "cdate": 1715801189591,
      "mdate": 1730874005401,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444261"
    },
    {
      "id": "M2UzLRoqic",
      "title": "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention",
      "abstract": "Key-value (KV) caching plays an essential role in accelerating decoding for transformer-based autoregressive large language models (LLMs). However, the amount of memory required to store the KV cache can become prohibitive at long sequence lengths and large batch sizes. Since the invention of the transformer, two of the most effective interventions discovered for reducing the size of the KV cache have been Multi-Query Attention (MQA) and its generalization, Grouped-Query Attention (GQA). MQA and GQA both modify the design of the attention block so that multiple query heads can share a single key/value head, reducing the number of distinct key/value heads by a large factor while only minimally degrading accuracy. In this paper, we show that it is possible to take Multi-Query Attention a step further by also sharing key and value heads between adjacent layers, yielding a new attention design we call Cross-Layer Attention (CLA). With CLA, we find that it is possible to reduce the size of the KV cache by another $2\\times$ while maintaining nearly the same accuracy as unmodified MQA. In experiments training 1B- and 3B-parameter models from scratch, we demonstrate that CLA provides a Pareto improvement over the memory/accuracy tradeoffs which are possible with traditional MQA, potentially enabling future models to operate at longer sequence lengths and larger batch sizes than would otherwise be possible.",
      "authors": [
        "William Brandon",
        "Mayank Mishra",
        "Aniruddha Nrusimha",
        "Rameswar Panda",
        "Jonathan Ragan-Kelley"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=M2UzLRoqic",
      "cdate": 1715801132479,
      "mdate": 1730874005289,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444267"
    },
    {
      "id": "qo7NtGMr2u",
      "title": "Symmetry Discovery Beyond Affine Transformations",
      "abstract": "Symmetry detection has been shown to improve various machine learning tasks. In the context of continuous symmetry detection, current state of the art experiments are limited to the detection of affine transformations. Under the manifold assumption, we outline a framework for discovering continuous symmetry in data beyond the affine transformation group. We also provide a similar framework for discovering discrete symmetry. We experimentally compare our method to an existing method known as LieGAN and show that our method is competitive at detecting affine symmetries for large sample sizes and superior than LieGAN for small sample sizes. We also show our method is able to detect continuous symmetries beyond the affine group and is  generally more computationally efficient than LieGAN.",
      "authors": [
        "Ben Shaw",
        "Abram Magner",
        "Kevin R. Moon"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qo7NtGMr2u",
      "cdate": 1715801107471,
      "mdate": 1730874005259,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444272"
    },
    {
      "id": "UwvjJZWjPT",
      "title": "Inductive biases of multi-task learning and finetuning: multiple regimes of feature reuse",
      "abstract": "Neural networks are often trained on multiple tasks, either simultaneously (multi-task learning, MTL) or sequentially (pretraining and subsequent finetuning, PT+FT). In particular, it is common practice to pretrain neural networks on a large auxiliary task before finetuning on a downstream task with fewer samples. Despite the prevalence of this approach, the inductive biases that arise from learning multiple tasks are poorly characterized. In this work, we address this gap. We describe novel implicit regularization penalties associated with MTL and PT+FT in diagonal linear networks and single-hidden-layer ReLU networks. These penalties indicate that MTL and PT+FT induce the network to reuse features in different ways. 1) Both MTL and PT+FT exhibit biases towards feature reuse between tasks, and towards sparsity in the set of learned features. We show a \"conservation law\" that implies a direct tradeoff between these two biases. 2) PT+FT exhibits a novel \"nested feature selection\" regime, not described by either the \"lazy\" or \"rich\" regimes identified in prior work, which biases it to *rely on a sparse subset* of the features learned during pretraining. This regime is much narrower for MTL. 3) PT+FT (but not MTL) in ReLU networks benefits from features that are correlated between the auxiliary and main task. We confirm these findings empirically with teacher-student models, and introduce a technique -- weight rescaling following pretraining -- that can elicit the nested feature selection regime. Finally, we validate our theory in deep neural networks trained on image classification. We find that weight rescaling improves performance when it causes models to display signatures of nested feature selection. Our results suggest that nested feature selection may be an important inductive bias for finetuning neural networks.",
      "authors": [
        "Samuel Lippl",
        "Jack Lindsey"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=UwvjJZWjPT",
      "cdate": 1715800795413,
      "mdate": 1730874005164,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444278"
    },
    {
      "id": "DlYNGpCuwa",
      "title": "Aligning LLM Agents by Learning Latent Preference from User Edits",
      "abstract": "We study interactive learning of language agents based on user edits made to the agent's output. In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their latent preference, in addition to improving the correctness. The edit feedback is naturally generated, making it a suitable candidate for improving the agent's alignment with the user's preference, and for reducing the cost of user edits over time. We propose a learning framework, PRELUDE that infers a description of the user's latent preference based on historic edit data and using it to define a prompt policy that drives future response generation. This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks. Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference. However, user preference can be complex and vary based on context, making it challenging to learn. To address this, we propose a simple yet effective algorithm named CIPHER that leverages a large language model (LLM) to infer the user preference for a given context based on user edits. In the future, CIPHER retrieves inferred preferences from the k-closest contexts in the history, and forms an aggregate preference for response generation. We introduce two interactive environments -- summarization and email writing, for evaluation using a GPT-4 simulated user. We compare with algorithms that directly retrieve user edits but do not learn descriptive preference, and algorithms that learn context-agnostic preference. On both tasks, CIPHER outperforms baselines by achieving the lowest edit distance cost. Meanwhile, CIPHER has a lower computational expense, as using learned preference results in a shorter prompt than directly using user edits. Our further analysis reports that the user preference learned by CIPHER shows significant similarity to the ground truth latent preference.",
      "authors": [
        "Ge Gao",
        "Alexey Taymanov",
        "Eduardo Salinas",
        "Paul Mineiro",
        "Dipendra Misra"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DlYNGpCuwa",
      "cdate": 1715800560222,
      "mdate": 1730874005090,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444282"
    },
    {
      "id": "1sLdprsbmk",
      "title": "Can Models Learn Skill Composition from Examples?",
      "abstract": "As large language models (LLMs) become increasingly advanced, their ability to exhibit compositional generalization---the capacity to combine learned skills in novel ways not encountered during training---has garnered significant attention. This type of generalization, particularly in scenarios beyond training data, is also of great interest in the study of AI safety and alignment. A recent study introduced the Skill-Mix evaluation, where models are tasked with composing a short paragraph demonstrating the use of a specified $k$-tuple of language skills. While small models struggled with composing even with $k=3$, larger models like GPT-4 performed reasonably well with $k=5$ and $6$.\n\nIn this paper, we employ a setup akin to Skill-Mix to evaluate the capacity of smaller models to learn compositional generalization from examples. Utilizing a diverse set of language skills---including rhetorical, literary, reasoning, theory of mind, and common sense---GPT was used to generate text samples that exhibit random subsets of $k$ skills. Subsequent fine-tuning of 7B and 13B parameter models on these combined skill texts, for increasing values of $k$, revealed the following findings: (1) Training on combinations of $k=2$ and $3$ skills results in noticeable improvements in the ability to compose texts with $k=4$ and $5$ skills, despite models never having seen such examples during training. (2) When skill categories are split into training and held-out groups, models significantly improve at composing texts with held-out skills during testing despite having only seen training skills during fine-tuning, illustrating the efficacy of the training approach even with previously unseen skills.\n\nThis study also suggests that incorporating skill-rich (potentially synthetic) text into training can substantially enhance the compositional capabilities of models.",
      "authors": [
        "Haoyu Zhao",
        "Simran Kaur",
        "Dingli Yu",
        "Anirudh Goyal",
        "Sanjeev Arora"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=1sLdprsbmk",
      "cdate": 1715800509500,
      "mdate": 1730874005060,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444288"
    },
    {
      "id": "pPSWHsgqRp",
      "title": "Smoothie: Label Free Language Model Routing",
      "abstract": "Large language models (LLMs) are increasingly used in applications where LLM inputs may span many different tasks. Recent work has found that the choice of LLM is consequential, and different LLMs may be good for different input samples. Prior approaches have thus explored how engineers might select an LLM to use for each sample (i.e. _routing_). While existing routing methods mostly require training auxiliary models on human-annotated data, our work explores whether it is possible to perform _unsupervised_ routing. We propose Smoothie, a weak supervision-inspired routing approach that requires no labeled data. Given a set of outputs from different LLMs, Smoothie constructs a latent variable graphical model over embedding representations of observable LLM outputs and unknown “true” outputs. Using this graphical model, we estimate sample-dependent quality scores for each LLM, and route each sample to the LLM with the highest corresponding score. We find that Smoothie's LLM quality-scores correlate with ground-truth model quality (correctly identifying the optimal model on 9/14 tasks), and that Smoothie outperforms baselines for routing by up to 10 points accuracy.",
      "authors": [
        "Neel Guha",
        "Mayee F Chen",
        "Trevor Chow",
        "Ishan S. Khare",
        "Christopher Re"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pPSWHsgqRp",
      "cdate": 1715800419678,
      "mdate": 1730874004949,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444293"
    },
    {
      "id": "XHCYZNmqnv",
      "title": "Detecting Brittle Decisions for Free: Leveraging Margin Consistency in Deep Robust Classifiers",
      "abstract": "Despite extensive research on adversarial training strategies to improve robustness, the decisions of even the most robust deep learning models can still be quite sensitive to imperceptible perturbations, creating serious risks when deploying them for high-stakes real-world applications. While detecting such cases may be critical, evaluating a model's vulnerability at a per-instance level using adversarial attacks is computationally too intensive and unsuitable for real-time deployment scenarios. The input space margin is the exact score to detect non-robust samples and is intractable for deep neural networks. This paper introduces the concept of margin consistency -- a property that links the input space margins and the logit margins in robust models -- for efficient detection of vulnerable samples. First, we establish that margin consistency is a necessary and sufficient condition to use a model's logit margin as a score for identifying non-robust samples. Next, through comprehensive empirical analysis of various robustly trained models on CIFAR10 and CIFAR100 datasets, we show that they indicate high margin consistency with a strong correlation between their input space margins and the logit margins. Then, we show that we can effectively use the logit margin to confidently detect brittle decisions with such models. Finally, we address cases where the model is not sufficiently margin-consistent by learning a pseudo-margin from the feature representation. Our findings highlight the potential of leveraging deep representations to efficiently assess adversarial vulnerability in deployment scenarios.",
      "authors": [
        "Jonas Ngnawe",
        "Sabyasachi Sahoo",
        "Yann Batiste Pequignot",
        "Frederic Precioso",
        "Christian Gagné"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=XHCYZNmqnv",
      "cdate": 1715800258958,
      "mdate": 1735148197286,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444298"
    },
    {
      "id": "HRnSVflpgt",
      "title": "Schur Nets: exploiting local structure for equivariance in higher order graph neural networks",
      "abstract": "Recent works have shown that extending the message passing paradigm to subgraphs communicating with other subgraphs, especially via higher order messages, can boost the expressivity of graph neural networks. In such architectures, to faithfully account for local structure such as cycles, the local operations must be equivariant to the automorphism group of the local environment. However, enumerating the automorphism groups of all subgraphs of interest and finding appropriate equivariant operations for each one of them separately is generally not feasible. In this paper we propose a solution to this problem based on spectral graph theory that bypasses \nhaving to determine the automorphism group entirely and constructs a basis for equivariant operations directly from the graph Laplacian. \nWe show that this approach can boost the performance of GNNs on some standard benchmarks.",
      "authors": [
        "QINGQI ZHANG",
        "Ruize Xu",
        "Risi Kondor"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=HRnSVflpgt",
      "cdate": 1715800220508,
      "mdate": 1737695693792,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444303"
    },
    {
      "id": "7sACcaOmGi",
      "title": "The Power of Resets in Online Reinforcement Learning",
      "abstract": "Simulators are a pervasive tool in reinforcement learning, but most existing algorithms cannot efficiently exploit simulator access -- particularly in high-dimensional domains that require general function approximation. We explore the power of simulators through online reinforcement learning with local simulator access (or, local planning), an RL protocol where the agent is allowed to reset to previously observed states and follow their dynamics during training. We use local simulator access to unlock new statistical guarantees that were previously out of reach:\n- We show that MDPs with low coverability (Xie et al. 2023) -- a general structural condition that subsumes Block MDPs and Low-Rank MDPs -- can be learned in a sample-efficient fashion with only Q⋆-realizability (realizability of the optimal state-value function); existing online RL algorithms require significantly stronger representation conditions.\n- As a consequence, we show that the notorious Exogenous Block MDP problem (Efroni et al. 2022) is tractable under local simulator access.\nThe results above are achieved through a computationally inefficient algorithm. We complement them with a more computationally efficient algorithm, RVFS (Recursive Value Function Search), which achieves provable sample complexity guarantees under a strengthened statistical assumption known as pushforward coverability. RVFS can be viewed as a principled, provable counterpart to a successful empirical paradigm that combines recursive search (e.g., MCTS) with value function approximation.",
      "authors": [
        "Zakaria Mhammedi",
        "Dylan J Foster",
        "Alexander Rakhlin"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7sACcaOmGi",
      "cdate": 1715800168986,
      "mdate": 1730874004729,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444309"
    },
    {
      "id": "MQIET1VfoV",
      "title": "Boosting Sample Efficiency and Generalization in Multi-agent Reinforcement Learning via Equivariance",
      "abstract": "Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency and poor generalization [1]. These challenges are partially due to a lack of structure or inductive bias in the neural networks typically used in learning the policy. One such form of structure that is commonly observed in multi-agent scenarios is symmetry. The field of Geometric Deep Learning has developed Equivariant Graph Neural Networks (EGNN) that are equivariant (or symmetric) to rotations, translations, and reflections of nodes. Incorporating equivariance has been shown to improve learning efficiency and decrease error [ 2 ]. In this paper, we demonstrate that EGNNs improve the sample efficiency and generalization in MARL. However, we also show that a naive application of EGNNs to MARL results in poor early exploration due to a bias in the EGNN structure. To mitigate this bias, we present Exploration-enhanced Equivariant Graph Neural Networks or E2GN2. We compare E2GN2 to other common function approximators using common MARL benchmarks MPE and SMACv2. E2GN2 demonstrates a significant improvement in sample efficiency, greater final reward convergence, and a 2x-5x gain in over standard GNNs in our generalization tests. These results pave the way for more reliable and effective solutions in complex multi-agent systems.",
      "authors": [
        "Joshua McClellan",
        "Naveed Haghani",
        "John Winder",
        "Furong Huang",
        "Pratap Tokekar"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MQIET1VfoV",
      "cdate": 1715800130224,
      "mdate": 1730874004681,
      "matched_keywords": [
        "reinforcement learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444314"
    },
    {
      "id": "hGgkdFF2hR",
      "title": "Low-Rank Optimal Transport through Factor Relaxation with Latent Coupling",
      "abstract": "Optimal transport (OT) is a general framework for finding a minimum-cost transport plan, or coupling, between probability distributions, and has many applications in machine learning. A key challenge in applying OT to massive datasets is the quadratic scaling of the coupling matrix with the size of the dataset. [Forrow et al. 2019] introduced a factored coupling for the k-Wasserstein barycenter problem, which [Scetbon et al. 2021] adapted to solve the primal low-rank OT problem. We derive an alternative parameterization of the low-rank problem based on the _latent coupling_ (LC) factorization previously introduced by [Lin et al. 2021] generalizing [Forrow et al. 2019]. The LC factorization has multiple  advantages for low-rank OT including decoupling the problem into three OT problems and greater flexibility and interpretability. We leverage these advantages to derive a new algorithm _Factor Relaxation with Latent Coupling_ (FRLC), which uses _coordinate_ mirror descent to compute the LC factorization. FRLC handles multiple OT objectives (Wasserstein, Gromov-Wasserstein, Fused Gromov-Wasserstein), and marginal constraints (balanced, unbalanced, and semi-relaxed) with linear space complexity. We provide theoretical results on FRLC, and demonstrate superior performance on diverse applications -- including graph clustering and spatial transcriptomics --  while demonstrating its interpretability.",
      "authors": [
        "Peter Halmos",
        "Xinhao Liu",
        "Julian Gold",
        "Benjamin Raphael"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hGgkdFF2hR",
      "cdate": 1715799946230,
      "mdate": 1730874004532,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444320"
    },
    {
      "id": "3CtTMF5zzM",
      "title": "On Tractable $\\Phi$-Equilibria in Non-Concave Games",
      "abstract": "While Online Gradient Descent and other no-regret learning procedures are known to efficiently converge to a coarse correlated equilibrium in games where each agent's utility is concave in their own strategy, this is not the case when utilities are non-concave -- a common scenario in machine learning applications involving strategies parameterized by deep neural networks, or when agents' utilities are computed by neural networks, or both. Non-concave games introduce significant game-theoretic and optimization challenges: (i) Nash equilibria may not exist; (ii) local Nash equilibria, though they exist, are intractable; and (iii) mixed Nash, correlated, and coarse correlated equilibria generally have infinite support and are intractable. To sidestep these challenges, we revisit the classical solution concept of $\\Phi$-equilibria introduced by Greenwald and Jafari [GJ03], which is guaranteed to exist for an arbitrary set of strategy modifications $\\Phi$ even in non-concave games [SL07]. However, the tractability of $\\Phi$-equilibria in such games remains elusive. In this paper, we initiate the study of tractable $\\Phi$-equilibria in non-concave games and examine several natural families of strategy modifications. We show that when $\\Phi$ is finite, there exists an efficient uncoupled learning algorithm that approximates the corresponding $\\Phi$-equilibria. Additionally, we explore cases where $\\Phi$ is infinite but consists of local modifications, showing that Online Gradient Descent can efficiently approximate $\\Phi$-equilibria in non-trivial regimes.",
      "authors": [
        "Yang Cai",
        "Constantinos Costis Daskalakis",
        "Haipeng Luo",
        "Chen-Yu Wei",
        "Weiqiang Zheng"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3CtTMF5zzM",
      "cdate": 1715799725558,
      "mdate": 1730874004458,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444325"
    },
    {
      "id": "pH3XAQME6c",
      "title": "Refusal in Language Models Is Mediated by a Single Direction",
      "abstract": "Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables a model's ability to refuse, with minimal effect on other capabilities. This interpretable rank-one weight edit results in an effective jailbreak technique that is simpler and more efficient than fine-tuning. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.",
      "authors": [
        "Andy Arditi",
        "Oscar Balcells Obeso",
        "Aaquib Syed",
        "Daniel Paleka",
        "Nina Rimsky",
        "Wes Gurnee",
        "Neel Nanda"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pH3XAQME6c",
      "cdate": 1715799591175,
      "mdate": 1730874004406,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444330"
    },
    {
      "id": "5bR2l1b2eh",
      "title": "SIRIUS : Contexual Sparisty with Correction for Efficient LLMs",
      "abstract": "With the blossom of large language models (LLM), inference efficiency becomes increasingly important. Various approximate methods are proposed to reduce the cost at inference time. Contextual Sparsity (CS) is appealing for its training-free nature and its ability to reach a higher compression ratio seemingly without significant performance degradation. However, after a comprehensive evaluation of contextual sparsity methods on various complex generation tasks, we find that although CS succeeds in prompt-understanding tasks, it significantly degrades the model performance for reasoning, deduction, and knowledge-based tasks. Despite the gap in end-to-end accuracy, we observed that sparse models and original models often share the general problem-solving logic and require only a few token corrections to recover the original model performance. This paper introduces SIRIUS, an efficient correction mechanism, which significantly boosts CS models on reasoning tasks while maintaining its efficiency gain. SIRIUS is evaluated on 6 models with 8 difficult generation tasks in reasoning, deduction, and coding and shows consistent effectiveness and efficiency. Also, we carefully develop a system implementation for SIRIUS and show that SIRIUS delivers theoretical latency reduction with roughly a 20% reduction in latency for 8B model on-chip and a 35% reduction in latency for 70B model offloading. We open-source our implementation of Sirius at https://github.com/Infini-AI-Lab/Sirius.git.",
      "authors": [
        "Yang Zhou",
        "Zhuoming Chen",
        "Zhaozhuo Xu",
        "Xi Victoria Lin",
        "Beidi Chen"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5bR2l1b2eh",
      "cdate": 1715799458810,
      "mdate": 1730874004319,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444335"
    },
    {
      "id": "hOcsUrOY0D",
      "title": "Attack-Aware Noise Calibration for Differential Privacy",
      "abstract": "Differential privacy (DP) is a widely used approach for mitigating privacy risks when training machine learning models on sensitive data. DP mechanisms add noise during training to limit the risk of information leakage. The scale of the added noise is critical, as it determines the trade-off between privacy and utility. The standard practice is to select the noise scale to satisfy a given privacy budget ε. This privacy budget is in turn interpreted in terms of operational attack risks, such as accuracy, sensitivity, and specificity of inference attacks aimed to recover\ninformation about the training data records. We show that first calibrating the noise scale to a privacy budget ε, and then translating ε to attack risk leads to overly conservative risk assessments and unnecessarily low utility. Instead, we propose methods to directly calibrate the noise scale to a desired attack risk level, bypassing the step of choosing ε. For a given notion of attack risk, our approach significantly\ndecreases noise scale, leading to increased utility at the same level of privacy. We empirically demonstrate that calibrating noise to attack sensitivity/specificity, rather than ε, when training privacy-preserving ML models substantially improves model accuracy for the same risk level. Our work provides a principled and practical way to improve the utility of privacy-preserving ML without compromising on privacy.",
      "authors": [
        "Bogdan Kulynych",
        "Juan Felipe Gomez",
        "Georgios Kaissis",
        "Flavio Calmon",
        "Carmela Troncoso"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hOcsUrOY0D",
      "cdate": 1715799446863,
      "mdate": 1730874004294,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444340"
    },
    {
      "id": "FZQYfmsmX9",
      "title": "A Critical Evaluation of AI Feedback for Aligning Large Language Models",
      "abstract": "Learning from AI feedback (LAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models. LAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL) or direct preference optimization (DPO), using feedback from a critic model. While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback. We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g., GPT-4) used for AI feedback generation. Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing LAIF pipelines. More generally, we find that the gains from LAIF vary substantially across base model families, test-time evaluation protocols, and critic models. Finally, we provide a mechanistic explanation for when SFT may outperform the full two-step LAIF pipeline as well as suggestions for making LAIF maximally useful in practice.",
      "authors": [
        "Archit Sharma",
        "Sedrick Keh",
        "Eric Mitchell",
        "Chelsea Finn",
        "Kushal Arora",
        "Thomas Kollar"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FZQYfmsmX9",
      "cdate": 1715799396282,
      "mdate": 1730874004204,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444346"
    },
    {
      "id": "LxxIiInmuF",
      "title": "Paths to Equilibrium in Games",
      "abstract": "In multi-agent reinforcement learning (MARL) and game theory, agents repeatedly interact and revise their strategies as new data arrives, producing a sequence of strategy profiles. This paper studies sequences of strategies satisfying a pairwise constraint inspired by policy updating in reinforcement learning, where an agent who is best responding in one period does not switch its strategy in the next period. This constraint merely requires that optimizing agents do not switch strategies, but does not constrain the non-optimizing agents in any way, and thus allows for exploration. Sequences with this property are called satisficing paths, and arise naturally in many MARL algorithms.  A fundamental question about strategic dynamics is such: for a given game and initial strategy profile, is it always possible to construct a satisficing path that terminates at an equilibrium? The resolution of this question has implications about the capabilities or limitations of a class of MARL algorithms. We answer this question in the affirmative for normal-form games. Our analysis reveals a counterintuitive insight that suboptimal, and perhaps even reward deteriorating, strategic updates are key to driving play to equilibrium along a satisficing path.",
      "authors": [
        "Bora Yongacoglu",
        "Gurdal Arslan",
        "Lacra Pavel",
        "Serdar Yuksel"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LxxIiInmuF",
      "cdate": 1715799377268,
      "mdate": 1730874004174,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444351"
    },
    {
      "id": "LYivxMp5es",
      "title": "Towards Effective Planning Strategies for Dynamic Opinion Networks",
      "abstract": "In this study, we investigate the under-explored intervention planning aimed at disseminating accurate information within dynamic opinion networks by leveraging learning strategies. Intervention planning involves identifying key nodes (search) and exerting control (e.g., disseminating accurate/official information through the nodes) to mitigate the influence of misinformation. However, as the network size increases, the problem becomes computationally intractable. To address this, we first introduce a ranking algorithm to identify key nodes for disseminating accurate information, which facilitates the training of neural network (NN) classifiers that provide generalized solutions for the search and planning problems. Second, we mitigate the complexity of label generation—which becomes challenging as the network grows—by developing a reinforcement learning (RL)-based centralized dynamic planning framework. We analyze these NN-based planners for opinion networks governed by two dynamic propagation models. Each model incorporates both binary and continuous opinion and trust representations. Our experimental results demonstrate that the ranking algorithm-based classifiers provide plans that enhance infection rate control, especially with increased action budgets for small networks. Further, we observe that the reward strategies focusing on key metrics, such as the number of susceptible nodes and infection rates, outperform those prioritizing faster blocking strategies. Additionally, our findings reveal that graph convolutional network (GCN)-based planners facilitate scalable centralized plans that achieve lower infection rates (higher control) across various network configurations (e.g., Watts-Strogatz topology, varying action budgets, varying initial infected nodes, and varying degree of infected nodes).",
      "authors": [
        "Bharath Chandra Muppasani",
        "Protik Nag",
        "Vignesh Narayanan",
        "Biplav Srivastava",
        "Michael Huhns"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LYivxMp5es",
      "cdate": 1715799302307,
      "mdate": 1730874004086,
      "matched_keywords": [
        "reinforcement learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444357"
    },
    {
      "id": "8B3sAX889P",
      "title": "Unified Insights: Harnessing Multi-modal Data for Phenotype Imputation via View Decoupling",
      "abstract": "Phenotype imputation plays a crucial role in improving comprehensive and accurate medical evaluation, which in turn can optimize patient treatment and bolster the reliability of clinical research. Despite the adoption of various techniques, multi-modal biological data, which can provide crucial insights into a patient's overall health, is often overlooked. With multi-modal biological data, patient characterization can be enriched from two distinct views: the biological view and the phenotype view. However, the heterogeneity and imprecise nature of the multimodal data still pose challenges in developing an effective method to model from two views. In this paper, we propose a novel framework to incorporate multi-modal biological data via view decoupling. Specifically, we segregate the modeling of biological data from phenotype data in a graph-based learning framework. From the biological view, the latent factors in biological data are discovered to model patient correlation. From the phenotype view, phenotype co-occurrence can be modeled to reveal patterns across patients. Then patients are encoded from these two distinct views. To mitigate the influence of noise and irrelevant information in biological data, we devise a cross-view contrastive knowledge distillation aimed at distilling insights from the biological view to enhance phenotype imputation. We show that phenotype imputation with the proposed model significantly outperforms the state-of-the-art models on the real-world biomedical database.",
      "authors": [
        "Qiannan Zhang",
        "Weishen Pan",
        "Zilong Bai",
        "Chang Su",
        "Fei Wang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=8B3sAX889P",
      "cdate": 1715799208988,
      "mdate": 1730874003969,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.444363"
    },
    {
      "id": "wZgw4CrxwK",
      "title": "Incentivizing Quality Text Generation via Statistical Contracts",
      "abstract": "While the success of large language models (LLMs) increases demand for machine-generated text, current pay-per-token pricing schemes create a misalignment of incentives known in economics as moral hazard: Text-generating agents have strong incentive to cut costs by preferring a cheaper model over the cutting-edge one, and this can be done “behind the scenes” since the agent performs inference internally. In this work, we approach this issue from an economic perspective, by proposing a pay-for-performance, contract-based framework for incentivizing quality. We study a principal-agent game where the agent generates text using costly inference, and the contract determines the principal’s payment for the text according to an automated quality evaluation. Since standard contract theory is inapplicable when internal inference costs are unknown, we introduce cost-robust contracts. As our main theoretical contribution, we characterize optimal cost-robust contracts through a direct correspondence to optimal composite hypothesis tests from statistics, generalizing a result of Saig et al. (NeurIPS’23). We evaluate our framework empirically by deriving contracts for a range of objectives and LLM evaluation benchmarks, and find that cost-robust contracts sacrifice only a marginal increase in objective value compared to their cost-aware counterparts.",
      "authors": [
        "Eden Saig",
        "Ohad Einav",
        "Inbal Talgam-Cohen"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=wZgw4CrxwK",
      "cdate": 1715799179751,
      "mdate": 1736899671328,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444367"
    },
    {
      "id": "IfZwSRpqHl",
      "title": "Dynamic Rescaling for Training GNNs",
      "abstract": "Graph neural networks (GNNs) with a rescale invariance, such as GATs, can be re-parameterized during optimization through dynamic rescaling of network parameters and gradients while keeping the loss invariant. In this work, we explore dynamic rescaling as a tool to influence GNN training dynamics in two key ways: i) balancing the network with respect to various criteria, and ii) controlling the relative learning speeds of different layers. We gain novel insights, unique to GNNs, that reveal distinct training modes for different tasks. For heterophilic graphs, achieving balance based on relative gradients leads to faster training and better generalization. In contrast, homophilic graphs benefit from delaying the learning of later layers. Additionally, we show that training in balance supports larger learning rates, which can improve generalization. Moreover, controlling layer-wise training speeds is linked to grokking-like phenomena, which may be of independent interest.",
      "authors": [
        "Nimrah Mustafa",
        "Rebekka Burkholz"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=IfZwSRpqHl",
      "cdate": 1715799149148,
      "mdate": 1730874003895,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444373"
    },
    {
      "id": "hB5NkiET32",
      "title": "Detecting Bugs with Substantial Monetary Consequences by LLM and Rule-based Reasoning",
      "abstract": "Financial transactions are increasingly being handled by automated programs called *smart contracts*. \nHowever, one challenge in the adaptation of smart contracts is the presence of vulnerabilities, which can cause significant monetary loss.\nIn  2024, $247.88 M was lost in 20 smart contract exploits.\nAccording to a recent study, accounting bugs (i.e., incorrect implementations of domain-specific financial models) are the most prevalent type of vulnerability, \nand are one of the most difficult to find, requiring substantial human efforts.\nWhile Large Language Models (LLMs) have shown promise in identifying these bugs, they often suffer from lack of generalization of vulnerability types, hallucinations, and problems with representing smart contracts in limited token context space.\nThis paper proposes a hybrid system combining LLMs and rule-based reasoning to detect accounting error vulnerabilities in smart contracts. \nIn particular, it utilizes the understanding capabilities of LLMs to annotate the financial meaning of variables in smart contracts, and employs rule-based reasoning to propagate the information throughout a contract's logic and to validate potential vulnerabilities.\nTo remedy hallucinations, we propose a feedback loop where validation is performed by providing the reasoning trace of vulnerabilities to the LLM for iterative self-reflection. \nWe achieve 75.6% accuracy on the labelling of financial meanings against human annotations. \nFurthermore, we achieve a recall of 90.5% from running on 23 real-world smart contract projects containing 21 accounting error vulnerabilities.\nFinally, we apply the automated technique on 8 recent projects, finding 4 known and 2 unknown bugs.",
      "authors": [
        "Brian Zhang",
        "ZHUO ZHANG"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hB5NkiET32",
      "cdate": 1715798955360,
      "mdate": 1730874003752,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444378"
    },
    {
      "id": "N2RaC7LO6k",
      "title": "Geometry of naturalistic object representations in recurrent neural network models of working memory",
      "abstract": "Working memory is a central cognitive ability crucial for intelligent decision-making. Recent experimental and computational work studying working memory has primarily used categorical (i.e., one-hot) inputs, rather than ecologically-relevant, multidimensional naturalistic ones. Moreover, studies have primarily investigated working memory during single or few number of cognitive tasks. As a result, an understanding of how naturalistic object information is maintained in working memory in neural networks is still lacking. To bridge this gap, we developed sensory-cognitive models, comprising of a convolutional neural network (CNN) coupled with a recurrent neural network (RNN), and trained them on nine distinct N-back tasks using naturalistic stimuli. By examining the RNN’s latent space, we found that: 1) Multi-task RNNs represent both task-relevant and irrelevant information simultaneously while performing tasks; 2) While the latent subspaces used to maintain specific object properties in vanilla RNNs are largely shared across tasks, they are highly task-specific in gated RNNs such as GRU and LSTM; 3) Surprisingly, RNNs embed objects in new representational spaces in which individual object features are less orthogonalized relative to the perceptual space; 4) Interestingly, the transformation of WM encodings (i.e., embedding of visual inputs in the RNN latent space) into memory was shared across stimuli, yet the transformations governing the retention of a memory in the face of incoming distractor stimuli were distinct across time. Our findings indicate that goal-driven RNNs employ chronological memory subspaces to track information over short time spans, enabling testable predictions with neural data.",
      "authors": [
        "Xiaoxuan Lei",
        "Takuya Ito",
        "Pouya Bashivan"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=N2RaC7LO6k",
      "cdate": 1715798756266,
      "mdate": 1730874003692,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444383"
    },
    {
      "id": "YO6GVPUrKN",
      "title": "On the Limitations of Fractal Dimension as a Measure of Generalization",
      "abstract": "Bounding and predicting the generalization gap of overparameterized neural networks remains a central open problem in theoretical machine learning. There is a recent and growing body of literature that proposes the framework of fractals to model optimization trajectories of neural networks, motivating generalization bounds and measures based on the fractal dimension of the trajectory. Notably, the persistent homology dimension has been proposed to correlate with the generalization gap. This paper performs an empirical evaluation of these persistent homology-based generalization measures, with an in-depth statistical analysis. Our study reveals confounding effects in the observed correlation between generalization and topological measures due to the variation of hyperparameters. We also observe that fractal dimension fails to predict generalization of models trained from poor initializations. We lastly reveal the intriguing manifestation of model-wise double descent in these topological generalization measures. Our work forms a basis for a deeper investigation of the causal relationships between fractal geometry, topological data analysis, and neural network optimization.",
      "authors": [
        "Charlie Tan",
        "Inés García-Redondo",
        "Qiquan Wang",
        "Michael M. Bronstein",
        "Anthea Monod"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=YO6GVPUrKN",
      "cdate": 1715798599552,
      "mdate": 1730874003619,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444388"
    },
    {
      "id": "7EQx56YSB2",
      "title": "Group and Shuffle: Efficient Structured Orthogonal Parametrization",
      "abstract": "The increasing size of neural networks has led to a growing demand for methods of efficient finetuning. Recently, an orthogonal finetuning paradigm was introduced that uses orthogonal matrices for adapting the weights of a pretrained model. In this paper, we introduce a new class of structured matrices, which unifies and generalizes structured classes from previous works. We examine properties of this class and build a structured orthogonal parametrization upon it.  We then use this parametrization to modify the orthogonal finetuning framework, improving parameter efficiency. We empirically validate our method on different domains, including adapting of text-to-image diffusion models and downstream task finetuning in language modeling. Additionally, we adapt our construction for orthogonal convolutions and conduct experiments with 1-Lipschitz neural networks.",
      "authors": [
        "Mikhail Gorbunov",
        "Kolya Yudin",
        "Vera Soboleva",
        "Aibek Alanov",
        "Alexey Naumov",
        "Maxim Rakhuba"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7EQx56YSB2",
      "cdate": 1715798290080,
      "mdate": 1730874003290,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444393"
    },
    {
      "id": "RnvgYd9RAh",
      "title": "LACIE: Listener-Aware Finetuning for Calibration in Large Language Models",
      "abstract": "When answering questions, large language models (LLMs) can convey not only an answer to the question, but a level of confidence about the answer being correct. This includes explicit markers of confidence (e.g. giving a numeric confidence score) as well as implicit markers, like using an authoritative tone or elaborating with additional knowledge of a subject. For LLMs to be trustworthy sources of knowledge, the confidence they convey should match their actual expertise on a topic; however, this is currently not the case, with most models tending towards overconfidence. To calibrate both implicit and explicit confidence markers, we introduce a pragmatic, listener-aware finetuning method (LACIE) that directly models the listener, considering not only whether an answer is right, but whether it will be accepted by a listener. Specifically, we cast calibration as a preference optimization problem, creating data via a two-agent speaker-listener game, where a speaker model’s outputs are judged by a simulated listener. We then finetune three different LLMs (Mistral-7B, Llama3-8B, Llama3-70B) with LACIE, and show that the models resulting from this multi-agent optimization are better calibrated on TriviaQA with respect to a simulated listener. Crucially, these trends transfer to human listeners, helping them correctly predict model correctness: we conduct a human evaluation where annotators accept or reject an LLM’s answers to trivia questions, finding that training with LACIE results in 47% fewer incorrect answers being accepted while maintaining the same level of acceptance for correct answers. Furthermore, LACIE generalizes to another dataset, resulting in a large increase in truthfulness on TruthfulQA when trained on TriviaQA. Our analysis indicates that LACIE leads to a better separation in confidence between correct and incorrect examples. Qualitatively, we find that a LACIE-trained model hedges more when uncertain and adopts implicit cues to signal certainty when it is correct, such as using an authoritative tone or including details. Finally, finetuning with our listener- aware method leads to an emergent increase in model abstention (e.g. saying “I don’t know”) for answers that are likely to be wrong, trading recall for precision.",
      "authors": [
        "Elias Stengel-Eskin",
        "Peter Hase",
        "Mohit Bansal"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=RnvgYd9RAh",
      "cdate": 1715798186155,
      "mdate": 1737122117258,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444398"
    },
    {
      "id": "tPdJ2qHkOB",
      "title": "Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing",
      "abstract": "Despite the impressive capabilities of Large Language Models (LLMs) on various tasks, they still struggle with scenarios that involves complex reasoning and planning. Self-correction and self-learning emerge as viable solutions, employing strategies that allow LLMs to refine their outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs in self-refining its response, particularly in complex reasoning and planning task, remains dubious. In this paper, we introduce AlphaLLM for the self-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with LLMs to establish a self-improving loop, thereby enhancing the capabilities of LLMs without additional annotations. Drawing inspiration from the success of AlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM for self-improvement, including data scarcity, the vastness search spaces of language tasks, and the subjective nature of feedback in language tasks. AlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach tailored for language tasks, and a trio of critic models for precise feedback. Our experimental results in mathematical reasoning tasks demonstrate that AlphaLLM significantly enhances the performance of LLMs without additional annotations, showing the potential for self-improvement in LLMs.",
      "authors": [
        "Ye Tian",
        "Baolin Peng",
        "Linfeng Song",
        "Lifeng Jin",
        "Dian Yu",
        "Lei Han",
        "Haitao Mi",
        "Dong Yu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tPdJ2qHkOB",
      "cdate": 1715798114540,
      "mdate": 1730874003096,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444403"
    },
    {
      "id": "UkauUrTbxx",
      "title": "ProTransformer: Robustify Transformers via Plug-and-Play Paradigm",
      "abstract": "Transformer-based architectures have dominated various areas of machine learning in recent years. In this paper, we introduce a novel robust attention mechanism designed to enhance the resilience of transformer-based architectures. Crucially, this technique can be integrated into existing transformers as a plug-and-play layer, improving their robustness without the need for additional training or fine-tuning. Through comprehensive experiments and ablation studies, we demonstrate that our ProTransformer significantly enhances the robustness of transformer models across a variety of prediction tasks, attack mechanisms, backbone architectures, and data domains. Notably, without further fine-tuning, the ProTransformer consistently improves the performance of vanilla transformers by 19.5\\%, 28.3\\%, 16.1\\%, and 11.4\\% for BERT, ALBERT, DistilBERT, and RoBERTa, respectively, under the classical TextFooler attack. Furthermore, ProTransformer shows promising resilience in large language models (LLMs) against prompting-based attacks, improving the performance of T5 and LLaMA by 24.8\\% and 17.8\\%, respectively, and enhancing Vicuna by an average of 10.4\\% against the Jailbreaking attack. Beyond the language domain, ProTransformer also demonstrates outstanding robustness in both vision and graph domains.",
      "authors": [
        "Zhichao Hou",
        "Weizhi Gao",
        "Yuchen Shen",
        "Feiyi Wang",
        "Xiaorui Liu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=UkauUrTbxx",
      "cdate": 1715798015578,
      "mdate": 1734715013400,
      "matched_keywords": [
        "large language model",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444409"
    },
    {
      "id": "nY0BrZdqLt",
      "title": "Time-Reversal Provides Unsupervised Feedback to LLMs",
      "abstract": "Large Language Models (LLMs) are typically trained to predict in the forward direction of time. However, recent works have shown that prompting these models to look back and critique their own generations can produce useful feedback. Motivated by this, we explore the question of whether LLMs can be empowered to think (predict and score) backwards to provide unsupervised feedback that complements forward LLMs. Towards this, we introduce Time Reversed Language Models (TRLMs), which can score and generate queries when conditioned on responses, effectively functioning in the reverse direction of time. Further, to effectively infer in the response to query direction, we pre-train and fine-tune a language model (TRLM-Ba) in the reverse token order from scratch. We show empirically (and theoretically in a stylized setting) that time-reversed models can indeed complement forward model predictions when used to score the query given response for re-ranking multiple forward generations. We obtain up to 5\\% improvement on the widely used AlpacaEval Leaderboard over the competent baseline of best-of-N re-ranking using self log-perplexity scores. We further show that TRLM scoring outperforms conventional forward scoring of response given query, resulting in significant gains in applications such as citation generation and passage retrieval. We next leverage the generative ability of TRLM to augment or provide unsupervised feedback to input safety filters of LLMs, demonstrating a drastic reduction in false negative rate with negligible impact on false positive rates against several attacks published on the popular JailbreakBench leaderboard.",
      "authors": [
        "Yerram Varun",
        "Rahul Madhavan",
        "Sravanti Addepalli",
        "Arun Suggala",
        "Karthikeyan Shanmugam",
        "Prateek Jain"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nY0BrZdqLt",
      "cdate": 1715797990836,
      "mdate": 1735397935152,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444414"
    },
    {
      "id": "aetbfmCcwg",
      "title": "Debiasing Synthetic Data Generated by Deep Generative Models",
      "abstract": "While synthetic data hold great promise for privacy protection, their statistical analysis poses significant challenges that necessitate innovative solutions. The use of deep generative models (DGMs) for synthetic data generation is known to induce considerable bias and imprecision into synthetic data analyses, compromising their inferential utility as opposed to original data analyses. This bias and uncertainty can be substantial enough to impede statistical convergence rates, even in seemingly straightforward analyses like mean calculation. The standard errors of such estimators then exhibit slower shrinkage with sample size than the typical 1 over root-$n$ rate. This complicates fundamental calculations like p-values and confidence intervals, with no straightforward remedy currently available. In response to these challenges, we propose a new strategy that targets synthetic data created by DGMs for specific data analyses. Drawing insights from debiased and targeted machine learning, our approach accounts for biases, enhances convergence rates, and facilitates the calculation of estimators with easily approximated large sample variances. We exemplify our proposal through a simulation study on toy data and two case studies on real-world data, highlighting the importance of tailoring DGMs for targeted data analysis. This debiasing strategy contributes to advancing the reliability and applicability of synthetic data in statistical inference.",
      "authors": [
        "Alexander Decruyenaere",
        "Heidelinde Dehaene",
        "Paloma Rabaey",
        "Johan Decruyenaere",
        "Christiaan Polet",
        "Thomas Demeester",
        "Stijn Vansteelandt"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=aetbfmCcwg",
      "cdate": 1715797914693,
      "mdate": 1737117735274,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444419"
    },
    {
      "id": "7ESHFpqjNO",
      "title": "Learning Place Cell Representations and Context-Dependent Remapping",
      "abstract": "Hippocampal place cells are known for their spatially selective firing patterns, which has led to the suggestion that they encode an animal's location. However, place cells also respond to contextual cues, such as smell. Furthermore, they have the ability to remap, wherein the firing fields and rates of cells change in response to changes in the environment. How place cell responses emerge, and how these representations remap is not fully understood. In this work, we propose a similarity-based objective function that translates proximity in space, to proximity in representation. We show that a neural network trained to minimize the proposed objective learns place-like representations. We also show that the proposed objective is easily extended to include other sources of information, such as context information, in the same way. When trained to encode multiple contexts, networks learn distinct representations, exhibiting remapping behaviors between contexts. The proposed objective is invariant to orthogonal transformations. Such transformations of the original trained representation (e.g. rotations), therefore yield new representations distinct from the original, without explicit relearning, akin to remapping. Our findings shed new light on the formation and encoding properties of place cells, and also demonstrate an interesting case of representational reuse.",
      "authors": [
        "Markus Pettersen",
        "Frederik Rogge",
        "Mikkel Elle Lepperød"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7ESHFpqjNO",
      "cdate": 1715797867909,
      "mdate": 1736773088810,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444424"
    },
    {
      "id": "fOQunr2E0T",
      "title": "Compositional Generalization Across Distributional Shifts with Sparse Tree Operations",
      "abstract": "Neural networks continue to struggle with compositional generalization, and this issue is exacerbated by a lack of massive pre-training. One successful approach for developing neural systems which exhibit human-like compositional generalization is $\\textit{hybrid}$ neurosymbolic techniques. However, these techniques run into the core issues that plague symbolic approaches to AI: scalability and flexibility. The reason for this failure is that at their core, hybrid neurosymbolic models perform symbolic computation and relegate the scalable and flexible neural computation to parameterizing a symbolic system. We investigate a $\\textit{unified}$ neurosymbolic system where transformations in the network can be interpreted simultaneously as both symbolic and neural computation. We extend a unified neurosymbolic architecture called the Differentiable Tree Machine in two central ways. First, we significantly increase the model’s efficiency through the use of sparse vector representations of symbolic structures. Second, we enable its application beyond the restricted set of tree2tree problems to the more general class of seq2seq problems. The improved model retains its prior generalization capabilities and, since there is a fully neural path through the network, avoids the pitfalls of other neurosymbolic techniques that elevate symbolic computation over neural computation.",
      "authors": [
        "Paul Soulos",
        "Henry Conklin",
        "Mattia Opper",
        "Paul Smolensky",
        "Jianfeng Gao",
        "Roland Fernandez"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fOQunr2E0T",
      "cdate": 1715797847938,
      "mdate": 1734553333166,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444429"
    },
    {
      "id": "kQPzFiwVIu",
      "title": "Synthetic Programming Elicitation for Text-to-Code in Very Low-Resource Programming and Formal Languages",
      "abstract": "Recent advances in large language models (LLMs) for code applications have demonstrated remarkable zero-shot fluency and instruction following on challenging code related tasks ranging from test case generation to self-repair. Unsurprisingly, however, models struggle to compose syntactically valid programs in programming languages unrepresented in pre-training, referred to as very low-resource Programming Languages (VLPLs). VLPLs appear in crucial settings, including domain-specific languages for internal tools, tool-chains for legacy languages, and formal verification frameworks. Inspired by a technique called natural programming elicitation, we propose designing an intermediate language that LLMs ``naturally'' know how to use and which can be automatically compiled to a target VLPL. When LLMs generate code that lies outside of this intermediate language, we use compiler techniques to repair the code into programs in the intermediate language. Overall, we introduce _synthetic programming elicitation and compilation_ (SPEAC), an approach that enables LLMs to generate syntactically valid code even for VLPLs. We empirically evaluate the performance of SPEAC in a case study for the UCLID5 formal verification language and find that, compared to existing retrieval and fine-tuning baselines, SPEAC produces syntactically correct programs more frequently and without sacrificing semantic correctness.",
      "authors": [
        "Federico Mora",
        "Justin Wong",
        "Haley Lepe",
        "Sahil Bhatia",
        "Karim Elmaaroufi",
        "George Varghese",
        "Joseph E. Gonzalez",
        "Elizabeth Polgreen",
        "Sanjit A. Seshia"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kQPzFiwVIu",
      "cdate": 1715797741126,
      "mdate": 1737594530693,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444435"
    },
    {
      "id": "f70e6YYFHF",
      "title": "The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More",
      "abstract": "Today's best language models still struggle with \"hallucinations\", factually incorrect generations, which impede their ability to reliably retrieve information seen during training. The *reversal curse*, where models cannot recall information when probed in a different order than was encountered during training, exemplifies limitations in information retrieval. \nTo better understand these limitations, we reframe the reversal curse as a *factorization curse* --- a failure of models to learn the same joint distribution under different factorizations.\nWe more closely simulate finetuning workflows which train pretrained models on specialized knowledge by introducing\n*WikiReversal*, a realistic testbed based on Wikipedia knowledge graphs. Through a series of controlled experiments with increasing levels of realism, including non-reciprocal relations, we find that reliable information retrieval is an inherent failure of the next-token prediction objective used in popular large language models. Moreover, we demonstrate reliable information retrieval cannot be solved with scale, reversed tokens, or even naive bidirectional-attention training. Consequently, various approaches to finetuning on specialized data would necessarily provide mixed results on downstream tasks, unless the model has already seen the right sequence of tokens. \nAcross five tasks of varying levels of complexity, our results uncover a promising path forward: factorization-agnostic objectives can significantly mitigate the reversal curse and hint at improved knowledge storage and planning capabilities.",
      "authors": [
        "Ouail Kitouni",
        "Niklas Nolte",
        "Adina Williams",
        "Michael Rabbat",
        "Diane Bouchacourt",
        "Mark Ibrahim"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=f70e6YYFHF",
      "cdate": 1715797731624,
      "mdate": 1731599901843,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444440"
    },
    {
      "id": "OcO2XakUUK",
      "title": "Realizable $H$-Consistent and Bayes-Consistent Loss Functions for Learning to Defer",
      "abstract": "We present a comprehensive study of surrogate loss functions for learning to defer. We introduce a broad family of surrogate losses, parameterized by a non-increasing function $\\Psi$, and establish their realizable $H$-consistency under mild conditions. For cost functions based on classification error, we further show that these losses admit $H$-consistency bounds when the hypothesis set is symmetric and complete, a property satisfied by common neural network and linear function hypothesis sets. Our results also resolve an open question raised in previous work [Mozannar et al., 2023] by proving the realizable $H$-consistency and Bayes-consistency of a specific surrogate loss. Furthermore, we identify choices of $\\Psi$ that lead to $H$-consistent surrogate losses for *any general cost function*, thus achieving Bayes-consistency, realizable $H$-consistency, and $H$-consistency bounds *simultaneously*. We also investigate the relationship between $H$-consistency bounds and realizable $H$-consistency in learning to defer, highlighting key differences from standard classification. Finally, we empirically evaluate our proposed surrogate losses and compare them with existing baselines.",
      "authors": [
        "Anqi Mao",
        "Mehryar Mohri",
        "Yutao Zhong"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OcO2XakUUK",
      "cdate": 1715797656539,
      "mdate": 1730874002567,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444445"
    },
    {
      "id": "AbTpJl7vN6",
      "title": "Flexible task abstractions emerge in linear networks with fast and bounded units",
      "abstract": "Animals survive in dynamic environments changing at arbitrary timescales, but such data distribution shifts are a challenge to neural networks. To adapt to change, neural systems may change a large number of parameters, which is a slow process involving forgetting past information. In contrast, animals leverage distribution changes to segment their stream of experience into tasks and associate them with internal task abstracts. Animals can then respond flexibly by selecting the appropriate task abstraction. However, how such flexible task abstractions may arise in neural systems remains unknown. Here, we analyze a linear gated network where the weights and gates are jointly optimized via gradient descent, but with neuron-like constraints on the gates including a faster timescale, non-negativity, and bounded activity. We observe that the weights self-organize into modules specialized for tasks or sub-tasks encountered, while the gates layer forms unique representations that switch the appropriate weight modules (task abstractions). We analytically reduce the learning dynamics to an effective eigenspace, revealing a virtuous cycle: fast adapting gates drive weight specialization by protecting previous knowledge, while weight specialization in turn increases the update rate of the gating layer. Task switching in the gating layer accelerates as a function of curriculum block size and task training, mirroring key findings in cognitive neuroscience. We show that the discovered task abstractions support generalization through both task and subtask composition, and we extend our findings to a non-linear network switching between two tasks. Overall, our work offers a theory of cognitive flexibility in animals as arising from joint gradient descent on synaptic and neural gating in a neural network architecture.",
      "authors": [
        "Kai Jappe Sandbrink",
        "Jan Philipp Bauer",
        "Alexandra Maria Proca",
        "Andrew M Saxe",
        "Christopher Summerfield",
        "Ali Hummos"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=AbTpJl7vN6",
      "cdate": 1715797559956,
      "mdate": 1736962975354,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444450"
    },
    {
      "id": "7Swrtm9Qsp",
      "title": "Stable Minima Cannot Overfit in Univariate ReLU Networks: Generalization by Large Step Sizes",
      "abstract": "We study the generalization of two-layer ReLU neural networks in a univariate nonparametric regression problem with noisy labels. This is a problem where kernels (\\emph{e.g.} NTK) are provably sub-optimal and benign overfitting does not happen, thus disqualifying existing theory for interpolating (0-loss, global optimal) solutions. We present a new theory of generalization for local minima that gradient descent with a constant learning rate can \\emph{stably} converge to.  We show that gradient descent with a fixed learning rate $\\eta$ can only find local minima that represent smooth functions with a certain weighted \\emph{first order total variation} bounded by $1/\\eta - 1/2 + \\widetilde{O}(\\sigma + \\sqrt{\\mathrm{MSE}})$ where $\\sigma$ is the label noise level, $\\mathrm{MSE}$ is short for mean squared error against the ground truth, and $\\widetilde{O}(\\cdot)$ hides a logarithmic factor. Under mild assumptions, we also prove a nearly-optimal MSE bound of $\\widetilde{O}(n^{-4/5})$  within the strict interior of the support of the $n$ data points. Our theoretical results are validated by extensive simulation that demonstrates large learning rate training induces sparse linear spline fits. To the best of our knowledge, we are the first to obtain generalization bound via minima stability in the non-interpolation case and the first to show ReLU NNs without regularization can achieve near-optimal rates in nonparametric regression.",
      "authors": [
        "Dan Qiao",
        "Kaiqi Zhang",
        "Esha Singh",
        "Daniel Soudry",
        "Yu-Xiang Wang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7Swrtm9Qsp",
      "cdate": 1715797552222,
      "mdate": 1730874002498,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444455"
    },
    {
      "id": "QyR1dNDxRP",
      "title": "Provable Tempered Overfitting of Minimal Nets and Typical Nets",
      "abstract": "We study the overfitting behavior of fully connected deep Neural Networks (NNs) with binary weights fitted to perfectly classify a noisy training set. We consider interpolation using both the smallest NN (having the minimal number of weights) and a random interpolating NN. For both learning rules, we prove overfitting is tempered. Our analysis rests on a new bound on the size of a threshold circuit consistent with a partial function. To the best of our knowledge, ours are the first theoretical results on benign or tempered overfitting that: (1) apply to deep NNs, and (2) do not require a very high or very low input dimension.",
      "authors": [
        "Itamar Harel",
        "William M. Hoza",
        "Gal Vardi",
        "Itay Evron",
        "Nathan Srebro",
        "Daniel Soudry"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=QyR1dNDxRP",
      "cdate": 1715797435991,
      "mdate": 1730874002390,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444460"
    },
    {
      "id": "i5PoejmWoC",
      "title": "Causal language modeling can elicit search and reasoning capabilities on logic puzzles",
      "abstract": "Causal language modeling using the Transformer architecture has yielded remarkable capabilities in Large Language Models (LLMs) over the last few years. However, the extent to which fundamental search and reasoning capabilities emerged within LLMs remains a topic of ongoing debate. In this work, we study if causal language modeling can learn a complex task such as solving Sudoku puzzles. To solve a Sudoku, the model is first required to search over all empty cells of the puzzle to decide on a cell to fill and then apply an appropriate strategy to fill the decided cell. Sometimes, the application of a strategy only results in thinning down the possible values in a cell rather than concluding the exact value of the cell. In such cases, multiple strategies are applied one after the other to fill a single cell. We observe that Transformer models trained on this synthetic task can indeed learn to solve Sudokus (our model solves $94.21\\%$ of the puzzles fully correctly) when trained on a logical sequence of steps taken by a solver. We find that training Transformers with the logical sequence of steps is necessary and without such training, they fail to learn Sudoku. We also extend our analysis to Zebra puzzles (known as Einstein puzzles) and show that the model solves $92.04 \\%$ of the puzzles fully correctly. In addition, we study the internal representations of the trained Transformer and find that through linear probing, we can decode information about the set of possible values in any given cell from them, pointing to the presence of a strong reasoning engine implicit in the Transformer weights.",
      "authors": [
        "Kulin Shah",
        "Nishanth Dikkala",
        "Xin Wang",
        "Rina Panigrahy"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=i5PoejmWoC",
      "cdate": 1715797350196,
      "mdate": 1730874002272,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444465"
    },
    {
      "id": "kfdEXQu6MC",
      "title": "A generalized neural tangent kernel for surrogate gradient learning",
      "abstract": "State-of-the-art neural network training methods depend on the gradient of the network function. Therefore, they cannot be applied to networks whose activation functions do not have useful derivatives, such as binary and discrete-time spiking neural networks. To overcome this problem, the activation function's derivative is commonly substituted with a surrogate derivative, giving rise to surrogate gradient learning (SGL). This method works well in practice but lacks theoretical foundation.\n\nThe neural tangent kernel (NTK) has proven successful in the analysis of gradient descent. Here, we provide a generalization of the NTK, which we call the surrogate gradient NTK, that enables the analysis of SGL. First, we study a naive extension of the NTK to activation functions with jumps, demonstrating that gradient descent for such activation functions is also ill-posed in the infinite-width limit. To address this problem, we generalize the NTK to gradient descent with surrogate derivatives, i.e., SGL. We carefully define this generalization and expand the existing key theorems on the NTK with mathematical rigor. Further, we illustrate our findings with numerical experiments. Finally, we numerically compare SGL in networks with sign activation function and finite width to kernel regression with the surrogate gradient NTK; the results confirm that the surrogate gradient NTK provides a good characterization of SGL.",
      "authors": [
        "Luke Eilers",
        "Raoul-Martin Memmesheimer",
        "Sven Goedeke"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kfdEXQu6MC",
      "cdate": 1715797322107,
      "mdate": 1736851290678,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444470"
    },
    {
      "id": "232VcN8tSx",
      "title": "GREATS: Online Selection of High-Quality Data for LLM Training in Every Iteration",
      "abstract": "Online batch selection methods offer an adaptive alternative to static training data selection by dynamically selecting data batches during training. However, existing methods either rely on impractical reference models or simple heuristics that may not capture true data informativeness. To address these limitations, we propose \\emph{GREedy Approximation Taylor Selection} (GREATS), a principled and efficient online batch selection method that applies greedy algorithm to optimize the data batch quality approximated by Taylor expansion. We develop a series of techniques to scale GREATS to large-scale model training. Extensive experiments with large language models (LLMs) demonstrate that GREATS significantly improves training convergence speed and generalization performance.",
      "authors": [
        "Jiachen T. Wang",
        "Tong Wu",
        "Dawn Song",
        "Prateek Mittal",
        "Ruoxi Jia"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=232VcN8tSx",
      "cdate": 1715797192808,
      "mdate": 1736999966554,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444475"
    },
    {
      "id": "e2R4WNHHGQ",
      "title": "Fair Bilevel Neural Network (FairBiNN): On Balancing fairness and accuracy via Stackelberg Equilibrium",
      "abstract": "The persistent challenge of bias in machine learning models necessitates robust solutions to ensure parity and equal treatment across diverse groups, particularly in classification tasks. Current methods for mitigating bias often result in information loss and an inadequate balance between accuracy and fairness. To address this, we propose a novel methodology grounded in bilevel optimization principles. Our deep learning-based approach concurrently optimizes for both accuracy and fairness objectives, and under certain assumptions, achieving proven Pareto optimal solutions while mitigating bias in the trained model. Theoretical analysis indicates that the upper bound on the loss incurred by this method is less than or equal to the loss of the Lagrangian approach, which involves adding a regularization term to the loss function. We demonstrate the efficacy of our model primarily on tabular datasets such as UCI Adult and Heritage Health. When benchmarked against state-of-the-art fairness methods, our model exhibits superior performance, advancing fairness-aware machine learning solutions and bridging the accuracy-fairness gap. The implementation of FairBiNN is available on https://github.com/yazdanimehdi/FairBiNN.",
      "authors": [
        "Mehdi Yazdani-Jahromi",
        "Ali Khodabandeh Yalabadi",
        "Amirarsalan Rajabi",
        "Aida Tayebi",
        "Ivan Garibay",
        "Ozlem Garibay"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=e2R4WNHHGQ",
      "cdate": 1715797144298,
      "mdate": 1730874002118,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444481"
    },
    {
      "id": "G2dYZJO4BE",
      "title": "Achievable distributional robustness when the robust risk is only partially identified",
      "abstract": "In safety-critical applications, machine learning models should generalize well under worst-case distribution shifts, that is, have a small robust risk. Invariance-based algorithms can provably take advantage of structural assumptions on the shifts when the training distributions are heterogeneous enough to identify the robust risk. However, in practice, such identifiability conditions are rarely satisfied – a scenario so far underexplored in the theoretical literature. In this paper, we aim to fill the gap and propose to study the more general setting of partially identifiable robustness. In particular, we define a new risk measure, the identifiable robust risk, and its corresponding (population) minimax quantity that is an algorithm-independent measure for the best achievable robustness under partial identifiability. We introduce these concepts broadly, and then study them within the framework of linear structural causal models for concreteness of the presentation. We use the introduced minimax quantity to show how previous approaches provably achieve suboptimal robustness in the partially identifiable case. We confirm our findings through empirical simulations and real-world experiments and demonstrate how the test error of existing robustness methods grows increasingly suboptimal as the proportion of previously unseen test directions increases.",
      "authors": [
        "Julia Kostin",
        "Nicola Gnecco",
        "Fanny Yang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=G2dYZJO4BE",
      "cdate": 1715797136058,
      "mdate": 1737029676255,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444487"
    },
    {
      "id": "rIOTceoNc8",
      "title": "Graph Coarsening with Message-Passing Guarantees",
      "abstract": "Graph coarsening aims to reduce the size of a large graph while preserving some of its key properties, which has been used in many applications to reduce computational load and memory footprint. For instance, in graph machine learning, training Graph Neural Networks (GNNs) on coarsened graphs leads to drastic savings in time and memory. However, GNNs rely on the Message-Passing (MP) paradigm, and classical spectral preservation guarantees for graph coarsening do not directly lead to theoretical guarantees when performing naive message-passing on the coarsened graph.\n\nIn this work, we propose a new message-passing operation specific to coarsened graphs, which exhibit theoretical guarantees on the preservation of the propagated signal. Interestingly, and in a sharp departure from previous proposals, this operation on coarsened graphs is oriented, even when the original graph is undirected. We conduct node classification tasks on synthetic and real data and observe improved results compared to performing naive message-passing on the coarsened graph.",
      "authors": [
        "Antonin Joly",
        "Nicolas Keriven"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rIOTceoNc8",
      "cdate": 1715797125618,
      "mdate": 1730874002018,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444491"
    },
    {
      "id": "FwhM1Zpyft",
      "title": "Scalable Neural Network Verification with Branch-and-bound Inferred Cutting Planes",
      "abstract": "Recently, cutting-plane methods such as GCP-CROWN have been explored to enhance neural network verifiers and made significant advancements. However, GCP-CROWN currently relies on ${\\it generic}$ cutting planes (\"cuts\") generated from external mixed integer programming (MIP) solvers. Due to the poor scalability of MIP solvers, large neural networks cannot benefit from these cutting planes. In this paper, we exploit the structure of the neural network verification problem to generate efficient and scalable cutting planes ${\\it specific}$ to this problem setting. We propose a novel approach, Branch-and-bound Inferred Cuts with COnstraint Strengthening (BICCOS), that leverages the logical relationships of neurons within verified subproblems in the branch-and-bound search tree, and we introduce cuts that preclude these relationships in other subproblems. We develop a mechanism that assigns influence scores to neurons in each path to allow the strengthening of these cuts. Furthermore, we design a multi-tree search technique to identify more cuts, effectively narrowing the search space and accelerating the BaB algorithm. Our results demonstrate that BICCOS can generate hundreds of useful cuts during the branch-and-bound process and consistently increase the number of verifiable instances compared to other state-of-the-art neural network verifiers on a wide range of benchmarks, including large networks that previous cutting plane methods could not scale to.",
      "authors": [
        "Duo Zhou",
        "Christopher Brix",
        "Grani A. Hanasusanto",
        "Huan Zhang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FwhM1Zpyft",
      "cdate": 1715797120283,
      "mdate": 1737115515952,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444497"
    },
    {
      "id": "itztwTAcN6",
      "title": "A Universal Growth Rate for Learning with Smooth Surrogate Losses",
      "abstract": "This paper presents a comprehensive analysis of the growth rate of $H$-consistency bounds (and excess error bounds) for various surrogate losses used in classification. We prove a square-root growth rate near zero for smooth margin-based surrogate losses in binary classification, providing both upper and lower bounds under mild assumptions. This result also translates to excess error bounds. Our lower bound requires weaker conditions than those in previous work for excess error bounds, and our upper bound is entirely novel. Moreover, we extend this analysis to multi-class classification with a series of novel results, demonstrating a universal square-root growth rate for smooth *comp-sum* and *constrained losses*, covering common choices for training neural networks in multi-class classification. Given this universal rate, we turn to the question of choosing among different surrogate losses. We first examine how $H$-consistency bounds vary across surrogates based on the number of classes. Next, ignoring constants and focusing on behavior near zero, we identify *minimizability gaps* as the key differentiating factor in these bounds. Thus, we thoroughly analyze these gaps, to guide surrogate loss selection, covering: comparisons across different comp-sum losses, conditions where gaps become zero, and general conditions leading to small gaps.  Additionally, we demonstrate the key role of minimizability gaps in comparing excess error bounds and $H$-consistency bounds.",
      "authors": [
        "Anqi Mao",
        "Mehryar Mohri",
        "Yutao Zhong"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=itztwTAcN6",
      "cdate": 1715797001167,
      "mdate": 1730874001887,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444502"
    },
    {
      "id": "wDDvJzvvBR",
      "title": "Learning Spatially-Aware Language and Audio Embeddings",
      "abstract": "Humans can picture a sound scene given an imprecise natural language description. For example, it is easy to imagine an acoustic environment given a phrase like \"the lion roar came from right behind me!\". For a machine to have the same degree of comprehension,  the machine must know what a lion is (semantic attribute), what the concept of \"behind\" is (spatial attribute) and how these pieces of linguistic information align with the semantic and spatial attributes of the sound (what a roar sounds like when its coming from behind). \nState-of-the-art audio foundation models, such as CLAP, which learn to map between audio scenes and natural textual descriptions, are trained on non-spatial audio and text pairs, and hence lack spatial awareness. In contrast, sound event localization and detection models are limited to recognizing sounds from a fixed number of classes, and they localize the source to absolute position (e.g., 0.2m) rather than a position described using natural language (e.g., \"next to me\"). To address these gaps, we present ELSA (Embeddings for Language and Spatial Audio), a spatially aware-audio and text embedding model trained using multimodal contrastive learning. ELSA supports non-spatial audio, spatial audio, and open vocabulary text captions describing both the spatial and semantic components of sound. To train ELSA: (a) we spatially augment  the audio and captions of three open-source audio datasets totaling 4,738 hours and 890,038 samples of audio comprised from 8,972 simulated spatial configurations, and (b) we design an encoder to capture the semantics of non-spatial audio, and the semantics and spatial attributes of spatial audio using contrastive learning. ELSA is a single model that is competitive with state-of-the-art for both semantic retrieval and 3D source localization.  In particular, ELSA achieves +2.8\\% mean audio-to-text and text-to-audio R@1 above the LAION-CLAP baseline, and outperforms by -11.6° mean-absolute-error in 3D source localization over the SeldNET baseline on the TUT Sound Events 2018 benchmark. Moreover, we show that the representation-space of ELSA is structured, enabling swapping of direction of audio via vector arithmetic of two directional text embeddings.",
      "authors": [
        "Bhavika Suresh Devnani",
        "Skyler Seto",
        "Zakaria Aldeneh",
        "Alessandro Toso",
        "YELENA MENYAYLENKO",
        "Barry-John Theobald",
        "Jonathan Sheaffer",
        "Miguel Sarabia"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=wDDvJzvvBR",
      "cdate": 1715796807924,
      "mdate": 1730874001804,
      "matched_keywords": [
        "foundation model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.444507"
    },
    {
      "id": "RQCmMSSzvI",
      "title": "Non-Asymptotic Uncertainty Quantification in High-Dimensional Learning",
      "abstract": "Uncertainty quantification (UQ) is a crucial but challenging task in many high-dimensional learning problems to increase the confidence of a given predictor. We develop a new data-driven approach for UQ in regression that applies both to classical optimization approaches such as the LASSO as well as to neural networks. One of the most notable UQ techniques is the debiased LASSO, which modifies the LASSO to allow for the construction of asymptotic confidence intervals by decomposing the estimation error into a Gaussian and an asymptotically vanishing bias component. However, in real-world problems with finite-dimensional data, the bias term is often too significant to disregard, resulting in overly narrow confidence intervals. Our work rigorously addresses this issue and derives a data-driven adjustment that corrects the confidence intervals for a large class of predictors by estimating the means and variances of the bias terms from training data, exploiting high-dimensional concentration phenomena. This gives rise to non-asymptotic confidence intervals, which can help avoid overestimating certainty in critical applications such as MRI diagnosis. Importantly, our analysis extends beyond sparse regression to data-driven predictors like neural networks, enhancing the reliability of model-based deep learning. Our findings bridge the gap between established theory and the practical applicability of such methods.",
      "authors": [
        "Frederik Hoppe",
        "Claudio Mayrink Verdun",
        "Hannah Laus",
        "Felix Krahmer",
        "Holger Rauhut"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=RQCmMSSzvI",
      "cdate": 1715796707779,
      "mdate": 1737024512767,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444512"
    },
    {
      "id": "QI1ScdeQjp",
      "title": "Upping the Game: How 2D U-Net Skip Connections Flip 3D Segmentation",
      "abstract": "In the present study, we introduce an innovative structure for 3D medical image segmentation that effectively integrates 2D U-Net-derived skip connections into the architecture of 3D convolutional neural networks (3D CNNs). Conventional 3D segmentation techniques predominantly depend on isotropic 3D convolutions for the extraction of volumetric features, which frequently engenders inefficiencies due to the varying information density across the three orthogonal axes in medical imaging modalities such as computed tomography (CT) and magnetic resonance imaging (MRI). This disparity leads to a decline in axial-slice plane feature extraction efficiency, with slice plane features being comparatively underutilized relative to features in the time-axial. To address this issue, we introduce the U-shaped Connection (uC), utilizing simplified 2D U-Net in place of standard skip connections to augment the extraction of the axial-slice plane features while concurrently preserving the volumetric context afforded by 3D convolutions. Based on uC, we further present uC 3DU-Net, an enhanced 3D U-Net backbone that integrates the uC approach to facilitate optimal axial-slice plane feature utilization. Through rigorous experimental validation on five publicly accessible datasets—FLARE2021, OIMHS, FeTA2021, AbdomenCT-1K, and BTCV, the proposed method surpasses contemporary state-of-the-art models. Notably, this performance is achieved while reducing the number of parameters and computational complexity. This investigation underscores the efficacy of incorporating 2D convolutions within the framework of 3D CNNs to overcome the intrinsic limitations of volumetric segmentation, thereby potentially expanding the frontiers of medical image analysis. Our implementation is available at https://github.com/IMOP-lab/U-Shaped-Connection.",
      "authors": [
        "Xingru Huang",
        "Yihao Guo",
        "Jian Huang",
        "Tianyun Zhang",
        "HE HONG",
        "Shaowei Jiang",
        "Yaoqi Sun"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=QI1ScdeQjp",
      "cdate": 1715796656018,
      "mdate": 1734574574599,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444518"
    },
    {
      "id": "G0LfcMiRkc",
      "title": "Linguistic Collapse: Neural Collapse in (Large) Language Models",
      "abstract": "Neural collapse ($\\mathcal{NC}$) is a phenomenon observed in classification tasks where top-layer representations collapse into their class means, which become equinorm, equiangular and aligned with the classifiers.\nThese behaviors -- associated with generalization and robustness -- would manifest under specific conditions: models are trained towards zero loss, with noise-free labels belonging to balanced classes, which do not outnumber the model's hidden dimension.\nRecent studies have explored $\\mathcal{NC}$ in the absence of one or more of these conditions to extend and capitalize on the associated benefits of ideal geometries.\nLanguage modeling presents a curious frontier, as \\textit{training by token prediction} constitutes a classification task where none of the conditions exist: the vocabulary is imbalanced and exceeds the embedding dimension; different tokens might correspond to similar contextual embeddings; and large language models (LLMs) in particular are typically only trained for a few epochs.\nThis paper empirically investigates the impact of scaling the architectures and training of causal language models (CLMs) on their progression towards $\\mathcal{NC}$.\nWe find that $\\mathcal{NC}$ properties that develop with scale (and regularization) are linked to generalization.\nMoreover, there is evidence of some relationship between $\\mathcal{NC}$ and generalization independent of scale.\nOur work thereby underscores the generality of $\\mathcal{NC}$ as it extends to the novel and more challenging setting of language modeling.\nDownstream, we seek to inspire further research on the phenomenon to deepen our understanding of LLMs -- and neural networks at large -- and improve existing architectures based on $\\mathcal{NC}$-related properties.\nOur code is hosted on GitHub: [`https://github.com/rhubarbwu/linguistic-collapse`](https://github.com/rhubarbwu/linguistic-collapse).",
      "authors": [
        "Robert Wu",
        "Vardan Papyan"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=G0LfcMiRkc",
      "cdate": 1715796647771,
      "mdate": 1737004638675,
      "matched_keywords": [
        "large language model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444523"
    },
    {
      "id": "udTwwF7tks",
      "title": "Iteratively Refined Early Interaction Alignment for Subgraph Matching based Graph Retrieval",
      "abstract": "Graph retrieval based on subgraph isomorphism has several real-world applications such as scene graph retrieval, molecular fingerprint detection and circuit design. Roy et al. [35] proposed IsoNet, a late interaction model for subgraph matching, which first computes the node and edge embeddings of each graph independently of paired graph  and then computes a trainable alignment map. Here, we present $\\texttt{IsoNet++}$, an early interaction graph neural network (GNN), based on several technical innovations. First, we compute embeddings of all nodes by passing messages within and across the two input graphs, guided by an *injective alignment* between their nodes. Second, we update this alignment in a lazy fashion over multiple *rounds*. Within each round, we run a layerwise GNN from scratch, based on the current state of the alignment. After the completion of one round of GNN, we use the last-layer embeddings to update the alignments, and proceed to the next round. Third, $\\texttt{IsoNet++}$ incorporates a novel notion of node-pair partner interaction. Traditional early interaction computes attention between a node and its potential partners in the other graph, the attention then controlling messages passed across graphs. We consider *node pairs* (not single nodes) as potential partners. Existence of an edge between the nodes in one graph and non-existence in the other provide vital signals for refining the alignment. Our experiments on several datasets show that the alignments get progressively refined with successive rounds,\nresulting in significantly better retrieval performance than existing methods. We demonstrate that all three innovations contribute to the enhanced accuracy. Our code and datasets are publicly available at https://github.com/structlearning/isonetpp.",
      "authors": [
        "Ashwin Ramachandran",
        "Vaibhav Raj",
        "Indradyumna Roy",
        "Soumen Chakrabarti",
        "Abir De"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=udTwwF7tks",
      "cdate": 1715796620946,
      "mdate": 1730874001454,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444529"
    },
    {
      "id": "A7wC1CTkYl",
      "title": "Efficient Lifelong Model Evaluation in an Era of Rapid Progress",
      "abstract": "Standardized benchmarks drive progress in machine learning. However, with repeated testing, the risk of overfitting grows as algorithms over-exploit benchmark idiosyncrasies. In our work, we seek to mitigate this challenge by compiling \\textit{ever-expanding} large-scale benchmarks called \\textit{Lifelong Benchmarks}. As exemplars of our approach, we create \\textit{Lifelong-CIFAR10} and \\textit{Lifelong-ImageNet}, containing (for now) 1.69M and 1.98M test samples, respectively. While reducing overfitting, lifelong benchmarks introduce a key challenge: the high cost of evaluating a growing number of models across an ever-expanding sample set. To address this challenge, we also introduce an efficient evaluation framework: \\textit{Sort \\& Search (S\\&S)}, which reuses previously evaluated models by leveraging dynamic programming algorithms to selectively rank and sub-select test samples, enabling cost-effective lifelong benchmarking. Extensive empirical evaluations across $\\sim$31,000 models demonstrate that \\textit{S\\&S} achieves highly-efficient approximate accuracy measurement, reducing compute cost from 180 GPU days to 5 GPU hours ($\\sim$1000x reduction) on a single A100 GPU, with low approximation error. As such, lifelong benchmarks offer a robust, practical solution to the ``benchmark exhaustion'' problem.",
      "authors": [
        "Ameya Prabhu",
        "Vishaal Udandarao",
        "Philip Torr",
        "Matthias Bethge",
        "Adel Bibi",
        "Samuel Albanie"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=A7wC1CTkYl",
      "cdate": 1715796559162,
      "mdate": 1730874001414,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444534"
    },
    {
      "id": "Hew2JSDycr",
      "title": "BiScope: AI-generated Text Detection by Checking Memorization of Preceding Tokens",
      "abstract": "Detecting text generated by Large Language Models (LLMs) is a pressing need in\n order to identify and prevent misuse of these powerful models in a wide range of\n applications, which have highly undesirable consequences such as misinformation\n and academic dishonesty. Given a piece of subject text, many existing detection\n methods work by measuring the difficulty of LLM predicting the next token in\n the text from their prefix. In this paper, we make a critical observation that\n how well the current token’s output logits memorizes the closely preceding input\n tokens also provides strong evidence. Therefore, we propose a novel bi-directional\n calculation method that measures the cross-entropy losses between an output\n logits and the ground-truth token (forward) and between the output logits and\n the immediately preceding input token (backward). A classifier is trained to\n make the final prediction based on the statistics of these losses. We evaluate our\n system, named BISCOPE, on texts generated by five latest commercial LLMs\n across five heterogeneous datasets, including both natural language and code.\n BISCOPE demonstrates superior detection accuracy and robustness compared to six\n existing baseline methods, exceeding the state-of-the-art non-commercial methods’\n detection accuracy by over 0.30 F1 score, achieving over 0.95 detection F1 score\n on average. It also outperforms the best commercial tool GPTZero that is based on\n a commercial LLM trained with an enormous volume of data. Code is available at https://github.com/MarkGHX/BiScope.",
      "authors": [
        "Hanxi Guo",
        "Siyuan Cheng",
        "Xiaolong Jin",
        "ZHUO ZHANG",
        "Kaiyuan Zhang",
        "Guanhong Tao",
        "Guangyu Shen",
        "Xiangyu Zhang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Hew2JSDycr",
      "cdate": 1715796536054,
      "mdate": 1730874001339,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444539"
    },
    {
      "id": "p3tSEFMwpG",
      "title": "Drift-Resilient TabPFN: In-Context Learning Temporal Distribution Shifts on Tabular Data",
      "abstract": "While most ML models expect independent and identically distributed data, this assumption is often violated in real-world scenarios due to distribution shifts, resulting in the degradation of machine learning model performance. Until now, no tabular method has consistently outperformed classical supervised learning, which ignores these shifts. To address temporal distribution shifts, we present Drift-Resilient TabPFN, a fresh approach based on In-Context Learning with a Prior-Data Fitted Network that learns the learning algorithm itself: it accepts the entire training dataset as input and makes predictions on the test set in a single forward pass. Specifically, it learns to approximate Bayesian inference on synthetic datasets drawn from a prior that specifies the model's inductive bias. This prior is based on structural causal models (SCM), which gradually shift over time. To model shifts of these causal models, we use a secondary SCM, that specifies changes in the primary model parameters. The resulting Drift-Resilient TabPFN can be applied to unseen data, runs in seconds on small to moderately sized datasets and needs no hyperparameter tuning. Comprehensive evaluations across 18 synthetic and real-world datasets demonstrate large performance improvements over a wide range of baselines, such as XGB, CatBoost, TabPFN, and applicable methods featured in the Wild-Time benchmark. Compared to the strongest baselines, it improves accuracy from 0.688 to 0.744 and ROC AUC from 0.786 to 0.832 while maintaining stronger calibration. This approach could serve as significant groundwork for further research on out-of-distribution prediction.",
      "authors": [
        "Kai Helli",
        "David Schnurr",
        "Noah Hollmann",
        "Samuel Müller",
        "Frank Hutter"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=p3tSEFMwpG",
      "cdate": 1715796318983,
      "mdate": 1735829321874,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444544"
    },
    {
      "id": "KrHFICMPjm",
      "title": "GUIDE: Real-Time Human-Shaped Agents",
      "abstract": "The recent rapid advancement of machine learning has been driven by increasingly powerful models with the growing availability of training data and computational resources. However, real-time decision-making tasks with limited time and sparse learning signals remain challenging. One way of improving the learning speed and performance of these agents is to leverage human guidance. In this work, we introduce GUIDE, a framework for real-time human-guided reinforcement learning by enabling continuous human feedback and grounding such feedback into dense rewards to accelerate policy learning. Additionally, our method features a simulated feedback module that learns and replicates human feedback patterns in an online fashion, effectively reducing the need for human input while allowing continual training. We demonstrate the performance of our framework on challenging tasks with sparse rewards and visual observations. Our human study involving 50 subjects offers strong quantitative and qualitative evidence of the effectiveness of our approach. With only 10 minutes of human feedback, our algorithm achieves up to 30\\% increase in success rate compared to its RL baseline.",
      "authors": [
        "Lingyu Zhang",
        "Zhengran Ji",
        "Nicholas R Waytowich",
        "Boyuan Chen"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=KrHFICMPjm",
      "cdate": 1715796298928,
      "mdate": 1730874001213,
      "matched_keywords": [
        "reinforcement learning",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444549"
    },
    {
      "id": "x9eFgahVBI",
      "title": "From Unstructured Data to In-Context Learning: Exploring What Tasks Can Be Learned and When",
      "abstract": "Large language models (LLMs) like transformers demonstrate impressive in-context learning (ICL) capabilities, allowing them to make\npredictions for new tasks based on prompt exemplars without parameter updates. While existing ICL theories often assume structured training data resembling ICL tasks (e.g., x-y pairs for linear regression), LLMs are typically trained unsupervised on unstructured text, such as web content, which lacks clear parallels to tasks like word analogy. To address this gap, we examine what enables ICL in models trained on unstructured data, focusing on critical sequence model requirements and training data structure. We find that many ICL capabilities can\nemerge simply from co-occurrence of semantically related word pairs in unstructured data; word analogy completion, for example, can provably arise purely through co-occurrence modeling, using classical language models like continuous bag of words (CBOW), without needing positional information or attention mechanisms. However, positional information becomes crucial for logic reasoning tasks requiring generalization to unseen tokens. Finally, we identify two cases where ICL fails: one in logic reasoning tasks that require generalizing to new, unseen patterns, and another in analogy completion where relevant word pairs appear only in fixed training positions. These findings suggest that LLMs' ICL abilities depend heavily on the structural elements within their training data.",
      "authors": [
        "Kevin Christian Wibisono",
        "Yixin Wang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=x9eFgahVBI",
      "cdate": 1715796191637,
      "mdate": 1730874001117,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444554"
    },
    {
      "id": "tWkL7k1u5v",
      "title": "Improving Equivariant Model Training via Constraint Relaxation",
      "abstract": "Equivariant neural networks have been widely used in a variety of applications due to their ability to generalize well in tasks where the underlying data symmetries are known. Despite their successes, such networks can be difficult to optimize and require careful hyperparameter tuning to train successfully. In this work, we propose a novel framework for improving the optimization of such models by relaxing the hard equivariance constraint during training: We relax the equivariance constraint of the network's intermediate layers by introducing an additional non-equivariant term that we progressively constrain until we arrive at an equivariant solution. By controlling the magnitude of the activation of the additional relaxation term, we allow the model to optimize over a larger hypothesis space containing approximate equivariant networks and converge back to an equivariant solution at the end of training. We provide experimental results on different state-of-the-art network architectures, demonstrating how this training framework can result in equivariant models with improved generalization performance. Our code is available at https://github.com/StefanosPert/Equivariant_Optimization_CR",
      "authors": [
        "Stefanos Pertigkiozoglou",
        "Evangelos Chatzipantazis",
        "Shubhendu Trivedi",
        "Kostas Daniilidis"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tWkL7k1u5v",
      "cdate": 1715796093135,
      "mdate": 1730874001069,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444560"
    },
    {
      "id": "9f5tOXKoMC",
      "title": "A Bayesian Approach to Data Point Selection",
      "abstract": "Data point selection (DPS) is becoming a critical topic in deep learning due to the ease of acquiring uncurated training data compared to the difficulty of obtaining curated or processed data. \nExisting approaches to DPS are predominantly based on a bi-level optimisation (BLO) formulation, which is demanding in terms of memory and computation, and exhibits some theoretical defects regarding minibatches.\nThus, we propose a novel Bayesian approach to DPS. We view the DPS problem as posterior inference in a novel Bayesian model where the posterior distributions of the instance-wise weights and the main neural network parameters are inferred under a reasonable prior and likelihood model.\nWe employ stochastic gradient Langevin MCMC sampling to learn the main network and instance-wise weights jointly, ensuring convergence even with minibatches. Our update equation is comparable to the widely used SGD and much more efficient than existing BLO-based methods. Through controlled experiments in both the vision and language domains, we present the proof-of-concept. Additionally, we demonstrate that our method scales effectively to large language models and facilitates automated per-task optimization for instruction fine-tuning datasets.",
      "authors": [
        "Xinnuo Xu",
        "Minyoung Kim",
        "Royson Lee",
        "Brais Martinez",
        "Timothy Hospedales"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9f5tOXKoMC",
      "cdate": 1715795964483,
      "mdate": 1730874001009,
      "matched_keywords": [
        "large language model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444565"
    },
    {
      "id": "LYx4w3CAgy",
      "title": "LLM-Check: Investigating Detection of Hallucinations in Large Language Models",
      "abstract": "While Large Language Models (LLMs) have become immensely popular due to their outstanding performance on a broad range of tasks, these models are prone to producing hallucinations— outputs that are fallacious or fabricated yet often appear plausible or tenable at a glance. In this paper, we conduct a comprehensive investigation into the nature of hallucinations within LLMs and furthermore explore effective techniques for detecting such inaccuracies in various real-world settings. Prior approaches to detect hallucinations in LLM outputs, such as consistency checks or retrieval-based methods, typically assume access to multiple model responses or large databases. These techniques, however, tend to be computationally expensive in practice, thereby limiting their applicability to real-time analysis. In contrast, in this work, we seek to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM. In addition, we also study hallucination detection in scenarios where ground-truth references are also available, such as in the setting of Retrieval-Augmented Generation (RAG). We demonstrate that the proposed detection methods are extremely compute-efficient, with speedups of up to 45x and 450x over other baselines, while achieving significant improvements in detection performance over diverse datasets.",
      "authors": [
        "Gaurang Sriramanan",
        "Siddhant Bharti",
        "Vinu Sankar Sadasivan",
        "Shoumik Saha",
        "Priyatham Kattakinda",
        "Soheil Feizi"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LYx4w3CAgy",
      "cdate": 1715795900488,
      "mdate": 1736461833752,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444570"
    },
    {
      "id": "qRnmLJQHgx",
      "title": "4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities",
      "abstract": "Current multimodal and multitask foundation models, like 4M or UnifiedIO, show promising results. However, their out-of-the-box abilities to accept diverse inputs and perform diverse tasks are limited by the (usually small) number of modalities and tasks they are trained on. In this paper, we develop a single any-to-any model trained on tens of highly diverse modalities and by performing co-training on large-scale multimodal datasets and text corpora. This includes training on images and text along with several semantic and geometric modalities, feature maps from recent state of the art models like DINOv2 and ImageBind, pseudo labels of specialist models like SAM and 4DHumans, and a range of new modalities that allow for novel ways to interact with the model and steer the generation, for example, image metadata or color palettes.\n\nA crucial step in this process is performing discrete tokenization on various modalities, whether they are image-like, neural network feature maps, vectors, structured data like instance segmentation or human poses, or data that can be represented as text.\n    \nThrough this, we show the possibility of training one model to solve at least 3x more tasks/modalities than existing models and doing so without a loss in performance. In addition, this enables more fine-grained and controllable multimodal generation capabilities and allows studying the distillation of models trained on diverse data and objectives into one unified model.\nWe scale the training to a three billion parameter and different datasets. The multimodal models and training code are open sourced at https://4m.epfl.ch/.",
      "authors": [
        "Roman Bachmann",
        "Oğuzhan Fatih Kar",
        "David Mizrahi",
        "Ali Garjani",
        "Mingfei Gao",
        "David Griffiths",
        "Jiaming Hu",
        "Afshin Dehghan",
        "Amir Zamir"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qRnmLJQHgx",
      "cdate": 1715795825147,
      "mdate": 1730874000876,
      "matched_keywords": [
        "foundation model",
        "multimodal",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444575"
    },
    {
      "id": "QEUntqKvmm",
      "title": "The surprising efficiency of temporal difference learning for rare event prediction",
      "abstract": "We quantify the efficiency of temporal difference (TD) learning over the direct, or Monte Carlo (MC), estimator for policy evaluation in reinforcement learning, with an emphasis on estimation of quantities related to rare events. Policy evaluation is complicated in the rare event setting by the long timescale of the event and by the need for \\emph{relative accuracy} in estimates of very small values.  Specifically, we focus on least-squares TD (LSTD) prediction for finite state Markov chains, and show that LSTD can achieve relative accuracy far more efficiently than MC.  We prove a central limit theorem for the LSTD estimator and upper bound the \n  \\emph{relative asymptotic variance}\n  by simple quantities characterizing the connectivity of states relative to the transition probabilities between them. Using this bound, we show that, even when both the timescale of the rare event and the relative accuracy of the MC estimator are exponentially large in the number of states, LSTD maintains a fixed level of relative accuracy with  a total number of observed transitions of the Markov chain that is only \\emph{polynomially} large in the number of states.",
      "authors": [
        "Xiaoou Cheng",
        "Jonathan Weare"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=QEUntqKvmm",
      "cdate": 1715795791472,
      "mdate": 1736999913188,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444580"
    },
    {
      "id": "e6WrwIvgzX",
      "title": "AutoMix: Automatically Mixing Language Models",
      "abstract": "Large language models (LLMs) are now available from cloud API providers in various sizes and configurations. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix are two key technical contributions. First, it has a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring extensive training. Second, given that self-verification can be noisy, it employs a POMDP based router that can effectively select an appropriately sized model, based on answer confidence. Experiments across five language models and five challenging datasets show that Automix consistently surpasses strong baselines, reducing computational cost by over 50\\% for comparable performance.",
      "authors": [
        "Pranjal Aggarwal",
        "Aman Madaan",
        "Ankit Anand",
        "Srividya Pranavi Potharaju",
        "Swaroop Mishra",
        "Pei Zhou",
        "Aditya Gupta",
        "Dheeraj Rajagopal",
        "Karthik Kappaganthu",
        "Yiming Yang",
        "Shyam Upadhyay",
        "Manaal Faruqui",
        "Mausam ."
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=e6WrwIvgzX",
      "cdate": 1715795789187,
      "mdate": 1730874000753,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444586"
    },
    {
      "id": "iF7MnXnxRw",
      "title": "Understanding the Differences in Foundation Models: Attention, State  Space Models, and Recurrent Neural Networks",
      "abstract": "Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.",
      "authors": [
        "Jerome Sieber",
        "Carmen Amo Alonso",
        "Alexandre Didier",
        "Melanie Zeilinger",
        "Antonio Orvieto"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iF7MnXnxRw",
      "cdate": 1715795587480,
      "mdate": 1730874000639,
      "matched_keywords": [
        "foundation model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444591"
    },
    {
      "id": "9OHXQybMZB",
      "title": "Aligning Model Properties via Conformal Risk Control",
      "abstract": "AI model alignment is crucial due to inadvertent biases in training data and the underspecified machine learning pipeline, where models with excellent test metrics may not meet end-user requirements. While post-training alignment via human feedback shows promise, these methods are often limited to generative AI settings where humans can interpret and provide feedback on model outputs. In traditional non-generative settings with numerical or categorical outputs, detecting misalignment through single-sample outputs remains challenging, and enforcing alignment during training requires repeating costly training processes.\nIn this paper we consider an alternative strategy. We propose interpreting model alignment through property testing, defining an aligned model $f$ as one belonging to a subset $\\mathcal{P}$ of functions that exhibit specific desired behaviors. We focus on post-processing a pre-trained model $f$ to better align with $\\mathcal{P}$ using conformal risk control. Specifically, we develop a general procedure for converting queries for testing a given property $\\mathcal{P}$ to a collection of loss functions suitable for use in a conformal risk control algorithm. We prove a probabilistic guarantee that the resulting conformal interval around $f$ contains a function approximately satisfying $\\mathcal{P}$. We exhibit applications of our methodology on a collection of supervised learning datasets for (shape-constrained) properties such as monotonicity and concavity. The general procedure is flexible and can be applied to a wide range of desired properties. Finally, we prove that pre-trained models will always require alignment techniques even as model sizes or training data increase, as long as the training data contains even small biases.",
      "authors": [
        "William Overman",
        "Jacqueline Jil Vallon",
        "Mohsen Bayati"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9OHXQybMZB",
      "cdate": 1715795529457,
      "mdate": 1736873998021,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444597"
    },
    {
      "id": "pMaCRgu8GV",
      "title": "Artificial Generational Intelligence: Cultural Accumulation in Reinforcement Learning",
      "abstract": "Cultural accumulation drives the open-ended and diverse progress in capabilities spanning human history. It builds an expanding body of knowledge and skills by combining individual exploration with inter-generational information transmission. Despite its widespread success among humans, the capacity for artificial learning agents to accumulate culture remains under-explored. In particular, approaches to reinforcement learning typically strive for improvements over only a single lifetime. Generational algorithms that do exist fail to capture the open-ended, emergent nature of cultural accumulation, which allows individuals to trade-off innovation and imitation. Building on the previously demonstrated ability for reinforcement learning agents to perform social learning, we find that training setups which balance this with independent learning give rise to cultural accumulation. These accumulating agents outperform those trained for a single lifetime with the same cumulative experience. We explore this accumulation by constructing two models under two distinct notions of a generation: episodic generations, in which accumulation occurs via in-context learning and train-time generations, in which accumulation occurs via in-weights learning. In-context and in-weights cultural accumulation can be interpreted as analogous to knowledge and skill accumulation, respectively. To the best of our knowledge, this work is the first to present general models that achieve emergent cultural accumulation in reinforcement learning, opening up new avenues towards more open-ended learning systems, as well as presenting new opportunities for modelling human culture.",
      "authors": [
        "Jonathan Cook",
        "Chris Lu",
        "Edward Hughes",
        "Joel Z Leibo",
        "Jakob Nicolaus Foerster"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pMaCRgu8GV",
      "cdate": 1715795481045,
      "mdate": 1730874000547,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444602"
    },
    {
      "id": "3Tzcot1LKb",
      "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward",
      "abstract": "Direct Preference Optimization (DPO) is a widely used offline preference optimization algorithm that reparameterizes reward functions in reinforcement learning from human feedback (RLHF) to enhance simplicity and training stability. In this work, we propose SimPO, a simpler yet more effective approach. The effectiveness of SimPO is attributed to a key design: using the _average_ log probability of a sequence as the implicit reward. This reward formulation better aligns with model generation and eliminates the need for a reference model, making it more compute and memory efficient. Additionally, we introduce a target reward margin to the Bradley-Terry objective to encourage a larger margin between the winning and losing responses, further improving the algorithm's performance. We compare SimPO to DPO and its latest variants across various state-of-the-art training setups, including both base and instruction-tuned models such as Mistral, Llama 3, and Gemma 2. We evaluate on extensive chat-based evaluation benchmarks, including AlpacaEval 2, MT-Bench, and Arena-Hard. Our results demonstrate that SimPO consistently and significantly outperforms existing approaches without substantially increasing response length. Specifically, SimPO outperforms DPO by up to 6.4 points on AlpacaEval 2 and by up to 7.5 points on Arena-Hard. Our top-performing model, built on Gemma-2-9B-it, achieves a 72.4\\% length-controlled win rate on AlpacaEval 2, a 59.1\\% win rate on Arena-Hard, and ranks 1st on Chatbot Arena among $<$10B models with real user votes.",
      "authors": [
        "Yu Meng",
        "Mengzhou Xia",
        "Danqi Chen"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3Tzcot1LKb",
      "cdate": 1715795397796,
      "mdate": 1730874000421,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444608"
    },
    {
      "id": "o3i1JEfzKw",
      "title": "Provable Partially Observable Reinforcement Learning with Privileged Information",
      "abstract": "Partial observability of the underlying states generally presents significant challenges for reinforcement learning (RL). In practice, certain *privileged information* , e.g., the access to states from simulators, has been exploited in training and achieved prominent empirical successes. To better understand the benefits of privileged information, we revisit and examine several simple and practically used paradigms in this setting, with both computation and sample efficiency analyses. Specifically, we first formalize the empirical paradigm of *expert distillation* (also known as  *teacher-student* learning), demonstrating its pitfall in finding near-optimal policies. We then identify a condition of the partially observable environment, the deterministic filter condition, under which expert distillation achieves sample and computational complexities that are *both* polynomial. Furthermore, we investigate another successful empirical paradigm of *asymmetric actor-critic*, and focus on the more challenging setting of observable partially observable Markov decision processes. We develop a belief-weighted optimistic asymmetric actor-critic algorithm with polynomial sample and quasi-polynomial computational complexities, where one key component is a new provable oracle for learning belief states that preserve *filter stability* under a misspecified model, which may be of independent interest. Finally, we also investigate the provable efficiency of partially observable multi-agent RL (MARL) with privileged information. We develop algorithms with the feature of centralized-training-with-decentralized-execution, a popular framework in empirical MARL, with polynomial sample and (quasi-)polynomial computational complexity in both paradigms above. Compared with a few recent related theoretical studies, our focus is on understanding practically inspired algorithmic paradigms, without computationally intractable oracles.",
      "authors": [
        "Yang Cai",
        "Xiangyu Liu",
        "Argyris Oikonomou",
        "Kaiqing Zhang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=o3i1JEfzKw",
      "cdate": 1715795396190,
      "mdate": 1736713962446,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444613"
    },
    {
      "id": "8mZc259r8X",
      "title": "Learning Cut Generating Functions for Integer Programming",
      "abstract": "The branch-and-cut algorithm is the method of choice to solve large scale integer programming problems in practice. A key ingredient of branch-and-cut is the use of *cutting planes* which are derived constraints that reduce the search space for an optimal solution. Selecting effective cutting planes to produce small branch-and-cut trees is a critical challenge in the branch-and-cut algorithm. Recent advances have employed a data-driven approach to select good cutting planes from a parameterized family, aimed at reducing the branch-and-bound tree size (in expectation) for a given distribution of integer programming instances. We extend this idea to the selection of the best cut generating function (CGF), which is a tool in the integer programming literature for generating a wide variety of cutting planes that generalize the well-known Gomory Mixed-Integer (GMI) cutting planes. We provide rigorous sample complexity bounds for the selection of an effective CGF from certain parameterized families that provably performs well for any specified distribution on the problem instances. Our empirical results show that the selected CGF can outperform the GMI cuts for certain distributions. Additionally, we explore the sample complexity of using neural networks for instance-dependent CGF selection.",
      "authors": [
        "Hongyu Cheng",
        "Amitabh Basu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=8mZc259r8X",
      "cdate": 1715795359687,
      "mdate": 1730874000286,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444618"
    },
    {
      "id": "fAlcxvrOEX",
      "title": "AdjointDEIS: Efficient Gradients for Diffusion Models",
      "abstract": "The optimization of the latents and parameters of diffusion models with respect to some differentiable metric defined on the output of the model is a challenging and complex problem. The sampling for diffusion models is done by solving either the *probability flow* ODE or diffusion SDE wherein a neural network approximates the score function allowing a numerical ODE/SDE solver to be used. However, naive backpropagation techniques are memory intensive, requiring the storage of all intermediate states, and face additional complexity in handling the injected noise from the diffusion term of the diffusion SDE. We propose a novel family of bespoke ODE solvers to the continuous adjoint equations for diffusion models, which we call *AdjointDEIS*. We exploit the unique construction of diffusion SDEs to further simplify the formulation of the continuous adjoint equations using *exponential integrators*. Moreover, we provide convergence order guarantees for our bespoke solvers. Significantly, we show that continuous adjoint equations for diffusion SDEs actually simplify to a simple ODE. Lastly, we demonstrate the effectiveness of AdjointDEIS for guided generation with an adversarial attack in the form of the face morphing problem. Our code will be released on our project page [https://zblasingame.github.io/AdjointDEIS/](https://zblasingame.github.io/AdjointDEIS/)",
      "authors": [
        "Zander W. Blasingame",
        "Chen Liu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fAlcxvrOEX",
      "cdate": 1715795359054,
      "mdate": 1736906669252,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444623"
    },
    {
      "id": "KYHVBsEHuC",
      "title": "DiffuPac: Contextual Mimicry in Adversarial Packets Generation via Diffusion Model",
      "abstract": "In domains of cybersecurity, recent advancements in Machine Learning (ML) and Deep Learning (DL) have significantly enhanced Network Intrusion Detection Systems (NIDS), improving the effectiveness of cybersecurity operations. However, attackers have also leveraged ML/DL to develop sophisticated models that generate adversarial packets capable of evading NIDS detection. Consequently, defenders must study and analyze these models to prepare for the evasion attacks that exploit NIDS detection mechanisms. Unfortunately, conventional generation models often rely on unrealistic assumptions about attackers' knowledge of NIDS components, making them impractical for real-world scenarios. To address this issue, we present DiffuPac, a first-of-its-kind generation model designed to generate adversarial packets that evade detection without relying on specific NIDS components. DiffuPac integrates a pre-trained Bidirectional Encoder Representations from Transformers (BERT) with diffusion model, which, through its capability for conditional denoising and classifier-free guidance, effectively addresses the real-world constraint of limited attacker knowledge. By concatenating malicious packets with contextually relevant normal packets and applying targeted noising only to the malicious packets, DiffuPac seamlessly blends adversarial packets into genuine network traffic. Through evaluations on real-world datasets, we demonstrate that DiffuPac achieves strong evasion capabilities against sophisticated NIDS, outperforming conventional methods by an average of 6.69 percentage points, while preserving the functionality and practicality of the generated adversarial packets.",
      "authors": [
        "Abdullah Bin Jasni",
        "Akiko Manada",
        "Kohei Watabe"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=KYHVBsEHuC",
      "cdate": 1715795355712,
      "mdate": 1730874000188,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444629"
    },
    {
      "id": "3hcn0UxP72",
      "title": "Topological obstruction to the training of shallow ReLU neural networks",
      "abstract": "Studying the interplay between the geometry of the loss landscape and the optimization trajectories of simple neural networks is a fundamental step for understanding their behavior in more complex settings.\nThis paper reveals the presence of topological obstruction in the loss landscape of shallow ReLU neural networks trained using gradient flow. We discuss how the homogeneous nature of the ReLU activation function constrains the training trajectories to lie on a product of quadric hypersurfaces whose shape depends on the particular initialization of the network's parameters. \nWhen the neural network's output is a single scalar, we prove that these quadrics can have multiple connected components, limiting the set of reachable parameters during training. We analytically compute the number of these components and discuss the possibility of mapping one to the other through neuron rescaling and permutation. In this simple setting, we find that the non-connectedness results in a topological obstruction, which, depending on the initialization, can make the global optimum unreachable. We validate this result with numerical experiments.",
      "authors": [
        "Marco Nurisso",
        "Pierrick Leroy",
        "Francesco Vaccarino"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3hcn0UxP72",
      "cdate": 1715795337799,
      "mdate": 1730874000156,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444634"
    },
    {
      "id": "aUHSwmHRVb",
      "title": "MotionTTT: 2D Test-Time-Training Motion Estimation for 3D Motion Corrected MRI",
      "abstract": "A major challenge of the long measurement times in magnetic resonance imaging (MRI), an important medical imaging technology, is that patients may move during data acquisition. This leads to severe motion artifacts in the reconstructed images and volumes. In this paper, we propose MotionTTT a deep learning-based test-time-training (TTT) method for accurate motion estimation. The key idea is that a neural network trained for motion-free reconstruction has a small loss if there is no motion, thus optimizing over motion parameters passed through the reconstruction network enables accurate estimation of motion. The estimated motion parameters enable to correct for the motion and to reconstruct accurate motion-corrected images. Our method uses 2D reconstruction networks to estimate rigid motion in 3D, and constitutes the first deep learning based method for 3D rigid motion estimation towards 3D-motion-corrected MRI. We show that our method can provably reconstruct motion parameters for a simple signal and neural network model. We demonstrate the effectiveness of our method for both retrospectively simulated motion and prospectively collected real motion-corrupted data. Code is available at \\url{https://github.com/MLI-lab/MRI_MotionTTT}.",
      "authors": [
        "Tobit Klug",
        "Kun Wang",
        "Stefan Ruschke",
        "Reinhard Heckel"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=aUHSwmHRVb",
      "cdate": 1715795307627,
      "mdate": 1730874000119,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444640"
    },
    {
      "id": "5PrShrKxoX",
      "title": "Transfer Q-star : Principled Decoding for LLM Alignment",
      "abstract": "Aligning foundation models is essential for their safe and trustworthy deployment. However, traditional fine-tuning methods are computationally intensive and require updating billions of model parameters. A promising alternative, alignment via decoding, adjusts the response distribution directly without model updates to maximize a target reward $r$, thus providing a lightweight and adaptable framework for alignment. However, principled decoding methods rely on oracle access to an optimal Q-function ($Q^*$), which is often unavailable in practice. Hence, prior SoTA methods either approximate this $Q^*$ using $Q^{\\pi_{\\text{sft}}}$ (derived from the reference $\\texttt{SFT}$ model) or rely on short-term rewards, resulting in sub-optimal decoding performance. In this work, we propose $\\texttt{Transfer Q}^*$, which implicitly estimates the optimal value function for a target reward $r$ through a baseline model $\\rho_{\\texttt{BL}}$  aligned with a baseline reward $r_{\\texttt{BL}}$ (which can be different from the target reward $r$). Theoretical analyses of $\\texttt{Transfer Q}^*$ provide a rigorous characterization of its optimality, deriving an upper bound on the sub-optimality gap and identifying a hyperparameter to control the deviation from the pre-trained reference $\\texttt{SFT}$ model based on user needs. Our approach significantly reduces the sub-optimality gap observed in prior SoTA methods and demonstrates superior empirical performance across key metrics such as coherence, diversity, and quality in extensive tests on several synthetic and real datasets.",
      "authors": [
        "Souradip Chakraborty",
        "Soumya Suvra Ghosal",
        "Ming Yin",
        "Dinesh Manocha",
        "Mengdi Wang",
        "Amrit Bedi",
        "Furong Huang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5PrShrKxoX",
      "cdate": 1715795285372,
      "mdate": 1730874000051,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444645"
    },
    {
      "id": "zDaD8zv8tG",
      "title": "A teacher-teacher framework for clinical language representation learning",
      "abstract": "In recent years, there has been a proliferation of ready-to-use large language models (LLMs) designed for various applications, both general-purpose and domain-specific. Instead of advocating for the development of a new model or continuous pretraining of an existing one, this paper introduces a pragmatic teacher-teacher framework to facilitate mutual learning between two pre-existing models.\nBy leveraging two teacher models possessing complementary knowledge, we introduce a LIghtweight kNowledge alignmEnt (LINE) module aimed at harmonizing their knowledge within a unified representation space. This framework is particularly valuable in clinical settings, where stringent regulations and privacy considerations dictate the handling of detailed clinical notes. Our trained LINE module excels in capturing critical information from clinical notes, leveraging highly de-identified data. Validation and downstream tasks further demonstrate the effectiveness of the proposed framework.",
      "authors": [
        "Feiqing Huang",
        "Shenghan Zhang",
        "Sara Morini Sweet",
        "Tianxi Cai"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=zDaD8zv8tG",
      "cdate": 1715795270835,
      "mdate": 1736943662167,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444650"
    },
    {
      "id": "vBah12uVbD",
      "title": "Conformalized Credal Set Predictors",
      "abstract": "Credal sets are sets of probability distributions that are considered as candidates for an imprecisely known ground-truth distribution. In machine learning, they have recently attracted attention as an appealing formalism for uncertainty representation, in particular, due to their ability to represent both the aleatoric and epistemic uncertainty in a prediction. However, the design of methods for learning credal set predictors remains a challenging problem. In this paper, we make use of conformal prediction for this purpose. More specifically, we propose a method for predicting credal sets in the classification task, given training data labeled by probability distributions. Since our method inherits the coverage guarantees of conformal prediction, our conformal credal sets are guaranteed to be valid with high probability (without any assumptions on model or distribution). We demonstrate the applicability of our method on ambiguous classification tasks for uncertainty quantification.",
      "authors": [
        "Alireza Javanmardi",
        "David Stutz",
        "Eyke Hüllermeier"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vBah12uVbD",
      "cdate": 1715795213977,
      "mdate": 1730874000004,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444655"
    },
    {
      "id": "WfpvtH7oC1",
      "title": "Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning",
      "abstract": "Exploration in sparse-reward reinforcement learning (RL) is difficult due to the need for long, coordinated sequences of actions in order to achieve any reward. Skill learning, from demonstrations or interaction, is a promising approach to address this, but skill extraction and inference are expensive for current methods. We present a novel method to extract skills from demonstrations for use in sparse-reward RL, inspired by the popular Byte-Pair Encoding (BPE) algorithm in natural language processing. With these skills, we show strong performance in a variety of tasks, 1000$\\times$ acceleration for skill-extraction and 100$\\times$ acceleration for policy inference. Given the simplicity of our method, skills extracted from 1\\% of the demonstrations in one task can be transferred to a new loosely related task. We also note that such a method yields a finite set of interpretable behaviors. Our code is available at https://github.com/dyunis/subwords_as_skills.",
      "authors": [
        "David Yunis",
        "Justin Jung",
        "Falcon Z Dai",
        "Matthew Walter"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=WfpvtH7oC1",
      "cdate": 1715794913264,
      "mdate": 1730873999712,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444661"
    },
    {
      "id": "uOvrwVW1yA",
      "title": "Sample Complexity of Algorithm Selection Using Neural Networks and Its Applications to Branch-and-Cut",
      "abstract": "Data-driven algorithm design is a paradigm that uses statistical and machine learning techniques to select from a class of algorithms for a computational problem an algorithm that has the best expected performance with respect to some (unknown) distribution on the instances of the problem. We build upon recent work in this line of research by considering the setup where, instead of selecting a single algorithm that has the best performance, we allow the possibility of selecting an algorithm based on the instance to be solved, using neural networks. In particular, given a representative sample of instances, we learn a neural network that maps an instance of the problem to the most appropriate algorithm *for that instance*. We formalize this idea and derive rigorous sample complexity bounds for this learning problem, in the spirit of recent work in data-driven algorithm design. We then apply this approach to the problem of making good decisions in the branch-and-cut framework for mixed-integer optimization (e.g., which cut to add?). In other words, the neural network will take as input a mixed-integer optimization instance and output a decision that will result in a small branch-and-cut tree for that instance. Our computational results provide evidence that our particular way of using neural networks for cut selection can make a significant impact in reducing branch-and-cut tree sizes, compared to previous data-driven approaches.",
      "authors": [
        "Hongyu Cheng",
        "Sammy Khalife",
        "Barbara Fiedorowicz",
        "Amitabh Basu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=uOvrwVW1yA",
      "cdate": 1715794850058,
      "mdate": 1730873999690,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444668"
    },
    {
      "id": "fYfliutfHX",
      "title": "Learning predictable and robust neural representations by straightening image sequences",
      "abstract": "Prediction is a fundamental capability of all living organisms, and has been proposed as an objective for learning sensory representations.  Recent work demonstrates that in primate visual systems, prediction is facilitated by neural representations that follow straighter temporal trajectories than their initial photoreceptor encoding, which allows for prediction by linear extrapolation. Inspired by these experimental findings, we develop a self-supervised learning (SSL) objective that explicitly quantifies and promotes straightening. We demonstrate the power of this objective in training deep feedforward neural networks on smoothly-rendered synthetic image sequences that mimic commonly-occurring properties of natural videos. The learned model contains neural embeddings that are predictive, but also factorize the geometric, photometric, and semantic attributes of objects. The representations also prove more robust to noise and adversarial attacks compared to previous SSL methods that optimize for invariance to random augmentations. Moreover, these beneficial properties can be transferred to other training procedures by using the straightening objective as a regularizer, suggesting a broader utility for straightening as a principle for robust unsupervised learning.",
      "authors": [
        "Xueyan Niu",
        "Cristina Savin",
        "Eero P Simoncelli"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fYfliutfHX",
      "cdate": 1715794730745,
      "mdate": 1736951509612,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444673"
    },
    {
      "id": "0bFXbEMz8e",
      "title": "FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions",
      "abstract": "Material discovery is a critical area of research with the potential to revolutionize various fields, including carbon capture, renewable energy, and electronics. However, the immense scale of the chemical space makes it challenging to explore all possible materials experimentally. In this paper, we introduce FlowLLM, a novel generative model that combines large language models (LLMs) and Riemannian flow matching (RFM) to design novel crystalline materials. FlowLLM first fine-tunes an LLM to learn an effective base distribution of meta-stable crystals in a text representation. After converting to a graph representation, the RFM model takes samples from the LLM and iteratively refines the coordinates and lattice parameters. Our approach significantly outperforms state-of-the-art methods, increasing the generation rate of stable materials by over three times and increasing the rate for stable, unique, and novel crystals by $\\sim50$% – a huge improvement on a difficult problem. Additionally, the crystals generated by FlowLLM are much closer to their relaxed state when compared with another leading model, significantly reducing post-hoc computational cost.",
      "authors": [
        "Anuroop Sriram",
        "Benjamin Kurt Miller",
        "Ricky T. Q. Chen",
        "Brandon M Wood"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0bFXbEMz8e",
      "cdate": 1715794550949,
      "mdate": 1730873999549,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444678"
    },
    {
      "id": "ceIO1w0PmT",
      "title": "OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents",
      "abstract": "This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model for open-world instruction-following agents in Minecraft. Compared to prior works that either emit textual goals to separate controllers or produce the control command directly, OmniJARVIS seeks a different path to ensure both strong reasoning and efficient decision-making capabilities via unified tokenization of multimodal interaction data. First, we introduce a self-supervised approach to learn a behavior encoder that produces discretized tokens for behavior trajectories $\\tau = \\{o_0, a_0, \\dots\\}$ and an imitation learning policy decoder conditioned on these tokens. These additional behavior tokens will be augmented to the vocabulary of pretrained Multimodal Language Models. With this encoder, we then pack long-term multimodal interactions involving task instructions, memories, thoughts, observations, textual responses, behavior trajectories, etc into unified token sequences and model them with autoregressive transformers. Thanks to the semantically meaningful behavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing chain-of-thoughts), plan, answer questions, and act (by producing behavior tokens for the imitation learning policy decoder). OmniJARVIS demonstrates excellent performances on a comprehensive collection of atomic, programmatic, and open-ended tasks in open-world Minecraft. Our analysis further unveils the crucial design principles in interaction data formation, unified tokenization, and its scaling potentials. The dataset, models, and code will be released at https://craftjarvis.org/OmniJARVIS.",
      "authors": [
        "Zihao Wang",
        "Shaofei Cai",
        "Zhancun Mu",
        "Haowei Lin",
        "Ceyao Zhang",
        "Xuejie Liu",
        "Qing Li",
        "Anji Liu",
        "Xiaojian Ma",
        "Yitao Liang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ceIO1w0PmT",
      "cdate": 1715794490726,
      "mdate": 1730873999473,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.444684"
    },
    {
      "id": "168NLzTpw8",
      "title": "Unleashing Region Understanding in Intermediate Layers for MLLM-based Referring Expression Generation",
      "abstract": "The Multi-modal Large Language Model (MLLM) based Referring Expression Generation (REG) task has gained increasing popularity, which aims to generate an unambiguous text description that applies to exactly one object or region in the image by leveraging foundation models. We empirically found that there exists a potential trade-off between the detailedness and the correctness of the descriptions for the referring objects. On the one hand, generating sentences with more details is usually required in order to provide more precise object descriptions. On the other hand, complicated sentences could easily increase the probability of hallucinations. To address this issue, we propose a training-free framework, named ``unleash-then-eliminate'', which first elicits the latent information in the intermediate layers, and then adopts a cycle-consistency-based decoding method to alleviate the production of hallucinations. Furthermore, to reduce the computational load of cycle-consistency-based decoding, we devise a Probing-based Importance Estimation method to statistically estimate the importance weights of intermediate layers within a subset. These importance weights are then incorporated into the decoding process over the entire dataset, intervening in the next token prediction from intermediate layers.\nExtensive experiments conducted on the RefCOCOg and PHD benchmarks show that our proposed framework could outperform existing methods on both semantic and hallucination-related metrics. Code will be made available in https://github.com/Glupayy/unleash-eliminate.",
      "authors": [
        "Yaoyuan Liang",
        "Zhuojun Cai",
        "Jian Xu",
        "Guanbo Huang",
        "Yiran Wang",
        "Xiao Liang",
        "Jiahao Liu",
        "Ziran Li",
        "Jingang Wang",
        "Shao-Lun Huang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=168NLzTpw8",
      "cdate": 1715794490712,
      "mdate": 1730873999436,
      "matched_keywords": [
        "large language model",
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444689"
    },
    {
      "id": "k6iyUfwdI9",
      "title": "To Believe or Not to Believe Your LLM: Iterative Prompting for Estimating Epistemic Uncertainty",
      "abstract": "We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.",
      "authors": [
        "Yasin Abbasi-Yadkori",
        "Ilja Kuzborskij",
        "András György",
        "Csaba Szepesvari"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=k6iyUfwdI9",
      "cdate": 1715794473977,
      "mdate": 1736939870091,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444694"
    },
    {
      "id": "JJGfCvjpTV",
      "title": "Hamiltonian Score Matching and Generative Flows",
      "abstract": "Classical Hamiltonian mechanics has been widely used in machine learning in the form of Hamiltonian Monte Carlo for applications with predetermined force fields. In this paper, we explore the potential of deliberately designing force fields for Hamiltonian systems, introducing Hamiltonian velocity predictors (HVPs) as a core tool for constructing energy-based and generative models. We present two innovations: Hamiltonian Score Matching (HSM), which utilizes score functions to augment data by simulating Hamiltonian trajectories, and Hamiltonian Generative Flows (HGFs), a novel generative model that encompasses diffusion models and OT-flow matching as HGFs with zero force fields. We showcase the extended design space of force fields by introducing Oscillation HGFs, a generative model inspired by harmonic oscillators. Our experiments demonstrate that HSM and HGFs rival leading score-matching and generative modeling techniques. Overall, our work systematically elucidates the synergy between Hamiltonian dynamics, force fields, and generative models, thereby opening new avenues for applications of machine learning in physical sciences and dynamical systems.",
      "authors": [
        "Peter Holderrieth",
        "Yilun Xu",
        "Tommi Jaakkola"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=JJGfCvjpTV",
      "cdate": 1715794179407,
      "mdate": 1730873999201,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444700"
    },
    {
      "id": "dYIqAZXQNV",
      "title": "Generalizing CNNs to graphs with learnable neighborhood quantization",
      "abstract": "Convolutional neural networks (CNNs) have led to a revolution in analyzing array data. However, many important sources of data, such as biological and social networks, are naturally structured as graphs rather than arrays, making the design of graph neural network (GNN) architectures that retain the strengths of CNNs an active and exciting area of research. Here, we introduce Quantized Graph Convolution Networks (QGCNs), the first framework for GNNs that formally and directly extends CNNs to graphs. QGCNs do this by decomposing the convolution operation into non-overlapping sub-kernels, allowing them to fit graph data while reducing to a 2D CNN layer on array data. We generalize this approach to graphs of arbitrary size and dimension by approaching sub-kernel assignment as a learnable multinomial assignment problem. Integrating this approach into a residual network architecture, we demonstrate performance that matches or exceeds other state-of-the-art GNNs on benchmark graph datasets and for predicting properties of nonlinear dynamics on a new finite element graph dataset. In summary, QGCNs are a novel GNN framework that generalizes CNNs and their strengths to graph data, allowing for more accurate and expressive models.",
      "authors": [
        "Isaac Osafo Nkansah",
        "Neil Gallagher",
        "Ruchi Sandilya",
        "Conor Liston",
        "Logan Grosenick"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=dYIqAZXQNV",
      "cdate": 1715794127962,
      "mdate": 1730873999132,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444705"
    },
    {
      "id": "BJndYScO6o",
      "title": "Model-based Diffusion for Trajectory Optimization",
      "abstract": "Recent advances in diffusion models have demonstrated their strong capabilities in generating high-fidelity samples from complex distributions through an iterative refinement process. Despite the empirical success of diffusion models in motion planning and control, the model-free nature of these methods does not leverage readily available model information and limits their generalization to new scenarios beyond the training data (e.g., new robots with different dynamics). In this work, we introduce Model-Based Diffusion (MBD), an optimization approach using the diffusion process to solve trajectory optimization (TO) problems without data. The key idea is to explicitly compute the score function by leveraging the model information in TO problems, which is why we refer to our approach as model-based diffusion. Moreover, although MBD does not require external data, it can be naturally integrated with data of diverse qualities to steer the diffusion process. We also reveal that MBD has interesting connections to sampling-based optimization. Empirical evaluations show that MBD outperforms state-of-the-art reinforcement learning and sampling-based TO methods in challenging contact-rich tasks. Additionally, MBD’s ability to integrate with data enhances its versatility and practical applicability, even with imperfect and infeasible data (e.g., partial-state demonstrations for high-dimensional humanoids), beyond the scope of standard diffusion models. Videos and codes are available in the supplementary materials.",
      "authors": [
        "Chaoyi Pan",
        "Zeji Yi",
        "Guanya Shi",
        "Guannan Qu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BJndYScO6o",
      "cdate": 1715794080474,
      "mdate": 1737135749293,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444711"
    },
    {
      "id": "kbBjVMcJ7G",
      "title": "Operator World Models for Reinforcement Learning",
      "abstract": "Policy Mirror Descent (PMD) is a powerful and theoretically sound methodology for sequential decision-making. However, it is not directly applicable to Reinforcement Learning (RL) due to the inaccessibility of explicit action-value functions. We address this challenge by introducing a novel approach based on learning a world model of the environment using conditional mean embeddings. Leveraging tools from operator theory we derive a closed-form expression of the action-value function in terms of the world model via simple matrix operations. Combining these estimators with PMD leads to POWR, a new RL algorithm for which we prove convergence rates to the global optimum. Preliminary experiments in finite and infinite state settings support the effectiveness of our method.",
      "authors": [
        "Pietro Novelli",
        "Marco Pratticò",
        "Massimiliano Pontil",
        "Carlo Ciliberto"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kbBjVMcJ7G",
      "cdate": 1715794053430,
      "mdate": 1740735280527,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444716"
    },
    {
      "id": "cpklMJqZDE",
      "title": "Unrolled denoising networks provably learn to perform optimal Bayesian inference",
      "abstract": "Much of Bayesian inference centers around the design of estimators for inverse problems which are optimal assuming the data comes from a known prior. But what do these optimality guarantees mean if the prior is unknown? In recent years, algorithm unrolling has emerged as deep learning's answer to this age-old question: design a neural network whose layers can in principle simulate iterations of inference algorithms and train on data generated by the unknown prior.  Despite its empirical success, however, it has remained unclear whether this method can provably recover the performance of its optimal, prior-aware counterparts.\n\nIn this work, we prove the first rigorous learning guarantees for neural networks based on unrolling approximate message passing (AMP). For compressed sensing, we prove that when trained on data drawn from a product prior, the layers of the network approximately converge to the same denoisers used in Bayes AMP. We also provide extensive numerical experiments for compressed sensing and rank-one matrix estimation demonstrating the advantages of our unrolled architecture \\--- in addition to being able to obliviously adapt to general priors, it exhibits improvements over Bayes AMP in more general settings of low dimensions, non-Gaussian designs, and non-product priors.",
      "authors": [
        "Aayush Karan",
        "Kulin Shah",
        "Sitan Chen",
        "Yonina C. Eldar"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=cpklMJqZDE",
      "cdate": 1715794026351,
      "mdate": 1730873998944,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444721"
    },
    {
      "id": "jwh9MHEfmY",
      "title": "Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs",
      "abstract": "Reward models trained on human preference data have been proven to effectively align Large Language Models (LLMs) with human intent within the framework of reinforcement learning from human feedback (RLHF). However, current reward models have limited generalization capabilities to unseen prompts and responses, which can lead to an unexpected phenomenon known as reward over-optimization, resulting in a decline in actual performance due to excessive optimization of rewards. While previous research has advocated for constraining policy optimization, our study introduces a novel approach to enhance the reward model's generalization ability against distribution shifts by regularizing the hidden states. Specifically, we retain the base model's language model head and incorporate a suite of text-generation losses to preserve the hidden states' text-generation capabilities, while concurrently learning a reward head behind the same hidden states. Our experimental results demonstrate that the introduced regularization technique markedly improves the accuracy of learned reward models across a variety of out-of-distribution (OOD) tasks and effectively alleviates the over-optimization issue in RLHF, offering a more reliable and robust preference learning paradigm.",
      "authors": [
        "Rui Yang",
        "Ruomeng Ding",
        "Yong Lin",
        "Huan Zhang",
        "Tong Zhang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jwh9MHEfmY",
      "cdate": 1715793935546,
      "mdate": 1730873998894,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444727"
    },
    {
      "id": "CbtkDWZzDq",
      "title": "Ex Uno Pluria: Insights on Ensembling in Low Precision Number Systems",
      "abstract": "While ensembling deep neural networks has shown promise in improving generalization performance, scaling current ensemble methods for large models remains challenging. Given that recent progress in deep learning is largely driven by the scale, exemplified by the widespread adoption of large-scale neural network architectures, scalability emerges an increasingly critical issue for machine learning algorithms in the era of large-scale models. In this work, we first showcase the potential of low precision ensembling, where ensemble members are derived from a single model within low precision number systems in a training-free manner. Our empirical analysis demonstrates the effectiveness of our proposed low precision ensembling method compared to existing ensemble approaches.",
      "authors": [
        "Giung Nam",
        "Juho Lee"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=CbtkDWZzDq",
      "cdate": 1715793785995,
      "mdate": 1730873998827,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444732"
    },
    {
      "id": "aq3I5B6GLG",
      "title": "Foundations of Multivariate Distributional Reinforcement Learning",
      "abstract": "In reinforcement learning (RL), the consideration of multivariate reward signals has led to fundamental advancements in multi-objective decision-making, transfer learning, and representation learning. This work introduces the first oracle-free and computationally-tractable algorithms for provably convergent multivariate *distributional* dynamic programming and temporal difference learning. Our convergence rates match the familiar rates in the scalar reward setting, and additionally provide new insights into the fidelity of approximate return distribution representations as a function of the reward dimension. Surprisingly, when the reward dimension is larger than $1$, we show that standard analysis of categorical TD learning fails, which we resolve with a novel projection onto the space of mass-$1$ signed measures. Finally, with the aid of our technical results and simulations, we identify tradeoffs between distribution representations that influence the performance of multivariate distributional RL in practice.",
      "authors": [
        "Harley Wiltzer",
        "Jesse Farebrother",
        "Arthur Gretton",
        "Mark Rowland"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=aq3I5B6GLG",
      "cdate": 1715793782917,
      "mdate": 1730873998808,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444737"
    },
    {
      "id": "FNtsZLwkGr",
      "title": "Pruning neural network models for gene regulatory dynamics using data and domain knowledge",
      "abstract": "The practical utility of machine learning models in the sciences often hinges on their interpretability. It is common to assess a model's merit for scientific discovery, and thus novel insights, by how well it aligns with already available domain knowledge - a dimension that is currently largely disregarded in the comparison of neural network models. While pruning can simplify deep neural network architectures and excels in identifying sparse models, as we show in the context of gene regulatory network inference, state-of-the-art techniques struggle with biologically meaningful structure learning. To address this issue, we propose DASH, a generalizable framework that guides network pruning by using domain-specific structural information in model fitting and leads to sparser, better interpretable models that are more robust to noise. Using both synthetic data with ground truth information, as well as real-world gene expression data, we show that DASH, using knowledge about gene interaction partners within the putative regulatory network, outperforms general pruning methods by a large margin and yields deeper insights into the biological systems being studied.",
      "authors": [
        "Intekhab Hossain",
        "Jonas Fischer",
        "Rebekka Burkholz",
        "John Quackenbush"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FNtsZLwkGr",
      "cdate": 1715793676859,
      "mdate": 1736418091792,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444742"
    },
    {
      "id": "OWwdlxwnFN",
      "title": "MonkeySee: Space-time-resolved reconstructions of natural images from macaque multi-unit activity",
      "abstract": "In this paper, we reconstruct naturalistic images directly from macaque brain signals using a convolutional neural network (CNN) based decoder. We investigate the ability of this CNN-based decoding technique to differentiate among neuronal populations from areas V1, V4, and IT, revealing distinct readout characteristics for each. This research marks a progression from low-level to high-level brain signals, thereby enriching the existing framework for utilizing CNN-based decoders to decode brain activity. Our results demonstrate high-precision reconstructions of naturalistic images, highlighting the efficiency of CNN-based decoders in advancing our knowledge of how the brain's representations translate into pixels. Additionally, we present a novel space-time-resolved decoding technique, demonstrating how temporal resolution in decoding can advance our understanding of neural representations. Moreover, we introduce a learned receptive field layer that sheds light on the CNN-based model's data processing during training, enhancing understanding of its structure and interpretive capacity.",
      "authors": [
        "Lynn Le",
        "Paolo Papale",
        "K. Seeliger",
        "Antonio Lozano",
        "Thirza Dado",
        "Feng Wang",
        "Pieter R. Roelfsema",
        "Marcel van Gerven",
        "Yağmur Güçlütürk",
        "Umut Güçlü"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OWwdlxwnFN",
      "cdate": 1715793625270,
      "mdate": 1730873998660,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444747"
    },
    {
      "id": "wSqpNeMVLU",
      "title": "A Theoretical Perspective for Speculative Decoding Algorithm",
      "abstract": "Transformer-based autoregressive sampling has been the major bottleneck for slowing down large language model inferences. One effective way to accelerate inference is Speculative Decoding, which employs a small model to sample a sequence of draft tokens and a large model to validate. Given its empirical effectiveness, the theoretical understanding of Speculative Decoding is falling behind. This paper tackles this gap by conceptualizing the decoding problem via markov chain abstraction and studying the key properties, output quality and inference acceleration, from a theoretical perspective. Our analysis covers the theoretical limits of speculative decoding, batch algorithms, and output quality-inference acceleration tradeoffs. Our results reveal the fundamental connections between different components of LLMs via total variation distances and show how they jointly affect the efficiency of decoding algorithms.",
      "authors": [
        "Ming Yin",
        "Minshuo Chen",
        "Kaixuan Huang",
        "Mengdi Wang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=wSqpNeMVLU",
      "cdate": 1715793615509,
      "mdate": 1730873998607,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444753"
    },
    {
      "id": "5d2eScRiRC",
      "title": "Imitating Language via Scalable Inverse Reinforcement Learning",
      "abstract": "The majority of language model training builds on imitation learning. It covers pretraining, supervised fine-tuning, and affects the starting conditions for reinforcement learning from human feedback (RLHF). The simplicity and scalability of maximum likelihood estimation (MLE) for next token prediction led to its role as predominant paradigm. However, the broader field of imitation learning can more effectively utilize the sequential structure underlying autoregressive generation. We focus on investigating the inverse reinforcement learning (IRL) perspective to imitation, extracting rewards and directly optimizing sequences instead of individual token likelihoods and evaluate its benefits for fine-tuning large language models.  We provide a new angle, reformulating inverse soft-Q-learning as a temporal difference regularized extension of MLE. This creates a principled connection between MLE and IRL and allows trading off added complexity with increased performance and diversity of generations in the supervised fine-tuning (SFT) setting. We find clear advantages for IRL-based imitation, in particular for retaining diversity while maximizing task performance, rendering IRL a strong alternative on fixed SFT datasets even without online data generation. Our analysis of IRL-extracted reward functions further indicates benefits for more robust reward functions via tighter integration of supervised and preference-based LLM post-training.",
      "authors": [
        "Markus Wulfmeier",
        "Michael Bloesch",
        "Nino Vieillard",
        "Arun Ahuja",
        "Jorg Bornschein",
        "Sandy Huang",
        "Artem Sokolov",
        "Matt Barnes",
        "Guillaume Desjardins",
        "Alex Bewley",
        "Sarah Maria Elisabeth Bechtle",
        "Jost Tobias Springenberg",
        "Nikola Momchev",
        "Olivier Bachem",
        "Matthieu Geist",
        "Martin Riedmiller"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5d2eScRiRC",
      "cdate": 1715793613124,
      "mdate": 1730873998565,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444758"
    },
    {
      "id": "CrADAX7h23",
      "title": "DAGER: Exact Gradient Inversion for Large Language Models",
      "abstract": "Federated learning works by aggregating locally computed gradients from multiple clients, thus enabling collaborative training without sharing private client data. However, prior work has shown that the data can actually be recovered by the server using so-called gradient inversion attacks. While these attacks perform well when applied on images, they are limited in the text domain and only permit approximate reconstruction of small batches and short input sequences. In this work, we propose DAGER, the first algorithm to recover whole batches of input text exactly. DAGER leverages the low-rank structure of self-attention layer gradients and the discrete nature of token embeddings to efficiently check if a given token sequence is part of the client data. We use this check to exactly recover full batches in the honest-but-curious setting without any prior on the data for both encoder and decoder-based architectures using exhaustive heuristic search and a greedy approach, respectively. We provide an efficient GPU implementation of DAGER and show experimentally that it recovers full batches of size up to 128 on large language models (LLMs), beating prior attacks in speed (20x at same batch size), scalability (10x larger batches), and reconstruction quality (ROUGE-1/2 > 0.99).",
      "authors": [
        "Ivo Petrov",
        "Dimitar Iliev Dimitrov",
        "Maximilian Baader",
        "Mark Niklas Mueller",
        "Martin Vechev"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=CrADAX7h23",
      "cdate": 1715793588432,
      "mdate": 1730873998541,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444764"
    },
    {
      "id": "k6ZHvF1vkg",
      "title": "Beyond Optimism: Exploration With Partially Observable Rewards",
      "abstract": "Exploration in reinforcement learning (RL) remains an open challenge.\nRL algorithms rely on observing rewards to train the agent, and if informative rewards are sparse the agent learns slowly or may not learn at all. \nTo improve exploration and reward discovery, popular algorithms rely on optimism. \nBut what if sometimes rewards are unobservable, e.g., situations of partial monitoring in bandits and the recent formalism of monitored Markov decision process? \nIn this case, optimism can lead to suboptimal behavior that does not explore further to collapse uncertainty.\nWith this paper, we present a novel exploration strategy that overcomes the limitations of existing methods and guarantees convergence to an optimal policy even when rewards are not always observable. \nWe further propose a collection of tabular environments for benchmarking exploration in RL (with and without unobservable rewards) and show that our method outperforms existing ones.",
      "authors": [
        "Simone Parisi",
        "Alireza Kazemipour",
        "Michael Bowling"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=k6ZHvF1vkg",
      "cdate": 1715793576716,
      "mdate": 1730873998494,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444769"
    },
    {
      "id": "xavWvnJTST",
      "title": "Feedback control guides credit assignment in recurrent neural networks",
      "abstract": "How do brain circuits learn to generate behaviour?\n  While significant strides have been made in understanding learning in artificial neural networks, applying this knowledge to biological networks remains challenging.\n  For instance, while backpropagation is known to perform accurate credit assignment of error in artificial neural networks, how a similarly powerful process can be realized within the constraints of biological circuits remains largely unclear.\n  One of the major challenges is that the brain's extensive recurrent connectivity requires the propagation of error through both space and time, a problem that is notoriously difficult to solve in vanilla recurrent neural networks.\n  Moreover, the extensive feedback connections in the brain are known to influence forward network activity, but the interaction between feedback-driven activity changes and local, synaptic plasticity-based learning is not fully understood.\n  Building on our previous work modelling motor learning, this work investigates the mechanistic properties of pre-trained networks with feedback control on a standard motor task.\n  We show that feedback control of the ongoing recurrent network dynamics approximates the optimal first-order gradient with respect to the network activities, allowing for rapid, ongoing movement correction.\n  Moreover, we show that trial-by-trial adaptation to a persistent perturbation using a local, biologically plausible learning rule that integrates recent activity and error feedback is both more accurate and more efficient with feedback control during learning, due to the decoupling of the recurrent network dynamics and the injection of an adaptive, second-order gradient into the network dynamics.\n  Thus, our results suggest that feedback control may guide credit assignment in biological recurrent neural networks, enabling both rapid and efficient learning in the brain.",
      "authors": [
        "Klara Kaleb",
        "Barbara Feulner",
        "Juan A. Gallego",
        "Claudia Clopath"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xavWvnJTST",
      "cdate": 1715793566879,
      "mdate": 1736954545308,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444774"
    },
    {
      "id": "oBvaZJ1C71",
      "title": "GAVEL: Generating Games via Evolution and Language Models",
      "abstract": "Automatically generating novel and interesting games is a complex task. Challenges include representing game rules in a computationally workable form, searching through the large space of potential games under most such representations, and accurately evaluating the originality and quality of previously unseen games. Prior work in automated game generation has largely focused on relatively restricted rule representations and relied on domain-specific heuristics. In this work, we explore the generation of novel games in the comparatively expansive Ludii game description language, which encodes the rules of over 1000 board games in a variety of styles and modes of play. We draw inspiration from recent advances in large language models and evolutionary computation in order to train a model that intelligently mutates and recombines games and mechanics expressed as code. We demonstrate both quantitatively and qualitatively that our approach is capable of generating new and interesting games, including in regions of the potential rules space not covered by existing games in the Ludii dataset.",
      "authors": [
        "Graham Todd",
        "Alexander George Padula",
        "Matthew Stephenson",
        "Eric Piette",
        "Dennis J. N. J. Soemers",
        "Julian Togelius"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=oBvaZJ1C71",
      "cdate": 1715793493284,
      "mdate": 1730873998403,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444778"
    },
    {
      "id": "spwE9sLrfg",
      "title": "Verified Code Transpilation with LLMs",
      "abstract": "Domain-specific languages (DSLs) have become integral to various software workflows. Such languages offer domain-specific optimizations and abstractions that improve code readability and maintainability.  However, leveraging these languages requires developers to rewrite existing code using the specific DSL's API. While large language models (LLMs) have shown some success in automatic code transpilation, none of them provide any functional correctness guarantees on the rewritten code. Another approach for automating this task is verified lifting, which relies on program synthesis to find programs in the target language that are functionally equivalent to the source language program. While several verified lifting tools have been developed for various application domains, they are specialized for specific source-target languages or require significant expertise in domain knowledge to make the search efficient. In this paper, leveraging recent advances in LLMs, we propose an LLM-based approach (LLMLift) to building verified lifting tools. We use the LLM's capabilities to reason about programs to translate a given program into its corresponding equivalent in the target language. Additionally, we use LLMs to generate proofs for functional equivalence. We develop lifting-based compilers for four DSLs targeting different application domains. Our approach not only outperforms previous symbolic-based tools in number of benchmarks transpiled and transpilation time, but also requires significantly less effort to build.",
      "authors": [
        "Sahil Bhatia",
        "Jie Qiu",
        "Niranjan Hasabnis",
        "Sanjit A. Seshia",
        "Alvin Cheung"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=spwE9sLrfg",
      "cdate": 1715793468755,
      "mdate": 1736920205871,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444784"
    },
    {
      "id": "6gMnj9oc6d",
      "title": "Scalable DP-SGD: Shuffling vs. Poisson Subsampling",
      "abstract": "We provide new lower bounds on the privacy guarantee of _multi-epoch_ Adaptive Batch Linear Queries (ABLQ) mechanism with _shuffled batch sampling_, demonstrating substantial gaps when compared to _Poisson subsampling_; prior analysis was limited to a single epoch.\nSince the privacy analysis of Differentially Private Stochastic Gradient Descent (DP-SGD) is obtained by analyzing the ABLQ mechanism, this brings into serious question the common practice of implementing Shuffling based DP-SGD, but reporting privacy parameters as if Poisson subsampling was used.\nTo understand the impact of this gap on the utility of trained machine learning models, we introduce a novel practical approach to implement Poisson subsampling _at scale_ using massively parallel computation, and efficiently train models with the same.\nWe provide a comparison between the utility of models trained with Poisson subsampling based DP-SGD, and the optimistic estimates of utility when using shuffling, via our new lower bounds on the privacy guarantee of ABLQ with shuffling.",
      "authors": [
        "Lynn Chua",
        "Badih Ghazi",
        "Pritish Kamath",
        "Ravi Kumar",
        "Pasin Manurangsi",
        "Amer Sinha",
        "Chiyuan Zhang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=6gMnj9oc6d",
      "cdate": 1715793466021,
      "mdate": 1730873998432,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444789"
    },
    {
      "id": "x2780VcMOI",
      "title": "A Polar coordinate system represents syntax in large language models",
      "abstract": "Originally formalized with symbolic representations, syntactic trees may also be effectively represented in the activations of large language models (LLMs). Indeed, a ''Structural Probe'' can find a subspace of neural activations, where syntactically-related words are relatively close to one-another. However, this syntactic code remains incomplete: the distance between the Structural Probe word embeddings can represent the \\emph{existence} but not the type and direction of syntactic relations. Here, we hypothesize that syntactic relations are, in fact, coded by the relative direction between nearby embeddings. To test this hypothesis, we introduce a ''Polar Probe'' trained to read syntactic relations from both the distance and the direction between word embeddings. Our approach reveals three main findings. First, our Polar Probe successfully recovers the type and direction of syntactic relations, and substantially outperforms the Structural Probe by nearly two folds. Second, we confirm that this polar coordinate system exists in a low-dimensional subspace of the intermediate layers of many LLMs and becomes increasingly precise in the latest frontier models. Third, we demonstrate with a new benchmark that similar syntactic relations are coded similarly across the nested levels of syntactic trees. Overall, this work shows that LLMs spontaneously learn a geometry of neural activations that explicitly represents the main symbolic structures of linguistic theory.",
      "authors": [
        "Pablo J. Diego Simon",
        "Stéphane d'Ascoli",
        "Emmanuel Chemla",
        "Yair Lakretz",
        "Jean-Remi King"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=x2780VcMOI",
      "cdate": 1715793364300,
      "mdate": 1736845021863,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444794"
    },
    {
      "id": "IdtoJVWVnX",
      "title": "Teach Better or Show Smarter? On Instructions and Exemplars in Automatic Prompt Optimization",
      "abstract": "Large language models have demonstrated remarkable capabilities but their performance is heavily reliant on effective prompt engineering. Automatic prompt optimization (APO) methods are designed to automate this and can be broadly categorized into those targeting instructions (instruction optimization, IO) vs. those targeting exemplars (exemplar optimization, EO). Despite their shared objective, these have evolved rather independently, with IO receiving more research attention recently. This paper seeks to bridge this gap by comprehensively comparing the performance of representative IO and EO techniques both isolation and combination on a diverse set of challenging tasks. Our findings reveal that intelligently reusing model-generated input-output pairs obtained from evaluating prompts on the validation set as exemplars, consistently improves performance on top of IO methods but is currently under-investigated. We also find that despite the recent focus on IO, how we select exemplars can outweigh how we optimize instructions, with EO strategies as simple as random search outperforming state-of-the-art IO methods with seed instructions without any optimization. Moreover, we observe a synergy between EO and IO, with optimal combinations surpassing the individual contributions. We conclude that studying exemplar optimization both as a standalone method and its optimal combination with instruction optimization remain a crucial aspect of APO and deserve greater consideration in future research, even in the era of highly capable instruction-following models.",
      "authors": [
        "Xingchen Wan",
        "Ruoxi Sun",
        "Hootan Nakhost",
        "Sercan O Arik"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=IdtoJVWVnX",
      "cdate": 1715793307011,
      "mdate": 1730873998248,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444799"
    },
    {
      "id": "rYjYwuM6yH",
      "title": "3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient Batching and Composability",
      "abstract": "Parameter-efficient finetuning (PEFT) methods effectively adapt large language models (LLMs) to diverse downstream tasks, reducing storage and GPU memory demands. Despite these advantages, several applications pose new challenges to PEFT beyond mere parameter efficiency. One notable challenge involves the efficient deployment of LLMs equipped with multiple task- or user-specific adapters, particularly when different adapters are needed for distinct requests within the same batch. Another challenge is the interpretability of LLMs, which is crucial for understanding how LLMs function. Previous studies introduced various approaches to address different challenges. In this paper, we introduce a novel method, RoAd, which employs a straightforward 2D rotation to adapt LLMs and addresses all the above challenges: (1) RoAd is remarkably parameter-efficient, delivering optimal performance on GLUE, eight commonsense reasoning tasks and four arithmetic reasoning tasks with <0.1% trainable parameters; (2) RoAd facilitates the efficient serving of requests requiring different adapters within a batch, with an overhead comparable to element-wise multiplication instead of batch matrix multiplication; (3) RoAd enhances LLM's interpretability through integration within a framework of distributed interchange intervention, demonstrated via composition experiments.",
      "authors": [
        "Baohao Liao",
        "Christof Monz"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rYjYwuM6yH",
      "cdate": 1715793288803,
      "mdate": 1730873998201,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444804"
    },
    {
      "id": "BDrWQTrfyI",
      "title": "BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts",
      "abstract": "Mixture of Experts (MoE) framework has become a popular architecture for large language models due to its superior performance compared to dense models. However, training MoEs from scratch in a large-scale regime is prohibitively expensive. Previous work addresses this challenge by independently training multiple dense expert models and using them to initialize an MoE. In particular, state-of-the-art approaches initialize MoE layers using experts' feed-forward parameters while merging all other parameters, limiting the advantages of the specialized dense models when upcycling them as MoEs.  We propose BAM (Branch-Attend-Mix), a simple yet effective improvement to MoE training. BAM makes full use of specialized dense models by not only using their feed-forward network (FFN) to initialize the MoE layers but also leveraging experts' attention weights fully by leveraging them as mixture-of-attention (MoA) layers. We explore two methods for upcycling MoA layers: 1) initializing separate attention experts from dense models including key, value, and query matrices; and 2) initializing only Q projections while sharing key-value pairs across all experts to facilitate efficient inference. Our experiments using seed models ranging from 590 million to 2 billion parameters show that our approach outperforms state-of-the-art approaches under the same data and compute budget in both perplexity and downstream tasks evaluations, confirming the effectiveness of BAM.",
      "authors": [
        "Qizhen Zhang",
        "Nikolas Gritsch",
        "Dwaraknath Gnaneshwar",
        "Simon Guo",
        "David Cairuz",
        "Bharat Venkitesh",
        "Jakob Nicolaus Foerster",
        "Phil Blunsom",
        "Sebastian Ruder",
        "Ahmet Üstün",
        "Acyr Locatelli"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BDrWQTrfyI",
      "cdate": 1715793137452,
      "mdate": 1730873998081,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444810"
    },
    {
      "id": "hUGD1aNMrp",
      "title": "Assouad, Fano, and Le Cam with Interaction: A Unifying Lower Bound Framework and Characterization for Bandit Learnability",
      "abstract": "We develop a unifying framework for information-theoretic lower bound in statistical estimation and interactive decision making. Classical lower bound techniques---such as Fano's method, Le Cam's method, and Assouad's lemma---are central to the study of minimax risk in statistical estimation, yet are insufficient to provide tight lower bounds for \\emph{interactive decision making} algorithms that collect data interactively (e.g., algorithms for bandits and reinforcement learning). Recent work of Foster et al. provides minimax lower bounds for interactive decision making using seemingly different analysis techniques from the classical methods. These results---which are proven using a complexity measure known as the \\emph{Decision-Estimation Coefficient} (DEC)---capture difficulties unique to interactive learning, yet do not recover the tightest known lower bounds for passive estimation. We propose a unified view of these distinct methodologies through a new lower bound approach called \\emph{interactive Fano method}. As an application, we introduce a novel complexity measure, the \\emph{Fractional Covering Number}, which facilitates the new lower bounds for interactive decision making that extend the DEC methodology by incorporating the complexity of estimation. Using the fractional covering number, we (i) provide a unified characterization of learnability for \\emph{any} stochastic bandit problem, (ii) close the remaining gap between the upper and lower bounds in Foster et al. (up to polynomial factors) for any interactive decision making problem in which the underlying model class is convex.",
      "authors": [
        "Fan Chen",
        "Dylan J Foster",
        "Yanjun Han",
        "Jian Qian",
        "Alexander Rakhlin",
        "Yunbei Xu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hUGD1aNMrp",
      "cdate": 1715792968226,
      "mdate": 1736929021766,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444815"
    },
    {
      "id": "W89fKKP2AO",
      "title": "Universal Neural Functionals",
      "abstract": "A challenging problem in many modern machine learning tasks is to process weight-space features, i.e., to transform or extract information from the weights and gradients of a neural network. Recent works have developed promising weight-space models that are equivariant to the permutation symmetries of simple feedforward networks. However, they are not applicable to general architectures, since the permutation symmetries of a weight space can be complicated by recurrence or residual connections. This work proposes an algorithm that automatically constructs permutation equivariant models, which we refer to as universal neural functionals (UNFs), for any weight space. Among other applications, we demonstrate how UNFs can be substituted into existing learned optimizer designs, and find promising improvements over prior methods when optimizing small image classifiers and language models. Our results suggest that learned optimizers can benefit from considering the (symmetry) structure of the weight space they optimize.",
      "authors": [
        "Allan Zhou",
        "Chelsea Finn",
        "James Harrison"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=W89fKKP2AO",
      "cdate": 1715792940344,
      "mdate": 1731699559884,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444820"
    },
    {
      "id": "4KeSvAvNMr",
      "title": "Treeffuser: probabilistic prediction via conditional diffusions with gradient-boosted trees",
      "abstract": "Probabilistic prediction aims to compute predictive distributions rather than single point predictions. These distributions enable practitioners to quantify uncertainty, compute risk, and detect outliers. However, most probabilistic methods assume parametric responses, such as Gaussian or Poisson distributions. When these assumptions fail, such models lead to bad predictions and poorly calibrated uncertainty.  In this paper, we propose Treeffuser, an easy-to-use method for probabilistic prediction on tabular data. The idea is to learn a conditional diffusion model where the score function is estimated using gradient-boosted trees. The conditional diffusion model makes Treeffuser flexible and non-parametric, while the gradient-boosted trees make it robust and easy to train on CPUs. Treeffuser learns well-calibrated predictive distributions and can handle a wide range of regression tasks---including those with multivariate, multimodal, and skewed responses. We study Treeffuser on synthetic and real data and show that it outperforms existing methods, providing better calibrated probabilistic predictions. We further demonstrate its versatility with an application to inventory allocation under uncertainty using sales data from Walmart. We implement Treeffuser in https://github.com/blei-lab/treeffuser.",
      "authors": [
        "Nicolas Beltran-Velez",
        "Alessandro Antonio Grande",
        "Achille Nazaret",
        "Alp Kucukelbir",
        "David Blei"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4KeSvAvNMr",
      "cdate": 1715792930691,
      "mdate": 1730873997805,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.444825"
    },
    {
      "id": "hpvJwmzEHX",
      "title": "RGFN: Synthesizable Molecular Generation Using GFlowNets",
      "abstract": "Generative models hold great promise for small molecule discovery, significantly increasing the size of search space compared to traditional in silico screening libraries. However, most existing machine learning methods for small molecule generation suffer from poor synthesizability of candidate compounds, making experimental validation difficult. In this paper we propose Reaction-GFlowNet (RGFN), an extension of the GFlowNet framework that operates directly in the space of chemical reactions, thereby allowing out-of-the-box synthesizability while maintaining comparable quality of generated candidates. We demonstrate that with the proposed set of reactions and building blocks, it is possible to obtain a search space of molecules orders of magnitude larger than existing screening libraries coupled with low cost of synthesis. We also show that the approach scales to very large fragment libraries, further increasing the number of potential molecules. We demonstrate the effectiveness of the proposed approach across a range of oracle models, including pretrained proxy models and GPU-accelerated docking.",
      "authors": [
        "Michał Koziarski",
        "Andrei Rekesh",
        "Dmytro Shevchuk",
        "Almer M. van der Sloot",
        "Piotr Gaiński",
        "Yoshua Bengio",
        "Cheng-Hao Liu",
        "Mike Tyers",
        "Robert A. Batey"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hpvJwmzEHX",
      "cdate": 1715792916110,
      "mdate": 1730873997743,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444831"
    },
    {
      "id": "G5lMFOtFHa",
      "title": "Where Do Large Learning Rates Lead Us?",
      "abstract": "It is generally accepted that starting neural networks training with large learning rates (LRs) improves generalization. Following a line of research devoted to understanding this effect, we conduct an empirical study in a controlled setting focusing on two questions: 1) how large an initial LR is required for obtaining optimal quality, and 2) what are the key differences between models trained with different LRs? We discover that only a narrow range of initial LRs slightly above the convergence threshold lead to optimal results after fine-tuning with a small LR or weight averaging. By studying the local geometry of reached minima, we observe that using LRs from this optimal range allows for the optimization to locate a basin that only contains high-quality minima. Additionally, we show that these initial LRs result in a sparse set of learned features, with a clear focus on those most relevant for the task. In contrast, starting training with too small LRs leads to unstable minima and attempts to learn all features simultaneously, resulting in poor generalization. Conversely, using initial LRs that are too large fails to detect a basin with good solutions and extract meaningful patterns from the data.",
      "authors": [
        "Ildus Sadrtdinov",
        "Maxim Kodryan",
        "Eduard Pokonechny",
        "Ekaterina Lobacheva",
        "Dmitry Vetrov"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=G5lMFOtFHa",
      "cdate": 1715792844333,
      "mdate": 1730873997703,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444836"
    },
    {
      "id": "LuCLf4BJsr",
      "title": "Chain of Agents: Large Language Models Collaborating on Long-Context Tasks",
      "abstract": "Addressing the challenge of effectively processing long contexts has become a critical issue for Large Language Models (LLMs). Two common strategies have emerged: 1) reducing the input length, such as retrieving relevant chunks by Retrieval-Augmented Generation (RAG), and 2) expanding the context window limit of LLMs. However, both strategies have drawbacks: input reduction has no guarantee of covering the part with needed information, while window extension struggles with focusing on the pertinent information for solving the task. To mitigate these limitations, we propose Chain-of-Agents (CoA), a novel framework that harnesses multi-agent collaboration through natural language to enable information aggregation and context reasoning across various LLMs over long-context tasks. CoA consists of multiple worker agents who sequentially communicate to handle different segmented portions of the text, followed by a manager agent who synthesizes these contributions into a coherent final output. CoA processes the entire input by interleaving reading and reasoning, and it mitigates long context focus issues by assigning each agent a short context. We perform a comprehensive evaluation of CoA on a wide range of long-context tasks in question answering, summarization, and code completion, demonstrating significant improvements by up to 10% over strong baselines of RAG, Full-Context, and multi-agent LLMs.",
      "authors": [
        "Yusen Zhang",
        "Ruoxi Sun",
        "Yanfei Chen",
        "Tomas Pfister",
        "Rui Zhang",
        "Sercan O Arik"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LuCLf4BJsr",
      "cdate": 1715792779552,
      "mdate": 1730873997664,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444841"
    },
    {
      "id": "uvFDaeFR9X",
      "title": "Exploring Jacobian Inexactness in Second-Order Methods for Variational Inequalities: Lower Bounds, Optimal Algorithms and Quasi-Newton Approximations",
      "abstract": "Variational inequalities represent a broad class of problems, including minimization and min-max problems, commonly found in machine learning. Existing second-order and high-order methods for variational inequalities require precise computation of derivatives, often resulting in prohibitively high iteration costs. In this work, we study the impact of Jacobian inaccuracy on second-order methods. For the smooth and monotone case, we establish a lower bound with explicit dependence on the level of Jacobian inaccuracy and propose an optimal algorithm for this key setting. When derivatives are exact, our method converges at the same rate as exact optimal second-order methods. To reduce the cost of solving the auxiliary problem, which arises in all high-order methods with global convergence, we introduce several Quasi-Newton approximations. Our method with Quasi-Newton updates achieves a global sublinear convergence rate. We extend our approach with a tensor generalization for inexact high-order derivatives and support the theory with experiments.",
      "authors": [
        "Artem Agafonov",
        "Petr Ostroukhov",
        "Roman Mozhaev",
        "Konstantin Yakovlev",
        "Eduard Gorbunov",
        "Martin Takáč",
        "Alexander Gasnikov",
        "Dmitry Kamzolov"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=uvFDaeFR9X",
      "cdate": 1715792764585,
      "mdate": 1730873997625,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444845"
    },
    {
      "id": "HCTikT7LS4",
      "title": "Enhancing Robustness in Deep Reinforcement Learning: A Lyapunov Exponent Approach",
      "abstract": "Deep reinforcement learning agents achieve state-of-the-art performance in a wide range of simulated control tasks. However, successful applications to real-world problems remain limited. One reason for this dichotomy is because the learnt policies are not robust to observation noise or adversarial attacks. In this paper, we investigate the robustness of deep RL policies to a single small state perturbation in deterministic continuous control tasks. We demonstrate that RL policies can be deterministically chaotic, as small perturbations to the system state have a large impact on subsequent state and reward trajectories. This unstable non-linear behaviour has two consequences: first, inaccuracies in sensor readings, or adversarial attacks, can cause significant performance degradation; second, even policies that show robust performance in terms of rewards may have unpredictable behaviour in practice. These two facets of chaos in RL policies drastically restrict the application of deep RL to real-world problems. To address this issue, we propose an improvement on the successful Dreamer V3 architecture, implementing Maximal Lyapunov Exponent regularisation. This new approach reduces the chaotic state dynamics, rendering the learnt policies more resilient to sensor noise or adversarial attacks and thereby improving the suitability of deep reinforcement learning for real-world applications.",
      "authors": [
        "Rory Young",
        "Nicolas Pugeault"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=HCTikT7LS4",
      "cdate": 1715792763691,
      "mdate": 1730873997599,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444851"
    },
    {
      "id": "U6oQEzSp8z",
      "title": "An eye for an ear: zero-shot audio description leveraging an image captioner with audio-visual token distribution matching",
      "abstract": "Multimodal large language models have fueled progress in image captioning. These models, fine-tuned on vast image datasets, exhibit a deep understanding of semantic concepts.\nIn this work, we show that this ability can be re-purposed for audio captioning, where the joint image-language decoder can be leveraged to describe auditory content associated with image sequences within videos featuring audiovisual content. This can be achieved via multimodal alignment.\nYet, this multimodal alignment task is non-trivial due to the inherent disparity between audible and visible elements in real-world videos. Moreover, multimodal representation learning often relies on contrastive learning, facing the challenge of the so-called modality gap which hinders smooth integration between modalities. In this work, we introduce a novel methodology for bridging the audiovisual modality gap by matching the distributions of tokens produced by an audio backbone and those of an image captioner. Our approach aligns the audio token distribution with that of the image tokens, enabling the model to perform zero-shot audio captioning in an unsupervised fashion. This alignment allows for the use of either audio or audiovisual input by combining or substituting the image encoder with the aligned audio encoder. Our method achieves significantly improved performances in zero-shot audio captioning, compared to existing approaches.",
      "authors": [
        "Hugo Malard",
        "Michel Olvera",
        "Stéphane Lathuilière",
        "Slim Essid"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=U6oQEzSp8z",
      "cdate": 1715792759828,
      "mdate": 1730873997556,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.444856"
    },
    {
      "id": "HkC4OYee3Q",
      "title": "SleeperNets: Universal Backdoor Poisoning Attacks Against  Reinforcement Learning Agents",
      "abstract": "Reinforcement learning (RL) is an actively growing field that is seeing increased usage in real-world, safety-critical applications -- making it paramount to ensure the robustness of RL algorithms against adversarial attacks. In this work we explore a particularly stealthy form of training-time attacks against RL -- backdoor poisoning. Here the adversary intercepts the training of an RL agent with the goal of reliably inducing a particular action when the agent observes a pre-determined trigger at inference time. We uncover theoretical limitations of prior work by proving their inability to generalize across domains and MDPs. Motivated by this, we formulate a novel poisoning attack framework which interlinks the adversary's objectives with those of finding an optimal policy -- guaranteeing attack success in the limit. Using insights from our theoretical analysis we develop \"SleeperNets\" as a universal backdoor attack which exploits a newly proposed threat model and leverages dynamic reward poisoning techniques. We evaluate our attack in 6 environments spanning multiple domains and demonstrate significant improvements in attack success over existing methods, while preserving benign episodic return.",
      "authors": [
        "Ethan Rathbun",
        "Christopher Amato",
        "Alina Oprea"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=HkC4OYee3Q",
      "cdate": 1715792456371,
      "mdate": 1730873997398,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444862"
    },
    {
      "id": "w2L3Ll1jbV",
      "title": "Adversarially Robust Multi-task Representation Learning",
      "abstract": "We study adversarially robust transfer learning, wherein, given labeled data on multiple (source) tasks, the goal is to train a model with small robust error on a previously unseen (target) task.\nIn particular, we consider a multi-task representation learning (MTRL) setting, i.e., we assume that the source and target tasks admit a simple (linear) predictor on top of a shared representation (e.g., the final hidden layer of a \ndeep neural network).\nIn this general setting, we provide rates on~the excess adversarial (transfer) risk for Lipschitz losses and smooth nonnegative losses.\nThese rates show that learning a representation using adversarial training on diverse tasks  helps protect against inference-time attacks in data-scarce environments.\nAdditionally, we provide novel rates for the single-task setting.",
      "authors": [
        "Austin Watkins",
        "Thanh Nguyen-Tang",
        "Enayat Ullah",
        "Raman Arora"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=w2L3Ll1jbV",
      "cdate": 1715792450260,
      "mdate": 1730873997357,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444867"
    },
    {
      "id": "TADTT9ughN",
      "title": "Deep Bayesian Active Learning for Preference Modeling in Large Language Models",
      "abstract": "Leveraging human preferences for steering the behavior of Large Language Models (LLMs) has demonstrated notable success in recent years. Nonetheless, data selection and labeling are still a bottleneck for these systems, particularly at large scale. Hence, selecting the most informative points for acquiring human feedback may considerably reduce the cost of preference labeling and unleash the further development of LLMs. Bayesian Active Learning provides a principled framework for addressing this challenge and has demonstrated remarkable success in diverse settings. However, previous attempts to employ it for Preference Modeling did not meet such expectations. In this work, we identify that naive epistemic uncertainty estimation leads to the acquisition of redundant samples. We address this by proposing the Bayesian Active Learner for Preference Modeling (BAL-PM), a novel stochastic acquisition policy that not only targets points of high epistemic uncertainty according to the preference model but also seeks to maximize the entropy of the acquired prompt distribution in the feature space spanned by the employed LLM. Notably, our experiments demonstrate that BAL-PM requires 33\\% to 68\\% fewer preference labels in two popular human preference datasets and exceeds previous stochastic Bayesian acquisition policies.",
      "authors": [
        "Luckeciano Carvalho Melo",
        "Panagiotis Tigas",
        "Alessandro Abate",
        "Yarin Gal"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=TADTT9ughN",
      "cdate": 1715792434916,
      "mdate": 1730873997349,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444872"
    },
    {
      "id": "3PqhU96Vvv",
      "title": "Flexible Context-Driven Sensory Processing in Dynamical Vision Models",
      "abstract": "Visual representations become progressively more abstract along the cortical hierarchy. These abstract representations define notions like objects and shapes, but at the cost of spatial specificity. By contrast, low-level regions represent spatially local but simple input features. How do spatially non-specific representations of abstract concepts in high-level areas flexibly modulate the low-level sensory representations in appropriate ways to guide context-driven and goal-directed behaviors across a range of tasks? We build a biologically motivated and trainable neural network model of dynamics in the visual pathway, incorporating local, lateral, and feedforward synaptic connections, excitatory and inhibitory neurons, and long-range top-down inputs conceptualized as low-rank modulations of the input-driven sensory responses by high-level areas. We study this ${\\bf D}$ynamical ${\\bf C}$ortical ${\\bf net}$work ($DCnet$) in a visual cue-delay-search task and show that the model uses its own cue representations to adaptively modulate its perceptual responses to solve the task, outperforming state-of-the-art DNN vision and LLM models. The model's population states over time shed light on the nature of contextual modulatory dynamics, generating predictions for experiments. We fine-tune the same model on classic psychophysics attention tasks, and find that the model closely replicates known reaction time results. This work represents a promising new foundation for understanding and making predictions about perturbations to visual processing in the brain.",
      "authors": [
        "Lakshmi Narasimhan Govindarajan",
        "Abhiram Iyer",
        "Valmiki Kothare",
        "Ila R Fiete"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3PqhU96Vvv",
      "cdate": 1715792361704,
      "mdate": 1730873997305,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444877"
    },
    {
      "id": "l6iICoILGB",
      "title": "Practical $0.385$-Approximation for Submodular Maximization Subject to a Cardinality Constraint",
      "abstract": "Non-monotone constrained submodular maximization plays a crucial role in various machine learning applications. However, existing algorithms often struggle with a trade-off between approximation guarantees and practical efficiency. The current state-of-the-art is a recent $0.401$-approximation algorithm, but its computational complexity makes it highly impractical. The best practical algorithms for the problem only guarantee $1/e$-approximation. In this work, we present a novel algorithm for submodular maximization subject to a cardinality constraint that combines a guarantee of $0.385$-approximation with a low and practical query complexity of $O(n+k^2)$. Furthermore, we evaluate our algorithm's performance through extensive machine learning applications, including Movie Recommendation, Image Summarization, and more. These evaluations demonstrate the efficacy of our approach.",
      "authors": [
        "Murad Tukan",
        "Loay Mualem",
        "Moran Feldman"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=l6iICoILGB",
      "cdate": 1715792331173,
      "mdate": 1730873997237,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444883"
    },
    {
      "id": "BRW0MKJ7Rr",
      "title": "Action Gaps and Advantages in Continuous-Time Distributional Reinforcement Learning",
      "abstract": "When decisions are made at high frequency, traditional reinforcement learning (RL) methods struggle to accurately estimate action values. In turn, their performance is inconsistent and often poor. Whether the performance of distributional RL (DRL) agents suffers similarly, however, is unknown. In this work, we establish that DRL agents *are* sensitive to the decision frequency. We prove that action-conditioned return distributions collapse to their underlying policy's return distribution as the decision frequency increases. We quantify the rate of collapse of these return distributions and exhibit that their statistics collapse at different rates. Moreover, we define distributional perspectives on action gaps and advantages. In particular, we introduce the *superiority* as a probabilistic generalization of the advantage---the core object of approaches to mitigating performance issues in high-frequency value-based RL. In addition, we build a superiority-based DRL algorithm. Through simulations in an option-trading domain, we validate that proper modeling of the superiority distribution produces improved controllers at high decision frequencies.",
      "authors": [
        "Harley Wiltzer",
        "Marc G Bellemare",
        "David Meger",
        "Patrick Shafto",
        "Yash Jhaveri"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BRW0MKJ7Rr",
      "cdate": 1715792291397,
      "mdate": 1736198666602,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444888"
    },
    {
      "id": "UPxmISfNCO",
      "title": "Efficiency for Free: Ideal Data Are Transportable Representations",
      "abstract": "Data, the seminal opportunity and challenge in modern machine learning, currently constrains the scalability of representation learning and impedes the pace of model evolution.\nIn this work, we investigate the efficiency properties of data from both optimization and generalization perspectives.\nOur theoretical and empirical analysis reveals an unexpected finding: for a given task, utilizing a publicly available, task- and architecture-agnostic model (referred to as the `prior model' in this paper) can effectively produce efficient data.\nBuilding on this insight, we propose the Representation Learning Accelerator (ReLA), which promotes the formation and utilization of efficient data, thereby accelerating representation learning.\nUtilizing a ResNet-18 pre-trained on CIFAR-10 as a prior model to inform ResNet-50 training on ImageNet-1K reduces computational costs by $50\\%$ while maintaining the same accuracy as the model trained with the original BYOL, which requires $100\\%$ cost.\nOur code is available at: \\url{https://github.com/LINs-lab/ReLA}.",
      "authors": [
        "Peng Sun",
        "Yi Jiang",
        "Tao Lin"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=UPxmISfNCO",
      "cdate": 1715792264072,
      "mdate": 1730873997097,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444893"
    },
    {
      "id": "sZ7jj9kqAy",
      "title": "SOI: Scaling Down Computational Complexity by Estimating Partial States of the Model",
      "abstract": "Consumer electronics used to follow the miniaturization trend described by Moore’s Law. Despite increased processing power in Microcontroller Units (MCUs), MCUs used in the smallest appliances are still not capable of running even moderately big, state-of-the-art artificial neural networks (ANNs) especially in time-sensitive scenarios. In this work, we present a novel method called Scattered Online Inference (SOI) that aims to reduce the computational complexity of ANNs. SOI leverages the continuity and seasonality of time-series data and model predictions, enabling extrapolation for processing speed improvements, particularly in deeper layers. By applying compression, SOI generates more general inner partial states of ANN, allowing skipping full model recalculation at each inference.",
      "authors": [
        "Grzegorz Stefański",
        "Paweł Daniluk",
        "Artur Szumaczuk",
        "Jakub Tkaczuk"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=sZ7jj9kqAy",
      "cdate": 1715792188111,
      "mdate": 1730873996996,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444898"
    },
    {
      "id": "aYqTwcDlCG",
      "title": "Learning World Models for Unconstrained Goal Navigation",
      "abstract": "Learning world models offers a promising avenue for goal-conditioned reinforcement learning with sparse rewards. By allowing agents to plan actions or exploratory goals without direct interaction with the environment, world models enhance exploration efficiency. The quality of a world model hinges on the richness of data stored in the agent's replay buffer, with expectations of reasonable generalization across the state space surrounding recorded trajectories. However, challenges arise in generalizing learned world models to state transitions backward along recorded trajectories or between states across different trajectories, hindering their ability to accurately model real-world dynamics. To address these challenges, we introduce a novel goal-directed exploration algorithm, MUN (short for \"World Models for Unconstrained Goal Navigation\"). This algorithm is capable of modeling state transitions between arbitrary subgoal states in the replay buffer, thereby facilitating the learning of policies to navigate between any \"key\" states. Experimental results demonstrate that MUN strengthens the reliability of world models and significantly improves the policy's capacity to generalize across new goal settings.",
      "authors": [
        "Yuanlin Duan",
        "Wensen Mao",
        "He Zhu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=aYqTwcDlCG",
      "cdate": 1715792187034,
      "mdate": 1736973330711,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444903"
    },
    {
      "id": "lDtABI541U",
      "title": "Quadratic Quantum Variational Monte Carlo",
      "abstract": "This paper introduces the Quadratic Quantum Variational Monte Carlo (Q$^2$VMC) algorithm, an innovative algorithm in quantum chemistry that significantly enhances the efficiency and accuracy of solving the Schrödinger equation. Inspired by the discretization of imaginary-time Schrödinger evolution, Q$^2$VMC employs a novel quadratic update mechanism that integrates seamlessly with neural network-based ansatzes. Our extensive experiments showcase Q$^2$VMC's superior performance, achieving faster convergence and lower ground state energies in wavefunction optimization across various molecular systems, without additional computational cost. This study not only advances the field of computational quantum chemistry but also highlights the important role of discretized evolution in variational quantum algorithms, offering a scalable and robust framework for future quantum research.",
      "authors": [
        "Baiyu Su",
        "qiang liu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lDtABI541U",
      "cdate": 1715792169231,
      "mdate": 1730873996901,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444908"
    },
    {
      "id": "qAP6RyYIJc",
      "title": "Stealth edits to large language models",
      "abstract": "We reveal the theoretical foundations of techniques for editing large language models, and present new methods which can do so without requiring retraining. Our theoretical insights show that a single metric (a measure of the intrinsic dimension of the model's features) can be used to assess a model's editability and reveals its previously unrecognised susceptibility to malicious *stealth attacks*. This metric is fundamental to predicting the success of a variety of editing approaches, and reveals new bridges between disparate families of editing methods. We collectively refer to these as *stealth editing* methods, because they directly update a model's weights to specify its response to specific known hallucinating prompts without affecting other model behaviour. By carefully applying our theoretical insights, we are able to introduce a new *jet-pack* network block which is optimised for highly selective model editing, uses only standard network operations, and can be inserted into existing networks. We also reveal the vulnerability of language models to stealth attacks: a small change to a model's weights which fixes its response to a single attacker-chosen prompt. Stealth attacks are computationally simple, do not require access to or knowledge of the model's training data, and therefore represent a potent yet previously unrecognised threat to redistributed foundation models. Extensive experimental results illustrate and support our methods and their theoretical underpinnings. Demos and source code are available at https://github.com/qinghua-zhou/stealth-edits.",
      "authors": [
        "Oliver Sutton",
        "Qinghua Zhou",
        "Wei Wang",
        "Desmond Higham",
        "Alexander N. Gorban",
        "Alexander Bastounis",
        "Ivan Y Tyukin"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qAP6RyYIJc",
      "cdate": 1715792145764,
      "mdate": 1730873996850,
      "matched_keywords": [
        "large language model",
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444913"
    },
    {
      "id": "0Gl5WxY6es",
      "title": "Grounding Multimodal Large Language Models in Actions",
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated a wide range of capabilities across many domains including Embodied AI. In this work, we study how to best ground a MLLM into different embodiments and their associated action spaces, including both continuous and discrete actions. For continuous actions, a set of learned tokenizations that capture an action at various resolutions allows for sufficient modeling precision, yielding the best performance on downstream tasks. For discrete actions, semantically aligning these actions with the native output token space of the MLLM leads to the strongest performance. We arrive at these lessons via a thorough study of seven action grounding approaches on five different environments, encompassing over 114 embodied tasks.",
      "authors": [
        "Andrew Szot",
        "Bogdan Mazoure",
        "Harsh Agrawal",
        "R Devon Hjelm",
        "Zsolt Kira",
        "Alexander T Toshev"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0Gl5WxY6es",
      "cdate": 1715792097663,
      "mdate": 1730873996770,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.444918"
    },
    {
      "id": "hEKSSsv5Q9",
      "title": "DALD: Improving Logits-based Detector without Logits from Black-box LLMs",
      "abstract": "The advent of Large Language Models (LLMs) has revolutionized text generation, producing outputs that closely mimic human writing. This blurring of lines between machine- and human-written text presents new challenges in distinguishing one from the other – a task further complicated by the frequent updates and closed nature of leading proprietary LLMs. Traditional logits-based detection methods leverage surrogate models for identifying LLM-generated content when the exact logits are unavailable from black-box LLMs. However, these methods grapple with the misalignment between the distributions of the surrogate and the often undisclosed target models, leading to performance degradation, particularly with the introduction of new, closed-source models. Furthermore, while current methodologies are generally effective when the source model is identified, they falter in scenarios where the model version remains unknown, or the test set comprises outputs from various source models. To address these limitations, we present \\textbf{D}istribution-\\textbf{A}ligned \\textbf{L}LMs \\textbf{D}etection (DALD), an innovative framework that redefines the state-of-the-art performance in black-box text detection even without logits from source LLMs. DALD is designed to align the surrogate model's distribution with that of unknown target LLMs, ensuring enhanced detection capability and resilience against rapid model iterations with minimal training investment. By leveraging corpus samples from publicly accessible outputs of advanced models such as ChatGPT, GPT-4 and Claude-3, DALD fine-tunes surrogate models to synchronize with unknown source model distributions effectively. Our approach achieves SOTA performance in black-box settings on different advanced closed-source and open-source models. The versatility of our method enriches widely adopted zero-shot detection frameworks (DetectGPT, DNA-GPT, Fast-DetectGPT) with a `plug-and-play' enhancement feature. \nExtensive experiments validate that our methodology reliably secures high detection precision for LLM-generated text and effectively detects text from diverse model origins through a singular detector.\nOur method is also robust under the revised text attack and non-English texts.",
      "authors": [
        "Cong Zeng",
        "Shengkun Tang",
        "Xianjun Yang",
        "Yuanzhou Chen",
        "Yiyou Sun",
        "zhiqiang xu",
        "Yao Li",
        "Haifeng Chen",
        "Wei Cheng",
        "Dongkuan Xu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hEKSSsv5Q9",
      "cdate": 1715792086391,
      "mdate": 1734681865057,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444923"
    },
    {
      "id": "NaCXcUKihH",
      "title": "Towards a theory of how the structure of language is acquired by deep neural networks",
      "abstract": "How much data is required to learn the structure of a language via next-token prediction? We study this question for synthetic datasets generated via a Probabilistic Context-Free Grammar (PCFG)---a hierarchical generative model that captures the tree-like structure of natural languages. We determine token-token correlations analytically in our model and show that they can be used to build a representation of the grammar's hidden variables, the longer the range the deeper the variable. In addition, a finite training set limits the resolution of correlations to an effective range, whose size grows with that of the training set. As a result, a Language Model trained with increasingly many examples can build a deeper representation of the grammar's structure, thus reaching good performance despite the high dimensionality of the problem. We conjecture that the relationship between training set size and effective range of correlations holds beyond our synthetic datasets, and we test it in a collection of lines from Shakespeare's plays. In particular, we show that reducing the input size leads to saturation of the test loss decay at a characteristic training set size that can be predicted in our framework.",
      "authors": [
        "Francesco Cagnetta",
        "Matthieu Wyart"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NaCXcUKihH",
      "cdate": 1715792012049,
      "mdate": 1730873996619,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444930"
    },
    {
      "id": "9hKN99RNdR",
      "title": "Exploring the Edges of Latent State Clusters for Goal-Conditioned Reinforcement Learning",
      "abstract": "Exploring unknown environments efficiently is a fundamental challenge in unsupervised goal-conditioned reinforcement learning. While selecting exploratory goals at the frontier of previously explored states is an effective strategy, the policy during training may still have limited capability of reaching rare goals on the frontier, resulting in reduced exploratory behavior. We propose \"Cluster Edge Exploration\" (CE$^2$), a new goal-directed exploration algorithm that when choosing goals in sparsely explored areas of the state space gives priority to goal states that remain accessible to the agent. The key idea is clustering to group states that are easily reachable from one another by the current policy under training in a latent space, and traversing to states holding significant exploration potential on the boundary of these clusters before doing exploratory behavior. In challenging robotics environments including navigating a maze with a multi-legged ant robot, manipulating objects with a robot arm on a cluttered tabletop, and rotating objects in the palm of an anthropomorphic robotic hand, CE$^2$ demonstrates superior efficiency in exploration compared to baseline methods and ablations.",
      "authors": [
        "Yuanlin Duan",
        "Guofeng Cui",
        "He Zhu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9hKN99RNdR",
      "cdate": 1715791982194,
      "mdate": 1736975029767,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444935"
    },
    {
      "id": "hocAc3Qit7",
      "title": "Flexible mapping of abstract domains by grid cells via self-supervised extraction and projection of generalized velocity signals",
      "abstract": "Grid cells in the medial entorhinal cortex create remarkable periodic maps of explored space during navigation. Recent studies show that they form similar maps of abstract cognitive spaces. Examples of such abstract environments include auditory tone sequences in which the pitch is continuously varied or images in which abstract features are continuously deformed (e.g., a cartoon bird whose legs stretch and shrink). Here, we hypothesize that the brain generalizes how it maps spatial domains to mapping abstract spaces. \nTo sidestep the computational cost of learning representations for each high-dimensional sensory input, the brain extracts self-consistent, low-dimensional descriptions of displacements across abstract spaces, leveraging the spatial velocity integration of grid cells to efficiently build maps of different domains.\nOur neural network model for abstract velocity extraction factorizes the content of these abstract domains from displacements within the domains to generate content-independent and self-consistent, low-dimensional velocity estimates. \nCrucially, it uses a self-supervised geometric consistency constraint that requires displacements along closed loop trajectories to sum to zero, an integration that is itself performed by the downstream grid cell circuit over learning. This process results in high fidelity estimates of velocities and allowed transitions in abstract domains, a crucial prerequisite for efficient map generation in these high-dimensional environments. We also show how our method outperforms traditional dimensionality reduction and deep-learning based motion extraction networks on the same set of tasks.\nThis is the first neural network model to explain how grid cells can flexibly represent different abstract spaces and makes the novel prediction that they should do so while maintaining their population correlation and manifold structure across domains. Fundamentally, our model sheds light on the mechanistic origins of cognitive flexibility and transfer of representations across vastly different domains in brains, providing a potential self-supervised learning (SSL) framework for leveraging similar ideas in transfer learning and data-efficient generalization in machine learning and robotics.",
      "authors": [
        "Abhiram Iyer",
        "Sarthak Chandra",
        "Sugandha Sharma",
        "Ila R Fiete"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hocAc3Qit7",
      "cdate": 1715791877919,
      "mdate": 1730873996329,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444940"
    },
    {
      "id": "P6nVDZRZRB",
      "title": "Are Uncertainty Quantification Capabilities of Evidential Deep Learning a Mirage?",
      "abstract": "This paper questions the effectiveness of a modern predictive uncertainty quantification approach, called *evidential deep learning* (EDL), in which a single neural network model is trained to learn a meta distribution over the predictive distribution by minimizing a specific objective function. Despite their perceived strong empirical performance on downstream tasks, a line of recent studies by Bengs et al. identify limitations of the existing methods to conclude their learned epistemic uncertainties are unreliable, e.g., in that they are non-vanishing even with infinite data. Building on and sharpening such analysis, we 1) provide a sharper understanding of the asymptotic behavior of a wide class of EDL methods by unifying various objective functions; 2) reveal that the EDL methods can be better interpreted as an out-of-distribution detection algorithm based on energy-based-models; and  3) conduct extensive ablation studies to better assess their empirical effectiveness with real-world datasets. \nThrough all these analyses, we conclude that even when EDL methods are empirically effective on downstream tasks, this occurs despite their poor uncertainty quantification capabilities. Our investigation suggests that incorporating model uncertainty can help EDL methods faithfully quantify uncertainties and further improve performance on representative downstream tasks, albeit at the cost of additional computational complexity.",
      "authors": [
        "Maohao Shen",
        "Jongha Jon Ryu",
        "Soumya Ghosh",
        "Yuheng Bu",
        "Prasanna Sattigeri",
        "Subhro Das",
        "Gregory W. Wornell"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=P6nVDZRZRB",
      "cdate": 1715791875780,
      "mdate": 1730873996292,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444945"
    },
    {
      "id": "4neqdBz8eG",
      "title": "Rethinking Weight Decay for Robust Fine-Tuning of Foundation Models",
      "abstract": "Modern optimizers such as AdamW, equipped with momentum and adaptive learning rate, are designed to escape local minima and explore the vast parameter space. This exploration is beneficial for finding good loss basins when training from scratch. It is not necessarily ideal when resuming from a powerful foundation model because it can lead to large deviations from the pre-trained initialization and, consequently, worse robustness and generalization. At the same time, strong regularization on all parameters can lead to under-fitting. We hypothesize that selectively regularizing the parameter space is the key to fitting and retraining the pre-trained knowledge. This paper proposes a new weight decay technique, Selective Projection Decay (SPD), that selectively imposes a strong penalty on certain layers while allowing others to change freely. Intuitively, SPD expands and contracts the parameter search space for layers with consistent and inconsistent loss reduction, respectively. Experimentally, when equipped with SPD, Adam consistently provides better in-distribution generalization and out-of-distribution robustness performance on multiple popular vision and language benchmarks.",
      "authors": [
        "Junjiao Tian",
        "Chengyue Huang",
        "Zsolt Kira"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4neqdBz8eG",
      "cdate": 1715791723331,
      "mdate": 1730873996216,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444951"
    },
    {
      "id": "fDiZJ7mmOV",
      "title": "Non-Stationary Learning of Neural Networks with Automatic Soft Parameter Reset",
      "abstract": "Neural networks are most often trained under the assumption that data come from a stationary distribution. However, settings in which this assumption is violated are of increasing importance; examples include supervised learning with distributional shifts, reinforcement learning, continual learning and non-stationary contextual bandits. Here, we introduce a novel learning approach that automatically models and adapts to non-stationarity by linking parameters through an Ornstein-Uhlenbeck process with an adaptive drift parameter. The adaptive drift draws the parameters towards the distribution used at initialisation, so the approach can be understood as a form of soft parameter reset. We show empirically that our approach performs well in non-stationary supervised, and off-policy reinforcement learning settings.",
      "authors": [
        "Alexandre Galashov",
        "Michalis Titsias",
        "András György",
        "Clare Lyle",
        "Razvan Pascanu",
        "Yee Whye Teh",
        "Maneesh Sahani"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fDiZJ7mmOV",
      "cdate": 1715791541935,
      "mdate": 1736890187751,
      "matched_keywords": [
        "reinforcement learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444956"
    },
    {
      "id": "KQp7dk5YYH",
      "title": "Task-Agnostic Machine-Learning-Assisted Inference",
      "abstract": "Machine learning (ML) is playing an increasingly important role in scientific research. In conjunction with classical statistical approaches, ML-assisted analytical strategies have shown great promise in accelerating research findings. This has also opened a whole field of methodological research focusing on integrative approaches that leverage both ML and statistics to tackle data science challenges. One type of study that has quickly gained popularity employs ML to predict unobserved outcomes in massive samples, and then uses predicted outcomes in downstream statistical inference. However, existing methods designed to ensure the validity of this type of post-prediction inference are limited to very basic tasks such as linear regression analysis. This is because any extension of these approaches to new, more sophisticated statistical tasks requires task-specific algebraic derivations and software implementations, which ignores the massive library of existing software tools already developed for the same scientific problem given observed data. This severely constrains the scope of application for post-prediction inference. To address this challenge, we introduce a novel statistical framework named PSPS for task-agnostic ML-assisted inference. It provides a post-prediction inference solution that can be easily plugged into almost any established data analysis routines. It delivers valid and efficient inference that is robust to arbitrary choice of ML model, allowing nearly all existing statistical frameworks to be incorporated into the analysis of ML-predicted data. Through extensive experiments, we showcase our method’s validity, versatility, and superiority compared to existing approaches. Our software is available at https://github.com/qlu-lab/psps.",
      "authors": [
        "Jiacheng Miao",
        "Qiongshi Lu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=KQp7dk5YYH",
      "cdate": 1715791499755,
      "mdate": 1730873996080,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444961"
    },
    {
      "id": "GnF9tavqgc",
      "title": "Physical Consistency Bridges Heterogeneous Data in Molecular Multi-Task Learning",
      "abstract": "In recent years, machine learning has demonstrated impressive capability in handling molecular science tasks. To support various molecular properties at scale, machine learning models are trained in the multi-task learning paradigm. Nevertheless, data of different molecular properties are often not aligned: some quantities, e.g. equilibrium structure, demand more cost to compute than others, e.g. energy, so their data are often generated by cheaper computational methods at the cost of lower accuracy, which cannot be directly overcome through multi-task learning. Moreover, it is not straightforward to leverage abundant data of other tasks to benefit a particular task. To handle such data heterogeneity challenges, we exploit the specialty of molecular tasks that there are physical laws connecting them, and design consistency training approaches that allow different tasks to exchange information directly so as to improve one another. Particularly, we demonstrate that the more accurate energy data can improve the accuracy of structure prediction. We also find that consistency training can directly leverage force and off-equilibrium structure data to improve structure prediction, demonstrating a broad capability for integrating heterogeneous data.",
      "authors": [
        "Yuxuan Ren",
        "Dihan Zheng",
        "Chang Liu",
        "Peiran Jin",
        "Yu Shi",
        "Lin Huang",
        "Jiyan He",
        "Shengjie Luo",
        "Tao Qin",
        "Tie-Yan Liu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GnF9tavqgc",
      "cdate": 1715791335476,
      "mdate": 1730873996023,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444966"
    },
    {
      "id": "76NKidadct",
      "title": "Improved Particle Approximation Error for Mean Field Neural Networks",
      "abstract": "Mean-field Langevin dynamics (MFLD) minimizes an entropy-regularized nonlinear convex functional defined over the space of probability distributions. MFLD has gained attention due to its connection with noisy gradient descent for mean-field two-layer neural networks. Unlike standard Langevin dynamics, the nonlinearity of the objective functional induces particle interactions, necessitating multiple particles to approximate the dynamics in a finite-particle setting. Recent works (Chen et al., 2022; Suzuki et al., 2023b) have demonstrated the uniform-in-time propagation of chaos for MFLD, showing that the gap between the particle system and its mean-field limit uniformly shrinks over time as the number of particles increases. In this work, we improve the dependence on logarithmic Sobolev inequality (LSI) constants in their particle approximation errors, which can exponentially deteriorate with the regularization coefficient. Specifically, we establish an LSI-constant-free particle approximation error concerning the objective gap by leveraging the problem structure in risk minimization. As the application, we demonstrate improved convergence of MFLD, sampling guarantee for the mean-field stationary distribution, and uniform-in-time Wasserstein propagation of chaos in terms of particle complexity.",
      "authors": [
        "Atsushi Nitanda"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=76NKidadct",
      "cdate": 1715791330596,
      "mdate": 1735955324908,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444971"
    },
    {
      "id": "vjCFnYTg67",
      "title": "Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature",
      "abstract": "Text watermarks for large language models (LLMs) have been commonly used to identify the origins of machine-generated content, which is promising for assessing liability when combating deepfake or harmful content. While existing watermarking techniques typically prioritize robustness against removal attacks, unfortunately, they are vulnerable to spoofing attacks: malicious actors can subtly alter the meanings of LLM-generated responses or even forge harmful content, potentially misattributing blame to the LLM developer. To overcome this, we introduce a bi-level signature scheme, Bileve, which embeds fine-grained signature bits for integrity checks (mitigating spoofing attacks) as well as a coarse-grained signal to trace text sources when the signature is invalid (enhancing detectability) via a novel rank-based sampling strategy. Compared to conventional watermark detectors that only output binary results, Bileve can differentiate 5 scenarios during detection, reliably tracing text provenance and regulating LLMs. The experiments conducted on OPT-1.3B and LLaMA-7B demonstrate the effectiveness of Bileve in defeating spoofing attacks with enhanced detectability.",
      "authors": [
        "Tong Zhou",
        "Xuandong Zhao",
        "Xiaolin Xu",
        "Shaolei Ren"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vjCFnYTg67",
      "cdate": 1715791107287,
      "mdate": 1730873995767,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.444976"
    },
    {
      "id": "XEbPJUQzs3",
      "title": "Prospective Learning: Learning for a Dynamic Future",
      "abstract": "In real-world applications, the distribution of the data, and our goals, evolve over time. The prevailing theoretical framework for studying machine learning, namely probably approximately correct (PAC) learning, largely ignores time. As a consequence, existing strategies to address the dynamic nature of data and goals exhibit poor real-world performance. This paper develops a theoretical framework called\n\"Prospective Learning\" that is tailored for situations when the optimal hypothesis changes over time. In PAC learning, empirical risk minimization (ERM) is known to be consistent. We develop a learner called Prospective ERM, which returns a sequence of predictors that  make predictions on future data.  We prove that the risk of prospective ERM converges to the Bayes risk under certain assumptions on the stochastic process  generating the data. Prospective ERM, roughly speaking, incorporates time as an input in addition to the data. We show that standard ERM as done in PAC learning, without incorporating time, can result in failure to learn when distributions are dynamic. Numerical experiments illustrate that prospective ERM can learn synthetic and visual recognition problems constructed from MNIST and CIFAR-10. Code at https://github.com/neurodata/prolearn.",
      "authors": [
        "Ashwin De Silva",
        "Rahul Ramesh",
        "Rubing Yang",
        "Siyu Yu",
        "Joshua T Vogelstein",
        "Pratik Chaudhari"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=XEbPJUQzs3",
      "cdate": 1715791007359,
      "mdate": 1736996401028,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444982"
    },
    {
      "id": "GuY0zB2xVU",
      "title": "Boosting Generalization in Parametric PDE Neural Solvers through Adaptive Conditioning",
      "abstract": "Solving parametric partial differential equations (PDEs) presents significant challenges for data-driven methods due to the sensitivity of spatio-temporal dynamics to variations in PDE parameters. Machine learning approaches often struggle to capture this variability. To address this, data-driven approaches learn parametric PDEs by sampling a very large variety of trajectories with varying PDE parameters. We first show that incorporating conditioning mechanisms for learning parametric PDEs is essential and that among them, \\textit{adaptive conditioning}, allows stronger generalization. As existing adaptive conditioning methods do not scale well with respect to the number of parameters to adapt in the neural solver, we propose GEPS, a simple adaptation mechanism to boost GEneralization in Pde Solvers via a first-order optimization and low-rank rapid adaptation of a small set of context parameters. We demonstrate the versatility of our approach for both fully data-driven and for physics-aware neural solvers. Validation performed on a whole range of spatio-temporal forecasting problems demonstrates excellent performance for generalizing to unseen conditions including initial conditions, PDE coefficients, forcing terms and solution domain. *Project page*: https://geps-project.github.io",
      "authors": [
        "Armand Kassaï Koupaï",
        "Jorge Mifsut Benet",
        "Yuan Yin",
        "Jean-Noël Vittaut",
        "Patrick Gallinari"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GuY0zB2xVU",
      "cdate": 1715790974798,
      "mdate": 1730873995657,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444987"
    },
    {
      "id": "ZtTWKr51yH",
      "title": "Constrained Adaptive Attack: Effective Adversarial Attack Against Deep Neural Networks for Tabular Data",
      "abstract": "State-of-the-art deep learning models for tabular data have recently achieved acceptable performance to be deployed in industrial settings. However, the robustness of these models remains scarcely explored. Contrary to computer vision, there are no effective attacks to properly evaluate the adversarial robustness of deep tabular models due to intrinsic properties of tabular data, such as categorical features, immutability, and feature relationship constraints. To fill this gap, we first propose CAPGD, a gradient attack that overcomes the failures of existing gradient attacks with adaptive mechanisms. This new attack does not require parameter tuning and further degrades the accuracy, up to 81\\% points compared to the previous gradient attacks. Second, we design CAA, an efficient evasion attack that combines our CAPGD attack and MOEVA, the best search-based attack.  We demonstrate the effectiveness of our attacks on five architectures and four critical use cases. Our empirical study demonstrates that CAA outperforms all existing attacks in 17 over the 20 settings, and leads to a drop in the accuracy by up to 96.1\\% points and 21.9\\% points compared to CAPGD and MOEVA respectively while being up to five times faster than MOEVA. Given the effectiveness and efficiency of our new attacks, we argue that they should become the minimal test for any new defense or robust architectures in tabular machine learning.",
      "authors": [
        "Thibault Simonetto",
        "Salah GHAMIZI",
        "Maxime Cordy"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ZtTWKr51yH",
      "cdate": 1715790971114,
      "mdate": 1736969104012,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.444992"
    },
    {
      "id": "dz6ex9Ee0Q",
      "title": "Robust Graph Neural Networks via Unbiased Aggregation",
      "abstract": "The adversarial robustness of Graph Neural Networks (GNNs) has been questioned due to the false sense of security uncovered by strong adaptive attacks despite the existence of numerous defenses.\nIn this work, we delve into the robustness analysis of representative robust GNNs and provide a unified robust estimation point of view to\nunderstand their robustness and limitations.\nOur novel analysis of estimation bias motivates the design of a \nrobust and unbiased graph signal estimator. \nWe then develop an efficient Quasi-Newton Iterative Reweighted Least Squares algorithm to solve the estimation problem, which is unfolded as robust unbiased aggregation layers in GNNs with theoretical guarantees.\nOur comprehensive experiments confirm the strong robustness of our proposed model under various scenarios, and the ablation study provides a deep understanding of its advantages.",
      "authors": [
        "Zhichao Hou",
        "Ruiqi Feng",
        "Tyler Derr",
        "Xiaorui Liu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=dz6ex9Ee0Q",
      "cdate": 1715790960086,
      "mdate": 1730873995552,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.444997"
    },
    {
      "id": "ujDKXWTbJX",
      "title": "JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models",
      "abstract": "Mathematical reasoning is an important capability of large language models~(LLMs) for real-world applications.\nTo enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\\eg GPT-4) to synthesize massive math problems. Both types of work generally lead to large costs in training or synthesis.\nTo reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data.\nTo achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM.\nConcretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels.\nBesides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts.\nThe both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM.\nWe leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model. The whole process only needs to invoke GPT-4 API 9.3k times and use 4.6B data for training.\nExperimental results have shown that JiuZhang3.0 achieves state-of-the-art performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings.\nOur code and data will be publicly released in \\url{https://github.com/RUCAIBox/JiuZhang3.0}.",
      "authors": [
        "Kun Zhou",
        "Beichen Zhang",
        "jiapeng wang",
        "Zhipeng Chen",
        "Xin Zhao",
        "Jing Sha",
        "Zhichao Sheng",
        "Shijin Wang",
        "Ji-Rong Wen"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ujDKXWTbJX",
      "cdate": 1715790661372,
      "mdate": 1730873995416,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445002"
    },
    {
      "id": "Vxijl0IOId",
      "title": "Learning Generalized Linear Programming Value Functions",
      "abstract": "We develop a theoretically-grounded learning method for the Generalized Linear Programming Value Function (GVF), which models the optimal value of a linear programming (LP) problem as its objective and constraint bounds vary. This function plays a fundamental role in algorithmic techniques for large-scale optimization, particularly in decomposition for two-stage mixed-integer linear programs (MILPs). This paper establishes a structural characterization of the GVF that enables it to be modeled as a particular neural network architecture, which we then use to learn the GVF in a way that benefits from three notable properties. First, our method produces a true under-approximation of the value function with respect to the constraint bounds. Second, the model is input-convex in the constraint bounds, which not only matches the structure of the GVF but also enables the trained model to be efficiently optimized over using LP. Finally, our learning method is unsupervised, meaning that training data generation does not require computing LP optimal values, which can be prohibitively expensive at large scales.  We numerically show that our method can approximate the GVF well, even when compared to supervised methods that collect training data by solving an LP for each data point. Furthermore, as an application of our framework, we develop a fast heuristic method for large-scale two-stage MILPs with continuous second-stage variables, via a compact reformulation that can be solved faster than the full model linear relaxation at large scales and orders of magnitude faster than the original model.",
      "authors": [
        "Tu Anh-Nguyen",
        "Joey Huchette",
        "Christian Tjandraatmadja"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Vxijl0IOId",
      "cdate": 1715790637182,
      "mdate": 1736903689852,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445007"
    },
    {
      "id": "lPTWdyIY4O",
      "title": "The Selective $G$-Bispectrum and its Inversion: Applications to $G$-Invariant Networks",
      "abstract": "An important problem in signal processing and deep learning is to achieve *invariance* to nuisance factors not relevant for the task. Since many of these factors are describable as the action of a group $G$ (e.g. rotations, translations, scalings), we want methods to be $G$-invariant. The $G$-Bispectrum extracts every characteristic of a given signal up to group action: for example, the shape of an object in an image, but not its orientation. Consequently, the $G$-Bispectrum has been incorporated into deep neural network architectures as a computational primitive for $G$-invariance\\textemdash akin to a pooling mechanism, but with greater selectivity and robustness. However, the computational cost of the $G$-Bispectrum ($\\mathcal{O}(|G|^2)$, with $|G|$ the size of the group) has limited its widespread adoption. Here, we show that the $G$-Bispectrum computation contains redundancies that can be reduced into a *selective $G$-Bispectrum* with $\\mathcal{O}(|G|)$ complexity. We prove desirable mathematical properties of the selective $G$-Bispectrum and demonstrate how its integration in neural networks enhances accuracy and robustness compared to traditional approaches, while enjoying considerable speeds-up compared to the full $G$-Bispectrum.",
      "authors": [
        "Simon Mataigne",
        "Johan Mathe",
        "Sophia Sanborn",
        "Christopher Hillar",
        "Nina Miolane"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lPTWdyIY4O",
      "cdate": 1715790598954,
      "mdate": 1730873995245,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445012"
    },
    {
      "id": "C0EhyoPpTN",
      "title": "Inferring stochastic low-rank recurrent neural networks from neural data",
      "abstract": "A central aim in computational neuroscience is to relate the activity of large populations of neurons to an underlying dynamical system. Models of these neural dynamics should ideally be both interpretable and fit the observed data well. Low-rank recurrent neural networks (RNNs) exhibit such interpretability by having tractable dynamics. However, it is unclear how to best fit low-rank RNNs to data consisting of noisy observations of an underlying stochastic system. Here, we propose to fit stochastic low-rank RNNs with variational sequential Monte Carlo methods. We validate our method on several datasets consisting of both continuous and spiking neural data, where we obtain lower dimensional latent dynamics than current state of the art methods. Additionally, for low-rank models with piecewise linear nonlinearities, we show how to efficiently identify all fixed points in polynomial rather than exponential cost in the number of units, making analysis of the inferred dynamics tractable for large RNNs. Our method both elucidates the dynamical systems underlying experimental recordings and provides a generative model whose trajectories match observed variability.",
      "authors": [
        "Matthijs Pals",
        "A Erdem Sağtekin",
        "Felix C Pei",
        "Manuel Gloeckler",
        "Jakob H. Macke"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=C0EhyoPpTN",
      "cdate": 1715790540796,
      "mdate": 1736955321923,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445018"
    },
    {
      "id": "Lzl8qJYXv5",
      "title": "Estimating the Hallucination Rate of Generative AI",
      "abstract": "This paper presents a method for estimating the hallucination rate for in-context learning (ICL) with generative AI. In ICL, a conditional generative model (CGM) is prompted with a dataset and a prediction question and asked to generate a response. One interpretation of ICL assumes that the CGM computes the posterior predictive of an unknown Bayesian model, which implicitly defines a joint distribution over observable datasets and latent mechanisms. This joint distribution factorizes into two components: the model prior over mechanisms and the model likelihood of datasets given a mechanism. With this perspective, we define a \\textit{hallucination} as a generated response to the prediction question with low model likelihood given the mechanism. We develop a new method that takes an ICL problem and estimates the probability that a CGM will generate a hallucination. Our method only requires generating prediction questions and responses from the CGM and evaluating its response log probability. We empirically evaluate our method using large language models for synthetic regression and natural language ICL tasks.",
      "authors": [
        "Andrew Jesson",
        "Nicolas Beltran-Velez",
        "Quentin Chu",
        "Sweta Karlekar",
        "Jannik Kossen",
        "Yarin Gal",
        "John Patrick Cunningham",
        "David Blei"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Lzl8qJYXv5",
      "cdate": 1715790429321,
      "mdate": 1735259939202,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445023"
    },
    {
      "id": "6HUJoD3wTj",
      "title": "Separations in the Representational Capabilities of Transformers and Recurrent Architectures",
      "abstract": "Transformer architectures have been widely adopted in foundation models. Due to their high inference costs, there is renewed interest in exploring the potential of efficient recurrent architectures (RNNs). In this paper, we analyze the differences in the representational capabilities of Transformers and RNNs across several tasks of practical relevance, including index lookup, nearest neighbor, recognizing bounded Dyck languages, and string equality.  For the tasks considered, our results show separations based on the size of the model required for different architectures. For example, we show that a one-layer Transformer of logarithmic width can perform index lookup, whereas an RNN requires a hidden state of linear size.  Conversely, while constant-size RNNs can recognize bounded Dyck languages, we show that one-layer Transformers require a linear size for this task. Furthermore, we show that two-layer Transformers of logarithmic size can perform decision tasks such as string equality or disjointness, whereas both one-layer Transformers and recurrent models require linear size for these tasks. We also show that a log-size two-layer Transformer can implement the nearest neighbor algorithm in its forward pass; on the other hand recurrent models require linear size. Our constructions are based on the existence of $N$ nearly orthogonal vectors in $O(\\log N)$ dimensional space and our lower bounds are based on reductions from communication complexity problems.  We supplement our theoretical results with experiments that highlight the differences in the performance of these architectures on practical-size sequences.",
      "authors": [
        "Satwik Bhattamishra",
        "Michael Hahn",
        "Phil Blunsom",
        "Varun Kanade"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=6HUJoD3wTj",
      "cdate": 1715790367247,
      "mdate": 1730873995006,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445029"
    },
    {
      "id": "k2hS5Rt1N0",
      "title": "Off-Dynamics Reinforcement Learning via Domain Adaptation and Reward Augmented Imitation",
      "abstract": "Training a policy in a source domain for deployment in the target domain under a dynamics shift can be challenging, often resulting in performance degradation. Previous work tackles this challenge by training on the source domain with modified rewards derived by matching distributions between the source and the target optimal trajectories. However, pure modified rewards only ensure the behavior of the learned policy in the source domain resembles trajectories produced by the target optimal policies, which does not guarantee optimal performance when the learned policy is actually deployed to the target domain. In this work, we propose to utilize imitation learning to transfer the policy learned from the reward modification to the target domain so that the new policy can generate the same trajectories in the target domain. Our approach, Domain Adaptation and Reward Augmented Imitation Learning (DARAIL), utilizes the reward modification for domain adaptation and follows the general framework of generative adversarial imitation learning from observation (GAIfO) by applying a reward augmented estimator for the policy optimization step. Theoretically, we present an error bound for our method under a mild assumption regarding the dynamics shift to justify the motivation of our method. Empirically, our method outperforms the pure modified reward method without imitation learning and also outperforms other baselines in benchmark off-dynamics environments.",
      "authors": [
        "Yihong Guo",
        "Yixuan Wang",
        "Yuanyuan Shi",
        "Pan Xu",
        "Anqi Liu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=k2hS5Rt1N0",
      "cdate": 1715790144391,
      "mdate": 1730873994800,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445034"
    },
    {
      "id": "aBMESB1Ajx",
      "title": "On the Sparsity of the Strong Lottery Ticket Hypothesis",
      "abstract": "Considerable research efforts have recently been made to show that a random neural network $N$ contains subnetworks capable of accurately approximating any given neural network that is sufficiently smaller than $N$, without any training. \nThis line of research, known as the Strong Lottery Ticket Hypothesis (SLTH), was originally motivated by the weaker Lottery Ticket Hypothesis, which states that a sufficiently large random neural network $N$ contains sparse subnetworks that can be trained efficiently to achieve performance comparable to that of training the entire network $N$.\nDespite its original motivation, results on the SLTH have so far not provided any guarantee on the size of subnetworks.\nSuch limitation is due to the nature of the main technical tool leveraged by these results, the Random Subset Sum (RSS) Problem.\nInformally, the RSS Problem asks how large a random i.i.d. sample $\\Omega$ should be so that we are able to approximate any number in $[-1,1]$, up to an error of $ \\epsilon$, as the sum of a suitable subset of $\\Omega$. \n\nWe provide the first proof of the SLTH in classical settings, such as dense and equivariant networks, with guarantees on the sparsity of the subnetworks. Central to our results, is the proof of an essentially tight bound on the Random Fixed-Size Subset Sum Problem (RFSS), a variant of the RSS Problem in which we only ask for subsets of a given size, which is of independent interest.",
      "authors": [
        "Emanuele Natale",
        "Davide Ferre'",
        "Giordano Giambartolomei",
        "Frédéric Giroire",
        "Frederik Mallmann-Trenn"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=aBMESB1Ajx",
      "cdate": 1715790024448,
      "mdate": 1730873994679,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445039"
    },
    {
      "id": "hVmi98a0ki",
      "title": "Optimizing Automatic Differentiation with Deep Reinforcement Learning",
      "abstract": "Computing Jacobians with automatic differentiation is ubiquitous in many scientific domains such as machine learning, computational fluid dynamics, robotics and finance. \nEven small savings in the number of computations or memory usage in Jacobian computations can already incur massive savings in energy consumption and runtime. \nWhile there exist many methods that allow for such savings, they generally trade computational efficiency for approximations of the exact Jacobian.\n\nIn this paper, we present a novel method to optimize the number of necessary multiplications for Jacobian computation by leveraging deep reinforcement learning (RL) and a concept called cross-country elimination while still computing the exact Jacobian. \nCross-country elimination is a framework for automatic differentiation that phrases Jacobian accumulation as ordered elimination of all vertices on the computational graph where every elimination incurs a certain computational cost.\nFinding the optimal elimination order that minimizes the number of necessary multiplications can be seen as a single player game which in our case is played by an RL agent.\nWe demonstrate that this method achieves up to 33% improvements over state-of-the-art methods on several relevant tasks taken from relevant domains.\nFurthermore, we show that these theoretical gains translate into actual runtime improvements by providing a cross-country elimination interpreter in JAX that can execute the obtained elimination orders.",
      "authors": [
        "Jamie Lohoff",
        "Emre Neftci"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hVmi98a0ki",
      "cdate": 1715789961651,
      "mdate": 1730873994652,
      "matched_keywords": [
        "reinforcement learning",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445044"
    },
    {
      "id": "Mrs9a1XQAp",
      "title": "Beyond Slow Signs in High-fidelity Model Extraction",
      "abstract": "Deep neural networks, costly to train and rich in intellectual property value, are\nincreasingly threatened by model extraction attacks that compromise their confiden-\ntiality. Previous attacks have succeeded in reverse-engineering model parameters\nup to a precision of float64 for models trained on random data with at most three\nhidden layers using cryptanalytical techniques. However, the process was identified\nto be very time consuming and not feasible for larger and deeper models trained on\nstandard benchmarks. Our study evaluates the feasibility of parameter extraction\nmethods of Carlini et al. [1] further enhanced by Canales-Martínez et al. [2] for\nmodels trained on standard benchmarks. We introduce a unified codebase that\nintegrates previous methods and reveal that computational tools can significantly\ninfluence performance. We develop further optimisations to the end-to-end attack\nand improve the efficiency of extracting weight signs by up to 14.8 times com-\npared to former methods through the identification of easier and harder to extract\nneurons. Contrary to prior assumptions, we identify extraction of weights, not\nextraction of weight signs, as the critical bottleneck. With our improvements, a\n16,721 parameter model with 2 hidden layers trained on MNIST is extracted within\nonly 98 minutes compared to at least 150 minutes previously. Finally, addressing\nmethodological deficiencies observed in previous studies, we propose new ways of\nrobust benchmarking for future model extraction attacks.",
      "authors": [
        "Hanna Foerster",
        "Robert D. Mullins",
        "Ilia Shumailov",
        "Jamie Hayes"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Mrs9a1XQAp",
      "cdate": 1715789815469,
      "mdate": 1737118745729,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445049"
    },
    {
      "id": "01XV5Za56k",
      "title": "Testing Calibration in Nearly-Linear Time",
      "abstract": "In the recent literature on machine learning and decision making, calibration has emerged as a desirable and widely-studied statistical property of the outputs of binary prediction models. However, the algorithmic aspects of measuring model calibration have remained relatively less well-explored. Motivated by Blasiok et al '23, which proposed a rigorous framework for measuring distances to calibration, we initiate the algorithmic study of calibration through the lens of property testing. We define the problem of calibration testing from samples where given $n$ draws from a distribution $\\mathcal{D}$ on $(\\text{predictions}, \\text{binary outcomes})$, our goal is to distinguish between the cases where $\\mathcal{D}$ is perfectly calibrated or $\\epsilon$-far from calibration. We make the simple observation that the empirical smooth calibration linear program can be reformulated as an instance of minimum-cost flow on a highly-structured graph, and design an exact dynamic programming-based solver for it which runs in time $O(n\\log^2(n))$, and solves the calibration testing problem information-theoretically optimally in the same time. This improves upon state-of-the-art black-box linear program solvers requiring $\\Omega(n^\\omega)$ time, where $\\omega > 2$ is the exponent of matrix multiplication. We also develop algorithms for tolerant variants of our testing problem improving upon black-box linear program solvers, and give sample complexity lower bounds for alternative calibration measures to the one considered in this work. Finally, we present experiments showing the testing problem we define faithfully captures standard notions of calibration, and that our algorithms scale efficiently to accommodate large sample sizes.",
      "authors": [
        "Lunjia Hu",
        "Arun Jambulapati",
        "Kevin Tian",
        "Chutong Yang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=01XV5Za56k",
      "cdate": 1715789735387,
      "mdate": 1730873994563,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445054"
    },
    {
      "id": "LfC5rujSTk",
      "title": "Can LLMs Implicitly Learn Numeric Parameter Constraints in Data Science APIs?",
      "abstract": "Data science (DS) programs, typically built on popular DS libraries (such as PyTorch and NumPy) with thousands of APIs, serve as the cornerstone for various mission-critical domains such as financial systems, autonomous driving software, and coding assistants. Recently, large language models (LLMs) have been widely applied to generate DS programs across diverse scenarios, such as assisting users for DS programming or detecting critical vulnerabilities in DS frameworks. Such applications have all operated under the assumption, that LLMs can implicitly model the numerical parameter constraints in DS library APIs and produce valid code. However, this assumption has not been rigorously studied in the literature. In this paper, we empirically investigate the proficiency of LLMs to handle these implicit numerical constraints when generating DS programs. We studied 28 widely used APIs from PyTorch and NumPy, and scrutinized the LLMs’ generation performance in different levels of granularity: full programs, all parameters, and individual parameters of a single API. We evaluated both state-of-the-art open-source and closed-source models. The results show that LLMs are great at generating simple DS programs, particularly those that follow common patterns seen in training data. However, as we increase the difficulty by providing more complex/unusual inputs, the performance of LLMs drops significantly. We also observe that GPT-4-Turbo can sustain much higher performance overall, but still cannot handle arithmetic API constraints well. In summary, while LLMs exhibit the ability to memorize common patterns of popular DS API usage through massive training, they overall lack genuine comprehension of the underlying numerical constraints.",
      "authors": [
        "Yinlin Deng",
        "Chunqiu Steven Xia",
        "Zhezhen Cao",
        "Meiziniu Li",
        "LINGMING ZHANG"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LfC5rujSTk",
      "cdate": 1715789723480,
      "mdate": 1730873994454,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445059"
    },
    {
      "id": "GkzrVxs9LS",
      "title": "Learning Low-Rank Feature for Thorax Disease Classification",
      "abstract": "Deep neural networks, including Convolutional Neural Networks (CNNs) and Visual Transformers (ViT), have achieved stunning success in the medical image domain. We study thorax disease classification in this paper. Effective extraction of features for the disease areas is crucial for disease classification on radiographic images. While various neural architectures and training techniques, such as self-supervised learning with contrastive/restorative learning, have been employed for disease classification on radiographic images, there are no principled methods that can effectively reduce the adverse effect of noise and background or non-disease areas on the radiographic images for disease classification. To address this challenge, we propose a novel Low-Rank Feature Learning (LRFL) method in this paper, which is universally applicable to the training of all neural networks. The LRFL method is both empirically motivated by a Low Frequency Property (LFP) and theoretically motivated by our sharp generalization bound for neural networks with low-rank features. LFP not only widely exists in deep neural networks for generic machine learning but also exists in all the thorax medical datasets studied in this paper. In the empirical study, using a neural network such as a ViT or a CNN pre-trained on unlabeled chest X-rays by Masked Autoencoders (MAE), our novel LRFL method is applied on the pre-trained neural network and demonstrates better classification results in terms of both multi-class area under the receiver operating curve (mAUC) and classification accuracy than the current state-of-the-art. The code of LRFL is available at \\url{https://github.com/Statistical-Deep-Learning/LRFL}.",
      "authors": [
        "Yancheng Wang",
        "Rajeev Goel",
        "Utkarsh Nath",
        "Alvin C Silva",
        "Teresa Wu",
        "Yingzhen Yang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GkzrVxs9LS",
      "cdate": 1715789657333,
      "mdate": 1734552786285,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445065"
    },
    {
      "id": "eNvVjpx97O",
      "title": "StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses",
      "abstract": "Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues. According to our observation, dialogue contexts are highly structured, and the special token of End-of-Utterance (EoU) in dialogues has the potential to aggregate information. We refer to the EoU tokens as ``conversational attention sinks'' (conv-attn sinks). Accordingly, we introduce StreamingDialogue, which compresses long dialogue history into conv-attn sinks with minimal losses, and thus reduces computational complexity quadratically with the number of sinks (i.e., the number of utterances). Current LLMs already demonstrate the ability to handle long context window, e.g., a window size of 200K or more. To this end, by compressing utterances into EoUs, our method has the potential to handle more than 200K of utterances, resulting in a prolonged dialogue learning. In order to minimize information losses from reconstruction after compression, we design two learning strategies of short-memory reconstruction (SMR) and long-memory reactivation (LMR). Our method outperforms strong baselines in dialogue tasks and achieves a 4 $\\times$ speedup while reducing memory usage by 18 $\\times$ compared to dense attention recomputation.",
      "authors": [
        "Jia-Nan Li",
        "Quan Tu",
        "Cunli Mao",
        "Zhengtao Yu",
        "Ji-Rong Wen",
        "Rui Yan"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=eNvVjpx97O",
      "cdate": 1715789639512,
      "mdate": 1730873994379,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445070"
    },
    {
      "id": "pEhvscmSgG",
      "title": "Constrained Latent Action Policies for Model-Based Offline Reinforcement Learning",
      "abstract": "In offline reinforcement learning, a policy is learned using a static dataset in the absence of costly feedback from the environment. In contrast to the online setting, only using static datasets poses additional challenges, such as policies generating out-of-distribution samples. Model-based offline reinforcement learning methods try to overcome these by learning a model of the underlying dynamics of the environment and using it to guide policy search. It is beneficial but, with limited datasets, errors in the model and the issue of value overestimation among out-of-distribution states can worsen performance. Current model-based methods apply some notion of conservatism to the Bellman update, often implemented using uncertainty estimation derived from model ensembles. In this paper, we propose Constrained Latent Action Policies (C-LAP) which learns a generative model of the joint distribution of observations and actions. We cast policy learning as a constrained objective to always stay within the support of the latent action distribution, and use the generative capabilities of the model to impose an implicit constraint on the generated actions. Thereby eliminating the need to use additional uncertainty penalties on the Bellman update and significantly decreasing the number of gradient steps required to learn a policy. We empirically evaluate C-LAP on the D4RL and V-D4RL benchmark, and show that C-LAP is competitive to state-of-the-art methods, especially outperforming on datasets with visual observations.",
      "authors": [
        "Marvin Alles",
        "Philip Becker-Ehmck",
        "Patrick van der Smagt",
        "Maximilian Karl"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pEhvscmSgG",
      "cdate": 1715789587530,
      "mdate": 1736892126819,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445076"
    },
    {
      "id": "SiALFXa0NN",
      "title": "Provably Safe Neural Network Controllers via Differential Dynamic Logic",
      "abstract": "While neural networks (NNs) have a large potential as autonomous controllers for Cyber-Physical Systems, verifying the safety of neural network based control systems (NNCSs) poses significant challenges for the practical use of NNs— especially when safety is needed for unbounded time horizons. One reason for this is the intractability of analyzing NNs, ODEs and hybrid systems. To this end, we introduce VerSAILLE (Verifiably Safe AI via Logically Linked Envelopes): The first general approach that allows reusing control theory literature for NNCS verification. By joining forces, we can exploit the efficiency of NN verification tools while retaining the rigor of differential dynamic logic (dL). Based on a provably safe control envelope in dL, we derive a specification for the NN which is proven with NN verification tools. We show that a proof of the NN’s adherence to the specification is then mirrored by a dL proof on the infinite-time safety of the NNCS.\n\nThe NN verification properties resulting from hybrid systems typically contain nonlinear arithmetic over formulas with arbitrary logical structure while efficient NN verification tools merely support linear constraints. To overcome this divide, we present Mosaic: An efficient, sound and complete verification approach for polynomial real arithmetic properties on piece-wise linear NNs. Mosaic partitions complex NN verification queries into simple queries and lifts off-the-shelf linear constraint tools to the nonlinear setting in a completeness-preserving manner by combining approximation with exact reasoning for counterexample regions. In our evaluation we demonstrate the versatility of VerSAILLE and Mosaic: We prove infinite-time safety on the classical Vertical Airborne Collision Avoidance NNCS verification benchmark for some scenarios while (exhaustively) enumerating counterexample regions in unsafe scenarios. We also show that our approach significantly outperforms the State-of-the-Art tools in closed-loop NNV",
      "authors": [
        "Samuel Teuber",
        "Stefan Mitsch",
        "Andre Platzer"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=SiALFXa0NN",
      "cdate": 1715789564393,
      "mdate": 1730873994228,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445081"
    },
    {
      "id": "jImXgQEmX3",
      "title": "AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback",
      "abstract": "The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks. We present AMOR, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process. AMOR builds reasoning logic over a finite state machine (FSM)\nthat solves problems through autonomous executions and transitions over disentangled modules. This allows humans to provide direct feedback to the individual modules, and thus naturally forms process supervision. Based on this reasoning and feedback framework, we develop AMOR through two-stage fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with examples automatically constructed from various public datasets, enabling AMOR to generalize across different knowledge environments, while the latter tailors AMOR to specific domains using process feedback. Extensive experiments across multiple domains demonstrate the advantage of AMOR to strong baselines, thanks to its FSM-based reasoning and process feedback mechanism. The code and data are publicly available at\nhttps://github.com/JianGuanTHU/AMOR.",
      "authors": [
        "Jian Guan",
        "Wei Wu",
        "zujie wen",
        "Peng Xu",
        "Hongning Wang",
        "Minlie Huang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jImXgQEmX3",
      "cdate": 1715789520014,
      "mdate": 1730873994091,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445087"
    },
    {
      "id": "HSRs6yyuUK",
      "title": "Preventing Model Collapse in Deep Canonical Correlation Analysis by Noise Regularization",
      "abstract": "Multi-View Representation Learning (MVRL) aims to learn a unified representation of an object from multi-view data.\nDeep Canonical Correlation Analysis (DCCA) and its variants share simple formulations and demonstrate state-of-the-art performance. However, with extensive experiments, we observe the issue of model collapse, i.e., the performance of DCCA-based methods will drop drastically when training proceeds. The model collapse issue could significantly hinder the wide adoption of DCCA-based methods because it is challenging to decide when to early stop. To this end, we develop NR-DCCA, which is equipped with a novel noise regularization approach to prevent model collapse. Theoretical analysis shows that the Correlation Invariant Property is the key to preventing model collapse, and our noise regularization forces the neural network to possess such a property. A framework to construct synthetic data with different common and complementary information is also developed to compare MVRL methods comprehensively. The developed NR-DCCA outperforms baselines stably and consistently in both synthetic and real-world datasets, and the proposed noise regularization approach can also be generalized to other DCCA-based methods such as DGCCA.",
      "authors": [
        "Junlin He",
        "Jinxiao Du",
        "Susu Xu",
        "Wei Ma"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=HSRs6yyuUK",
      "cdate": 1715789411486,
      "mdate": 1737294595254,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445092"
    },
    {
      "id": "Ehsd856Ltb",
      "title": "Revisiting K-mer Profile for Effective and Scalable Genome Representation Learning",
      "abstract": "Obtaining effective representations of DNA sequences is crucial for genome analysis. Metagenomic binning, for instance, relies on genome representations to cluster complex mixtures of DNA fragments from biological samples with the aim of determining their microbial compositions. In this paper, we revisit k-mer-based representations of genomes and provide a theoretical analysis of their use in representation learning. Based on the analysis, we propose a lightweight and scalable model for performing metagenomic binning at the genome read level, relying only on the k-mer compositions of the DNA fragments. We compare the model to recent genome foundation models and demonstrate that while the models are comparable in performance, the proposed model is significantly more effective in terms of scalability, a crucial aspect for performing metagenomic binning of real-world data sets.",
      "authors": [
        "Abdulkadir Celikkanat",
        "Andres R Masegosa",
        "Thomas Dyhre Nielsen"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Ehsd856Ltb",
      "cdate": 1715789257770,
      "mdate": 1730873993894,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445097"
    },
    {
      "id": "WxLVYZbIew",
      "title": "Building on Efficient Foundations: Effective Training of LLMs with Structured Feedforward Layers",
      "abstract": "State-of-the-art results in large language models (LLMs) often rely on scale, which\nbecomes computationally expensive. This has sparked a research agenda to reduce\nthese models’ parameter counts and computational costs without significantly\nimpacting their performance. Our study focuses on transformer-based LLMs,\nspecifically targeting the computationally intensive feedforward networks (FFNs),\nwhich are less studied than attention blocks. We consider three structured linear\nparameterizations of the FFN using efficient low-rank and block-diagonal matrices.\nIn contrast to many previous works that examined these approximations, our study\ni) explores these structures from a training-from-scratch perspective, ii) scales up\nto 1.3B parameters, and iii) is conducted within recent Transformer-based LLMs\nrather than convolutional architectures. We demonstrate that these structures can\nlead to actual computational gains in various scenarios, including online decoding\nwhen using a pre-merge technique. Additionally, we propose a novel training\nregime, called self-guided training, aimed at improving the poor training dynamics\nthat these approximations exhibit when used from initialization. Interestingly,\nthe scaling performance of structured matrices is explored, revealing steeper\ncurves in scaling training FLOPs, along with a favorable scaling trend in the\novertraining regime. Specifically, we show that wide and structured networks\ncan utilize training FLOPs more efficiently, with fewer parameters and lower\nloss than dense models at their optimal trade-off. Our code is available at\nhttps://github.com/CLAIRE-Labo/StructuredFFN/tree/main.",
      "authors": [
        "Xiuying Wei",
        "Skander Moalla",
        "Razvan Pascanu",
        "Caglar Gulcehre"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=WxLVYZbIew",
      "cdate": 1715789211083,
      "mdate": 1730873993868,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445102"
    },
    {
      "id": "oDeqjIM9Sk",
      "title": "Weight decay induces low-rank attention layers",
      "abstract": "The effect of regularizers such as weight decay when training deep neural networks is not well understood. We study the influence of weight decay as well as $L2$-regularization when training neural network models in which parameter matrices interact multiplicatively. This combination is of particular interest as this parametrization is common in attention layers, the workhorse of transformers. Here, key-query, as well as value-projection parameter matrices, are multiplied directly with each other: $W_K^TW_Q$ and $PW_V$. \nWe extend previous results and show on one hand that any local minimum of a $L2$-regularized loss of the form $L(AB^\\top) + \\lambda (\\|A\\|^2 + \\|B\\|^2)$ coincides with a minimum of the nuclear norm-regularized loss $L(AB^\\top) + \\lambda\\|AB^\\top\\|_*$, and on the other hand that the 2 losses become identical exponentially quickly during training. We thus complement existing works linking $L2$-regularization with low-rank regularization, and in particular, explain why such regularization on the matrix product affects early stages of training.\nBased on these theoretical insights, we verify empirically that the key-query and value-projection matrix products $W_K^TW_Q, PW_V$ within attention layers, when optimized with weight decay, as usually done in vision tasks and language modelling, indeed induce a significant reduction in the rank of $W_K^TW_Q$ and $PW_V$, even in fully online training.\nWe find that, in accordance with existing work, inducing low rank in attention matrix products can damage language model performance, and observe advantages when decoupling weight decay in attention layers from the rest of the parameters.",
      "authors": [
        "Seijin Kobayashi",
        "Yassir Akram",
        "Johannes von Oswald"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=oDeqjIM9Sk",
      "cdate": 1715789180249,
      "mdate": 1747068378590,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445107"
    },
    {
      "id": "VFRyS7Wx08",
      "title": "Rethinking Inverse Reinforcement Learning: from Data Alignment to Task Alignment",
      "abstract": "Many imitation learning (IL) algorithms use inverse reinforcement learning (IRL) to infer a reward function that aligns with the demonstration.\nHowever, the inferred reward functions often fail to capture the underlying task objectives.\nIn this paper, we propose a novel framework for IRL-based IL that prioritizes task alignment over conventional data alignment. Our framework is a semi-supervised approach that leverages expert demonstrations as weak supervision to derive a set of candidate reward functions that align with the task rather than only with the data. It then adopts an adversarial mechanism to train a policy with this set of reward functions to gain a collective validation of the policy's ability to accomplish the task. We provide theoretical insights into this framework's ability to mitigate task-reward misalignment and present a practical implementation. Our experimental results show that our framework outperforms conventional IL baselines in complex and transfer learning scenarios.",
      "authors": [
        "Weichao Zhou",
        "Wenchao Li"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=VFRyS7Wx08",
      "cdate": 1715789160417,
      "mdate": 1736991753715,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445112"
    },
    {
      "id": "4fN2REs0Ma",
      "title": "Unveiling Induction Heads: Provable Training Dynamics and Feature Learning in Transformers",
      "abstract": "In-context learning (ICL) is a cornerstone of large language model (LLM) functionality, yet its theoretical foundations remain elusive due to the complexity of transformer architectures. In particular, most existing work only theoretically explains how the attention mechanism facilitates ICL under certain data models. It remains unclear how the other building blocks of the transformer contribute to ICL. To address this question, we study how a two-attention-layer transformer is trained to perform ICL on $n$-gram Markov chain data, where each token in the Markov chain statistically depends on the previous n tokens. \nWe analyze a sophisticated transformer model featuring relative positional embedding, multi-head softmax attention,  and a feed-forward layer with normalization. \nWe prove that the gradient flow with respect to a cross-entropy ICL loss converges to a limiting model that performs a generalized version of the \"induction head\" mechanism with a learned feature, resulting from the congruous contribution of all the building blocks. \nSpecifically, the first attention layer acts as a copier, copying past tokens within a given window to each position, and the feed-forward network with normalization acts as a selector that generates a feature vector by only looking at informationally relevant parents from the window. \nFinally, the second attention layer is a classifier that\ncompares these features with the feature at the output position, and uses the resulting similarity scores to generate the desired output. Our theory is further validated by simulation experiments.",
      "authors": [
        "Siyu Chen",
        "Heejune Sheen",
        "Tianhao Wang",
        "Zhuoran Yang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4fN2REs0Ma",
      "cdate": 1715789146750,
      "mdate": 1730873993757,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445118"
    },
    {
      "id": "iD18l6prA7",
      "title": "$C^2M^3$: Cycle-Consistent Multi-Model Merging",
      "abstract": "In this paper, we present a novel data-free method for merging neural networks in weight space. Our method optimizes for the permutations of network neurons while ensuring global coherence across all layers, and it outperforms recent layer-local approaches in a set of challenging scenarios. We then generalize the formulation to the $N$-models scenario to enforce cycle consistency of the permutations with guarantees, allowing circular compositions of permutations to be computed without accumulating error along the path. \n    We qualitatively and quantitatively motivate the need for such a constraint, showing its benefits when merging homogeneous sets of models in scenarios spanning varying architectures and datasets. We finally show that, when coupled with activation renormalization, the approach yields the best results in the task.",
      "authors": [
        "Donato Crisostomi",
        "Marco Fumero",
        "Daniele Baieri",
        "Florian Bernard",
        "Emanuele Rodolà"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iD18l6prA7",
      "cdate": 1715789092692,
      "mdate": 1730873993679,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445124"
    },
    {
      "id": "npJQ6qS4bg",
      "title": "Understanding and Minimising Outlier Features in Transformer Training",
      "abstract": "Outlier Features (OFs) are neurons whose activation magnitudes significantly exceed the average over a neural network's (NN) width. They are well known to emerge during standard transformer training and have the undesirable effect of hindering quantisation in afflicted models. Despite their practical importance, little is known behind why OFs emerge during training, nor how one can minimise them.\n\nOur work focuses on the above questions, first identifying several quantitative metrics, such as the kurtosis over neuron activation norms, to measure OFs. With these metrics, we study how architectural and optimisation choices influence OFs, and provide practical insights to minimise OFs during training. As highlights, we introduce a novel unnormalised transformer block, the Outlier Protected block, and present a previously unknown benefit of non-diagonal preconditioning optimisers, finding both approaches to significantly reduce OFs and improve quantisation without compromising convergence speed, at scales of up to 7B parameters. Notably, our combination of OP block and non-diagonal preconditioner (SOAP) achieves 14.87 weight-and-activation int8 perplexity (from 14.71 in standard precision), compared to 63.4 int8 perplexity (from 16.00) with a default OF-prone combination of Pre-Norm model and Adam, when quantising OPT-125m models post-training.",
      "authors": [
        "Bobby He",
        "Lorenzo Noci",
        "Daniele Paliotta",
        "Imanol Schlag",
        "Thomas Hofmann"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=npJQ6qS4bg",
      "cdate": 1715788916026,
      "mdate": 1730873993613,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445129"
    },
    {
      "id": "T9PfJViMiJ",
      "title": "HHD-GP: Incorporating Helmholtz-Hodge Decomposition into Gaussian Processes for Learning Dynamical Systems",
      "abstract": "Machine learning models provide alternatives for efficiently recognizing complex patterns from data, but the main concern in applying them to modeling physical systems stems from their physics-agnostic design, leading to learning methods that lack interpretability, robustness, and data efficiency. This paper mitigates this concern by incorporating the Helmholtz-Hodge decomposition into a Gaussian process model, leading to a versatile framework that simultaneously learns the curl-free and divergence-free components of a dynamical system. Learning a predictive model in this form facilitates the exploitation of symmetry priors. In addition to improving predictive power, these priors make the model indentifiable, thus the identified features can be linked to comprehensible scientific properties of the system. We show that compared to baseline models, our model achieves better predictive performance on several benchmark dynamical systems while allowing physically meaningful decomposition of the systems from noisy and sparse data.",
      "authors": [
        "Hao Xu",
        "Jia Pan"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=T9PfJViMiJ",
      "cdate": 1715788856039,
      "mdate": 1730873993479,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445135"
    },
    {
      "id": "XF1jpo5k6l",
      "title": "fMRI predictors based on language models of increasing complexity recover brain left lateralization",
      "abstract": "Over the past decade, studies of naturalistic language processing where participants are scanned while listening to continuous text have flourished. Using word embeddings at first, then large language models, researchers have created encoding models to analyze the brain signals. Presenting these models with the same text as the participants allows to identify brain areas where there is a significant correlation between the functional magnetic resonance imaging (fMRI) time series and the ones predicted by the models' artificial neurons. One intriguing finding from these studies is that they have revealed highly symmetric bilateral activation patterns, somewhat at odds with the well-known left lateralization of language processing. Here, we report analyses of an fMRI dataset where we manipulate the complexity of large language models, testing 28 pretrained models from 8 different families, ranging from 124M to 14.2B parameters. First, we observe that the performance of models in predicting brain responses follows a scaling law, where the fit with brain activity increases linearly with the logarithm of the number of parameters of the model (and its performance on natural language processing tasks). Second, although this effect is present in both hemispheres, it is stronger in the left than in the right hemisphere. Specifically, the left-right difference in brain correlation follows a scaling law with the number of parameters. This finding reconciles computational analyses of brain activity using large language models with the classic observation from aphasic patients showing left hemisphere dominance for language.",
      "authors": [
        "Laurent Bonnasse-Gahot",
        "Christophe Pallier"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=XF1jpo5k6l",
      "cdate": 1715788769979,
      "mdate": 1730873993314,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445139"
    },
    {
      "id": "V6hrg4O9gg",
      "title": "CodeRosetta: Pushing the Boundaries of Unsupervised Code Translation for Parallel Programming",
      "abstract": "Automatic translation of programming languages has garnered renewed interest, driven by recent advancements in large language models (LLMs). Encoder-decoder transformer models, in particular, have shown promise in translating between different programming languages. However, translating between a language and its high-performance computing (HPC) extension remains underexplored due to inherent challenges like complex parallel semantics understanding. In this paper, we introduce CodeRosetta, an encoder-decoder transformer model explicitly designed for translating between programming languages and also their HPC extensions. CodeRosetta is evaluated on C++ to CUDA and Fortran to C++ translation.\nIt employs a customized learning-based framework with tailored pretraining and training objectives that enable it to effectively capture code semantics and parallel structural nuances, allowing for bidirectional code translation. Our results show that CodeRosetta outperforms state-of-the-art baselines in C++ to CUDA translation by 2.9 BLEU and 1.72 CodeBLUE points while improving compilation accuracy by 6.05%. Compared to general closed-source LLMs, our proposed bidirectional learning-based method improves C++ to CUDA translation by 22.08 BLEU and 14.39 CodeBLUE with 2.75% higher compilation accuracy.\nFinally, CodeRosetta exhibits proficiency in Fortran to parallel C++ translation, marking it, to our knowledge, as the first encoder-decoder model for such a complex translation task, improving CodeBLEU at least by 4.63 points compared to closed-source LLMs and Open Code LLM.",
      "authors": [
        "Ali TehraniJamsaz",
        "Arijit Bhattacharjee",
        "Le Chen",
        "Nesreen K. Ahmed",
        "Amir Yazdanbakhsh",
        "Ali Jannesari"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=V6hrg4O9gg",
      "cdate": 1715788761033,
      "mdate": 1730873993279,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445145"
    },
    {
      "id": "ZbjJE6Nq5k",
      "title": "Normalization and effective learning rates in reinforcement learning",
      "abstract": "Normalization layers have recently experienced a renaissance in the deep reinforcement learning and continual learning literature, with several works highlighting diverse benefits such as improving loss landscape conditioning and combatting overestimation bias. However, normalization brings with it a subtle but important side effect: an equivalence between growth in the norm of the network parameters and decay in the effective learning rate. This becomes problematic in continual learning settings, where the resulting learning rate schedule may decay to near zero too quickly relative to the timescale of the learning problem. We propose to make the learning rate schedule explicit with a simple re-parameterization which we call  Normalize-and-Project (NaP), which couples the insertion of normalization layers with weight projection, ensuring that the effective learning rate remains constant throughout training. This technique reveals itself as a powerful analytical tool to better understand learning rate schedules in deep reinforcement learning, and as a means of improving robustness to nonstationarity in synthetic plasticity loss benchmarks along with both the single-task and sequential variants of the Arcade Learning Environment. We also show that our approach can be easily applied to popular architectures such as ResNets and transformers while recovering and in some cases even slightly improving the performance of the base model in common stationary benchmarks.",
      "authors": [
        "Clare Lyle",
        "Zeyu Zheng",
        "Khimya Khetarpal",
        "James Martens",
        "Hado van Hasselt",
        "Razvan Pascanu",
        "Will Dabney"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ZbjJE6Nq5k",
      "cdate": 1715788744265,
      "mdate": 1730873993213,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445150"
    },
    {
      "id": "ofjTu2ktxO",
      "title": "Carrot and Stick: Eliciting Comparison Data and Beyond",
      "abstract": "Comparison data elicited from people are fundamental to many machine learning tasks, including reinforcement learning from human feedback for large language models and estimating ranking models. They are typically subjective and not directly verifiable. How to truthfully elicit such comparison data from rational individuals? We design peer prediction mechanisms for eliciting comparison data using a bonus-penalty payment. Our design leverages on the strong stochastic transitivity for comparison data to create symmetrically strongly truthful mechanisms such that truth-telling 1) forms a strict Bayesian Nash equilibrium, and 2) yields the highest payment among all symmetric equilibria. Each individual only needs to evaluate one pair of items and report her comparison in our mechanism.\n\nWe further extend the bonus-penalty payment concept to eliciting networked data, designing a symmetrically strongly truthful mechanism when agents’ private signals are sampled according to the Ising models. We provide the necessary and sufficient conditions for our bonus-penalty payment to have truth-telling as a strict Bayesian Nash equilibrium. Experiments on two real-world datasets further support our theoretical discoveries.",
      "authors": [
        "Yiling Chen",
        "Shi Feng",
        "Fang-Yi Yu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ofjTu2ktxO",
      "cdate": 1715788739859,
      "mdate": 1731566908860,
      "matched_keywords": [
        "large language model",
        "reinforcement learning",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445155"
    },
    {
      "id": "cr5EQRJlRn",
      "title": "Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for Large Language Models",
      "abstract": "Fine-tuning is a crucial process for adapting large language models (LLMs) to diverse applications. In certain scenarios, such as multi-tenant serving, deploying multiple LLMs becomes necessary to meet complex demands. Recent studies suggest decomposing a fine-tuned LLM into a base model and corresponding delta weights, which are then compressed using low-rank or low-bit approaches to reduce costs. In this work, we observe that existing low-rank and low-bit compression methods can significantly harm the model performance for task-specific fine-tuned LLMs (e.g., WizardMath for math problems). Motivated by the long-tail distribution of singular values in the delta weights, we propose a delta quantization approach using mixed-precision. This method employs higher-bit representation for singular vectors corresponding to larger singular values. We evaluate our approach on various fine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs. Experimental results demonstrate that our approach performs comparably to full fine-tuned LLMs, surpassing both low-rank and low-bit baselines by a considerable margin. Additionally, we show that our method is compatible with various backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its generalizability.",
      "authors": [
        "Bowen Ping",
        "Shuo Wang",
        "Hanqing Wang",
        "Xu Han",
        "Yuzhuang Xu",
        "Yukun Yan",
        "Yun Chen",
        "Baobao Chang",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=cr5EQRJlRn",
      "cdate": 1715788563202,
      "mdate": 1730873992955,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445161"
    },
    {
      "id": "nw9JmfL99s",
      "title": "Nonlinear dynamics of localization in neural receptive fields",
      "abstract": "Localized receptive fields—neurons that are selective for certain contiguous spatiotemporal features of their input—populate early sensory regions of the mammalian brain. Unsupervised learning algorithms that optimize explicit sparsity or independence criteria replicate features of these localized receptive fields, but fail to explain directly how localization arises through learning without efficient coding, as occurs in early layers of deep neural networks and might occur in early sensory regions of biological systems. We consider an alternative model in which localized receptive fields emerge without explicit top-down efficiency constraints—a feed-forward neural network trained on a data model inspired by the structure of natural images. Previous work identified the importance of non-Gaussian statistics to localization in this setting but left open questions about the mechanisms driving dynamical emergence. We address these questions by deriving the effective learning dynamics for a single nonlinear neuron, making precise how higher-order statistical properties of the input data drive emergent localization, and we demonstrate that the predictions of these effective dynamics extend to the many-neuron setting. Our analysis provides an alternative explanation for the ubiquity of localization as resulting from the nonlinear dynamics of learning in neural circuits",
      "authors": [
        "Leon Lufkin",
        "Andrew M Saxe",
        "Erin Grant"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nw9JmfL99s",
      "cdate": 1715788338241,
      "mdate": 1730873992850,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445166"
    },
    {
      "id": "7aFEqIb1dp",
      "title": "Untrained Neural Nets for Snapshot Compressive Imaging: Theory and Algorithms",
      "abstract": "Snapshot compressive imaging (SCI) recovers high-dimensional (3D) data cubes from a single 2D measurement, enabling diverse applications like video and hyperspectral imaging to go beyond standard techniques in terms of acquisition speed and efficiency. In this paper, we focus on SCI recovery algorithms that employ untrained neural networks (UNNs), such as deep image prior (DIP), to model source structure. Such UNN-based methods are appealing as they have the potential of avoiding the computationally intensive retraining required for different source models and different measurement scenarios. We first develop a theoretical framework for characterizing the performance of such UNN-based methods. The theoretical framework, on the one hand, enables us to optimize the parameters of data-modulating masks, and on the other hand, provides a fundamental connection between the number of data frames that can be recovered from a single measurement to the parameters of the untrained NN. We also employ the recently proposed bagged-deep-image-prior (bagged-DIP) idea to develop SCI Bagged Deep Video Prior (SCI-BDVP) algorithms that address the common challenges faced by standard UNN solutions. Our experimental results show that in video SCI our proposed solution achieves state-of-the-art among UNN methods, and in the case of noisy measurements, it even outperforms supervised solutions. Code is publicly available at [https://github.com/Computational-Imaging-RU/SCI-BDVP](https://github.com/Computational-Imaging-RU/SCI-BDVP).",
      "authors": [
        "Mengyu Zhao",
        "Xi Chen",
        "Xin Yuan",
        "Shirin Jalali"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7aFEqIb1dp",
      "cdate": 1715788284897,
      "mdate": 1730873992763,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445172"
    },
    {
      "id": "zXfhHJnMB2",
      "title": "Neural Conditional Probability for Uncertainty Quantification",
      "abstract": "We introduce Neural Conditional Probability (NCP), an operator-theoretic approach to learning conditional distributions with \na focus on statistical inference tasks. NCP can be used to build conditional confidence regions and extract key statistics such as \nconditional quantiles, mean, and covariance. It offers streamlined learning via a single unconditional training phase, allowing \nefficient inference without the need for retraining even when conditioning changes. By leveraging the approximation \ncapabilities of neural networks, NCP efficiently handles a wide variety of complex probability distributions. \nWe provide theoretical guarantees that ensure both optimization consistency and statistical accuracy. \nIn experiments, we show that NCP with a 2-hidden-layer network matches or outperforms leading methods. \nThis demonstrates that a a minimalistic architecture with a theoretically grounded loss can achieve \ncompetitive results,  even in the face of more complex architectures.",
      "authors": [
        "Vladimir R Kostic",
        "gregoire pacreau",
        "Giacomo Turri",
        "Pietro Novelli",
        "Karim Lounici",
        "Massimiliano Pontil"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=zXfhHJnMB2",
      "cdate": 1715788209140,
      "mdate": 1736961258653,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445177"
    },
    {
      "id": "BrvLTxEx08",
      "title": "Learning Equilibria in Adversarial Team Markov Games: A Nonconvex-Hidden-Concave Min-Max Optimization Problem",
      "abstract": "We study the problem of learning a Nash equilibrium (NE) in Markov games which is a cornerstone in multi-agent reinforcement learning (MARL). In particular, we focus on infinite-horizon adversarial team Markov games (ATMGs) in which agents that share a common reward function compete against a single opponent, *the adversary*. These games unify two-player zero-sum Markov games and Markov potential games, resulting in a setting that encompasses both collaboration and competition. Kalogiannis et al. (2023) provided an efficient equilibrium computation algorithm for ATMGs which presumes knowledge of the reward and transition functions and has no sample complexity guarantees. We contribute a learning algorithm that utilizes MARL policy gradient methods with iteration and sample complexity that is polynomial in the approximation error $\\epsilon$ and the natural parameters of the ATMG, resolving the main caveats of the solution by (Kalogiannis et al., 2023). It is worth noting that previously, the existence of learning algorithms for NE was known for Markov two-player zero-sum and potential games but not for ATMGs.\n    \nSeen through the lens of min-max optimization, computing a NE in these games consists a nonconvex--nonconcave saddle-point problem. Min-max optimization has received an extensive study. Nevertheless, the case of nonconvex--nonconcave landscapes remains elusive: in full generality, finding saddle-points is computationally intractable (Daskalakis et al., 2021). We circumvent the aforementioned intractability by developing techniques that exploit the hidden structure of the objective function via a nonconvex--concave reformulation. However, this introduces a challenge of a feasibility set with coupled constraints. We tackle these challenges by establishing novel techniques for optimizing weakly-smooth nonconvex functions, extending the framework of (Devolder et al., 2014).",
      "authors": [
        "Fivos Kalogiannis",
        "Jingming Yan",
        "Ioannis Panageas"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BrvLTxEx08",
      "cdate": 1715788192539,
      "mdate": 1730873992653,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445182"
    },
    {
      "id": "VZQmIoDGBG",
      "title": "SafeWorld: Geo-Diverse Safety Alignment",
      "abstract": "In the rapidly evolving field of Large Language Models (LLMs), ensuring safety is a crucial and widely discussed topic. However, existing works often overlooks the geo-diversity of cultural and legal standards across the world. To reveal the chal5 lenges posed by geo-diverse safety standards, we introduce SafeWorld, a novel benchmark specifically designed to evaluate LLMs’ ability to generate responses that are not only helpful but also culturally sensitive and legally compliant across diverse global contexts. SafeWorld encompasses 2,775 test user queries, each grounded in high-quality, human-verified cultural norms and legal policies from 50 countries and 493 regions/races. On top of it, we propose a multi-dimensional automatic safety evaluation framework that assesses the contextual appropriateness, accuracy, and comprehensiveness of responses. Our evaluations reveal that current LLMs struggle to meet these criteria effectively. To enhance LLMs’ alignment with geo-diverse safety standards, we synthesize helpful preference pairs for Direct Preference Optimization (DPO) alignment. The preference pair construction aims to encourage LLMs to behave appropriately and provide precise references to relevant cultural norms and policies when necessary. Our trained SafeWorldLM outperforms all competing models, including GPT-4o on all the three evaluation dimensions by a large margin. Global human evaluators also note a nearly 20% higher winning rate in helpfulness and harmfulness evaluation.",
      "authors": [
        "Da Yin",
        "Haoyi Qiu",
        "Kung-Hsiang Huang",
        "Kai-Wei Chang",
        "Nanyun Peng"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=VZQmIoDGBG",
      "cdate": 1715788075944,
      "mdate": 1730873992562,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445188"
    },
    {
      "id": "JL2eMCfDW8",
      "title": "Federated Learning over Connected Modes",
      "abstract": "Statistical heterogeneity in federated learning poses two major challenges: slow global training due to conflicting gradient signals, and the need of personalization for local distributions. In this work, we tackle both challenges by leveraging recent advances in \\emph{linear mode connectivity} --- identifying a linearly connected low-loss region in the parameter space of neural networks, which we call solution simplex. We propose federated learning over connected modes (\\textsc{Floco}), where clients are assigned local subregions in this simplex based on their gradient signals, and together learn the shared global solution simplex. This allows personalization of the client models to fit their local distributions within the degrees of freedom in the solution simplex and homogenizes the update signals for the global simplex training.  Our experiments show that \\textsc{Floco} accelerates the global training process, and significantly improves the local accuracy with minimal computational overhead in cross-silo federated learning settings.",
      "authors": [
        "Dennis Grinwald",
        "Philipp Wiesner",
        "Shinichi Nakajima"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=JL2eMCfDW8",
      "cdate": 1715787930543,
      "mdate": 1736168848485,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445192"
    },
    {
      "id": "LLuSjg59an",
      "title": "Where does In-context Learning Happen in Large Language Models?",
      "abstract": "Self-supervised large language models have demonstrated the ability to perform various tasks via in-context learning, but little is known about where the model locates the task with respect to prompt instructions and demonstration examples. In this work, we attempt to characterize the region where large language models transition from recognizing the task to performing the task. Through a series of layer-wise context-masking experiments on GPTNeo2.7B, Bloom3B, Starcoder2-7B, Llama3.1-8B, Llama3.1-8B-Instruct, on Machine Translation and Code generation, we demonstrate evidence of a \"task recognition\" point where the task is encoded into the input representations and attention to context is no longer necessary. Taking advantage of this redundancy results in 45% computational savings when prompting with 5 examples, and task recognition achieved at layer 14 / 32 using an example with Machine Translation. Our findings also have implications for resource and parameter efficient fine-tuning; we observe a correspondence between strong fine-tuning performance of individual LoRA layers and the task recognition layers.",
      "authors": [
        "Suzanna Sia",
        "David Mueller",
        "Kevin Duh"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LLuSjg59an",
      "cdate": 1715787799207,
      "mdate": 1737026593423,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445198"
    },
    {
      "id": "Wc0vlQuoLb",
      "title": "I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token",
      "abstract": "Large Language Models are known to capture real-world knowledge, allowing them to excel in many downstream tasks. Despite recent advances, these models are still prone to what are commonly known as hallucinations, causing them to emit unwanted and factually incorrect text. In this work, we propose a novel calibration method that can be used to combat hallucinations. \nWe add a special [IDK] (“I Don't Know”) token to the model's vocabulary and introduce an objective function that shifts probability mass to the [IDK] token for incorrect predictions. \nThis approach allows the model to express uncertainty in its output explicitly. \nWe evaluate our proposed method across multiple model architectures and factual downstream tasks.\nWe find that models trained with our method are able to express uncertainty in places where they would previously make mistakes while suffering only a small loss of encoded knowledge. We further perform extensive ablation studies of multiple variations of our approach and provide a detailed analysis of the precision-recall tradeoff of our method.",
      "authors": [
        "Roi Cohen",
        "Konstantin Dobler",
        "Eden Biran",
        "Gerard de Melo"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Wc0vlQuoLb",
      "cdate": 1715787786179,
      "mdate": 1737024389627,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445203"
    },
    {
      "id": "PsPR4NOiRC",
      "title": "Generative Hierarchical Materials Search",
      "abstract": "Generative models trained at scale can now produce novel text, video, and more recently, scientific data such as crystal structures. The ultimate goal for materials discovery, however, goes beyond generation: we desire a fully automated system that proposes, generates, and verifies crystal structures given a high-level user instruction. In this work, we formulate end-to-end language-to-structure generation as a multi-objective optimization problem, and propose Generative Hierarchical Materials Search (GenMS) for controllable generation of crystal structures. GenMS consists of (1) a language model that takes high-level natural language as input and generates intermediate textual information about a crystal (e.g., chemical formulae), and (2) a diffusion model that takes intermediate information as input and generates low-level continuous value crystal structures. GenMS additionally uses a graph neural network to predict properties (e.g., formation energy) from the generated crystal structures. During inference, GenMS leverages all three components to conduct a forward tree search over the space of possible structures. Experiments show that GenMS outperforms other alternatives both in satisfying user request and in generating low-energy structures. GenMS is able to generate complex structures such as double perovskites (or elpasolites), layered structures, and spinels, solely from natural language input.",
      "authors": [
        "Sherry Yang",
        "Simon Batzner",
        "Ruiqi Gao",
        "Muratahan Aykol",
        "Alexander L Gaunt",
        "Brendan McMorrow",
        "Danilo Jimenez Rezende",
        "Dale Schuurmans",
        "Igor Mordatch",
        "Ekin Dogus Cubuk"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=PsPR4NOiRC",
      "cdate": 1715787743021,
      "mdate": 1731729499533,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445209"
    },
    {
      "id": "pGR5X4e1gy",
      "title": "Learning on Large Graphs using Intersecting Communities",
      "abstract": "Message Passing Neural Networks (MPNNs) are a staple of graph machine learning. MPNNs iteratively update each node’s representation in an input graph by aggregating messages from the node’s neighbors, which necessitates a memory complexity of the order of the __number of graph edges__. This complexity might quickly become  prohibitive for large graphs provided they are not very sparse.  In this paper, we propose a novel approach to alleviate this problem by  approximating the input graph as an  intersecting community graph (ICG) -- a combination of intersecting cliques. The key insight is that the number of communities required to approximate a graph  __does not depend on the graph size__. We develop a new constructive version of the Weak Graph Regularity Lemma to efficiently construct an approximating ICG for any input graph. We then devise an efficient graph learning algorithm operating directly on ICG in linear memory and time with respect to the __number of nodes__ (rather than edges).  This offers a new and fundamentally different pipeline for learning on very large non-sparse graphs, whose applicability is demonstrated empirically on node classification tasks and spatio-temporal data processing.",
      "authors": [
        "Ben Finkelshtein",
        "Ismail Ilkan Ceylan",
        "Michael M. Bronstein",
        "Ron Levie"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pGR5X4e1gy",
      "cdate": 1715787735056,
      "mdate": 1734866551851,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445215"
    },
    {
      "id": "poE54GOq2l",
      "title": "SnapKV: LLM Knows What You are Looking for Before Generation",
      "abstract": "Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications.\n\nWe discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an `observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications.",
      "authors": [
        "Yuhong Li",
        "Yingbing Huang",
        "Bowen Yang",
        "Bharat Venkitesh",
        "Acyr Locatelli",
        "Hanchen Ye",
        "Tianle Cai",
        "Patrick Lewis",
        "Deming Chen"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=poE54GOq2l",
      "cdate": 1715787489400,
      "mdate": 1730873991906,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445220"
    },
    {
      "id": "8rcFOqEud5",
      "title": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search",
      "abstract": "Recent methodologies in LLM self-training mostly rely on LLM generating responses and filtering those with correct output answers as training data. This approach often yields a low-quality fine-tuning training set (e.g., incorrect plans or intermediate reasoning). In this paper, we develop a reinforced self-training approach, called ReST-MCTS*, based on integrating process reward guidance with tree search MCTS* for collecting higher-quality reasoning traces as well as per-step value to train policy and reward models. ReST-MCTS* circumvents the per-step manual annotation typically used to train process rewards by tree-search-based reinforcement learning: Given oracle final correct answers, ReST-MCTS* is able to infer the correct process rewards by estimating the probability this step can help lead to the correct answer. These inferred rewards serve dual purposes: they act as value targets for further refining the process reward model and also facilitate the selection of high-quality traces for policy model self-training. We first show that the tree-search policy in ReST-MCTS* achieves higher accuracy compared with prior LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same search budget. We then show that by using traces searched by this tree-search policy as training data, we can continuously enhance the three language models for multiple iterations, and outperform other self-training algorithms such as ReST$^\\text{EM}$ and Self-Rewarding LM.",
      "authors": [
        "Dan Zhang",
        "Sining Zhoubian",
        "Ziniu Hu",
        "Yisong Yue",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=8rcFOqEud5",
      "cdate": 1715787396544,
      "mdate": 1737023726531,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445225"
    },
    {
      "id": "ZxtaNh5UYB",
      "title": "Learn more, but bother less: parameter efficient continual learning",
      "abstract": "Large Language Models (LLMs) have demonstrated profound capabilities due to their extensive pre-training on diverse corpora. However, LLMs often struggle with catastrophic forgetting when engaged in sequential task learning. In this paper, we propose a novel parameter-efficient approach for continual learning in LLMs, which empirically investigates knowledge transfer from previously learned tasks to new tasks through low-rank matrix parameters, enhancing the learning of new tasks without significant interference. Our method employs sensitivity-based analysis of low-rank matrix parameters to identify knowledge-specific parameters between sequential tasks, which are used to initialize the low-rank matrix parameters in new tasks. To maintain orthogonality and minimize forgetting, we further involve the gradient projection technique that keeps the low-rank subspaces of each new task orthogonal to those of previous tasks. Our experimental results on continual learning benchmarks validate the efficacy of our proposed method, which outperforms existing state-of-the-art methods in reducing forgetting, enhancing task performance, and preserving the model's ability to generalize to unseen tasks.",
      "authors": [
        "Fuli Qiao",
        "Mehrdad Mahdavi"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ZxtaNh5UYB",
      "cdate": 1715787234444,
      "mdate": 1730873991651,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445231"
    },
    {
      "id": "IxEhb4NCvy",
      "title": "SSDM: Scalable Speech Dysfluency Modeling",
      "abstract": "Speech dysfluency modeling is the core module for spoken language learning, and speech therapy. However, there are three challenges. First, current state-of-the-art solutions~~\\cite{lian2023unconstrained-udm, lian-anumanchipalli-2024-towards-hudm} suffer from poor scalability. Second, there is a lack of a large-scale dysfluency corpus. Third, there is not an effective learning framework. In this paper, we propose \\textit{SSDM: Scalable Speech Dysfluency Modeling}, which (1) adopts articulatory gestures as scalable forced alignment; (2) introduces connectionist subsequence aligner (CSA) to achieve dysfluency alignment; (3) introduces a large-scale simulated dysfluency corpus called Libri-Dys; and (4) develops an end-to-end system by leveraging the power of large language models (LLMs). We expect SSDM to serve as a standard in the area of dysfluency modeling. Demo is available at \\url{https://berkeley-speech-group.github.io/SSDM/}.",
      "authors": [
        "Jiachen Lian",
        "Xuanru Zhou",
        "Zoe Ezzes",
        "Jet M.J. Vonk",
        "Brittany T. Morin",
        "David Paul Galang Baquirin",
        "Zachary A. Miller",
        "Maria Luisa Gorno-Tempini",
        "Gopala Anumanchipalli"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=IxEhb4NCvy",
      "cdate": 1715787113034,
      "mdate": 1730873991476,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445237"
    },
    {
      "id": "DX5GUwMFFb",
      "title": "Deep Policy Gradient Methods Without Batch Updates, Target Networks, or Replay Buffers",
      "abstract": "Modern deep policy gradient methods achieve effective performance on simulated robotic tasks, but they all require large replay buffers or expensive batch updates, or both, making them incompatible for real systems with resource-limited computers. We show that these methods fail catastrophically when limited to small replay buffers or during *incremental learning*, where updates only use the most recent sample without batch updates or a replay buffer. We propose a novel incremental deep policy gradient method --- *Action Value Gradient (AVG)* and a set of normalization and scaling techniques to address the challenges of instability in incremental learning. On robotic simulation benchmarks, we show that AVG is the only incremental method that learns effectively, often achieving final performance comparable to batch policy gradient methods. This advancement enabled us to show for the first time effective deep reinforcement learning with real robots using only incremental updates, employing a robotic manipulator and a mobile robot.",
      "authors": [
        "Gautham Vasan",
        "Mohamed Elsayed",
        "Seyed Alireza Azimi",
        "Jiamin He",
        "Fahim Shahriar",
        "Colin Bellinger",
        "Martha White",
        "A. Rupam Mahmood"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DX5GUwMFFb",
      "cdate": 1715787044844,
      "mdate": 1730873991474,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445242"
    },
    {
      "id": "aVK4JFpegy",
      "title": "Evaluating the World Model Implicit in a Generative Model",
      "abstract": "Recent work suggests that large language models may implicitly learn world models. How should we assess this possibility? We formalize this question for the case where the underlying reality is governed by a deterministic finite automaton. This includes problems as diverse as simple logical reasoning, geographic navigation, game-playing, and chemistry. We propose new evaluation metrics for world model recovery inspired by the classic Myhill-Nerode theorem from language theory. We illustrate their utility in three domains: game playing, logic puzzles, and navigation. In all domains, the generative models we consider do well on existing diagnostics for assessing world models, but our evaluation metrics reveal their world models to be far less coherent than they appear. Such incoherence creates fragility: using a generative model to solve related but subtly different tasks can lead to failures. Building generative models that meaningfully capture the underlying logic of the domains they model would be immensely valuable; our results suggest new ways to assess how close a given model is to that goal.",
      "authors": [
        "Keyon Vafa",
        "Justin Y. Chen",
        "Ashesh Rambachan",
        "Jon Kleinberg",
        "Sendhil Mullainathan"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=aVK4JFpegy",
      "cdate": 1715786828757,
      "mdate": 1734562796876,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445248"
    },
    {
      "id": "sQApQMBqiP",
      "title": "Learning Human-like Representations to Enable Learning Human Values",
      "abstract": "How can we build AI systems that can learn any set of individual human values both quickly and safely, avoiding causing harm or violating societal standards for acceptable behavior during the learning process? We explore the effects of representational alignment between humans and AI agents on learning human values. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We demonstrate that this kind of representational alignment can also support safely learning and exploring human values in the context of personalization. We begin with a theoretical prediction, show that it applies to learning human morality judgments, then show that our results generalize to ten different aspects of human values -- including ethics, honesty, and fairness -- training AI agents on each set of values in a multi-armed bandit setting, where rewards reflect human value judgments over the chosen action. Using a set of textual action descriptions, we collect value judgments from humans, as well as similarity judgments from both humans and multiple language models, and demonstrate that representational alignment enables both safe exploration and improved generalization when learning human values.",
      "authors": [
        "Andrea Wynn",
        "Ilia Sucholutsky",
        "Thomas L. Griffiths"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=sQApQMBqiP",
      "cdate": 1715786718653,
      "mdate": 1730873991325,
      "matched_keywords": [
        "ai agent"
      ],
      "fetched_at": "2025-08-10T23:47:09.445254"
    },
    {
      "id": "Nmmiyjw7Xg",
      "title": "Safe and Sparse Newton Method for Entropic-Regularized Optimal Transport",
      "abstract": "Computational optimal transport (OT) has received massive interests in the machine learning community, and great advances have been gained in the direction of entropic-regularized OT. The Sinkhorn algorithm, as well as its many improved versions, has become the *de facto* solution to large-scale OT problems. However, most of the existing methods behave like first-order methods, which typically require a large number of iterations to converge. More recently, Newton-type methods using sparsified Hessian matrices have demonstrated promising results on OT computation, but there still remain a lot of unresolved open questions. In this article, we make major new progresses towards this direction: first, we propose a novel Hessian sparsification scheme that promises a strict control of the approximation error; second, based on this sparsification scheme, we develop a *safe* Newton-type method that is guaranteed to avoid singularity in computing the search directions; third, the developed algorithm has a clear implementation for practical use, avoiding most hyperparameter tuning; and remarkably, we provide rigorous global and local convergence analysis of the proposed algorithm, which is lacking in the prior literature. Various numerical experiments are conducted to demonstrate the effectiveness of the proposed algorithm in solving large-scale OT problems.",
      "authors": [
        "Zihao Tang",
        "Yixuan Qiu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Nmmiyjw7Xg",
      "cdate": 1715786410273,
      "mdate": 1737006965349,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445259"
    },
    {
      "id": "J0Itri0UiN",
      "title": "Counterfactual Fairness by Combining Factual and Counterfactual Predictions",
      "abstract": "In high-stakes domains such as healthcare and hiring, the role of machine learning (ML) in decision-making raises significant fairness concerns. \nThis work focuses on Counterfactual Fairness (CF), which posits that an ML model's outcome on any individual should remain unchanged if they had belonged to a different demographic group.\nPrevious works have proposed methods that guarantee CF. \nNotwithstanding, their effects on the model's predictive performance remain largely unclear.\nTo fill this gap, we provide a theoretical study on the inherent trade-off between CF and predictive performance in a model-agnostic manner. \nWe first propose a simple but effective method to cast an optimal but potentially unfair predictor into a fair one with a minimal loss of performance.\nBy analyzing the excess risk incurred by perfect CF, we quantify this inherent trade-off. \nFurther analysis on our method's performance with access to only incomplete causal knowledge is also conducted. \nBuilt upon this, we propose a practical algorithm that can be applied in such scenarios. \nExperiments on both synthetic and semi-synthetic datasets demonstrate the validity of our analysis and methods.",
      "authors": [
        "Zeyu Zhou",
        "Tianci Liu",
        "Ruqi Bai",
        "Jing Gao",
        "Murat Kocaoglu",
        "David I. Inouye"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=J0Itri0UiN",
      "cdate": 1715786336047,
      "mdate": 1737016367816,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445265"
    },
    {
      "id": "9c3IiAWeiN",
      "title": "IPM-LSTM: A Learning-Based Interior Point Method for Solving Nonlinear Programs",
      "abstract": "Solving constrained nonlinear programs (NLPs) is of great importance in various domains such as power systems, robotics, and wireless communication networks. One widely used approach for addressing NLPs is the interior point method (IPM). The most computationally expensive procedure in IPMs is to solve systems of linear equations via matrix factorization. Recently, machine learning techniques have been adopted to expedite classic optimization algorithms. In this work, we propose using Long Short-Term Memory (LSTM) neural networks to approximate the solution of linear systems and integrate this approximating step into an IPM. The resulting approximate NLP solution is then utilized to warm-start an interior point solver. Experiments on various types of NLPs, including Quadratic Programs and Quadratically Constrained Quadratic Programs, show that our approach can significantly accelerate NLP solving, reducing iterations by up to 60% and solution time by up to 70% compared to the default solver.",
      "authors": [
        "Xi Gao",
        "Jinxin Xiong",
        "Akang Wang",
        "Qihong Duan",
        "Jiang Xue",
        "Qingjiang Shi"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9c3IiAWeiN",
      "cdate": 1715786294563,
      "mdate": 1730873991086,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445270"
    },
    {
      "id": "93HCE8vTye",
      "title": "Transformers need glasses! Information over-squashing in language tasks",
      "abstract": "We study how information propagates in decoder-only Transformers, which are the architectural foundation of most existing frontier large language models (LLMs). We rely on a theoretical signal propagation analysis---specifically, we analyse the representations of the last token in the final layer of the Transformer, as this is the representation used for next-token prediction. Our analysis reveals a representational collapse phenomenon: we prove that certain distinct pairs of inputs to the Transformer can yield arbitrarily close representations in the final token. This effect is exacerbated by the low-precision floating-point formats frequently used in modern LLMs. As a result, the model is provably unable to respond to these sequences in different ways---leading to errors in, e.g., tasks involving counting or copying. Further, we show that decoder-only Transformer language models can lose sensitivity to specific tokens in the input, which relates to the well-known phenomenon of over-squashing in graph neural networks. We provide empirical evidence supporting our claims on contemporary LLMs. Our theory points to simple solutions towards ameliorating these issues.",
      "authors": [
        "Federico Barbero",
        "Andrea Banino",
        "Steven Kapturowski",
        "Dharshan Kumaran",
        "João Guilherme Madeira Araújo",
        "Alex Vitvitskyi",
        "Razvan Pascanu",
        "Petar Veličković"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=93HCE8vTye",
      "cdate": 1715786265825,
      "mdate": 1730873990985,
      "matched_keywords": [
        "large language model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445275"
    },
    {
      "id": "848vuK2cKp",
      "title": "Offline Oracle-Efficient Learning for Contextual MDPs via Layerwise Exploration-Exploitation Tradeoff",
      "abstract": "Motivated by the recent discovery of a statistical and computational reduction from contextual bandits to offline regression \\citep{simchi2020bypassing}, we address the general (stochastic) Contextual Markov Decision Process (CMDP) problem with horizon $H$ (as known as CMDP with $H$ layers). In this paper, we introduce a reduction from CMDPs to offline density estimation under the realizability assumption, i.e., a model class $\\mathcal{M}$ containing the true underlying CMDP is provided in advance. We develop an efficient, statistically near-optimal algorithm requiring only $O(H \\log T)$ calls to an offline density estimation algorithm (or oracle) across all $T$ rounds. This number can be further reduced to $O(H \\log \\log T)$ if $T$ is known in advance. Our results mark the first efficient and near-optimal reduction from CMDPs to offline density estimation without imposing any structural assumptions on the model class. A notable feature of our algorithm is the design of a layerwise exploration-exploitation tradeoff tailored to address the layerwise structure of CMDPs. Additionally, our algorithm is versatile and applicable to pure exploration tasks in reward-free reinforcement learning.",
      "authors": [
        "Jian Qian",
        "Haichen Hu",
        "David Simchi-Levi"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=848vuK2cKp",
      "cdate": 1715786045970,
      "mdate": 1734728164102,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445281"
    },
    {
      "id": "hw76X5uWrc",
      "title": "Unlocking the Potential of Global Human Expertise",
      "abstract": "Solving societal problems on a global scale requires the collection and processing of ideas and methods from diverse sets of international experts. As the number and diversity of human experts increase, so does the likelihood that elements in this collective knowledge can be combined and refined to discover novel and better solutions. However, it is difficult to identify, combine, and refine complementary information in an increasingly large and diverse knowledge base. This paper argues that artificial intelligence (AI) can play a crucial role in this process. An evolutionary AI framework, termed RHEA, fills this role by distilling knowledge from diverse models created by human experts into equivalent neural networks, which are then recombined and refined in a population-based search. The framework was implemented in a formal synthetic domain, demonstrating that it is transparent and systematic. It was then applied to the results of the XPRIZE Pandemic Response Challenge, in which over 100 teams of experts across 23 countries submitted models based on diverse methodologies to predict COVID-19 cases and suggest non-pharmaceutical intervention policies for 235 nations, states, and regions across the globe. Building upon this expert knowledge, by recombining and refining the 169 resulting policy suggestion models, RHEA discovered a broader and more effective set of policies than either AI or human experts alone, as evaluated based on real-world data. The results thus suggest that AI can play a crucial role in realizing the potential of human expertise in global problem-solving.",
      "authors": [
        "Elliot Meyerson",
        "Olivier Francon",
        "Darren Sargent",
        "Babak Hodjat",
        "Risto Miikkulainen"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hw76X5uWrc",
      "cdate": 1715786017018,
      "mdate": 1730873990842,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445286"
    },
    {
      "id": "fpxRpPbF1t",
      "title": "Differentiable Modal Synthesis for Physical Modeling of Planar String Sound and Motion Simulation",
      "abstract": "While significant advancements have been made in music generation and differentiable sound synthesis within machine learning and computer audition, the simulation of instrument vibration guided by physical laws has been underexplored. To address this gap, we introduce a novel model for simulating the spatio-temporal motion of nonlinear strings, integrating modal synthesis and spectral modeling within a neural network framework. Our model leverages mechanical properties and fundamental frequencies as inputs, outputting string states across time and space that solve the partial differential equation characterizing the nonlinear string. Empirical evaluations demonstrate that the proposed architecture achieves superior accuracy in string motion simulation compared to existing baseline architectures. The code and demo are available online.",
      "authors": [
        "Jin Woo Lee",
        "Jaehyun Park",
        "Min Jun Choi",
        "Kyogu Lee"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fpxRpPbF1t",
      "cdate": 1715785983496,
      "mdate": 1730873990725,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445291"
    },
    {
      "id": "A969ouPqEs",
      "title": "DiffLight: A Partial Rewards Conditioned Diffusion Model for Traffic Signal Control with Missing Data",
      "abstract": "The application of reinforcement learning in traffic signal control (TSC) has been extensively researched and yielded notable achievements. However, most existing works for TSC assume that traffic data from all surrounding intersections is fully and continuously available through sensors. In real-world applications, this assumption often fails due to sensor malfunctions or data loss, making TSC with missing data a critical challenge. To meet the needs of practical applications, we introduce DiffLight, a novel conditional diffusion model for TSC under data-missing scenarios in the offline setting. Specifically, we integrate two essential sub-tasks, i.e., traffic data imputation and decision-making, by leveraging a Partial Rewards Conditioned Diffusion (PRCD) model to prevent missing rewards from interfering with the learning process. Meanwhile, to effectively capture the spatial-temporal dependencies among intersections, we design a Spatial-Temporal transFormer (STFormer) architecture. In addition, we propose a Diffusion Communication Mechanism (DCM) to promote better communication and control performance under data-missing scenarios. Extensive experiments on five datasets with various data-missing scenarios demonstrate that DiffLight is an effective controller to address TSC with missing data. The code of DiffLight is released at https://github.com/lokol5579/DiffLight-release.",
      "authors": [
        "Hanyang Chen",
        "Yang Jiang",
        "Shengnan Guo",
        "Xiaowei Mao",
        "Youfang Lin",
        "Huaiyu Wan"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=A969ouPqEs",
      "cdate": 1715785968496,
      "mdate": 1730873990655,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445296"
    },
    {
      "id": "sOhFyFFnxT",
      "title": "Exploring the Precise Dynamics of Single-Layer GAN Models: Leveraging Multi-Feature Discriminators for High-Dimensional Subspace Learning",
      "abstract": "Subspace learning is a critical endeavor in contemporary machine learning, particularly given the vast dimensions of modern datasets. In this study, we delve into the training dynamics of a single-layer GAN model from the perspective of subspace learning, framing these GANs as a novel approach to this fundamental task. Through a rigorous scaling limit analysis, we offer insights into the behavior of this model. Extending beyond prior research that primarily focused on sequential feature learning, we investigate the non-sequential scenario, emphasizing the pivotal role of inter-feature interactions in expediting training and enhancing performance, particularly with an uninformed initialization strategy. Our investigation encompasses both synthetic and real-world datasets, such as MNIST and Olivetti Faces, demonstrating the robustness and applicability of our findings to practical scenarios. By bridging our analysis to the realm of subspace learning, we systematically compare the efficacy of GAN-based methods against conventional approaches, both theoretically and empirically. Notably, our results unveil that while all methodologies successfully capture the underlying subspace, GANs exhibit a remarkable capability to acquire a more informative basis, owing to their intrinsic ability to generate new data samples. This elucidates the unique advantage of GAN-based approaches in subspace learning tasks.",
      "authors": [
        "Andrew Bond",
        "Zafer Dogan"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=sOhFyFFnxT",
      "cdate": 1715785959147,
      "mdate": 1737382300266,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445301"
    },
    {
      "id": "nK6OnCpd3n",
      "title": "Text-Aware Diffusion for Policy Learning",
      "abstract": "Training an agent to achieve particular goals or perform desired behaviors is often accomplished through reinforcement learning, especially in the absence of expert demonstrations.  However, supporting novel goals or behaviors through reinforcement learning requires the ad-hoc design of appropriate reward functions, which quickly becomes intractable. To address this challenge, we propose Text-Aware Diffusion for Policy Learning (TADPoLe), which uses a pretrained, frozen text-conditioned diffusion model to compute dense zero-shot reward signals for text-aligned policy learning.  We hypothesize that large-scale pretrained generative models encode rich priors that can supervise a policy to behave not only in a text-aligned manner, but also in alignment with a notion of naturalness summarized from internet-scale training data.  In our experiments, we demonstrate that TADPoLe is able to learn policies for novel goal-achievement and continuous locomotion behaviors specified by natural language, in both Humanoid and Dog environments. The behaviors are learned zero-shot without ground-truth rewards or expert demonstrations, and are qualitatively more natural according to human evaluation. We further show that TADPoLe performs competitively when applied to robotic manipulation tasks in the Meta-World environment, without having access to any in-domain demonstrations.",
      "authors": [
        "Calvin Luo",
        "Mandy He",
        "Zilai Zeng",
        "Chen Sun"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nK6OnCpd3n",
      "cdate": 1715785955943,
      "mdate": 1730873990569,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445307"
    },
    {
      "id": "9VbGjXLzig",
      "title": "No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance",
      "abstract": "Web-crawled pretraining datasets underlie the impressive \"zero-shot\" evaluation performance of multimodal models, such as CLIP for classification and Stable-Diffusion for image generation. However, it is unclear how meaningful the notion of \"zero-shot\" generalization is for such multimodal models, as it is not known to what extent their pretraining datasets encompass the downstream concepts targeted for during \"zero-shot\" evaluation. In this work, we ask: How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets?\n\nWe comprehensively investigate this question across 34 models and 5 standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics), generating over 300GB of data artifacts. We consistently find that, far from exhibiting \"zero-shot\" generalization, multimodal models require exponentially more data to achieve linear improvements in downstream \"zero-shot\" performance, following a sample inefficient log-linear scaling trend. This trend persists even when controlling for sample-level similarity between pretraining and downstream datasets, and testing on purely synthetic data distributions. Furthermore, upon benchmarking models on long-tailed data sampled based on our analysis, we demonstrate that multimodal models across the board perform poorly. We contribute this long-tail test set as the Let it Wag! benchmark to further research in this direction. Taken together, our study reveals an exponential need for training data which implies that the key to \"zero-shot\" generalization capabilities under large-scale training data and compute paradigms remains to be found.",
      "authors": [
        "Vishaal Udandarao",
        "Ameya Prabhu",
        "Adhiraj Ghosh",
        "Yash Sharma",
        "Philip Torr",
        "Adel Bibi",
        "Samuel Albanie",
        "Matthias Bethge"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9VbGjXLzig",
      "cdate": 1715785924320,
      "mdate": 1730873990407,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.445312"
    },
    {
      "id": "NCX3Kgb1nh",
      "title": "Multivariate Stochastic Dominance via Optimal Transport and Applications to Models Benchmarking",
      "abstract": "Stochastic dominance is an important concept in probability theory, econometrics and social choice theory for robustly modeling agents' preferences between random outcomes.  While many works have been dedicated to the univariate case,\nlittle has been done in the multivariate scenario, wherein an agent has to decide between different multivariate outcomes. By exploiting a  characterization  of  multivariate first stochastic dominance in terms of couplings, we  introduce  a statistic that assesses multivariate almost stochastic dominance under the framework of Optimal Transport  with a smooth cost. Further, we introduce an entropic regularization of this statistic, and establish a central limit theorem (CLT) and consistency of the bootstrap procedure for the empirical statistic. Armed with this CLT, we propose a hypothesis testing framework as well as an efficient implementation using the Sinkhorn algorithm. We showcase our  method in comparing and benchmarking Large Language Models that are evaluated on multiple metrics. Our multivariate stochastic dominance test allows us to capture the dependencies between the metrics in order to make an informed and statistically significant  decision on the relative performance of the models.",
      "authors": [
        "Gabriel Rioux",
        "Apoorva Nitsure",
        "Mattia Rigotti",
        "Kristjan Greenewald",
        "Youssef Mroueh"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NCX3Kgb1nh",
      "cdate": 1715785882184,
      "mdate": 1736852035216,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445317"
    },
    {
      "id": "OSHaRf4TVU",
      "title": "AMAGO-2: Breaking the Multi-Task Barrier in Meta-Reinforcement Learning with Transformers",
      "abstract": "Language models trained on diverse datasets unlock generalization by in-context learning. Reinforcement Learning (RL) policies can achieve a similar effect by meta-learning within the memory of a sequence model. However, meta-RL research primarily focuses on adapting to minor variations of a single task. It is difficult to scale towards more general behavior without confronting challenges in multi-task optimization, and few solutions are compatible with meta-RL's goal of learning from large training sets of unlabeled tasks. To address this challenge, we revisit the idea that multi-task RL is bottlenecked by imbalanced training losses created by uneven return scales across different tasks. We build upon recent advancements in Transformer-based (in-context) meta-RL and evaluate a simple yet scalable solution where both an agent's actor and critic objectives are converted to classification terms that decouple optimization from the current scale of returns. Large-scale comparisons in Meta-World ML45, Multi-Game Procgen, Multi-Task POPGym, Multi-Game Atari, and BabyAI find that this design unlocks significant progress in online multi-task adaptation and memory problems without explicit task labels.",
      "authors": [
        "Jake Grigsby",
        "Justin Sasek",
        "Samyak Parajuli",
        "Daniel Adebi",
        "Amy Zhang",
        "Yuke Zhu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OSHaRf4TVU",
      "cdate": 1715785827932,
      "mdate": 1730873990169,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445323"
    },
    {
      "id": "MOFwt8OeXr",
      "title": "Generalizing Consistency Policy to Visual RL with Prioritized Proximal Experience Regularization",
      "abstract": "With high-dimensional state spaces, visual reinforcement learning (RL) faces significant challenges in exploitation and exploration, resulting in low sample efficiency and training stability. As a time-efficient diffusion model, although consistency models have been validated in online state-based RL, it is still an open question whether it can be extended to visual RL. In this paper, we investigate the impact of non-stationary distribution and the actor-critic framework on consistency policy in online RL, and find that consistency policy was unstable during the training, especially in visual RL with the high-dimensional state space. To this end, we suggest sample-based entropy regularization to stabilize the policy training, and propose a consistency policy with prioritized proximal experience regularization (CP3ER) to improve sample efficiency. CP3ER achieves new state-of-the-art (SOTA) performance in 21 tasks across DeepMind control suite and Meta-world. To our knowledge, CP3ER is the first method to apply diffusion/consistency models to visual RL and demonstrates the potential of consistency models in visual RL.",
      "authors": [
        "Haoran Li",
        "Zhennan Jiang",
        "YUHUI CHEN",
        "Dongbin Zhao"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MOFwt8OeXr",
      "cdate": 1715785789996,
      "mdate": 1730873990099,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445328"
    },
    {
      "id": "Oo7HY9kmK6",
      "title": "Mean-Field Langevin Dynamics for Signed Measures via a Bilevel Approach",
      "abstract": "Mean-field Langevin dynamics (MLFD) is a class of interacting particle methods that tackle convex optimization over probability measures on a manifold, which are scalable, versatile, and enjoy computational guarantees. However, some important problems -- such as risk minimization for infinite width two-layer neural networks, or sparse deconvolution -- are originally defined over the set of signed, rather than probability, measures. In this paper, we investigate how to extend the MFLD framework to convex optimization problems over signed measures.\nAmong two known reductions from signed to probability measures -- the lifting and the bilevel approaches -- we show that the bilevel reduction leads to stronger guarantees and faster rates (at the price of a higher per-iteration complexity).\nIn particular, we investigate the convergence rate of MFLD applied to the bilevel reduction in the low-noise regime and obtain two results. First, this dynamics is amenable to an annealing schedule, adapted from [Suzuki et al., 2023], that results in polynomial convergence rates to a fixed multiplicative accuracy. Second, we investigate the problem of learning a single neuron with the bilevel approach and obtain local exponential convergence rates that depend polynomially on the dimension and noise level (to compare with the exponential dependence that would result from prior analyses).",
      "authors": [
        "Guillaume Wang",
        "Alireza Mousavi-Hosseini",
        "Lénaïc Chizat"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Oo7HY9kmK6",
      "cdate": 1715785682574,
      "mdate": 1730873989985,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445333"
    },
    {
      "id": "FExX8pMrdT",
      "title": "AutoSurvey: Large Language Models Can Automatically Write Surveys",
      "abstract": "This paper introduces AutoSurvey, a speedy and well-organized methodology for automating the creation of comprehensive literature surveys in rapidly evolving fields like artificial intelligence. Traditional survey paper creation faces challenges due to the vast volume and complexity of information, prompting the need for efficient survey methods. While large language models (LLMs) offer promise in automating this process, challenges such as context window limitations, parametric knowledge constraints, and the lack of evaluation benchmarks remain. AutoSurvey addresses these challenges through a systematic approach that involves initial retrieval and outline generation, subsection drafting by specialized LLMs, integration and refinement, and rigorous evaluation and iteration. Our contributions include a comprehensive solution to the survey problem, a reliable evaluation method, and experimental validation demonstrating AutoSurvey's effectiveness.",
      "authors": [
        "Yidong Wang",
        "Qi Guo",
        "Wenjin Yao",
        "Hongbo Zhang",
        "Xin Zhang",
        "Zhen Wu",
        "Meishan Zhang",
        "Xinyu Dai",
        "Min zhang",
        "Qingsong Wen",
        "Wei Ye",
        "Shikun Zhang",
        "Yue Zhang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FExX8pMrdT",
      "cdate": 1715785654082,
      "mdate": 1730873989956,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445339"
    },
    {
      "id": "DqiggGDOmA",
      "title": "EASI: Evolutionary Adversarial Simulator Identification for Sim-to-Real Transfer",
      "abstract": "Reinforcement Learning (RL) controllers have demonstrated remarkable performance in complex robot control tasks. However, the presence of reality gap often leads to poor performance when deploying policies trained in simulation directly onto real robots. Previous sim-to-real algorithms like Domain Randomization (DR) requires domain-specific expertise and suffers from issues such as reduced control performance and high training costs. In this work, we introduce Evolutionary Adversarial Simulator Identification (EASI), a novel approach that combines Generative Adversarial Network (GAN) and Evolutionary Strategy (ES) to address sim-to-real challenges. Specifically, we consider the problem of sim-to-real as a search problem, where ES acts as a generator in adversarial competition with a neural network discriminator, aiming to find physical parameter distributions that make the state transitions between simulation and reality as similar as possible. The discriminator serves as the fitness function, guiding the evolution of the physical parameter distributions. EASI features simplicity, low cost, and high fidelity, enabling the construction of a more realistic simulator with minimal requirements for real-world data, thus aiding in transferring simulated-trained policies to the real world. We demonstrate the performance of EASI in both sim-to-sim and sim-to-real tasks, showing superior performance compared to existing sim-to-real algorithms.",
      "authors": [
        "Haoyu Dong",
        "Huiqiao Fu",
        "Wentao Xu",
        "Zhehao Zhou",
        "Chunlin Chen"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DqiggGDOmA",
      "cdate": 1715785573843,
      "mdate": 1730873989883,
      "matched_keywords": [
        "reinforcement learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445344"
    },
    {
      "id": "oNMnR0NJ2e",
      "title": "A Label is Worth A Thousand Images in Dataset Distillation",
      "abstract": "Data *quality* is a crucial factor in the performance of machine learning models, a principle that dataset distillation methods exploit by compressing training datasets into much smaller counterparts that maintain similar downstream performance. Understanding how and why data distillation methods work is vital not only for improving these methods but also for revealing fundamental characteristics of \"good” training data. However, a major challenge in achieving this goal is the observation that distillation approaches, which rely on sophisticated but mostly disparate methods to generate synthetic data, have little in common with each other. In this work, we highlight a largely overlooked aspect common to most of these methods: the use of soft (probabilistic) labels. Through a series of ablation experiments, we study the role of soft labels in depth. Our results reveal that the main factor explaining the performance of state-of-the-art distillation methods is not the specific techniques used to generate synthetic data but rather the use of soft labels. Furthermore, we demonstrate that not all soft labels are created equal; they must contain *structured information* to be beneficial. We also provide empirical scaling laws that characterize the effectiveness of soft labels as a function of images-per-class in the distilled dataset and establish an empirical Pareto frontier for data-efficient learning. Combined, our findings challenge conventional wisdom in dataset distillation, underscore the importance of soft labels in learning, and suggest new directions for improving distillation methods. Code for all experiments is available at https://github.com/sunnytqin/no-distillation.",
      "authors": [
        "Tian Qin",
        "Zhiwei Deng",
        "David Alvarez-Melis"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=oNMnR0NJ2e",
      "cdate": 1715785540816,
      "mdate": 1730873989848,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445349"
    },
    {
      "id": "7UyBKTFrtd",
      "title": "Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)",
      "abstract": "CLIP embeddings have demonstrated remarkable performance across a wide range of multimodal applications. However, these high-dimensional, dense vector representations are not easily interpretable,  limiting our understanding of the rich structure of CLIP and its use in downstream applications that require transparency. \nIn this work, we show that the semantic structure of CLIP's latent space can be leveraged to provide interpretability, allowing for the decomposition of representations into semantic concepts. \nWe formulate this problem as one of sparse recovery and propose a novel method, Sparse Linear Concept Embeddings (SpLiCE), for transforming CLIP representations into sparse linear combinations of human-interpretable concepts. Distinct from previous work, \\method is task-agnostic and can be used, without training, \nto explain and even replace traditional dense CLIP representations, maintaining high downstream performance while significantly improving their interpretability. We also demonstrate significant use cases of \\method representations including detecting spurious correlations and model editing. Code is provided at https://github.com/AI4LIFE-GROUP/SpLiCE.",
      "authors": [
        "Usha Bhalla",
        "Alex Oesterling",
        "Suraj Srinivas",
        "Flavio Calmon",
        "Himabindu Lakkaraju"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7UyBKTFrtd",
      "cdate": 1715785434560,
      "mdate": 1730873989737,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.445354"
    },
    {
      "id": "t7SGOv5W5z",
      "title": "UQE: A Query Engine for Unstructured Databases",
      "abstract": "Analytics on structured data is a mature field with many successful methods.\nHowever, most real world data exists in unstructured form, such as images and conversations.\nWe investigate the potential of Large Language Models (LLMs) to enable unstructured data analytics.\nIn particular, we propose a new Universal Query Engine (UQE) that directly interrogates and draws insights from unstructured data collections.\nThis engine accepts queries in a Universal Query Language (UQL), a dialect of SQL that provides full natural language flexibility in specifying conditions and operators.\nThe new engine leverages the ability of LLMs to conduct analysis of unstructured data, while also allowing us to exploit advances in sampling and optimization techniques to achieve efficient and accurate query execution.\nIn addition, we borrow techniques from classical compiler theory to better orchestrate the workflow between sampling methods and foundation model calls.\nWe demonstrate the efficiency of UQE on data analytics across different modalities, including images, dialogs and reviews, across a range of useful query types, including conditional aggregation, semantic retrieval and abstraction aggregation.",
      "authors": [
        "Hanjun Dai",
        "Bethany Yixin Wang",
        "Xingchen Wan",
        "Bo Dai",
        "Sherry Yang",
        "Azade Nova",
        "Pengcheng Yin",
        "Phitchaya Mangpo Phothilimthana",
        "Charles Sutton",
        "Dale Schuurmans"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=t7SGOv5W5z",
      "cdate": 1715785429924,
      "mdate": 1730873989641,
      "matched_keywords": [
        "large language model",
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445359"
    },
    {
      "id": "fu0xdh4aEJ",
      "title": "Bigger, Regularized, Optimistic: scaling for compute and sample efficient continuous control",
      "abstract": "Sample efficiency in Reinforcement Learning (RL) has traditionally been driven by algorithmic enhancements. In this work, we demonstrate that scaling can also lead to substantial improvements.  We conduct a thorough investigation into the interplay of scaling model capacity and domain-specific RL enhancements. These empirical findings inform the design choices underlying our proposed BRO (Bigger, Regularized, Optimistic) algorithm. The key innovation behind BRO is that strong regularization allows for effective scaling of the critic networks, which, paired with optimistic exploration, leads to superior performance. BRO achieves state-of-the-art results, significantly outperforming the leading model-based and model-free algorithms across 40 complex tasks from the DeepMind Control, MetaWorld, and MyoSuite benchmarks. BRO is the first model-free algorithm to achieve near-optimal policies in the notoriously challenging Dog and Humanoid tasks.",
      "authors": [
        "Michal Nauman",
        "Mateusz Ostaszewski",
        "Krzysztof Jankowski",
        "Piotr Miłoś",
        "Marek Cygan"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fu0xdh4aEJ",
      "cdate": 1715785389604,
      "mdate": 1736130052719,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445365"
    },
    {
      "id": "ihEHCbqZEx",
      "title": "Flex-MoE: Modeling Arbitrary Modality Combination via the Flexible Mixture-of-Experts",
      "abstract": "Multimodal learning has gained increasing importance across various fields, offering the ability to integrate data from diverse sources such as images, text, and personalized records, which are frequently observed in medical domains. However, in scenarios where some modalities are missing, many existing frameworks struggle to accommodate arbitrary modality combinations, often relying heavily on a single modality or complete data. This oversight of potential modality combinations limits their applicability in real-world situations. To address this challenge, we propose Flex-MoE (Flexible Mixture-of-Experts), a new framework designed to flexibly incorporate arbitrary modality combinations while maintaining robustness to missing data. The core idea of Flex-MoE is to first address missing modalities using a new missing modality bank that integrates observed modality combinations with the corresponding missing ones. This is followed by a uniquely designed Sparse MoE framework. Specifically, Flex-MoE first trains experts using samples with all modalities to inject generalized knowledge through the generalized router ($\\mathcal{G}$-Router). The $\\mathcal{S}$-Router then specializes in handling fewer modality combinations by assigning the top-1 gate to the expert corresponding to the observed modality combination. We evaluate Flex-MoE on the ADNI dataset, which encompasses four modalities in the Alzheimer's Disease domain, as well as on the MIMIC-IV dataset. The results demonstrate the effectiveness of Flex-MoE, highlighting its ability to model arbitrary modality combinations in diverse missing modality scenarios. Code is available at: \\url{https://github.com/UNITES-Lab/flex-moe}.",
      "authors": [
        "Sukwon Yun",
        "Inyoung Choi",
        "Jie Peng",
        "Yangfan Wu",
        "Jingxuan Bao",
        "Qiyiwen Zhang",
        "Jiayi Xin",
        "Qi Long",
        "Tianlong Chen"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ihEHCbqZEx",
      "cdate": 1715785385982,
      "mdate": 1730873989563,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.445369"
    },
    {
      "id": "VqxODXhU4k",
      "title": "Nonparametric Instrumental Variable Regression through Stochastic Approximate Gradients",
      "abstract": "Instrumental variables (IVs) provide a powerful strategy for identifying causal effects in the presence of unobservable confounders. Within the nonparametric setting (NPIV), recent methods have been based on nonlinear generalizations of Two-Stage Least Squares and on minimax formulations derived from moment conditions or duality. In a novel direction, we show how to formulate a functional stochastic gradient descent algorithm to tackle NPIV regression by directly minimizing the populational risk. We provide theoretical support in the form of bounds on the excess risk, and conduct numerical experiments showcasing our method's superior stability and competitive performance relative to current state-of-the-art alternatives. This algorithm enables flexible estimator choices, such as neural networks or kernel based methods, as well as non-quadratic loss functions, which may be suitable for structural equations beyond the setting of continuous outcomes and additive noise. Finally, we demonstrate this flexibility of our framework by presenting how it naturally addresses the important case of binary outcomes, which has received far less attention by recent developments in the NPIV literature.",
      "authors": [
        "Yuri Fonseca",
        "Caio Peixoto",
        "Yuri Saporito"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=VqxODXhU4k",
      "cdate": 1715785376085,
      "mdate": 1734960532926,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445374"
    },
    {
      "id": "L8ifDX5XNq",
      "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
      "abstract": "The machine learning community has witnessed impressive advancements since large language models (LLMs) first appeared. Yet, their massive memory consumption has become a significant roadblock to large-scale training. For instance, a 7B model typically requires at least 60 GB of GPU memory with full parameter training, which presents challenges for researchers without access to high-resource environments. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem. However, in most large-scale fine-tuning settings, their performance does not reach the level of full parameter training because they confine the parameter search to a low-rank subspace. Attempting to complement this deficiency, we investigate the layerwise properties of LoRA on fine-tuning tasks and observe an unexpected but consistent skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of importance sampling to different layers in LLMs and randomly freeze most middle layers during optimization. Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream fine-tuning tasks, where LISA consistently outperforms LoRA by over 10%-35% in terms of MT-Bench score while achieving on-par or better performance in MMLU, AGIEval and WinoGrande. On large models, specifically LLaMA-2-70B, LISA surpasses LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating its effectiveness across different domains.",
      "authors": [
        "Rui Pan",
        "Xiang Liu",
        "Shizhe Diao",
        "Renjie Pi",
        "Jipeng Zhang",
        "Chi Han",
        "Tong Zhang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=L8ifDX5XNq",
      "cdate": 1715785198361,
      "mdate": 1734676603679,
      "matched_keywords": [
        "large language model",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445379"
    },
    {
      "id": "RtMyTzIW6l",
      "title": "SymILO: A Symmetry-Aware Learning Framework for Integer Linear Optimization",
      "abstract": "Integer linear programs (ILPs) are commonly employed to model diverse practical problems such as scheduling and planning. \nRecently, machine learning techniques have been utilized to solve ILPs.  A straightforward idea is to train a model via supervised learning, with an ILP as the input and an optimal solution as the label. An ILP is symmetric if its variables can be permuted without changing the problem structure, resulting in numerous equivalent and optimal solutions. Randomly selecting an optimal solution as the label can introduce variability in the training data, which may hinder the model from learning stable patterns. In this work, we incorporate the intrinsic symmetry of ILPs and propose a novel training framework called SymILO. Specifically, we modify the learning task by introducing solution permutation along with neural network weights as learnable parameters and then design an alternating algorithm to jointly optimize the loss function.\nWe conduct extensive experiments on ILPs involving different symmetries and the computational results demonstrate that our symmetry-aware approach significantly outperforms three existing methods----achieving $50.3\\\\%$, $66.5\\\\%$, and $45.4\\\\%$ average improvements, respectively.",
      "authors": [
        "Qian Chen",
        "Tianjian Zhang",
        "Linxin Yang",
        "Qingyu Han",
        "Akang Wang",
        "Ruoyu Sun",
        "Xiaodong Luo",
        "Tsung-Hui Chang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=RtMyTzIW6l",
      "cdate": 1715785151575,
      "mdate": 1735459465287,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445384"
    },
    {
      "id": "204YOrDHny",
      "title": "Reparameterization invariance in approximate Bayesian inference",
      "abstract": "Current approximate posteriors in Bayesian neural networks (BNNs) exhibit a crucial limitation: they fail to maintain invariance under reparameterization, i.e. BNNs assign different posterior densities to different parametrizations of identical functions. This creates a fundamental flaw in the application of Bayesian principles as it breaks the correspondence between uncertainty over the parameters with uncertainty over the parametrized function. In this paper, we investigate this issue in the context of the increasingly popular linearized Laplace approximation. Specifically, it has been observed that linearized predictives alleviate the common underfitting problems of the Laplace approximation. We develop a new geometric view of reparametrizations from which we explain the success of linearization. Moreover, we demonstrate that these reparameterization invariance properties can be extended to the original neural network predictive using a Riemannian diffusion process giving a straightforward algorithm for approximate posterior sampling, which empirically improves posterior fit.",
      "authors": [
        "Hrittik Roy",
        "Marco Miani",
        "Carl Henrik Ek",
        "Philipp Hennig",
        "Marvin Pförtner",
        "Lukas Tatzel",
        "Søren Hauberg"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=204YOrDHny",
      "cdate": 1715785118804,
      "mdate": 1731568009791,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445389"
    },
    {
      "id": "j2wCrWmgMX",
      "title": "Kernel Language Entropy: Fine-grained Uncertainty Quantification for LLMs from Semantic Similarities",
      "abstract": "Uncertainty quantification in Large Language Models (LLMs) is crucial for applications where safety and reliability are important. In particular, uncertainty can be used to improve the trustworthiness of LLMs by detecting factually incorrect model responses, commonly called hallucinations. Critically, one should seek to capture the model's semantic uncertainty, i.e., the uncertainty over the meanings of LLM outputs, rather than uncertainty over lexical or syntactic variations that do not affect answer correctness.\nTo address this problem, we propose Kernel Language Entropy (KLE), a novel method for uncertainty estimation in white- and black-box LLMs. KLE defines positive semidefinite unit trace kernels to encode the semantic similarities of LLM outputs and quantifies uncertainty using the von Neumann entropy. It considers pairwise semantic dependencies between answers (or semantic clusters), providing more fine-grained uncertainty estimates than previous methods based on hard clustering of answers. We theoretically prove that KLE generalizes the previous state-of-the-art method called semantic entropy and empirically demonstrate that it improves uncertainty quantification performance across multiple natural language generation datasets and LLM architectures.",
      "authors": [
        "Alexander V Nikitin",
        "Jannik Kossen",
        "Yarin Gal",
        "Pekka Marttinen"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=j2wCrWmgMX",
      "cdate": 1715785076620,
      "mdate": 1730873989343,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445394"
    },
    {
      "id": "jeWZStUavo",
      "title": "Reinforcement Learning Policy as Macro Regulator Rather than Macro Placer",
      "abstract": "In modern chip design, placement aims at placing millions of circuit modules, which is an essential step that significantly influences power, performance, and area (PPA) metrics. Recently, reinforcement learning (RL) has emerged as a promising technique for improving placement quality, especially macro placement. However, current RL-based placement methods suffer from long training times, low generalization ability, and inability to guarantee PPA results. A key issue lies in the problem formulation, i.e., using RL to place from scratch, which results in limits useful information and inaccurate rewards during the training process. In this work, we propose an approach that utilizes RL for the refinement stage, which allows the RL policy to learn how to adjust existing placement layouts, thereby receiving sufficient information for the policy to act and obtain relatively dense and precise rewards. Additionally, we introduce the concept of regularity during training, which is considered an important metric in the chip design industry but is often overlooked in current RL placement methods. We evaluate our approach on the ISPD 2005 and ICCAD 2015 benchmark, comparing the global half-perimeter wirelength and regularity of our proposed method against several competitive approaches. Besides, we test the PPA performance using commercial software, showing that RL as a regulator can achieve significant PPA improvements. Our RL regulator can fine-tune placements from any method and enhance their quality. Our work opens up new possibilities for the application of RL in placement, providing a more effective and efficient approach to optimizing chip design. Our code is available at \\url{https://github.com/lamda-bbo/macro-regulator}.",
      "authors": [
        "Ke Xue",
        "Ruo-Tong Chen",
        "Xi Lin",
        "Yunqi Shi",
        "Shixiong Kai",
        "Siyuan Xu",
        "Chao Qian"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jeWZStUavo",
      "cdate": 1715784947917,
      "mdate": 1730873989267,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445399"
    },
    {
      "id": "UZIHW8eFRp",
      "title": "A Tractable Inference Perspective of Offline RL",
      "abstract": "A popular paradigm for offline Reinforcement Learning (RL) tasks is to first fit the offline trajectories to a sequence model, and then prompt the model for actions that lead to high expected return. In addition to obtaining accurate sequence models, this paper highlights that tractability, the ability to exactly and efficiently answer various probabilistic queries, plays an important role in offline RL. Specifically, due to the fundamental stochasticity from the offline data-collection policies and the environment dynamics, highly non-trivial conditional/constrained generation is required to elicit rewarding actions. While it is still possible to approximate such queries, we observe that such crude estimates undermine the benefits brought by expressive sequence models. To overcome this problem, this paper proposes Trifle (Tractable Inference for Offline RL), which leverages modern tractable generative models to bridge the gap between good sequence models and high expected returns at evaluation time. Empirically, Trifle achieves $7$ state-of-the-art scores and the highest average scores in $9$ Gym-MuJoCo benchmarks against strong baselines. Further, Trifle significantly outperforms prior approaches in stochastic environments and safe RL tasks with minimum algorithmic modifications.",
      "authors": [
        "Xuejie Liu",
        "Anji Liu",
        "Guy Van den Broeck",
        "Yitao Liang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=UZIHW8eFRp",
      "cdate": 1715784944368,
      "mdate": 1730873989238,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445405"
    },
    {
      "id": "snxWD0Q4EI",
      "title": "The Iterative Optimal Brain Surgeon: Faster Sparse Recovery by Leveraging Second-Order Information",
      "abstract": "The rising footprint of machine learning has led to a focus on imposing model sparsity as a means of reducing computational and memory costs. For deep neural networks (DNNs), the state-of-the-art accuracy-vs-sparsity is achieved by heuristics inspired by the classical Optimal Brain Surgeon (OBS) framework [LeCun et al., 1989, Hassibi and Stork, 1992, Hassibi et al., 1993], which leverages loss curvature information to make better pruning decisions. Yet, these results still lack a solid theoretical understanding, and it is unclear whether they can be improved by leveraging connections to the wealth of work on sparse recovery algorithms. In this paper, we draw new connections between these two areas and present new sparse recovery algorithms inspired by the OBS framework that come with theoretical guarantees under reasonable assumptions and have strong practical performance. Specifically, our work starts from the observation that we can leverage curvature information in OBS-like fashion upon the projection step of classic iterative sparse recovery algorithms such as IHT. We show for the first time that this leads both to improved convergence bounds in well-behaved settings and to stronger practical convergence. Furthermore, we present extensions of this approach to training accurate sparse DNNs, and validate it experimentally at scale.",
      "authors": [
        "Diyuan Wu",
        "Ionut-Vlad Modoranu",
        "Mher Safaryan",
        "Denis Kuznedelev",
        "Dan Alistarh"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=snxWD0Q4EI",
      "cdate": 1715784847654,
      "mdate": 1730873989146,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445410"
    },
    {
      "id": "DexM7d1H6e",
      "title": "Animal-Bench: Benchmarking Multimodal Video Models for Animal-centric Video Understanding",
      "abstract": "With the emergence of large pre-trained multimodal video models, multiple benchmarks have been proposed to evaluate model capabilities. However, most of the benchmarks are human-centric, with evaluation data and tasks centered around human applications. Animals are an integral part of the natural world, and animal-centric video understanding is crucial for animal welfare and conservation efforts. Yet, existing benchmarks overlook evaluations focused on animals, limiting the application of the models. To address this limitation, our work established an animal-centric benchmark, namely Animal-Bench, to allow for a comprehensive evaluation of model capabilities in real-world contexts, overcoming agent-bias in previous benchmarks. Animal-Bench includes 13 tasks encompassing both common tasks shared with humans and special tasks relevant to animal conservation, spanning 7 major animal categories and 819 species, comprising a total of 41,839 data entries. To generate this benchmark, we defined a task system centered on animals and proposed an automated pipeline for animal-centric data processing. To further validate the robustness of models against real-world challenges, we utilized a video editing approach to simulate realistic scenarios like weather changes and shooting parameters due to animal movements. We evaluated 8 current multimodal video models on our benchmark and found considerable room for improvement. We hope our work provides insights for the community and opens up new avenues for research in multimodal video models. Our data and code will be released at https://github.com/PRIS-CV/Animal-Bench.",
      "authors": [
        "Yinuo Jing",
        "Ruxu Zhang",
        "Kongming Liang",
        "Yongxiang Li",
        "Zhongjiang He",
        "Zhanyu Ma",
        "Jun Guo"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DexM7d1H6e",
      "cdate": 1715784807987,
      "mdate": 1730873989114,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.445415"
    },
    {
      "id": "DONsOc7rY1",
      "title": "HORSE: Hierarchical Representation for Large-Scale Neural Subset Selection",
      "abstract": "Subset selection tasks, such as anomaly detection and compound selection in AI-assisted drug discovery, are crucial for a wide range of applications. Learning subset-valued functions with neural networks has achieved great success by incorporating permutation invariance symmetry into the architecture. However, existing neural set architectures often struggle to either capture comprehensive information from the superset or address complex interactions within the input. Additionally, they often fail to perform in scenarios where superset sizes surpass available memory capacity. To address these challenges, we introduce the novel concept of the Identity Property, which requires models to integrate information from the originating set, resulting in the development of neural networks that excel at performing effective subset selection from large supersets. Moreover, we present the Hierarchical Representation of Neural Subset Selection (HORSE), an attention-based method that learns complex interactions and retains information from both the input set and the optimal subset supervision signal. Specifically, HORSE enables the partitioning of the input ground set into manageable chunks that can be processed independently and then aggregated, ensuring consistent outcomes across different partitions. Through extensive experimentation, we demonstrate that HORSE significantly enhances neural subset selection performance by capturing more complex information and surpasses state-of-the-art methods in handling large-scale inputs by a margin of up to 20%.",
      "authors": [
        "Binghui Xie",
        "Yixuan Wang",
        "Yongqiang Chen",
        "Kaiwen Zhou",
        "Yu Li",
        "Wei Meng",
        "James Cheng"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DONsOc7rY1",
      "cdate": 1715784750247,
      "mdate": 1730873989055,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445420"
    },
    {
      "id": "nQl8EjyMzh",
      "title": "On conditional diffusion models for PDE simulations",
      "abstract": "Modelling partial differential equations (PDEs) is of crucial importance in science and engineering, and it includes tasks ranging from forecasting to inverse problems, such as data assimilation. However, most previous numerical and machine learning approaches that target forecasting cannot be applied out-of-the-box for data assimilation. Recently, diffusion models have emerged as a powerful tool for conditional generation, being able to flexibly incorporate observations without retraining. In this work, we perform a comparative study of score-based diffusion models for forecasting and assimilation of sparse observations.  In particular, we focus on diffusion models that are either trained in a conditional manner, or conditioned after unconditional training. We address the shortcomings of existing models by proposing 1) an autoregressive sampling approach, that significantly improves performance in forecasting, 2) a new training strategy for conditional score-based models that achieves stable performance over a range of history lengths, and 3) a hybrid model which employs flexible pre-training conditioning on initial conditions and flexible post-training conditioning to handle data assimilation. We empirically show that these modifications are crucial for successfully tackling the combination of forecasting and data assimilation, a task commonly encountered in real-world scenarios.",
      "authors": [
        "Aliaksandra Shysheya",
        "Cristiana Diaconu",
        "Federico Bergamin",
        "Paris Perdikaris",
        "José Miguel Hernández-Lobato",
        "Richard E. Turner",
        "Emile Mathieu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nQl8EjyMzh",
      "cdate": 1715784669360,
      "mdate": 1730873988996,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445425"
    },
    {
      "id": "NAcHv7vtL2",
      "title": "Scaling laws for learning with real and surrogate data",
      "abstract": "Collecting large quantities of high-quality data can be  prohibitively expensive or impractical, and a bottleneck in machine learning. One may instead augment a small set of $n$ data points from the target distribution with data from more accessible sources, e.g. data collected under different circumstances or synthesized by generative models. We refer to such data  as `surrogate data'. We study a weighted empirical risk minimization (ERM) approach for integrating surrogate data into training. We analyze mathematically this method under several classical statistical models, and validate our findings empirically on datasets from different domains. Our main findings are: $(i)$ Integrating surrogate data can significantly reduce the test error on the original distribution. Surprisingly, this can happen even when the surrogate data is unrelated to the original ones. We trace back this behavior to the classical Stein's paradox. $(ii)$ In order to reap the benefit of surrogate data, it is crucial to use optimally weighted ERM. $(iii)$ The test error of models trained on mixtures of real and surrogate data is approximately described by a scaling law. This scaling law can be used to predict the optimal weighting scheme, and to choose the amount of surrogate data to add.",
      "authors": [
        "Ayush Jain",
        "Andrea Montanari",
        "Eren Sasoglu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NAcHv7vtL2",
      "cdate": 1715784638067,
      "mdate": 1736980946903,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445430"
    },
    {
      "id": "h1grUs6CjN",
      "title": "The Price of Implicit Bias in Adversarially Robust Generalization",
      "abstract": "We study the implicit bias of optimization in robust empirical risk minimization (robust ERM) and its connection with robust generalization.  In classification settings under adversarial perturbations with linear models, we study what type of regularization should ideally be applied for a given perturbation set to improve (robust) generalization. We then show that the implicit bias of optimization in robust ERM can significantly affect the robustness of the model and identify two ways this can happen; either through the optimization algorithm or the architecture. We verify our predictions in simulations with synthetic data and experimentally study the importance of implicit bias in robust ERM with deep neural networks.",
      "authors": [
        "Nikolaos Tsilivis",
        "Natalie Frank",
        "Nathan Srebro",
        "Julia Kempe"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=h1grUs6CjN",
      "cdate": 1715784491056,
      "mdate": 1730873988657,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445435"
    },
    {
      "id": "JD3NYpeQ3R",
      "title": "Large language model validity via enhanced conformal prediction methods",
      "abstract": "We develop new conformal inference methods for obtaining validity guarantees on the output of large language models (LLMs). Prior work in conformal language modeling identifies a subset of the text that satisfies a high-probability guarantee of correctness. These methods work by filtering claims from the LLM's original response if a scoring function evaluated on the claim fails to exceed a threshold calibrated via split conformal prediction. Existing methods in this area suffer from two deficiencies. First, the guarantee stated is not conditionally valid. The trustworthiness of the filtering step may vary based on the topic of the response. Second, because the scoring function is imperfect, the filtering step can remove many valuable and accurate claims. We address both of these challenges via two new conformal methods. First, we generalize the conditional conformal procedure of Gibbs et al. (2023) in order to adaptively issue weaker guarantees when they are required to preserve the utility of the output. Second, we show how to systematically improve the quality of the scoring function via a novel algorithm for differentiating through the conditional conformal procedure. We demonstrate the efficacy of our approach on biography and medical question-answering datasets.",
      "authors": [
        "John Cherian",
        "Isaac Gibbs",
        "Emmanuel Candes"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=JD3NYpeQ3R",
      "cdate": 1715784277092,
      "mdate": 1730873988520,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445440"
    },
    {
      "id": "uCvdw0IOuU",
      "title": "Addressing Asynchronicity in Clinical Multimodal Fusion via Individualized Chest X-ray Generation",
      "abstract": "Integrating multi-modal clinical data, such as electronic health records (EHR) and chest X-ray images (CXR), is particularly beneficial for clinical prediction tasks. However, in a temporal setting, multi-modal data are often inherently asynchronous. EHR can be continuously collected but CXR is generally taken with a much longer interval due to its high cost and radiation dose. When clinical prediction is needed, the last available CXR image might have been outdated, leading to suboptimal predictions. To address this challenge, we propose DDL-CXR, a method that dynamically generates an up-to-date latent representation of the individualized CXR images. Our approach leverages latent diffusion models for patient-specific generation strategically conditioned on a previous CXR image and EHR time series, providing information regarding anatomical structures and disease progressions, respectively. In this way, the interaction across modalities could be better captured by the latent CXR generation process, ultimately improving the prediction performance. Experiments using MIMIC datasets show that the proposed model could effectively address asynchronicity in multimodal fusion and consistently outperform existing methods.",
      "authors": [
        "Wenfang Yao",
        "Chen Liu",
        "Kejing Yin",
        "William K. Cheung",
        "Jing Qin"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=uCvdw0IOuU",
      "cdate": 1715783906712,
      "mdate": 1730873988189,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.445446"
    },
    {
      "id": "OU1uqd1vyw",
      "title": "CLUES: Collaborative Private-domain High-quality Data Selection for LLMs via Training Dynamics",
      "abstract": "Recent research has highlighted the importance of data quality in scaling large language models (LLMs). However, automated data quality control faces unique challenges in collaborative settings where sharing is not allowed directly between data silos. To tackle this issue, this paper proposes a novel data quality control technique based on the notion of data influence on the training dynamics of LLMs, that high quality data are more likely to have similar training dynamics to the anchor dataset. We then leverage the influence of the training dynamics to select high-quality data from different private domains, with centralized model updates on the server side in a collaborative training fashion by either model merging or federated learning. As for the data quality indicator, we compute the per-sample gradients with respect to the private data and the anchor dataset, and use the trace of the accumulated inner products as a measurement of data quality. In addition, we develop a quality control evaluation tailored for collaborative settings with heterogeneous medical domain data. Experiments show that training on the high-quality data selected by our method can often outperform other data selection methods for collaborative fine-tuning of LLMs, across diverse private domain datasets, in medical, multilingual and financial settings.",
      "authors": [
        "Wanru Zhao",
        "Hongxiang Fan",
        "Shell Xu Hu",
        "Wangchunshu Zhou",
        "Nicholas Donald Lane"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OU1uqd1vyw",
      "cdate": 1715783905657,
      "mdate": 1730873988153,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445451"
    },
    {
      "id": "f63DKIpx0I",
      "title": "Self-Healing Machine Learning: A Framework for Autonomous Adaptation in Real-World Environments",
      "abstract": "Real-world machine learning systems often encounter model performance degradation due to distributional shifts in the underlying data generating process (DGP). Existing approaches to addressing shifts, such as concept drift adaptation, are limited by their *reason-agnostic* nature. By choosing from a pre-defined set of actions, such methods implicitly assume that the causes of model degradation are irrelevant to what actions should be taken, limiting their ability to select appropriate adaptations. In this paper, we propose an alternative paradigm to overcome these limitations, called *self-healing machine learning* (SHML). Contrary to previous approaches, SHML autonomously diagnoses the reason for degradation and proposes diagnosis-based corrective actions. We formalize SHML as an optimization problem over a space of adaptation actions to minimize the expected risk under the shifted DGP.  We introduce a theoretical framework for self-healing systems and build an agentic self-healing solution *$\\mathcal{H}$-LLM* which uses large language models to perform self-diagnosis by reasoning about the structure underlying the DGP, and self-adaptation by proposing and evaluating corrective actions. Empirically, we analyze different components of *$\\mathcal{H}$-LLM* to understand *why* and *when* it works, demonstrating the potential of self-healing ML.",
      "authors": [
        "Paulius Rauba",
        "Nabeel Seedat",
        "Krzysztof Kacprzyk",
        "Mihaela van der Schaar"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=f63DKIpx0I",
      "cdate": 1715783793765,
      "mdate": 1730873987950,
      "matched_keywords": [
        "large language model",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445457"
    },
    {
      "id": "njvPjG0BfK",
      "title": "HardCore Generation: Generating Hard UNSAT Problems for Data Augmentation",
      "abstract": "Efficiently determining the satisfiability of a boolean equation --- known as the SAT problem for brevity --- is crucial in various industrial problems.  Recently, the advent of deep learning methods has introduced significant potential for enhancing SAT solving. However, a major barrier to the advancement of this field has been the scarcity of large, realistic datasets.  The majority of current public datasets are either randomly generated or extremely limited, containing only a few examples from unrelated problem families. These datasets are inadequate for meaningful training of deep learning methods.  In light of this, researchers have started exploring generative techniques to create data that more accurately reflect SAT problems encountered in practical situations. These methods have so far suffered from either the inability to produce challenging SAT problems or time-scalability obstacles.  In this paper we address both by identifying and manipulating the key contributors to a problem's ``hardness'', known as cores. Although some previous work has addressed cores, the time costs are unacceptably high due to the expense of traditional heuristic core detection techniques. We introduce a fast core detection procedure that uses a graph neural network. Our empirical results demonstrate that we can efficiently generate problems that remain hard to solve and retain key attributes of the original example problems. We show via experiment that the generated synthetic SAT problems can be used in a data augmentation setting to provide improved prediction of solver runtimes.",
      "authors": [
        "Joseph Cotnareanu",
        "Zhanguang Zhang",
        "Hui-Ling Zhen",
        "Yingxue Zhang",
        "Mark Coates"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=njvPjG0BfK",
      "cdate": 1715783720826,
      "mdate": 1730873987817,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445463"
    },
    {
      "id": "lPDxPVS6ix",
      "title": "SPEAR: Exact Gradient Inversion of Batches in Federated Learning",
      "abstract": "Federated learning is a framework for collaborative machine learning where clients only share gradient updates and not their private data with a server. However, it was recently shown that gradient inversion attacks can reconstruct this data from the shared gradients. In the important honest-but-curious setting, existing attacks enable exact reconstruction only for batch size of $b=1$, with larger batches permitting only approximate reconstruction. In this work, we propose SPEAR, *the first algorithm reconstructing whole batches with $b >1$ exactly*. SPEAR combines insights into the explicit low-rank structure of gradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable. We provide an efficient GPU implementation for fully connected networks and show that it recovers high-dimensional ImageNet inputs in batches of up to $b \\lesssim 25$ exactly while scaling to large networks. Finally, we show theoretically that much larger batches can be reconstructed with high probability given exponential time.",
      "authors": [
        "Dimitar Iliev Dimitrov",
        "Maximilian Baader",
        "Mark Niklas Mueller",
        "Martin Vechev"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lPDxPVS6ix",
      "cdate": 1715783548788,
      "mdate": 1730873987707,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445468"
    },
    {
      "id": "KyVBzkConO",
      "title": "Injecting Undetectable Backdoors in Obfuscated Neural Networks and Language Models",
      "abstract": "As ML models become increasingly complex and integral to high-stakes domains such as finance and healthcare, they also become more susceptible to sophisticated adversarial attacks. We investigate the threat posed by undetectable backdoors, as defined in Goldwasser et al. [2022], in models developed by insidious external expert firms. When such backdoors exist, they allow the designer of the model to sell information on how to slightly perturb their input to change the outcome of the model. We develop a general strategy to plant backdoors to obfuscated neural networks, that satisfy the security properties of the celebrated notion of indistinguishability obfuscation. Applying obfuscation before releasing neural networks is a strategy that is well motivated to protect sensitive information of the external expert firm. Our method to plant backdoors ensures that even if the weights and architecture of the obfuscated model are accessible, the existence of\nthe backdoor is still undetectable. Finally, we introduce the notion of undetectable backdoors to language models and extend our neural network backdoor attacks to such models based on the existence of steganographic functions.",
      "authors": [
        "Alkis Kalavasis",
        "Amin Karbasi",
        "Argyris Oikonomou",
        "Katerina Sotiraki",
        "Grigoris Velegkas",
        "Manolis Zampetakis"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=KyVBzkConO",
      "cdate": 1715783450030,
      "mdate": 1730873987641,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445472"
    },
    {
      "id": "nv7ox1vd3q",
      "title": "Precise asymptotics of reweighted least-squares algorithms for linear diagonal networks",
      "abstract": "The classical iteratively reweighted least-squares (IRLS) algorithm aims to recover an unknown signal from linear measurements by performing a sequence of weighted least squares problems, where the weights are recursively updated at each step. Varieties of this algorithm have been shown to achieve favorable empirical performance and theoretical guarantees for sparse recovery and $\\ell_p$-norm minimization. Recently, some preliminary connections have also been made between IRLS and certain types of non-convex linear neural network architectures that are observed to exploit low-dimensional structure in high-dimensional linear models. In this work, we provide a unified asymptotic analysis for a family of algorithms that encompasses IRLS, the recently proposed lin-RFM algorithm (which was motivated by feature learning in neural networks), and the alternating minimization algorithm on linear diagonal neural networks. Our analysis operates in a \"batched\" setting with i.i.d. Gaussian covariates and shows that, with appropriately chosen reweighting policy, the algorithm can achieve favorable performance in only a handful of iterations. We also extend our results to the case of group-sparse recovery and show that leveraging this structure in the reweighting scheme provably improves test error compared to coordinate-wise reweighting.",
      "authors": [
        "Chiraag Kaushik",
        "Justin Romberg",
        "Vidya Muthukumar"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nv7ox1vd3q",
      "cdate": 1715783355072,
      "mdate": 1730873987542,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445477"
    },
    {
      "id": "RwK0tgfptL",
      "title": "Shaving Weights with Occam's Razor: Bayesian Sparsification for Neural Networks using the Marginal Likelihood",
      "abstract": "Neural network sparsification is a promising avenue to save computational time and memory costs, especially in an age where many successful AI models are becoming too large to naively deploy on consumer hardware. While much work has focused on different weight pruning criteria, the overall sparsifiability of the network, i.e., its capacity to be pruned without quality loss, has often been overlooked. We present Sparsifiability via the Marginal likelihood (SpaM), a sparsification framework that highlights the effectiveness of using the Bayesian marginal likelihood in conjunction with sparsity-inducing priors for making neural networks more sparsifiable. Our approach implements an automatic Occam's razor that selects the most sparsifiable model that still explains the data well, both for structured and unstructured sparsification. In addition, we demonstrate that the pre-computed posterior precision from the Laplace approximation can be re-used to define a cheap pruning criterion, which outperforms many existing (more expensive) approaches. We demonstrate the effectiveness of our framework, especially at high sparsity levels, across a range of different neural network architectures and datasets.",
      "authors": [
        "Rayen Dhahri",
        "Alexander Immer",
        "Bertrand Charpentier",
        "Stephan Günnemann",
        "Vincent Fortuin"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=RwK0tgfptL",
      "cdate": 1715783335571,
      "mdate": 1730873987435,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445482"
    },
    {
      "id": "h15RyEj151",
      "title": "Provable Benefits of Complex Parameterizations for Structured State Space Models",
      "abstract": "Structured state space models (SSMs), the core engine behind prominent neural networks such as S4 and Mamba, are linear dynamical systems adhering to a specified structure, most notably diagonal. In contrast to typical neural network modules, whose parameterizations are real, SSMs often use complex parameterizations. Theoretically explaining the benefits of complex parameterizations for SSMs is an open problem. The current paper takes a step towards its resolution, by establishing formal gaps between real and complex diagonal SSMs. Firstly, we prove that while a moderate dimension suffices in order for a complex SSM to express all mappings of a real SSM, a much higher dimension is needed for a real SSM to express mappings of a complex SSM. Secondly, we prove that even if the dimension of a real SSM is high enough to express a given mapping, typically, doing so requires the parameters of the real SSM to hold exponentially large values, which cannot be learned in practice. In contrast, a complex SSM can express any given mapping with moderate parameter values. Experiments corroborate our theory, and suggest a potential extension of the theory that accounts for selectivity, a new architectural feature yielding state of the art performance.",
      "authors": [
        "Yuval Ran-Milo",
        "Eden Lumbroso",
        "Edo Cohen-Karlik",
        "Raja Giryes",
        "Amir Globerson",
        "Nadav Cohen"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=h15RyEj151",
      "cdate": 1715783212826,
      "mdate": 1730873987363,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445487"
    },
    {
      "id": "NhyDfZXjQX",
      "title": "A Local Method for Satisfying Interventional Fairness with Partially Known Causal Graphs",
      "abstract": "Developing fair automated machine learning algorithms is critical in making safe and trustworthy decisions. Many causality-based fairness notions have been proposed to address the above issues by quantifying the causal connections between sensitive attributes and decisions, and when the true causal graph is fully known, certain algorithms that achieve interventional fairness have been proposed. However, when the true causal graph is unknown, it is still challenging to effectively and efficiently exploit partially directed acyclic graphs (PDAGs) to achieve interventional fairness. To exploit the PDAGs for achieving interventional fairness, previous methods have been built on variable selection or causal effect identification, but limited to reduced prediction accuracy or strong assumptions. In this paper, we propose a general min-max optimization framework that can achieve interventional fairness with promising prediction accuracy and can be extended to maximally oriented PDAGs (MPDAGs) with added background knowledge. Specifically, we first estimate all possible treatment effects of sensitive attributes on a given prediction model from all possible adjustment sets of sensitive attributes via an efficient local approach. Next, we propose to alternatively update the prediction model and possible estimated causal effects, where the prediction model is trained via a min-max loss to control the worst-case fairness violations. Extensive experiments on synthetic and real-world datasets verify the superiority of our methods. To benefit the research community, we have released our project at https://github.com/haoxuanli-pku/NeurIPS24-Interventional-Fairness-with-PDAGs.",
      "authors": [
        "Haoxuan Li",
        "Yue Liu",
        "Zhi Geng",
        "Kun Zhang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NhyDfZXjQX",
      "cdate": 1715783202370,
      "mdate": 1736963759097,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445492"
    },
    {
      "id": "clDGHpx2la",
      "title": "InversionView: A General-Purpose Method for Reading Information from Neural Activations",
      "abstract": "The inner workings of neural networks can be better understood if we can fully decipher the information encoded in neural activations. In this paper, we argue that this information is embodied by the subset of inputs that give rise to similar activations.  We propose InversionView, which allows us to practically inspect this subset by sampling from a trained decoder model conditioned on activations. This helps uncover the information content of activation vectors, and facilitates understanding of the algorithms implemented by transformer models. We present four case studies where we investigate models ranging from small transformers to GPT-2. In these studies, we show that InversionView can reveal clear information contained in activations, including basic information about tokens appearing in the context, as well as more complex information, such as the count of certain tokens, their relative positions, and abstract knowledge about the subject. We also provide causally verified circuits to confirm the decoded information.",
      "authors": [
        "Xinting Huang",
        "Madhur Panwar",
        "Navin Goyal",
        "Michael Hahn"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=clDGHpx2la",
      "cdate": 1715783064473,
      "mdate": 1730873987283,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445497"
    },
    {
      "id": "pCVxYw6FKg",
      "title": "The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof",
      "abstract": "Many algorithms and observed phenomena in deep learning appear to be affected by parameter symmetries --- transformations of neural network parameters that do not change the underlying neural network function. These include linear mode connectivity, model merging, Bayesian neural network inference, metanetworks, and several other characteristics of optimization or loss-landscapes. However, theoretical analysis of the relationship between parameter space symmetries and these phenonmena is difficult. In this work, we empirically investigate the impact of neural parameter symmetries by introducing new neural network architectures that have reduced parameter space symmetries. We develop two methods, with some provable guarantees, of modifying standard neural networks to reduce parameter space symmetries.  With these new methods, we conduct a comprehensive experimental study consisting of multiple tasks aimed at assessing the effect of removing parameter symmetries. Our experiments reveal several interesting observations on the empirical impact of parameter symmetries; for instance, we observe linear mode connectivity between our networks without alignment of weight spaces, and we find that our networks allow for faster and more effective Bayesian neural network training.",
      "authors": [
        "Derek Lim",
        "Theo Putterman",
        "Robin Walters",
        "Haggai Maron",
        "Stefanie Jegelka"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pCVxYw6FKg",
      "cdate": 1715783052345,
      "mdate": 1730873987254,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445502"
    },
    {
      "id": "NG16csOmcA",
      "title": "Neural Residual Diffusion Models for Deep Scalable Vision Generation",
      "abstract": "The most advanced diffusion models have recently adopted increasingly deep stacked networks (e.g., U-Net or Transformer) to promote the generative emergence capabilities of vision generation models similar to large language models (LLMs). However, progressively deeper stacked networks will intuitively cause numerical propagation errors and reduce noisy prediction capabilities on generative data, which hinders massively deep scalable training of vision generation models. In this paper, we first uncover the nature that neural networks being able to effectively perform generative denoising lies in the fact that the intrinsic residual unit has consistent dynamic property with the input signal's reverse diffusion process, thus supporting excellent generative abilities.\nAfterwards, we stand on the shoulders of two common types of deep stacked networks to propose a unified and massively scalable Neural Residual Diffusion Models framework (Neural-RDM for short), which is a simple yet meaningful change to the common architecture of deep generative networks by introducing a series of learnable gated residual parameters that conform to the generative dynamics. Experimental results on various generative tasks show that the proposed neural residual models obtain state-of-the-art scores on image's and video's generative benchmarks. Rigorous theoretical proofs and extensive experiments also demonstrate the advantages of this simple gated residual mechanism consistent with dynamic modeling in improving the  fidelity and consistency of generated content and supporting large-scale scalable training.",
      "authors": [
        "Zhiyuan Ma",
        "Liangliang Zhao",
        "Biqing Qi",
        "Bowen Zhou"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NG16csOmcA",
      "cdate": 1715783044710,
      "mdate": 1730873987242,
      "matched_keywords": [
        "large language model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445506"
    },
    {
      "id": "Hz6cSigMyU",
      "title": "Reinforcing LLM Agents via Policy Optimization with Action Decomposition",
      "abstract": "Language models as intelligent agents push the boundaries of sequential decision-making agents but struggle with limited knowledge of environmental dynamics and exponentially huge action space. Recent efforts like GLAM and TWOSOME manually constrain the action space to a restricted subset and employ reinforcement learning to align agents' knowledge with specific environments. However, they overlook fine-grained credit assignments for intra-action tokens, which is essential for efficient language agent optimization, and rely on human's prior knowledge to restrict action space. This paper proposes decomposing language agent optimization from the action level to the token level, offering finer supervision for each intra-action token and manageable optimization complexity in environments with unrestricted action spaces. Beginning with the simplification of flattening all actions, we theoretically explore the discrepancies between action-level optimization and this naive token-level optimization. We then derive the Bellman backup with Action Decomposition (BAD) to integrate credit assignments for both intra-action and inter-action tokens, effectively eliminating the discrepancies. Implementing BAD within the PPO algorithm, we introduce Policy Optimization with Action Decomposition (POAD). POAD benefits from a finer-grained credit assignment process and lower optimization complexity, leading to enhanced learning efficiency and generalization abilities in aligning language agents with interactive environments. We validate POAD across diverse testbeds, with results affirming the advantages of our approach and the correctness of our theoretical analysis. The source code can be accessed directly with this link: https://github.com/morning9393/ADRL.",
      "authors": [
        "Muning Wen",
        "Ziyu Wan",
        "Jun Wang",
        "Weinan Zhang",
        "Ying Wen"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Hz6cSigMyU",
      "cdate": 1715782932676,
      "mdate": 1730873987111,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445511"
    },
    {
      "id": "bJddXCyosA",
      "title": "VisMin: Visual Minimal-Change Understanding",
      "abstract": "Fine-grained understanding of objects, attributes, and relationships between objects is crucial for visual-language models (VLMs). To evaluate VLMs' fine-grained understanding, existing benchmarks primarily focus on evaluating VLMs' capability to distinguish between two very similar captions given an image. In this paper, our focus is on evaluating VLMs' capability to distinguish between two very similar images given a caption. To this end, we introduce a new, challenging benchmark termed Visual Minimal-Change Understanding (VisMin), which requires models to predict the correct image-caption match given two images and two captions. Importantly, the image pair (as well as the caption pair) contains minimal changes, i.e., between the two images (as well as between the two captions), only one aspect changes at a time from among the following possible types of changes: object, attribute, count, and spatial relation. These four types of minimal changes are specifically designed to test the models' understanding of objects, attributes of objects (such as color, material, shape), counts of objects, and spatial relationships between objects. To curate our benchmark, we built an automatic pipeline using large language models and diffusion models, followed by a rigorous 4-step verification process by human annotators. Empirical experiments reveal that current VLMs exhibit notable deficiencies in understanding spatial relationships and counting abilities. Furthermore, leveraging the automated nature of our data creation process, we generate a large-scale training dataset, which we use to finetune CLIP (a foundational VLM) and Idefics2 (a multimodal large language model). Our findings show that both these models benefit significantly from fine-tuning on this data, as evident by marked improvements in fine-grained understanding across a wide range of benchmarks. Additionally, such fine-tuning improves CLIP's general image-text alignment capabilities too. All resources including the benchmark, the training data, and the finetuned model checkpoints will be released.",
      "authors": [
        "Rabiul Awal",
        "Saba Ahmadi",
        "Le Zhang",
        "Aishwarya Agrawal"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=bJddXCyosA",
      "cdate": 1715782884656,
      "mdate": 1730873987139,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.445516"
    },
    {
      "id": "b8jwgZrAXG",
      "title": "MatrixNet: Learning over symmetry groups using learned group representations",
      "abstract": "Group theory has been used in machine learning to provide a theoretically grounded approach for incorporating known symmetry transformations in tasks from robotics to protein modeling. In these applications, equivariant neural networks use known\nsymmetry groups with predefined representations to learn over geometric input data. We propose MatrixNet, a neural network architecture that learns matrix representations of group element inputs instead of using predefined representations. MatrixNet achieves higher sample efficiency and generalization over several standard baselines in prediction tasks over the several finite groups and the Artin braid group. We also show that MatrixNet respects group relations allowing generalization to group elements of greater word length than in the training set. Our code is available at https://github.com/lucas-laird/MatrixNet.",
      "authors": [
        "Lucas Laird",
        "Circe Hsu",
        "Asilata Bapat",
        "Robin Walters"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=b8jwgZrAXG",
      "cdate": 1715782598848,
      "mdate": 1730873986795,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445520"
    },
    {
      "id": "BROvXhmzYK",
      "title": "SELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures",
      "abstract": "We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2’s performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share commonalities with human reasoning patterns.",
      "authors": [
        "Pei Zhou",
        "Jay Pujara",
        "Xiang Ren",
        "Xinyun Chen",
        "Heng-Tze Cheng",
        "Quoc V Le",
        "Ed H. Chi",
        "Denny Zhou",
        "Swaroop Mishra",
        "Steven Zheng"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BROvXhmzYK",
      "cdate": 1715782568812,
      "mdate": 1730873986722,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445525"
    },
    {
      "id": "xojbzSYIVS",
      "title": "LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential Recommendation",
      "abstract": "Sequential recommender systems (SRS) aim to predict users' subsequent choices based on their historical interactions and have found applications in diverse fields such as e-commerce and social media. However, in real-world systems, most users interact with only a handful of items, while the majority of items are seldom consumed. These two issues, known as the long-tail user and long-tail item challenges, often pose difficulties for existing SRS. These challenges can adversely affect user experience and seller benefits, making them crucial to address. Though a few works have addressed the challenges, they still struggle with the seesaw or noisy issues due to the intrinsic scarcity of interactions. The advancements in large language models (LLMs) present a promising solution to these problems from a semantic perspective. As one of the pioneers in this field, we propose the Large Language Models Enhancement framework for Sequential Recommendation (LLM-ESR). This framework utilizes semantic embeddings derived from LLMs to enhance SRS without adding extra inference load. To address the long-tail item challenge, we design a dual-view modeling framework that combines semantics from LLMs and collaborative signals from conventional SRS. For the long-tail user challenge, we propose a retrieval augmented self-distillation method to enhance user preference representation using more informative interactions from similar users. To verify the effectiveness and versatility of our proposed enhancement framework, we conduct extensive experiments on three real-world datasets using three popular SRS models. The results consistently show that our method surpasses existing baselines. The implementation code is available in Supplementary Material.",
      "authors": [
        "Qidong Liu",
        "Xian Wu",
        "Yejing Wang",
        "Zijian Zhang",
        "Feng Tian",
        "Yefeng Zheng",
        "Xiangyu Zhao"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xojbzSYIVS",
      "cdate": 1715782512496,
      "mdate": 1735802568900,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445531"
    },
    {
      "id": "Luxk3z1tSG",
      "title": "Can Graph Neural Networks Expose Training Data Properties? An Efficient Risk Assessment Approach",
      "abstract": "Graph neural networks (GNNs) have attracted considerable attention due to their diverse applications. However, the scarcity and quality limitations of graph data present challenges to their training process in practical settings. To facilitate the development of effective GNNs, companies and researchers often seek external collaboration. Yet, directly sharing data raises privacy concerns, motivating data owners to train GNNs on their private graphs and share the trained models. Unfortunately, these models may still inadvertently disclose sensitive properties of their training graphs (\\textit{e.g.}, average default rate in a transaction network), leading to severe consequences for data owners. \nIn this work, we study graph property inference attack to identify the risk of sensitive property information leakage from shared models.\nExisting approaches typically train numerous shadow models for developing such attack, which is computationally intensive and impractical. To address this issue, we propose an efficient graph property inference attack by leveraging model approximation techniques. Our method only requires training a small set of models on graphs, while generating a sufficient number of approximated shadow models for attacks.\nTo enhance diversity while reducing errors in the approximated models, we apply edit distance to quantify the diversity within a group of approximated models and introduce a theoretically guaranteed criterion to evaluate each model's error. Subsequently, we propose a novel selection mechanism to ensure that the retained approximated models achieve high diversity and low error.\nExtensive experiments across six real-world scenarios demonstrate our method's substantial improvement, with average increases of 2.7\\% in attack accuracy and 4.1\\% in ROC-AUC, while being 6.5$\\times$ faster compared to the best baseline.",
      "authors": [
        "Hanyang Yuan",
        "Jiarong Xu",
        "Renhong Huang",
        "Mingli Song",
        "Chunping Wang",
        "Yang Yang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Luxk3z1tSG",
      "cdate": 1715782413577,
      "mdate": 1730873986625,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445536"
    },
    {
      "id": "4t3ox9hj3z",
      "title": "When are dynamical systems learned from time series data statistically accurate?",
      "abstract": "Conventional notions of generalization often fail to describe the ability of learned models to capture meaningful information from dynamical data. A neural network that learns complex dynamics with a small test error may still fail to reproduce its \\emph{physical} behavior, including associated statistical moments and Lyapunov exponents. To address this gap, we propose an ergodic theoretic approach to generalization of complex dynamical models learned from time series data. Our main contribution is to define and analyze generalization of a broad suite of neural representations of classes of ergodic systems, including chaotic systems, in a way that captures emulating underlying invariant, physical measures. Our results provide theoretical justification for why regression methods for generators of dynamical systems (Neural ODEs) fail to generalize, and why their statistical accuracy improves upon adding Jacobian information during training. We verify our results on a number of ergodic chaotic systems and neural network parameterizations, including MLPs, ResNets, Fourier Neural layers, and RNNs.",
      "authors": [
        "Jeongjin Park",
        "Nicole Tianjiao Yang",
        "Nisha Chandramoorthy"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4t3ox9hj3z",
      "cdate": 1715782342664,
      "mdate": 1730873986578,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445542"
    },
    {
      "id": "eFrdRuyHR9",
      "title": "Transition Constrained Bayesian Optimization via Markov Decision Processes",
      "abstract": "Bayesian optimization is a methodology to optimize black-box functions. Traditionally, it focuses on the setting where you can arbitrarily query the search space. However, many real-life problems do not offer this flexibility; in particular, the search space of the next query may depend on previous ones. Example challenges arise in the physical sciences in the form of local movement constraints, required monotonicity in certain variables, and transitions influencing the accuracy of measurements. Altogether, such *transition constraints* necessitate a form of planning. This work extends classical Bayesian optimization via the framework of Markov Decision Processes. We iteratively solve a tractable linearization of our utility function using reinforcement learning to obtain a policy that plans ahead for the entire horizon. This is a parallel to the optimization of an *acquisition function in policy space*. The resulting policy is potentially history-dependent and non-Markovian. We showcase applications in chemical reactor optimization, informative path planning, machine calibration, and other synthetic examples.",
      "authors": [
        "Jose Pablo Folch",
        "Calvin Tsay",
        "Robert Matthew Lee",
        "Behrang Shafei",
        "Weronika Ormaniec",
        "Andreas Krause",
        "Mark van der Wilk",
        "Ruth Misener",
        "Mojmir Mutny"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=eFrdRuyHR9",
      "cdate": 1715781949412,
      "mdate": 1730873986336,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445547"
    },
    {
      "id": "3uI4ceR4iz",
      "title": "SA3DIP: Segment Any 3D Instance with Potential 3D Priors",
      "abstract": "The proliferation of 2D foundation models has sparked research into adapting them for open-world 3D instance segmentation. Recent methods introduce a paradigm that leverages superpoints as geometric primitives and incorporates 2D multi-view masks from Segment Anything model (SAM) as merging guidance, achieving outstanding zero-shot instance segmentation results. However, the limited use of 3D priors restricts the segmentation performance. Previous methods calculate the 3D superpoints solely based on estimated normal from spatial coordinates, resulting in under-segmentation for instances with similar geometry. Besides, the heavy reliance on SAM and hand-crafted algorithms in 2D space suffers from over-segmentation due to SAM's inherent part-level segmentation tendency. To address these issues, we propose SA3DIP, a novel method for Segmenting Any 3D Instances via exploiting potential 3D Priors. Specifically, on one hand, we generate complementary 3D primitives based on both geometric and textural priors, which reduces the initial errors that accumulate in subsequent procedures. On the other hand, we introduce supplemental constraints from the 3D space by using a 3D detector to guide a further merging process. Furthermore, we notice a considerable portion of low-quality ground truth annotations in ScanNetV2 benchmark, which affect the fair evaluations. Thus, we present ScanNetV2-INS with complete ground truth labels and supplement additional instances for 3D class-agnostic instance segmentation. Experimental evaluations on various 2D-3D datasets demonstrate the effectiveness and robustness of our approach. Our code and proposed ScanNetV2-INS dataset are available HERE.",
      "authors": [
        "Xi Yang",
        "Xu Gu",
        "Xingyilang Yin",
        "Xinbo Gao"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3uI4ceR4iz",
      "cdate": 1715781755996,
      "mdate": 1730873986184,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445552"
    },
    {
      "id": "RB1F2h5YEx",
      "title": "Parseval Regularization for Continual Reinforcement Learning",
      "abstract": "Plasticity loss, trainability loss, and primacy bias have been identified as issues arising when training deep neural networks on sequences of tasks---referring to the increased difficulty in training on new tasks.\nWe propose to use Parseval regularization, which maintains orthogonality of weight matrices, to preserve useful optimization properties and improve training in a continual reinforcement learning setting.\nWe show that it provides significant benefits to RL agents on a suite of gridworld, CARL and MetaWorld tasks.\nWe conduct comprehensive ablations to identify the source of its benefits and investigate the effect of certain metrics associated to network trainability including weight matrix rank, weight norms and policy entropy.",
      "authors": [
        "Wesley Chung",
        "Lynn Cherif",
        "Doina Precup",
        "David Meger"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=RB1F2h5YEx",
      "cdate": 1715781726616,
      "mdate": 1730873986096,
      "matched_keywords": [
        "reinforcement learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445557"
    },
    {
      "id": "EfpZNpkrm2",
      "title": "QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation",
      "abstract": "We propose **Quan**tum-informed **T**ensor **A**daptation (**QuanTA**), a novel, easy-to-implement, fine-tuning method with no inference overhead for large-scale pre-trained language models. By leveraging quantum-inspired methods derived from quantum circuit structures, QuanTA enables efficient *high-rank* fine-tuning, surpassing the limitations of Low-Rank Adaptation (LoRA)---low-rank approximation may fail for complicated downstream tasks. Our approach is theoretically supported by the universality theorem and the rank representation theorem to achieve efficient high-rank adaptations. Experiments demonstrate that QuanTA significantly enhances commonsense reasoning, arithmetic reasoning, and scalability compared to traditional methods. Furthermore, QuanTA shows superior performance with fewer trainable parameters compared to other approaches and can be designed to integrate with existing fine-tuning algorithms for further improvement, providing a scalable and efficient solution for fine-tuning large language models and advancing state-of-the-art in natural language processing.",
      "authors": [
        "Zhuo Chen",
        "Rumen Dangovski",
        "Charlotte Loh",
        "Owen M Dugan",
        "Di Luo",
        "Marin Soljacic"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EfpZNpkrm2",
      "cdate": 1715781723395,
      "mdate": 1736963754168,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445563"
    },
    {
      "id": "ni3Ud2BV3G",
      "title": "On the Impacts of the Random Initialization in the Neural Tangent Kernel Theory",
      "abstract": "This paper aims to discuss the impact of random initialization of neural networks in the neural tangent kernel (NTK) theory, which is ignored by most recent works in the NTK theory. It is well known that as the network's width tends to infinity, the neural network with random initialization converges to a Gaussian process \\(f^{\\mathrm{GP}}\\), which takes values in \\(L^{2}(\\mathcal{X})\\), where \\(\\mathcal{X}\\) is the domain of the data. In contrast, to adopt the traditional theory of kernel regression, most recent works introduced a special mirrored architecture and a mirrored (random) initialization to ensure the network's output is identically zero at initialization. Therefore, it remains a question whether the conventional setting and mirrored initialization would make wide neural networks exhibit different generalization capabilities. In this paper, we first show that the training dynamics of the gradient flow of neural networks with random initialization converge uniformly to that of the corresponding NTK regression with random initialization \\(f^{\\mathrm{GP}}\\). We then show that \\(\\mathbf{P}(f^{\\mathrm{GP}} \\in [\\mathcal{H}^{\\mathrm{NT}}]^{s}) = 1\\) for any \\(s < \\frac{3}{d+1}\\) and \\(\\mathbf{P}(f^{\\mathrm{GP}} \\in [\\mathcal{H}^{\\mathrm{NT}}]^{s}) = 0\\) for any \\(s \\geq \\frac{3}{d+1}\\), where \\([\\mathcal{H}^{\\mathrm{NT}}]^{s}\\) is the real interpolation space of the RKHS \\(\\mathcal{H}^{\\mathrm{NT}}\\) associated with the NTK. Consequently, the generalization error of the wide neural network trained by gradient descent is \\(\\Omega(n^{-\\frac{3}{d+3}})\\), and it still suffers from the curse of dimensionality. Thus, the NTK theory may not explain the superior performance of neural networks.",
      "authors": [
        "Guhan Chen",
        "Yicheng Li",
        "Qian Lin"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ni3Ud2BV3G",
      "cdate": 1715781569192,
      "mdate": 1737117478489,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445567"
    },
    {
      "id": "ORQiboaRqY",
      "title": "On the Power of Small-size Graph Neural Networks for Linear Programming",
      "abstract": "Graph neural networks (GNNs) have recently emerged as powerful tools for addressing complex optimization problems. It has been theoretically demonstrated that GNNs can universally approximate the solution mapping functions of linear programming (LP) problems. However, these theoretical results typically require GNNs to have large parameter sizes. Conversely, empirical experiments have shown that relatively small GNNs can solve LPs effectively, revealing a significant discrepancy between theoretical predictions and practical observations. In this work, we aim to bridge this gap by providing a theoretical foundation for the effectiveness of small-size GNNs. We prove that polylogarithmic-depth, constant-width GNNs are sufficient to solve packing and covering LPs, two widely used classes of LPs. Our proof leverages the capability of GNNs to simulate a variant of the gradient descent algorithm on a carefully selected potential function. Additionally, we introduce a new GNN architecture, termed GD-Net. Experimental results demonstrate that GD-Net significantly outperforms conventional GNN structures while using fewer parameters.",
      "authors": [
        "Qian Li",
        "Tian Ding",
        "Linxin Yang",
        "Minghui Ouyang",
        "Qingjiang Shi",
        "Ruoyu Sun"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ORQiboaRqY",
      "cdate": 1715781539762,
      "mdate": 1734688624801,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445573"
    },
    {
      "id": "44WWOW4GPF",
      "title": "Learning symmetries via weight-sharing with doubly stochastic tensors",
      "abstract": "Group equivariance has emerged as a valuable inductive bias in deep learning, enhancing generalization, data efficiency, and robustness. Classically, group equivariant methods require the groups of interest to be known beforehand, which may not be realistic for real-world data. Additionally, baking in fixed group equivariance may impose overly restrictive constraints on model architecture. This highlights the need for methods that can dynamically discover and apply symmetries as soft constraints. For neural network architectures, equivariance is commonly achieved through group transformations of a canonical weight tensor, resulting in weight sharing over a given group $G$. In this work, we propose to *learn* such a weight-sharing scheme by defining a collection of learnable doubly stochastic matrices that act as soft permutation matrices on canonical weight tensors, which can take regular group representations as a special case. This yields learnable kernel transformations that are jointly optimized with downstream tasks. We show that when the dataset exhibits strong symmetries, the permutation matrices will converge to regular group representations and our weight-sharing networks effectively become regular group convolutions. Additionally, the flexibility of the method enables it to effectively pick up on partial symmetries.",
      "authors": [
        "Putri A Van der Linden",
        "Alejandro García Castellanos",
        "Sharvaree Vadgama",
        "Thijs P. Kuipers",
        "Erik J Bekkers"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=44WWOW4GPF",
      "cdate": 1715781412325,
      "mdate": 1737123729248,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445578"
    },
    {
      "id": "8koaqRdRYH",
      "title": "Improving Neural Network Surface Processing with Principal Curvatures",
      "abstract": "The modern study and use of surfaces is a research topic grounded in centuries of mathematical and empirical inquiry. From a mathematical point of view, curvature is an invariant that characterises the intrinsic geometry and the extrinsic shape of a surface. Yet, in modern applications the focus has shifted away from finding expressive representations of surfaces, and towards the design of efficient neural network architectures to process them. The literature suggests a tendency to either overlook the representation of the processed surface, or use overcomplicated representations whose ability to capture the essential features of a surface is opaque. We propose using curvature as the input of neural network architectures for surface processing, and explore this proposition through experiments making use of the shape operator. Our results show that using curvature as input leads to significant a increase in performance on segmentation and classification tasks, while allowing far less computational overhead than current methods.",
      "authors": [
        "Josquin Harrison",
        "James Benn",
        "Maxime Sermesant"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=8koaqRdRYH",
      "cdate": 1715781359848,
      "mdate": 1730873985631,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445583"
    },
    {
      "id": "LvAy07mCxU",
      "title": "The Surprising Ineffectiveness of Pre-Trained Visual Representations for Model-Based Reinforcement Learning",
      "abstract": "Visual Reinforcement Learning (RL) methods often require extensive amounts of data. As opposed to model-free RL, model-based RL (MBRL) offers a potential solution with efficient data utilization through planning. Additionally, RL lacks generalization capabilities for real-world tasks. Prior work has shown that incorporating pre-trained visual representations (PVRs) enhances sample efficiency and generalization. While PVRs have been extensively studied in the context of model-free RL, their potential in MBRL remains largely unexplored. In this paper, we benchmark a set of PVRs on challenging control tasks in a model-based RL setting. We investigate the data efficiency, generalization capabilities, and the impact of different properties of PVRs on the performance of model-based agents. Our results, perhaps surprisingly, reveal that for MBRL current PVRs are not more sample efficient than learning representations from scratch, and that they do not generalize better to out-of-distribution (OOD) settings. To explain this, we analyze the quality of the trained dynamics model. Furthermore, we show that data diversity and network architecture are the most important contributors to OOD generalization performance.",
      "authors": [
        "Moritz Schneider",
        "Robert Krug",
        "Narunas Vaskevicius",
        "Luigi Palmieri",
        "Joschka Boedecker"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LvAy07mCxU",
      "cdate": 1715781054323,
      "mdate": 1736953732125,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445589"
    },
    {
      "id": "aBtcfcrjM3",
      "title": "Geometry-aware training of factorized layers in tensor Tucker format",
      "abstract": "Reducing parameter redundancies in neural network architectures is crucial for achieving feasible computational and memory requirements during train and inference of large networks. Given its easy implementation and flexibility, one promising approach is layer factorization, which reshapes weight tensors into a matrix format and parameterizes it as the product of two rank-r matrices. However, this family of approaches often requires an initial full-model warm-up phase, prior knowledge of a feasible rank, and it is sensitive to parameter initialization.\nIn this work, we introduce a novel approach to train the factors of a Tucker decomposition of the weight tensors. Our training proposal proves to be optimal in locally approximating the original unfactorized dynamics and stable for the initialization. Furthermore, the rank of each mode is dynamically updated during training.\nWe provide a theoretical analysis of the algorithm, showing convergence, approximation and local descent guarantees. The method's performance is further illustrated through a variety of experiments, showing remarkable training compression rates and comparable or even better performance than the full baseline and alternative layer factorization strategies.",
      "authors": [
        "Emanuele Zangrando",
        "Steffen Schotthöfer",
        "Gianluca Ceruti",
        "Jonas Kusch",
        "Francesco Tudisco"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=aBtcfcrjM3",
      "cdate": 1715781051504,
      "mdate": 1730873985491,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445594"
    },
    {
      "id": "g1HxCIc0wi",
      "title": "Speculative Monte-Carlo Tree Search",
      "abstract": "Monte-Carlo tree search (MCTS) is an influential sequential decision-making algorithm notably employed in AlphaZero. Despite its success, the primary challenge in AlphaZero training lies in its prolonged time-to-solution due to the high latency imposed by the sequential MCTS process. To address this challenge, this paper proposes and evaluates an inter-decision parallelization strategy called speculative MCTS, a new type of parallelism in AlphaZero which implements speculative execution. This approach allows for the parallel execution of future moves before the current MCTS computations are completed, thus reducing the latency. Additionally, we analyze factors contributing to the overall speedup by studying the synergistic effects of speculation and neural network caching in MCTS. We also provide an analytical model that can be used to evaluate the potential of different speculation strategies before they are implemented and deployed. Our empirical findings indicate that the proposed speculative MCTS can reduce training latency by 5.81$\\times$ in 9x9 Go games. Moreover, our study shows that speculative execution can enhance the NN cache hit rate by 26\\% during midgame. Overall, our end-to-end evaluation indicates 1.91$\\times$ speedup in 19x19 Go training time, compared to the state-of-the-art KataGo program.",
      "authors": [
        "Scott Cheng",
        "Mahmut Kandemir",
        "Ding-Yong Hong"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=g1HxCIc0wi",
      "cdate": 1715781037717,
      "mdate": 1730873985406,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445599"
    },
    {
      "id": "kZ4Kc5GhGB",
      "title": "Rethinking Model-based, Policy-based, and Value-based Reinforcement Learning via the Lens of Representation Complexity",
      "abstract": "Reinforcement Learning (RL) encompasses diverse paradigms, including model-based RL, policy-based RL, and value-based RL, each tailored to approximate the model, optimal policy, and optimal value function, respectively. This work investigates the potential hierarchy of representation complexity among these RL paradigms. By utilizing computational complexity measures, including time complexity and circuit complexity, we theoretically unveil a potential representation complexity hierarchy within RL. We find that representing the model emerges as the easiest task, followed by the optimal policy, while representing the optimal value function presents the most intricate challenge. Additionally, we reaffirm this hierarchy from the perspective of the expressiveness of Multi-Layer Perceptrons (MLPs), which align more closely with practical deep RL and contribute to a completely new perspective in theoretical studying representation complexity in RL. Finally, we conduct deep RL experiments to validate our theoretical findings.",
      "authors": [
        "Guhao Feng",
        "Han Zhong"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kZ4Kc5GhGB",
      "cdate": 1715781020728,
      "mdate": 1730873985388,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445603"
    },
    {
      "id": "uHml6eyoVF",
      "title": "Learning from higher-order correlations, efficiently: hypothesis tests, random features, and neural networks",
      "abstract": "Neural networks excel at discovering statistical patterns in\nhigh-dimensional data sets. In practice, higher-order cumulants, which quantify\nthe non-Gaussian correlations between three or more variables, are particularly\nimportant for the performance of neural networks. But how efficient are neural\nnetworks at extracting features from higher-order cumulants? We study this\nquestion in the spiked cumulant model, where the statistician needs to recover a\nprivileged direction or \"spike'' from the order-$p\\ge 4$ cumulants\nof $d$-dimensional inputs. \nWe first discuss the fundamental statistical and\ncomputational limits of recovering the spike by analysing the number of\n samples $n$ required to strongly distinguish between inputs from the spiked\ncumulant model and isotropic Gaussian inputs. \nExisting literature established the presence of a wide statistical-to-computational gap in this problem. We deepen this line of work by finding an exact formula for the likelihood ratio norm which proves that statistical\ndistinguishability requires $n\\gtrsim d$ samples, while distinguishing the two\ndistributions in polynomial time requires $n \\gtrsim d^2$ samples for a wide\nclass of algorithms, i.e. those covered by the low-degree conjecture. \nNumerical experiments show that neural networks do indeed learn to distinguish\nthe two distributions with quadratic sample complexity, while ``lazy'' methods\nlike random features are not better than random guessing in this regime. Our\nresults show that neural networks extract information from higher-order\ncorrelations in the spiked cumulant model efficiently, and reveal a large gap in\nthe amount of data required by neural networks and random features to learn from\nhigher-order cumulants.",
      "authors": [
        "Eszter Szekely",
        "Lorenzo Bardone",
        "Federica Gerace",
        "Sebastian Goldt"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=uHml6eyoVF",
      "cdate": 1715781016449,
      "mdate": 1730873985337,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445608"
    },
    {
      "id": "1vPqOmqSfO",
      "title": "Sketched Lanczos uncertainty score: a low-memory summary of the Fisher information",
      "abstract": "Current uncertainty quantification is memory and compute expensive, which hinders practical uptake. To counter, we develop Sketched Lanczos Uncertainty (SLU): an architecture-agnostic uncertainty score that can be applied to pre-trained neural networks with minimal overhead. Importantly, the memory use of SLU only grows logarithmically with the number of model parameters. We combine Lanczos' algorithm with dimensionality reduction techniques to compute a sketch of the leading eigenvectors of a matrix. Applying this novel algorithm to the Fisher information matrix yields a cheap and reliable uncertainty score. Empirically, SLU yields well-calibrated uncertainties, reliably detects out-of-distribution examples, and consistently outperforms existing methods in the low-memory regime.",
      "authors": [
        "Marco Miani",
        "Lorenzo Beretta",
        "Søren Hauberg"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=1vPqOmqSfO",
      "cdate": 1715780878640,
      "mdate": 1731897496280,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.445613"
    },
    {
      "id": "Cp7HD618bd",
      "title": "A Metalearned Neural Circuit for Nonparametric Bayesian Inference",
      "abstract": "Most applications of machine learning to classification assume a closed set of balanced classes. This is at odds with the real world, where class occurrence statistics often follow a long-tailed power-law distribution and it is unlikely that all classes are seen in a single sample. Nonparametric Bayesian models naturally capture this phenomenon, but have significant practical barriers to widespread adoption, namely implementation complexity and computational inefficiency. To address this, we present a method for extracting the inductive bias from a nonparametric Bayesian model and transferring it to an artificial neural network. By simulating data with a nonparametric Bayesian prior, we can metalearn a sequence model that performs inference over an unlimited set of classes. After training, this \"neural circuit\" has distilled the corresponding inductive bias and can successfully perform sequential inference over an open set of classes. Our experimental results show that the metalearned neural circuit achieves comparable or better performance than particle filter-based methods for inference in these models while being faster and simpler to use than methods that explicitly incorporate Bayesian nonparametric inference.",
      "authors": [
        "Jake Snell",
        "Gianluca Bencomo",
        "Thomas L. Griffiths"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Cp7HD618bd",
      "cdate": 1715780817844,
      "mdate": 1730873985254,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445618"
    },
    {
      "id": "GtEmIzLZmR",
      "title": "Achievable Fairness on Your Data With Utility Guarantees",
      "abstract": "In machine learning fairness, training models that minimize disparity across different sensitive groups often leads to diminished accuracy, a phenomenon known as the fairness-accuracy trade-off. The severity of this trade-off inherently depends on dataset characteristics such as dataset imbalances or biases and therefore, using a uniform fairness requirement across diverse datasets remains questionable. To address this, we present a computationally efficient approach to approximate the fairness-accuracy trade-off curve tailored to individual datasets, backed by rigorous statistical guarantees. By utilizing the You-Only-Train-Once (YOTO) framework, our approach mitigates the computational burden of having to train multiple models when approximating the trade-off curve. Crucially, we introduce a novel methodology for quantifying uncertainty in our estimates, thereby providing practitioners with a robust framework for auditing model fairness while avoiding false conclusions due to estimation errors. Our experiments spanning tabular (e.g., Adult), image (CelebA), and language (Jigsaw) datasets underscore that our approach not only reliably quantifies the optimum achievable trade-offs across various data modalities but also helps detect suboptimality in SOTA fairness methods.",
      "authors": [
        "Muhammad Faaiz Taufiq",
        "Jean-Francois Ton",
        "Yang Liu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GtEmIzLZmR",
      "cdate": 1715780774369,
      "mdate": 1730873985183,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445624"
    },
    {
      "id": "aaUVnpQvbZ",
      "title": "Learning Elastic Costs to Shape Monge Displacements",
      "abstract": "Given a source and a target probability measure, the Monge problem studies efficient ways to map the former onto the latter.\nThis efficiency is quantified by defining a *cost* function between source and target data. \nSuch a cost is often set by default in the machine learning literature to the squared-Euclidean distance, $\\ell^2\\_2(\\mathbf{x},\\mathbf{y}):=\\tfrac12\\|\\mathbf{x}-\\mathbf{y}\\|\\_2^2$.\nThe benefits of using *elastic* costs, defined using a regularizer $\\tau$ as $c(\\mathbf{x},\\mathbf{y}):=\\ell^2_2(\\mathbf{x},\\mathbf{y})+\\tau(\\mathbf{x}-\\mathbf{y})$, was recently highlighted in (Cuturi et al. 2023). Such costs shape the *displacements* of Monge maps $T$, namely the difference between a source point and its image $T(\\mathbf{x})-\\mathbf{x}$, by giving them a structure that matches that of the proximal operator of $\\tau$.\nIn this work, we make two important contributions to the study of elastic costs:*(i)* For any elastic cost, we propose a numerical method to compute Monge maps that are provably optimal. This provides a much-needed routine to create synthetic problems where the ground-truth OT map is known, by analogy to the Brenier theorem, which states that the gradient of any convex potential is always a valid Monge map for the $\\ell_2^2$ cost; *(ii)* We propose a loss to *learn* the parameter $\\theta$ of a parameterized regularizer $\\tau_\\theta$, and apply it in the case where $\\tau_{A}({\\bf z}):=\\|A^\\perp {\\bf z}\\|^2_2$. This regularizer promotes displacements that lie on a low-dimensional subspace of $\\mathbb{R}^d$, spanned by the $p$ rows of $A\\in\\mathbb{R}^{p\\times d}$. We illustrate the soundness of our procedure on synthetic data, generated using our first contribution, in which we show near-perfect recovery of $A$'s subspace using only samples. We demonstrate the applicability of this method by showing predictive improvements on single-cell data tasks.",
      "authors": [
        "Michal Klein",
        "Aram-Alexandre Pooladian",
        "Pierre Ablin",
        "Eugene Ndiaye",
        "Jonathan Niles-Weed",
        "marco cuturi"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=aaUVnpQvbZ",
      "cdate": 1715780563960,
      "mdate": 1737980413589,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445629"
    },
    {
      "id": "GZnsqBwHAG",
      "title": "Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models",
      "abstract": "Safety alignment is crucial to ensure that large language models (LLMs) behave in ways that align with human preferences and prevent harmful actions during inference. However, recent studies show that the alignment can be easily compromised through finetuning with only a few adversarially designed training examples. We aim to measure the risks in finetuning LLMs through navigating the LLM safety landscape. We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as “safety basin”: random perturbations to model weights maintain the safety level of the original aligned model within its local neighborhood. However, outside this local region, safety is fully compromised, exhibiting a sharp, step-like drop. This safety basin contrasts sharply with the LLM capability landscape, where model performance peaks at the origin and gradually declines as random perturbation increases. Our discovery inspires us to propose the new VISAGE safety metric that measures the safety in LLM finetuning by probing its safety landscape. Visualizing the safety landscape of the aligned model enables us to understand how finetuning compromises safety by dragging the model away from the safety basin. The LLM safety landscape also highlights the system prompt’s critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin. These observations from our safety landscape research provide new\ninsights for future work on LLM safety community. Our code is publicly available at https://github.com/ShengYun-Peng/llm-landscape.",
      "authors": [
        "ShengYun Peng",
        "Pin-Yu Chen",
        "Matthew Daniel Hull",
        "Duen Horng Chau"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GZnsqBwHAG",
      "cdate": 1715780465217,
      "mdate": 1730873985027,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.445635"
    },
    {
      "id": "YfVMcbcDqo",
      "title": "Physics-Regularized Multi-Modal Image Assimilation for Brain Tumor Localization",
      "abstract": "Physical models in the form of partial differential equations serve as important priors for many under-constrained problems. One such application is tumor treatment planning, which relies on accurately estimating the spatial distribution of tumor cells within a patient’s anatomy. While medical imaging can detect the bulk of a tumor, it cannot capture the full extent of its spread, as low-concentration tumor cells often remain undetectable, particularly in glioblastoma, the most common primary brain tumor. Machine learning approaches struggle to estimate the complete tumor cell distribution due to a lack of appropriate training data. Consequently, most existing methods rely on physics-based simulations to generate anatomically and physiologically plausible estimations. However, these approaches face challenges with complex and unknown initial conditions and are constrained by overly rigid physical models. In this work, we introduce a novel method that integrates data-driven and physics-based cost functions, akin to Physics-Informed Neural Networks (PINNs). However, our approach parametrizes the solution directly on a dynamic discrete mesh, allowing for the effective modeling of complex biomechanical behaviors. Specifically, we propose a unique discretization scheme that quantifies how well the learned spatiotemporal distributions of tumor and brain tissues adhere to their respective growth and elasticity equations. This quantification acts as a regularization term, offering greater flexibility and improved integration of patient data compared to existing models. We demonstrate enhanced coverage of tumor recurrence areas using real-world data from a patient cohort, highlighting the potential of our method to improve model-driven treatment planning for glioblastoma in clinical practice.",
      "authors": [
        "Michal Balcerak",
        "Tamaz Amiranashvili",
        "Andreas Wagner",
        "Jonas Weidner",
        "Petr Karnakov",
        "Johannes C. Paetzold",
        "Ivan Ezhov",
        "Petros Koumoutsakos",
        "Benedikt Wiestler",
        "bjoern menze"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=YfVMcbcDqo",
      "cdate": 1715779737834,
      "mdate": 1730873984686,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.445640"
    },
    {
      "id": "8iytZCnXIu",
      "title": "BricksRL: A Platform for Democratizing Robotics and  Reinforcement Learning Research and Education with LEGO",
      "abstract": "We present BricksRL, a platform designed to democratize access to robotics for reinforcement learning research and education. BricksRL facilitates the creation, design, and training of custom LEGO robots in the real world by interfacing them with the TorchRL library for reinforcement learning agents. The integration of TorchRL with the LEGO hubs, via Bluetooth bidirectional communication, enables state-of-the-art reinforcement learning training on GPUs for a wide variety of LEGO builds. This offers a flexible and cost-efficient approach for scaling and also provides a robust infrastructure for robot-environment-algorithm communication. We present various experiments across tasks and robot configurations, providing built plans and training results. Furthermore, we demonstrate that inexpensive LEGO robots can be trained end-to-end in the real world to achieve simple tasks, with training times typically under 120 minutes on a normal laptop. Moreover, we show how users can extend the capabilities, exemplified by the successful integration of non-LEGO sensors. By enhancing accessibility to both robotics and reinforcement learning, BricksRL establishes a strong foundation for democratized robotic learning in research and educational settings.",
      "authors": [
        "Sebastian Dittert",
        "Vincent Moens",
        "Gianni De Fabritiis"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=8iytZCnXIu",
      "cdate": 1715779535977,
      "mdate": 1730873984504,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446672"
    },
    {
      "id": "lflwtGE6Vf",
      "title": "(FL)$^2$: Overcoming Few Labels in Federated Semi-Supervised Learning",
      "abstract": "Federated Learning (FL) is a distributed machine learning framework that trains accurate global models while preserving clients' privacy-sensitive data. However, most FL approaches assume that clients possess labeled data, which is often not the case in practice. Federated Semi-Supervised Learning (FSSL) addresses this label deficiency problem, targeting situations where only the server has a small amount of labeled data while clients do not. However, a significant performance gap exists between Centralized Semi-Supervised Learning (SSL) and FSSL. This gap arises from confirmation bias, which is more pronounced in FSSL due to multiple local training epochs and the separation of labeled and unlabeled data. We propose $(FL)^2$, a robust training method for unlabeled clients using sharpness-aware consistency regularization. We show that regularizing the original pseudo-labeling loss is suboptimal, and hence we carefully select unlabeled samples for regularization. We further introduce client-specific adaptive thresholding and learning status-aware aggregation to adjust the training process based on the learning progress of each client. Our experiments on three benchmark datasets demonstrate that our approach significantly improves performance and bridges the gap with SSL, particularly in scenarios with scarce labeled data.",
      "authors": [
        "Seungjoo Lee",
        "Thanh-Long V. Le",
        "Jaemin Shin",
        "Sung-Ju Lee"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lflwtGE6Vf",
      "cdate": 1715779452110,
      "mdate": 1730873984438,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446683"
    },
    {
      "id": "fc88ANWvdF",
      "title": "Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices",
      "abstract": "Dense linear layers are the dominant computational bottleneck in large neural networks, presenting a critical need for more efficient alternatives. Previous efforts to develop alternatives have focused on a small number of hand-crafted structured matrices, and have neglected to investigate whether these structures can surpass dense layers in terms of compute-optimal scaling laws when both the model size and training examples are optimally allocated. In this work, we present a unifying framework that enables searching among all linear operators expressible via an Einstein summation. This framework encompasses many previously proposed structures, such as low-rank, Kronecker, Tensor-Train, and Monarch, along with many novel structures. We develop a taxonomy of all such operators based on their computational and algebraic properties, which provides insights into their scaling laws. Combining these insights with empirical evaluation, we identify a subset of structures that achieve equal or better performance than dense layers as a function of training compute. To further improve their compute efficiency, we develop a natural extension of these performant structures that convert them into a sparse Mixture-of-Experts layer. The resulting layer significantly outperforms dense layers in compute-optimal training efficiency for GPT-2 language models.",
      "authors": [
        "Andres Potapczynski",
        "Shikai Qiu",
        "Marc Anton Finzi",
        "Christopher Ferri",
        "Zixi Chen",
        "Micah Goldblum",
        "C. Bayan Bruss",
        "Christopher De Sa",
        "Andrew Gordon Wilson"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fc88ANWvdF",
      "cdate": 1715779438210,
      "mdate": 1730873984391,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.446688"
    },
    {
      "id": "BrPZMOQiSN",
      "title": "SequentialAttention++ for Block Sparsification: Differentiable Pruning Meets Combinatorial Optimization",
      "abstract": "Neural network pruning is a key technique towards engineering large yet scalable, interpretable, and generalizable models. Prior work on the subject has developed largely along two orthogonal directions: (1) differentiable pruning for efficiently and accurately scoring the importance of parameters, and (2) combinatorial optimization for efficiently searching over the space of sparse models. We unite the two approaches, both theoretically and empirically, to produce a coherent framework for structured neural network pruning in which differentiable pruning guides combinatorial optimization algorithms to select the most important sparse set of parameters. Theoretically, we show how many existing differentiable pruning techniques can be understood as nonconvex regularization for group sparse optimization, and prove that for a wide class of nonconvex regularizers, the global optimum is unique, group-sparse, and provably yields an approximate solution to a sparse convex optimization problem. The resulting algorithm that we propose, SequentialAttention++, advances the state of the art in large-scale neural network block-wise pruning tasks on the ImageNet and Criteo datasets.",
      "authors": [
        "Taisuke Yasuda",
        "Kyriakos Axiotis",
        "Gang Fu",
        "Mohammadhossein Bateni",
        "Vahab Mirrokni"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BrPZMOQiSN",
      "cdate": 1715779374920,
      "mdate": 1730873984338,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.446693"
    },
    {
      "id": "mhhlZeAr67",
      "title": "Reciprocal Learning",
      "abstract": "We demonstrate that numerous machine learning algorithms are specific instances of one single paradigm: reciprocal learning. These instances range from active learning over multi-armed bandits to self-training. We show that all these algorithms not only learn parameters from data but also vice versa: They iteratively alter training data in a way that depends on the current model fit. We introduce reciprocal learning as a generalization of these algorithms using the language of decision theory. This allows us to study under what conditions they converge. The key is to guarantee that reciprocal learning contracts such that the Banach fixed-point theorem applies. In this way, we find that reciprocal learning converges at linear rates to an approximately optimal model under some assumptions on the loss function, if their predictions are probabilistic and the sample adaption is both non-greedy and either randomized or regularized. We interpret these findings and provide corollaries that relate them to active learning, self-training, and bandits.",
      "authors": [
        "Julian Rodemann",
        "Christoph Jansen",
        "Georg Schollmeyer"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=mhhlZeAr67",
      "cdate": 1715779224112,
      "mdate": 1734690715036,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446698"
    },
    {
      "id": "XY2qrq7cXM",
      "title": "Gradient Rewiring for Editable Graph Neural Network Training",
      "abstract": "Deep neural networks are ubiquitously adopted in many applications, such as computer vision, natural language processing, and graph analytics. However, well-trained neural networks can make prediction errors after deployment as the world changes. \\textit{Model editing} involves updating the base model to correct prediction errors with less accessible training data and computational resources.\nDespite recent advances in model editors in computer vision and natural language processing, editable training in graph neural networks (GNNs) is rarely explored. The challenge with editable GNN training lies in the inherent information aggregation across neighbors, which can lead model editors to affect the predictions of other nodes unintentionally. In this paper, we first observe the gradient of cross-entropy loss for the target node and training nodes with significant inconsistency, which indicates that directly fine-tuning the base model using the loss on the target node deteriorates the performance on training nodes. Motivated by the gradient inconsistency observation, we propose a simple yet effective \\underline{G}radient \\underline{R}ewiring method for \\underline{E}ditable graph neural network training, named \\textbf{GRE}. Specifically, we first store the anchor gradient of the loss on training nodes to preserve the locality. Subsequently, we rewire the gradient of the loss on the target node to preserve performance on the training node using anchor gradient. Experiments demonstrate the effectiveness of GRE on various model architectures and graph datasets in terms of multiple editing situations. The source code is available at \\url{https://github.com/zhimengj0326/Gradient_rewiring_editing}.",
      "authors": [
        "Zhimeng Jiang",
        "Zirui Liu",
        "Xiaotian Han",
        "Qizhang Feng",
        "Hongye Jin",
        "Qiaoyu Tan",
        "Kaixiong Zhou",
        "Na Zou",
        "Xia Hu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=XY2qrq7cXM",
      "cdate": 1715779089388,
      "mdate": 1730873984136,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.446703"
    },
    {
      "id": "m296WJXyzQ",
      "title": "Scanning Trojaned Models Using Out-of-Distribution Samples",
      "abstract": "Scanning for trojan (backdoor) in deep neural networks is crucial due to their significant real-world applications. There has been an increasing focus on developing effective general trojan scanning methods across various trojan attacks. Despite advancements, there remains a shortage of methods that perform effectively without preconceived assumptions about the backdoor attack method. Additionally, we have observed that current methods struggle to identify classifiers trojaned using adversarial training. Motivated by these challenges, our study introduces a novel scanning method named TRODO (TROjan scanning by Detection of adversarial shifts in Out-of-distribution samples). TRODO leverages the concept of \"blind spots\"—regions where trojaned classifiers erroneously identify out-of-distribution (OOD) samples as in-distribution (ID). We scan for these blind spots by adversarially shifting OOD samples towards in-distribution. The increased likelihood of perturbed OOD samples being classified as ID serves as a signature for trojan detection. TRODO is both trojan and label mapping agnostic, effective even against adversarially trained trojaned classifiers. It is applicable even in scenarios where training data is absent, demonstrating high accuracy and adaptability across various scenarios and datasets, highlighting its potential as a robust trojan scanning strategy.",
      "authors": [
        "Hossein Mirzaei",
        "Ali Ansari",
        "Bahar Dibaei Nia",
        "Mojtaba Nafez",
        "Moein Madadi",
        "Sepehr Rezaee",
        "Zeinab Sadat Taghavi",
        "Arad Maleki",
        "Kian Shamsaie",
        "Mahdi Hajialilue",
        "Jafar Habibi",
        "Mohammad Sabokrou",
        "Mohammad Hossein Rohban"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=m296WJXyzQ",
      "cdate": 1715779070933,
      "mdate": 1730873984114,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.446708"
    },
    {
      "id": "TwrnhZfD6a",
      "title": "Test Where Decisions Matter: Importance-driven Testing for Deep Reinforcement Learning",
      "abstract": "In many Deep Reinforcement Learning (RL) problems, decisions in a trained policy vary in significance for the expected safety and performance of the policy. Since RL policies are very complex, testing efforts should concentrate on states in which the agent's decisions have the highest impact on the expected outcome. In this paper, we propose a novel model-based method to rigorously compute a ranking of state importance across the entire state space. We then focus our testing efforts on the highest-ranked states. In this paper, we focus on testing for safety. However, the proposed methods can be easily adapted to test for performance. In each iteration, our testing framework computes optimistic and pessimistic safety estimates. These estimates provide lower and upper bounds on the expected outcomes of the policy execution across all modeled states in the state space. Our approach divides the state space into safe and unsafe regions upon convergence, providing clear insights into the policy's weaknesses. Two important properties characterize our approach. (1) Optimal Test-Case Selection: At any time in the testing process, our approach evaluates the policy in the states that are most critical for safety. (2) Guaranteed Safety: Our approach can provide formal verification guarantees over the entire state space by sampling only a fraction of the policy. Any safety properties assured by the pessimistic estimate are formally proven to hold for the policy. We provide a detailed evaluation of our framework on several examples, showing that our method discovers unsafe policy behavior with low testing effort.",
      "authors": [
        "Stefan Pranger",
        "Hana Chockler",
        "Martin Tappler",
        "Bettina Könighofer"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=TwrnhZfD6a",
      "cdate": 1715778917154,
      "mdate": 1736764066171,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446714"
    },
    {
      "id": "ds6xMV3yVV",
      "title": "Nature-Inspired Local Propagation",
      "abstract": "The spectacular results achieved in machine learning, including the recent advances in generative AI, rely on large data collections. On the opposite, intelligent processes in nature arises without the need for such collections, but simply by on-line processing of the environmental information. In particular, natural learning processes rely on mechanisms where data representation and learning are intertwined in such a way to respect spatiotemporal locality. This paper shows that such a feature arises from a pre-algorithmic view of learning that is inspired by related studies in Theoretical Physics. We show that the algorithmic interpretation of the derived “laws of learning”, which takes the structure of Hamiltonian equations, reduces to Backpropagation when the speed of propagation goes to infinity. This opens the doors to machine learning studies based on full on-line information processing that are based on the replacement of Backpropagation with the proposed spatiotemporal local algorithm.",
      "authors": [
        "Alessandro Betti",
        "Marco Gori"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ds6xMV3yVV",
      "cdate": 1715778811374,
      "mdate": 1730873983839,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446719"
    },
    {
      "id": "HavKlV22xJ",
      "title": "Model-free Low-Rank Reinforcement Learning via Leveraged Entry-wise Matrix Estimation",
      "abstract": "We consider the problem of learning an $\\varepsilon$-optimal policy in controlled dynamical systems with low-rank latent structure. \nFor this problem, we present LoRa-PI (Low-Rank Policy Iteration), a model-free learning algorithm alternating between policy improvement and policy evaluation steps. In the latter, the algorithm estimates the low-rank matrix corresponding to the (state, action) value function of the current policy using the following two-phase procedure. The entries of the matrix are first sampled uniformly at random to estimate, via a spectral method, the *leverage scores* of its rows and columns. These scores are then used to extract a few important rows and columns whose entries are further sampled. The algorithm exploits these new samples to  complete the matrix estimation using a CUR-like method. For this leveraged matrix estimation procedure, we establish entry-wise guarantees that remarkably, do not depend on the coherence of the matrix but only on its spikiness. These guarantees imply that LoRa-PI learns an $\\varepsilon$-optimal policy using $\\tilde{\\cal O}({(S+A)\\over \\mathrm{poly}(1-\\gamma)\\varepsilon^2})$ samples where $S$ (resp. $A$) denotes the number of states (resp. actions) and $\\gamma$ the discount factor. Our algorithm achieves this order-optimal (in $S$, $A$ and $\\varepsilon$) sample complexity under milder conditions than those assumed in previously proposed approaches.",
      "authors": [
        "Stefan Stojanovic",
        "Yassir Jedra",
        "Alexandre Proutiere"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=HavKlV22xJ",
      "cdate": 1715778549477,
      "mdate": 1730873983646,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446725"
    },
    {
      "id": "u5enPCwaLt",
      "title": "Towards Estimating Bounds on the Effect of Policies under Unobserved Confounding",
      "abstract": "As many practical fields transition to provide personalized decisions, data is increasingly relevant to support the evaluation of candidate plans and policies (e.g., guidelines for the treatment of disease, government directives, etc.). In the machine learning literature, significant efforts have been put into developing machinery to predict the effectiveness of policies efficiently. The challenge is that, in practice, the effectiveness of a candidate policy is not always identifiable, i.e., not uniquely estimable from the combination of the available data and assumptions about the domain at hand (e.g., encoded in a causal graph). In this paper, we develop graphical characterizations and estimation tools to bound the effect of policies given a causal graph and observational data collected in non-identifiable settings. Specifically, our contributions are two-fold: (1) we derive analytical bounds for general probabilistic and conditional policies that are tighter than existing results, (2) we develop an estimation framework to estimate bounds from finite samples, applicable in higher-dimensional spaces and continuously-valued data. We further show that the resulting estimators have favourable statistical properties such as fast convergence and robustness to model misspecification.",
      "authors": [
        "Alexis Bellot",
        "Silvia Chiappa"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=u5enPCwaLt",
      "cdate": 1715778401974,
      "mdate": 1730873983604,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446731"
    },
    {
      "id": "GtbwJ6mruI",
      "title": "Skill-aware Mutual Information Optimisation for Zero-shot Generalisation in Reinforcement Learning",
      "abstract": "Meta-Reinforcement Learning (Meta-RL) agents can struggle to operate across tasks with varying environmental features that require different optimal skills (i.e., different modes of behaviour). Using context encoders based on contrastive learning to enhance the generalisability of Meta-RL agents is now widely studied but faces challenges such as the requirement for a large sample size, also referred to as the $\\log$-$K$ curse. To improve RL generalisation to different tasks, we first introduce Skill-aware Mutual Information (SaMI), an optimisation objective that aids in distinguishing context embeddings according to skills, thereby equipping RL agents with the ability to identify and execute different skills across tasks. We then propose Skill-aware Noise Contrastive Estimation (SaNCE), a $K$-sample estimator used to optimise the SaMI objective. We provide a framework for equipping an RL agent with SaNCE in practice and conduct experimental validation on modified MuJoCo and Panda-gym benchmarks. We empirically find that RL agents that learn by maximising SaMI achieve substantially improved zero-shot generalisation to unseen tasks. Additionally, the context encoder trained with SaNCE demonstrates greater robustness to a reduction in the number of available samples, thus possessing the potential to overcome the $\\log$-$K$ curse.",
      "authors": [
        "Xuehui Yu",
        "Mhairi Dunion",
        "Xin Li",
        "Stefano V Albrecht"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GtbwJ6mruI",
      "cdate": 1715777886207,
      "mdate": 1734960139423,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446736"
    },
    {
      "id": "SOsiObSdU2",
      "title": "Automatically Learning Hybrid Digital Twins of Dynamical Systems",
      "abstract": "Digital Twins (DTs) are computational models that simulate the states and temporal dynamics of real-world systems, playing a crucial role in prediction, understanding, and decision-making across diverse domains. However, existing approaches to DTs often struggle to generalize to unseen conditions in data-scarce settings, a crucial requirement for such models. To address these limitations, our work begins by establishing the essential desiderata for effective DTs. Hybrid Digital Twins (**HDTwins**) represent a promising approach to address these requirements, modeling systems using a composition of both mechanistic and neural components. This hybrid architecture simultaneously leverages (partial) domain knowledge and neural network expressiveness to enhance generalization, with its modular design facilitating improved evolvability. While existing hybrid models rely on expert-specified architectures with only parameters optimized on data, *automatically* specifying and optimizing HDTwins remains intractable due to the complex search space and the need for flexible integration of domain priors. To overcome this complexity, we propose an evolutionary algorithm (**HDTwinGen**) that employs Large Language Models (LLMs) to autonomously propose, evaluate, and optimize HDTwins. Specifically, LLMs iteratively generate novel model specifications, while offline tools are employed to optimize emitted parameters. Correspondingly, proposed models are evaluated and evolved based on targeted feedback, enabling the discovery of increasingly effective hybrid models. Our empirical results reveal that HDTwinGen produces generalizable, sample-efficient, and evolvable models, significantly advancing DTs' efficacy in real-world applications.",
      "authors": [
        "Samuel Holt",
        "Tennison Liu",
        "Mihaela van der Schaar"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=SOsiObSdU2",
      "cdate": 1715777666675,
      "mdate": 1730873983394,
      "matched_keywords": [
        "large language model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.446741"
    },
    {
      "id": "NJUClFbosX",
      "title": "Discrete Dictionary-based Decomposition Layer for Structured Representation Learning",
      "abstract": "Neuro-symbolic neural networks have been extensively studied to integrate symbolic operations with neural networks, thereby improving systematic generalization. Specifically, Tensor Product Representation (TPR) framework enables neural networks to perform differentiable symbolic operations by encoding the symbolic structure of data within vector spaces. However, TPR-based neural networks often struggle to decompose unseen data into structured TPR representations, undermining their symbolic operations. To address this decomposition problem, we propose a Discrete Dictionary-based Decomposition (D3) layer designed to enhance the decomposition capabilities of TPR-based models. D3 employs discrete, learnable key-value dictionaries trained to capture symbolic features essential for decomposition operations. It leverages the prior knowledge acquired during training to generate structured TPR representations by mapping input data to pre-learned symbolic features within these dictionaries. D3 is a straightforward drop-in layer that can be seamlessly integrated into any TPR-based model without modifications. Our experimental results demonstrate that D3 significantly improves the systematic generalization of various TPR-based models while requiring fewer additional parameters. Notably, D3 outperforms baseline models on the synthetic task that demands the systematic decomposition of unseen combinatorial data.",
      "authors": [
        "Taewon Park",
        "Hyun-Chul Kim",
        "Minho Lee"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NJUClFbosX",
      "cdate": 1715777588668,
      "mdate": 1730873983374,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.446746"
    },
    {
      "id": "AYq6GxxrrY",
      "title": "Transferable Boltzmann Generators",
      "abstract": "The generation of equilibrium samples of molecular systems has been a long-standing problem in statistical physics. Boltzmann Generators are a generative machine learning method that addresses this issue by learning a transformation via a normalizing flow from a simple prior distribution to the target Boltzmann distribution of interest. Recently, flow matching has been employed to train Boltzmann Generators for small molecular systems in Cartesian coordinates. We extend this work and propose a first framework for Boltzmann Generators that are transferable across chemical space, such that they predict zero-shot Boltzmann distributions for test molecules without being retraining for these systems. These transferable Boltzmann Generators allow approximate sampling from the target distribution of unseen systems, as well as efficient reweighting to the target Boltzmann distribution. The transferability of the proposed framework is evaluated on dipeptides, where we show that it generalizes efficiently to unseen systems.\nFurthermore, we demonstrate that our proposed architecture enhances the efficiency of Boltzmann Generators trained on single molecular systems.",
      "authors": [
        "Leon Klein",
        "Frank Noe"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=AYq6GxxrrY",
      "cdate": 1715777556795,
      "mdate": 1736939447715,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446751"
    },
    {
      "id": "dqdffX3BS5",
      "title": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning",
      "abstract": "Graph incremental learning has gained widespread attention for its ability to mitigate catastrophic forgetting for graph neural networks (GNN). Conventional methods typically require numerous labels for node classification. However, obtaining abundant labels is often challenging in practice, which makes graph few-shot incremental learning necessary. Current approaches rely on large number of samples from meta-learning to construct memories, and heavy fine-tuning of the GNN parameters that lead to the loss of past knowledge. These result in significant memory consumption and loss of past knowledge information, respectively. To tackle these issues, We introduce Mecoin to efficient construct and Preserve memory. For efficient storage and update of class prototypes, Mecoin use Structured Memory Unit (SMU) to cache prototypes of the seen classes and update new class prototypes through interaction between nodes and the cached prototypes by Memory Construction module(MeCo). Besides, to avoid extensive parameter fine-tuning and forgetting, we introduce a Memory Representation Adaptive Module called MRaM to separate the learning of prototypes and class representations and use Graph Knowledge Interchange Module (GKIM) to injects past knowledge information into GNN. We analyze the effectiveness of our paradigm from the perspectives of generalization error, and discuss the impact of different distillation methods on model performance through experiments and VC-dimension. By comparison with other related methods, we validate that Mecoin achieves higher accuracy and lower forgetting rate.",
      "authors": [
        "Dong Li",
        "Aijia Zhang",
        "Junqi Gao",
        "Biqing Qi"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=dqdffX3BS5",
      "cdate": 1715777545058,
      "mdate": 1730873983254,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.446756"
    },
    {
      "id": "PAWQvrForJ",
      "title": "Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration",
      "abstract": "Membership Inference Attacks (MIA) aim to infer whether a target data record has been utilized for model training or not. Existing MIAs designed for large language models (LLMs) can be bifurcated into two types: reference-free and reference-based attacks. Although reference-based attacks appear promising performance by calibrating the probability measured on the target model with reference models, this illusion of privacy risk heavily depends on a reference dataset that closely resembles the training set. Both two types of attacks are predicated on the hypothesis that training records consistently maintain a higher probability of being sampled. However, this hypothesis heavily relies on the overfitting of target models, which will be mitigated by multiple regularization methods and the generalization of LLMs. Thus, these reasons lead to high false-positive rates of MIAs in practical scenarios.\nWe propose a Membership Inference Attack based on Self-calibrated Probabilistic Variation (SPV-MIA). \nSpecifically, we introduce a self-prompt approach, which constructs the dataset to fine-tune the reference model by prompting the target LLM itself. In this manner, the adversary can collect a dataset with a similar distribution from public APIs.\nFurthermore, we introduce probabilistic variation, a more reliable membership signal based on LLM memorization rather than overfitting, from which we rediscover the neighbour attack with theoretical grounding. \nComprehensive evaluation conducted on three datasets and four exemplary LLMs shows that SPV-MIA raises the AUC of MIAs from 0.7 to a significantly high level of 0.9. Our code and dataset are available at: https://github.com/tsinghua-fib-lab/NeurIPS2024_SPV-MIA",
      "authors": [
        "Wenjie Fu",
        "Huandong Wang",
        "Chen Gao",
        "Guanghua Liu",
        "Yong Li",
        "Tao Jiang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=PAWQvrForJ",
      "cdate": 1715777542300,
      "mdate": 1735031732997,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.446761"
    },
    {
      "id": "ZYNYhh3ocW",
      "title": "Towards Next-Generation Logic Synthesis: A Scalable Neural Circuit Generation Framework",
      "abstract": "Logic Synthesis (LS) aims to generate an optimized logic circuit satisfying a given functionality, which generally consists of circuit translation and optimization. It is a challenging and fundamental combinatorial optimization problem in integrated circuit design. Traditional LS approaches rely on manually designed heuristics to tackle the LS task, while machine learning recently offers a promising approach towards next-generation logic synthesis by neural circuit generation and optimization. In this paper, we first revisit the application of differentiable neural architecture search (DNAS) methods to circuit generation and found from extensive experiments that existing DNAS methods struggle to exactly generate circuits, scale poorly to large circuits, and exhibit high sensitivity to hyper-parameters. Then we provide three major insights for these challenges from extensive empirical analysis: 1) DNAS tends to overfit to too many skip-connections, consequently wasting a significant portion of the network's expressive capabilities; 2) DNAS suffers from the structure bias between the network architecture and the circuit inherent structure, leading to inefficient search; 3) the learning difficulty of different input-output examples varies significantly, leading to severely imbalanced learning. To address these challenges in a systematic way, we propose a novel regularized triangle-shaped circuit network generation framework, which leverages our key insights for completely accurate and scalable circuit generation. Furthermore, we propose an evolutionary algorithm assisted by reinforcement learning agent restarting technique for efficient and effective neural circuit optimization. Extensive experiments on four different circuit benchmarks demonstrate that our method can precisely generate circuits with up to 1200 nodes. Moreover, our synthesized circuits significantly outperform the state-of-the-art results from several competitive winners in IWLS 2022 and 2023 competitions.",
      "authors": [
        "Zhihai Wang",
        "Jie Wang",
        "Qingyue Yang",
        "Yinqi Bai",
        "Xing Li",
        "Lei Chen",
        "Jianye HAO",
        "Mingxuan Yuan",
        "Bin Li",
        "Yongdong Zhang",
        "Feng Wu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ZYNYhh3ocW",
      "cdate": 1715777489388,
      "mdate": 1730873983142,
      "matched_keywords": [
        "reinforcement learning",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446766"
    },
    {
      "id": "ZgDNrpS46k",
      "title": "Analyzing & Reducing the Need for Learning Rate Warmup in GPT Training",
      "abstract": "Learning Rate Warmup is a popular heuristic for training neural networks, especially at larger batch sizes, despite limited understanding of its benefits. Warmup decreases the update size $\\Delta \\mathbf{w}_t = \\eta_t \\mathbf{u}_t$ early in training by using lower values for the learning rate $\\eta_t$. In this work we argue that warmup benefits training by keeping the overall size of $\\Delta \\mathbf{w}_t$ limited, counteracting large initial values of $\\mathbf{u}_t$. Focusing on small-scale GPT training with AdamW/Lion, we explore the following question: *Why and by which criteria are early updates $\\mathbf{u}_t$ too large?* We analyze different metrics for the update size including the $\\ell_2$-norm, resulting directional change, and impact on the representations of the network, providing a new perspective on warmup. In particular, we find that warmup helps counteract large angular updates as well as a limited critical batch size early in training. Finally, we show that the need for warmup can be significantly reduced or eliminated by modifying the optimizer to explicitly normalize $\\mathbf{u}_t$ based on the aforementioned metrics.",
      "authors": [
        "Atli Kosson",
        "Bettina Messmer",
        "Martin Jaggi"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ZgDNrpS46k",
      "cdate": 1715777206351,
      "mdate": 1730873982988,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.446771"
    },
    {
      "id": "NKpPnb3YNg",
      "title": "Time-Constrained Robust MDPs",
      "abstract": "Robust reinforcement learning is essential for deploying reinforcement learning algorithms in real-world scenarios where environmental uncertainty predominates.\nTraditional robust reinforcement learning often depends on rectangularity assumptions, where adverse probability measures of outcome states are assumed to be independent across different states and actions. \nThis assumption, rarely fulfilled in practice, leads to overly conservative policies. \nTo address this problem, we introduce a new time-constrained robust MDP (TC-RMDP) formulation that considers multifactorial, correlated, and time-dependent disturbances, thus more accurately reflecting real-world dynamics. This formulation goes beyond the conventional rectangularity paradigm, offering new perspectives and expanding the analytical framework for robust RL.\nWe propose three distinct algorithms, each using varying levels of environmental information, and evaluate them extensively on continuous control benchmarks. \nOur results demonstrate that these algorithms yield an efficient tradeoff between performance and robustness, outperforming traditional deep robust RL methods in time-constrained environments while preserving robustness in classical benchmarks.\nThis study revisits the prevailing assumptions in robust RL and opens new avenues for developing more practical and realistic RL applications.",
      "authors": [
        "Adil Zouitine",
        "David Bertoin",
        "Pierre Clavier",
        "Matthieu Geist",
        "Emmanuel Rachelson"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=NKpPnb3YNg",
      "cdate": 1715776821863,
      "mdate": 1730873982894,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446776"
    },
    {
      "id": "HjeKHxK2VH",
      "title": "WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off",
      "abstract": "Watermarking is a technical means to dissuade malfeasant usage of Large Language Models.\nThis paper proposes a novel watermarking scheme, so-called WaterMax, that enjoys high detectability while sustaining the quality of the generated text of the original LLM.\nIts new design leaves the LLM untouched (no modification of the weights, logits or temperature).\nWaterMax balances robustness and  computational complexity contrary to the watermarking techniques of the literature inherently provoking a trade-off between quality and robustness.\nIts performance is both theoretically proven and experimentally validated.\nIt outperforms all the SotA techniques under the most complete benchmark suite.",
      "authors": [
        "Eva Giboulot",
        "Teddy Furon"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=HjeKHxK2VH",
      "cdate": 1715776246130,
      "mdate": 1730873982705,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.446781"
    },
    {
      "id": "fHxmoekQBh",
      "title": "MaVEn: An Effective Multi-granularity Hybrid Visual Encoding Framework for Multimodal Large Language Model",
      "abstract": "This paper presents MaVEn, an innovative Multi-granularity Visual Encoding framework designed to enhance the capabilities of Multimodal Large Language Models (MLLMs) in multi-image reasoning. Current MLLMs primarily focus on single-image visual understanding, limiting their ability to interpret and integrate information across multiple images. MaVEn addresses this limitation by combining discrete visual symbol sequences, which abstract coarse-grained semantic concepts, with traditional continuous representation sequences that model fine-grained features. This dual approach bridges the semantic gap between visual and textual data, thereby improving the model's ability to process and interpret information from multiple images effectively. Additionally, we design a dynamic reduction mechanism by for long-sequence continuous features to enhance multi-image processing efficiency. Experimental results demonstrate that MaVEn significantly enhances MLLMs' understanding in complex multi-image scenarios, while also improving performance in single-image contexts.",
      "authors": [
        "Chaoya Jiang",
        "Jia Hongrui",
        "Haiyang Xu",
        "Wei Ye",
        "Mengfan Dong",
        "Ming Yan",
        "Ji Zhang",
        "Fei Huang",
        "Shikun Zhang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fHxmoekQBh",
      "cdate": 1715776177177,
      "mdate": 1736154873645,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.446795"
    },
    {
      "id": "REVdYKGcfb",
      "title": "What Factors Affect Multi-Modal In-Context Learning? An In-Depth Exploration",
      "abstract": "Recently, rapid advancements in Multi-Modal In-Context Learning (MM-ICL) have achieved notable success, which is capable of achieving superior performance across various tasks without requiring additional parameter tuning. However, the underlying rules for the effectiveness of MM-ICL remain under-explored. To fill this gap, this work aims to investigate the research question: \"_What factors affect the performance of MM-ICL?_\" To this end, we investigate extensive experiments on the three core steps of MM-ICL including demonstration retrieval, demonstration ordering, and prompt construction using 6 vision large language models and 20 strategies. Our findings highlight (1) the necessity of a multi-modal retriever for demonstration retrieval, (2) the importance of intra-demonstration ordering over inter-demonstration ordering, and (3) the enhancement of task comprehension through introductory instructions in prompts. We hope this study can serve as a foundational guide for optimizing MM-ICL strategies in future research.",
      "authors": [
        "Libo Qin",
        "Qiguang Chen",
        "Hao Fei",
        "Zhi Chen",
        "Min Li",
        "Wanxiang Che"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=REVdYKGcfb",
      "cdate": 1715775753569,
      "mdate": 1735192114399,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.446800"
    },
    {
      "id": "Nycj81Z692",
      "title": "UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction",
      "abstract": "Urban knowledge graph has recently worked as an emerging building block to distill critical knowledge from multi-sourced urban data for diverse urban application scenarios. Despite its promising benefits, urban knowledge graph construction (UrbanKGC) still heavily relies on manual effort, hindering its potential advancement. This paper presents UrbanKGent, a unified large language model agent framework, for urban knowledge graph construction. Specifically, we first construct the knowledgeable instruction set for UrbanKGC tasks (such as relational triplet extraction and knowledge graph completion) via heterogeneity-aware and geospatial-infused instruction generation. Moreover, we propose a tool-augmented iterative trajectory refinement module to enhance and refine the trajectories distilled from GPT-4. Through hybrid instruction fine-tuning with augmented trajectories on Llama 2 and Llama 3 family, we obtain UrbanKGC agent family, consisting of UrbanKGent-7/8/13B version. We perform a comprehensive evaluation on two real-world datasets using both human and GPT-4 self-evaluation. The experimental results demonstrate that UrbanKGent family can not only significantly outperform 31 baselines in UrbanKGC tasks, but also surpass the state-of-the-art LLM, GPT-4, by more than 10% with approximately 20 times lower cost. Compared with the existing benchmark, the UrbanKGent family could help construct an UrbanKG with hundreds of times richer relationships using only one-fifth of the data. Our data and code are available at https://github.com/usail-hkust/UrbanKGent.",
      "authors": [
        "Yansong Ning",
        "Hao Liu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Nycj81Z692",
      "cdate": 1715775646072,
      "mdate": 1730873982475,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.446805"
    },
    {
      "id": "y2fAmldTIf",
      "title": "HEPrune: Fast Private Training of Deep Neural Networks With Encrypted Data Pruning",
      "abstract": "Non-interactive cryptographic computing, Fully Homomorphic Encryption (FHE), provides a promising solution for private neural network training on encrypted data. One challenge of FHE-based private training is its large computational overhead, especially the multiple rounds of forward and backward execution on each encrypted data sample. Considering the existence of largely redundant data samples, pruning them will significantly speed up the training, as proven in plain non-FHE training. \nExecuting the data pruning of encrypted data on the server side is not trivial since the knowledge calculation of data pruning needs complex and expensive executions on encrypted data. There is a lack of FHE-based data pruning protocol for efficient, private training. In this paper, we propose, \\textit{HEPrune}, to construct a FHE data-pruning protocol and then design an FHE-friendly data-pruning algorithm under client-aided or non-client-aided settings, respectively. We also observed that data sample pruning may not always remove ciphertexts, leaving large empty slots and limiting the effects of data pruning. Thus, in HEPrune, we further propose ciphertext-wise pruning to reduce ciphertext computation numbers without hurting accuracy. Experimental results show that our work can achieve a $16\\times$ speedup with only a $0.6\\%$ accuracy drop over prior work. \nThe code is publicly available at \\href{https://github.com/UCF-Lou-Lab-PET/Private-Data-Prune}.",
      "authors": [
        "Yancheng Zhang",
        "Mengxin Zheng",
        "Yuzhang Shang",
        "Xun Chen",
        "Qian Lou"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=y2fAmldTIf",
      "cdate": 1715775616209,
      "mdate": 1730873982383,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.446810"
    },
    {
      "id": "recsheQ7e8",
      "title": "Aligning to Thousands of Preferences via System Message Generalization",
      "abstract": "Although humans inherently have diverse values, current large language model (LLM) alignment methods often assume that aligning LLMs with the general public’s preferences is optimal. A major challenge in adopting a more individualized approach to LLM alignment is its lack of scalability, as it involves repeatedly acquiring preference data and training new reward models and LLMs for each individual’s preferences. To address these challenges, we propose a new paradigm where users specify what they value most within the system message, steering the LLM’s generation behavior to better align with the user’s intentions. However, a naive application of such an approach is non-trivial since LLMs are typically trained on a uniform system message (e.g., “You are a helpful assistant”), which limits\ntheir ability to generalize to diverse, unseen system messages. To improve this generalization, we create Multifaceted Collection, augmenting 66k user instructions into 197k system messages through hierarchical user value combinations. Using this dataset, we train a 7B LLM called Janus and test it on 921 prompts from 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct)\nby adding system messages that reflect unseen user values. JANUS achieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct v0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks focused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto v0.1), JANUS also outperforms LLaMA 3 8B Instruct by a +4.0%p, +0.1%p, +3.0%p margin, underscoring that training with a vast array of system messages could also enhance alignment to the general public’s preference as well. Our code, dataset, benchmark, and models are available at https://lklab.kaist.ac.kr/Janus/.",
      "authors": [
        "Seongyun Lee",
        "Sue Hyun Park",
        "Seungone Kim",
        "Minjoon Seo"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=recsheQ7e8",
      "cdate": 1715775615018,
      "mdate": 1730873982364,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.446815"
    },
    {
      "id": "MXze4H7opg",
      "title": "SLTrain: a sparse plus low rank approach for parameter and memory efficient pretraining",
      "abstract": "Large language models (LLMs) have shown impressive capabilities across various tasks. However, training LLMs from scratch requires significant computational power and extensive memory capacity. Recent studies have explored low-rank structures on weights for efficient fine-tuning in terms of parameters and memory, either through low-rank adaptation or factorization. While effective for fine-tuning, low-rank structures are generally less suitable for pretraining because they restrict parameters to a low-dimensional subspace. In this work, we propose to parameterize the weights as a sum of low-rank and sparse matrices for pretraining, which we call SLTrain. The low-rank component is learned via matrix factorization, while for the sparse component, we employ a simple strategy of uniformly selecting the sparsity support at random and learning only the non-zero entries with the fixed support. While being simple, the random fixed-support sparse learning strategy significantly enhances pretraining when combined with low-rank learning. Our results show that SLTrain adds minimal extra parameters and memory costs compared to pretraining with low-rank parameterization, yet achieves substantially better performance, which is comparable to full-rank training. Remarkably, when combined with quantization and per-layer updates, SLTrain can reduce memory requirements by up to 73% when pretraining the LLaMA 7B model.",
      "authors": [
        "Andi Han",
        "Jiaxiang Li",
        "Wei Huang",
        "Mingyi Hong",
        "Akiko Takeda",
        "Pratik Jawanpuria",
        "Bamdev Mishra"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MXze4H7opg",
      "cdate": 1715775501369,
      "mdate": 1730873982316,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.446820"
    },
    {
      "id": "pC44UMwy2v",
      "title": "Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought",
      "abstract": "Chain-of-Thought (CoT) reasoning has emerged as a promising approach for enhancing the performance of large language models (LLMs) on complex reasoning tasks. Recently, a series of studies attempt to explain the mechanisms underlying CoT, aiming to deepen the understanding of its efficacy. Nevertheless, the existing research faces two major challenges: (1) a lack of quantitative metrics to assess CoT capabilities and (2) a dearth of guidance on optimizing CoT performance. Motivated by this, in this work, we introduce a novel reasoning boundary framework (RBF) to address these challenges. To solve the lack of quantification, we first define a reasoning boundary (RB) to quantify the upper-bound of CoT and establish a combination law for RB, enabling a practical quantitative approach applicable to various real-world CoT tasks. To address the lack of optimization, we propose three categories of RBs. We further optimize these categories with combination laws focused on RB promotion and reasoning path optimization for CoT improvement. Through extensive experiments on 27 models and 5 tasks, the study validates the existence and rationality of the proposed framework. Furthermore, it explains the effectiveness of 10 CoT strategies and guides optimization from two perspectives. We hope this work can provide a comprehensive understanding of the boundaries and optimization strategies for reasoning in LLMs. Our code and data are available at https://github.com/LightChen233/reasoning-boundary.",
      "authors": [
        "Qiguang Chen",
        "Libo Qin",
        "Jiaqi WANG",
        "Jingxuan Zhou",
        "Wanxiang Che"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pC44UMwy2v",
      "cdate": 1715775218923,
      "mdate": 1735191692619,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.446825"
    },
    {
      "id": "7YdafFbhxL",
      "title": "Provably and Practically Efficient Adversarial Imitation Learning with General Function Approximation",
      "abstract": "As a prominent category of imitation learning methods, adversarial imitation learning (AIL) has garnered significant practical success powered by neural network approximation. However, existing theoretical studies on AIL are primarily limited to simplified scenarios such as tabular and linear function approximation and involve complex algorithmic designs that hinder practical implementation, highlighting a gap between theory and practice. In this paper, we explore the theoretical underpinnings of online AIL with general function approximation. We introduce a new method called optimization-based AIL (OPT-AIL), which centers on performing online optimization for reward functions and optimism-regularized Bellman error minimization for Q-value functions. Theoretically, we prove that OPT-AIL achieves polynomial expert sample complexity and interaction complexity for learning near-expert policies. To our best knowledge, OPT-AIL is the first provably efficient AIL method with general function approximation. Practically, OPT-AIL only requires the approximate optimization of two objectives, thereby facilitating practical implementation. Empirical studies demonstrate that OPT-AIL outperforms previous state-of-the-art deep AIL methods in several challenging tasks.",
      "authors": [
        "Tian Xu",
        "Zhilong Zhang",
        "Ruishuo Chen",
        "Yihao Sun",
        "Yang Yu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7YdafFbhxL",
      "cdate": 1715775183101,
      "mdate": 1730873982116,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.446830"
    },
    {
      "id": "FTpKGuxEfy",
      "title": "Vision Foundation Model Enables Generalizable Object Pose Estimation",
      "abstract": "Object pose estimation plays a crucial role in robotic manipulation, however, its practical applicability still suffers from limited generalizability. This paper addresses the challenge of generalizable object pose estimation, particularly focusing on category-level object pose estimation for unseen object categories. Current methods either require impractical instance-level training or are confined to predefined categories, limiting their applicability. We propose VFM-6D, a novel framework that explores harnessing existing vision and language models, to elaborate object pose estimation into two stages: category-level object viewpoint estimation and object coordinate map estimation. Based on the two-stage framework, we introduce a 2D-to-3D feature lifting module and a shape-matching module, both of which leverage pre-trained vision foundation models to improve object representation and matching accuracy. VFM-6D is trained on cost-effective synthetic data and exhibits superior generalization capabilities. It can be applied to both instance-level unseen object pose estimation and category-level object pose estimation for novel categories. Evaluations on benchmark datasets demonstrate the effectiveness and versatility of VFM-6D in various real-world scenarios.",
      "authors": [
        "Kai Chen",
        "Yiyao Ma",
        "Xingyu Lin",
        "Stephen James",
        "Jianshu Zhou",
        "Yun-Hui Liu",
        "Pieter Abbeel",
        "Qi Dou"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FTpKGuxEfy",
      "cdate": 1715775149482,
      "mdate": 1736388057450,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:09.446836"
    },
    {
      "id": "8x48XFLvyd",
      "title": "Globally Convergent Variational Inference",
      "abstract": "In variational inference (VI), an approximation of the posterior distribution is selected from a family of distributions through numerical optimization. With the most common variational objective function, known as the evidence lower bound (ELBO), only convergence to a *local* optimum can be guaranteed. In this work, we instead establish the *global* convergence of a particular VI method. This VI method, which may be considered an instance of neural posterior estimation (NPE), minimizes an expectation of the inclusive (forward) KL divergence to fit a variational distribution that is parameterized by a neural network. Our convergence result relies on the neural tangent kernel (NTK) to characterize the gradient dynamics that arise from considering the variational objective in function space. In the asymptotic regime of a fixed, positive-definite neural tangent kernel, we establish conditions under which the variational objective admits a unique solution in a reproducing kernel Hilbert space (RKHS). Then, we show that the gradient descent dynamics in function space converge to this unique function. In ablation studies and practical problems, we demonstrate that our results explain the behavior of NPE in non-asymptotic finite-neuron settings, and show that NPE outperforms ELBO-based optimization, which often converges to shallow local optima.",
      "authors": [
        "Declan McNamara",
        "Jackson Loper",
        "Jeffrey Regier"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=8x48XFLvyd",
      "cdate": 1715775022411,
      "mdate": 1736869520254,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.446840"
    },
    {
      "id": "mVfRrMfGdY",
      "title": "Unified Mechanism-Specific Amplification by Subsampling and Group Privacy Amplification",
      "abstract": "Amplification by subsampling is one of the main primitives in machine learning with differential privacy (DP): Training a model on random batches instead of complete datasets results in stronger privacy. This is traditionally formalized via mechanism-agnostic subsampling guarantees that express the privacy parameters of a subsampled mechanism as a function of the original mechanism's privacy parameters. We propose the first general framework for deriving mechanism-specific guarantees, which leverage additional information beyond these parameters to more tightly characterize the subsampled mechanism's privacy. Such guarantees are of particular importance for privacy accounting, i.e., tracking privacy over multiple iterations. Overall, our framework based on conditional optimal transport lets us derive existing and novel guarantees for approximate DP, accounting with Renyi DP, and accounting with dominating pairs in a unified, principled manner. As an application, we analyze how subsampling affects the privacy of groups of multiple users. Our tight mechanism-specific bounds outperform tight mechanism-agnostic bounds and classic group privacy results.",
      "authors": [
        "Jan Schuchardt",
        "Mihail Stoian",
        "Arthur Kosmala",
        "Stephan Günnemann"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=mVfRrMfGdY",
      "cdate": 1715774989423,
      "mdate": 1730873981992,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446845"
    },
    {
      "id": "nqWaya7hiX",
      "title": "Wings: Learning Multimodal LLMs without Text-only Forgetting",
      "abstract": "Multimodal large language models (MLLMs), initiated with a trained LLM, first align images with text and then fine-tune on multimodal mixed inputs. However, during the continued training, the MLLM catastrophically forgets the text-only instructions that the initial LLM masters. In this paper, we present Wings, a novel MLLM that excels in both text-only and multimodal instructions. By examining attention across layers of MLLM, we find that *text-only forgetting* is related to the attention shifts from pre-image to post-image text. From that, we construct an additional Low-Rank Residual Attention (LoRRA) block that acts as the \"modality learner\" to expand the learnable space and compensate for the attention shift. The complementary learners, like \"wings\" on either side, are connected in parallel to each layer's attention block. The LoRRA mirrors the structure of attention but utilizes low-rank connections to ensure efficiency. Initially, image and text inputs are aligned with visual learners operating alongside the main attention, balancing focus on visual elements. Later, textual learners are integrated with token-wise routing, blending the outputs of both modality learners collaboratively. Our experimental results demonstrate that Wings outperforms equally-scaled MLLMs in both text-only and visual question-answering tasks. Wings with *compensation of learners* addresses text-only forgetting during visual modality expansion in general MLLMs.",
      "authors": [
        "Yi-Kai Zhang",
        "Shiyin Lu",
        "Yang Li",
        "YanQing Ma",
        "Qing-Guo Chen",
        "Zhao Xu",
        "Weihua Luo",
        "Kaifu Zhang",
        "De-Chuan Zhan",
        "Han-Jia Ye"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nqWaya7hiX",
      "cdate": 1715774964675,
      "mdate": 1736995608488,
      "matched_keywords": [
        "large language model",
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.446850"
    },
    {
      "id": "Y13gSfTjGr",
      "title": "Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations",
      "abstract": "Scale has become a main ingredient in obtaining strong machine learning models. As a result, understanding a model's scaling properties is key to effectively designing both the right training setup as well as future generations of architectures. In this work, we argue that scale and training research has been needlessly complex due to reliance on the cosine schedule, which prevents training across different lengths for the same model size. We investigate the training behavior of a direct alternative --- constant learning rate and cooldowns --- and find that it scales predictably and reliably similar to cosine. Additionally, we show that stochastic weight averaging yields improved performance along the training trajectory, without additional training costs, across different scales. Importantly, with these findings we demonstrate that scaling experiments can be performed with significantly reduced compute and GPU hours by utilizing fewer but reusable training runs. Our code is available at https://github.com/epfml/schedules-and-scaling/.",
      "authors": [
        "Alexander Hägele",
        "Elie Bakouch",
        "Atli Kosson",
        "Loubna Ben allal",
        "Leandro Von Werra",
        "Martin Jaggi"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Y13gSfTjGr",
      "cdate": 1715774956285,
      "mdate": 1730873981926,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446855"
    },
    {
      "id": "KYHma7hzjr",
      "title": "Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?",
      "abstract": "Recently, interpretable machine learning has re-explored concept bottleneck models (CBM). An advantage of this model class is the user's ability to intervene on predicted concept values, affecting the downstream output. In this work, we introduce a method to perform such concept-based interventions on *pretrained* neural networks, which are not interpretable by design, only given a small validation set with concept labels. Furthermore, we formalise the notion of *intervenability* as a measure of the effectiveness of concept-based interventions and leverage this definition to fine-tune black boxes. Empirically, we explore the intervenability of black-box classifiers on synthetic tabular and natural image benchmarks. We focus on backbone architectures of varying complexity, from simple, fully connected neural nets to Stable Diffusion. We demonstrate that the proposed fine-tuning improves intervention effectiveness and often yields better-calibrated predictions. To showcase the practical utility of our techniques, we apply them to deep chest X-ray classifiers and show that fine-tuned black boxes are more intervenable than CBMs. Lastly, we establish that our methods are still effective under vision-language-model-based concept annotations, alleviating the need for a human-annotated validation set.",
      "authors": [
        "Sonia Laguna",
        "Ričards Marcinkevičs",
        "Moritz Vandenhirtz",
        "Julia E Vogt"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=KYHma7hzjr",
      "cdate": 1715774947505,
      "mdate": 1730873981880,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446860"
    },
    {
      "id": "wQiJNyPENt",
      "title": "Batched Energy-Entropy acquisition for Bayesian Optimization",
      "abstract": "Bayesian optimization (BO) is an attractive machine learning framework for performing sample-efficient global optimization of black-box functions. The optimization process is guided by an acquisition function that selects points to acquire in each round of BO. In batched BO, when multiple points are acquired in parallel, commonly used acquisition functions are often high-dimensional and intractable, leading to the use of sampling-based alternatives. We propose a statistical physics inspired acquisition function that can natively handle batches. Batched Energy-Entropy acquisition for BO (BEEBO) enables tight control of the explore-exploit trade-off of the optimization process and generalizes to heteroskedastic black-box problems. We demonstrate the applicability of BEEBO on a range of problems, showing competitive performance to existing acquisition functions.",
      "authors": [
        "Felix Teufel",
        "Carsten Stahlhut",
        "Jesper Ferkinghoff-Borg"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=wQiJNyPENt",
      "cdate": 1715774931956,
      "mdate": 1730873981848,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446865"
    },
    {
      "id": "MwJo3zuiTm",
      "title": "Free-Rider and Conflict Aware Collaboration Formation for Cross-Silo Federated Learning",
      "abstract": "Federated learning (FL) is a machine learning paradigm that allows multiple FL participants (FL-PTs) to collaborate on training models without sharing private data. Due to data heterogeneity, negative transfer may occur in the FL training process. This necessitates FL-PT selection based on their data complementarity. In cross-silo FL, organizations that engage in business activities are key sources of FL-PTs. The resulting FL ecosystem has two features: (i) self-interest, and (ii) competition among FL-PTs. This requires the desirable FL-PT selection strategy to simultaneously mitigate the problems of free riders and conflicts of interest among competitors. To this end, we propose an optimal FL collaboration formation strategy -FedEgoists- which ensures that: (1) a FL-PT can benefit from FL if and only if it benefits the FL ecosystem, and (2) a FL-PT will not contribute to its competitors or their supporters. It provides an efficient clustering solution to group FL-PTs into coalitions, ensuring that within each coalition, FL-PTs share the same interest. We theoretically prove that the FL-PT coalitions formed are optimal since no coalitions can collaborate together to improve the utility of any of their members. Extensive experiments on widely adopted benchmark datasets demonstrate the effectiveness of FedEgoists compared to nine state-of-the-art baseline methods, and its ability to establish efficient collaborative networks in cross-silos FL with FL-PTs that engage in business activities.",
      "authors": [
        "Mengmeng Chen",
        "Xiaohu Wu",
        "Xiaoli Tang",
        "Tiantian He",
        "Yew-Soon Ong",
        "QIQI LIU",
        "Qicheng Lao",
        "Han Yu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MwJo3zuiTm",
      "cdate": 1715774896357,
      "mdate": 1734914867705,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446869"
    },
    {
      "id": "32g9BWTndc",
      "title": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings",
      "abstract": "Zero-shot graph machine learning, especially with graph neural networks (GNNs), has garnered significant interest due to the challenge of scarce labeled data. While methods like self-supervised learning and graph prompt learning have been extensively explored, they often rely on fine-tuning with task-specific labels, limiting their effectiveness in zero-shot scenarios. Inspired by the zero-shot capabilities of instruction-fine-tuned large language models (LLMs), we introduce a novel framework named Token Embedding-Aligned Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and cross-task zero-shot learners for graph machine learning. Concretely, we pretrain a GNN, aligning its representations with token embeddings of an LLM. We then train a linear projector that transforms the GNN's representations into a fixed number of graph token embeddings without tuning the LLM. A unified instruction is designed for various graph tasks at different levels, such as node classification (node-level) and link prediction (edge-level). These design choices collectively enhance our method's effectiveness in zero-shot learning, setting it apart from existing methods. Experiments show that our graph token embeddings help the LLM predictor achieve state-of-the-art performance on unseen datasets and tasks compared to other methods using LLMs as predictors. Our code is available at https://github.com/W-rudder/TEA-GLM.",
      "authors": [
        "Duo Wang",
        "Yuan Zuo",
        "Fengzhi Li",
        "Junjie Wu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=32g9BWTndc",
      "cdate": 1715774852741,
      "mdate": 1734625787020,
      "matched_keywords": [
        "large language model",
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446874"
    },
    {
      "id": "4oAt5L4lYe",
      "title": "ArkVale: Efficient Generative LLM Inference with Recallable Key-Value Eviction",
      "abstract": "Large Language Models (LLMs) are widely used in today's tasks of natural language processing. \nTo support applications like multi-turn chats, document understanding, and content generation, models with long context lengths are growing in importance.\nHowever, managing long contexts brings substantial challenges due to the expansion of key-value cache (KV cache). Longer KV cache requires larger memory, limiting the batch-size thus decreasing throughput. Also, computing attention over long KV cache incurs more memory access, hurting the end-to-end latency.\nPrior works find that it is sufficient to use only the recent and high-impact tokens for attention computation, allowing the eviction of less vital tokens to shrink cache size.\nNonetheless, we observe a dynamic shift in token importance across different decoding steps. Tokens initially evicted might regain importance after certain decoding steps.\nTo address this, we propose ArkVale, a page-based KV cache manager that can recognize and recall currently important tokens evicted before. We asynchronously copy the filled page into external memory (e.g., CPU memory) as backup and summarize it into a much smaller digest by constructing the bounding-volume of its keys. Before attention computation, we measure all pages' importance based on their digests, recall the important ones, evict the unimportant ones, and select the top-ranked pages for attention computation. \nExperiment results show that ArkVale performs well on various long context tasks with negligible accuracy loss under 2k$\\sim$4k cache budget and can improve decoding latency to $2.2\\times$ and batching throughput to $4.6\\times$ because it applies attention on only a small subset of pages and reduce per-sample memory usage of KV cache.",
      "authors": [
        "Renze Chen",
        "Zhuofeng Wang",
        "Beiquan Cao",
        "Tong Wu",
        "Size Zheng",
        "Xiuhong Li",
        "Xuechao Wei",
        "Shengen Yan",
        "Meng Li",
        "Yun Liang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=4oAt5L4lYe",
      "cdate": 1715774809903,
      "mdate": 1730873981686,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.446879"
    },
    {
      "id": "rQYyWGYuzK",
      "title": "Monomial Matrix Group Equivariant Neural Functional Networks",
      "abstract": "Neural functional networks (NFNs) have recently gained significant attention due to their diverse applications, ranging from predicting network generalization and network editing to classifying implicit neural representation. Previous NFN designs often depend on permutation symmetries in neural networks' weights, which traditionally arise from the unordered arrangement of neurons in hidden layers. However, these designs do not take into account the weight scaling symmetries of $\\operatorname{ReLU}$ networks, and the weight sign flipping symmetries of $\\operatorname{sin}$ or $\\operatorname{Tanh}$ networks. In this paper, we extend the study of the group action on the network weights from the group of permutation matrices to the group of monomial matrices by incorporating scaling/sign-flipping symmetries. Particularly, we encode these scaling/sign-flipping symmetries by designing our corresponding equivariant and invariant layers. We name our new family of NFNs the Monomial Matrix Group Equivariant Neural Functional Networks (Monomial-NFN). Because of the expansion of the symmetries, Monomial-NFN has much fewer independent trainable parameters compared to the baseline NFNs in the literature, thus enhancing the model's efficiency. Moreover, for fully connected and convolutional neural networks, we theoretically prove that all groups that leave these networks invariant while acting on their weight spaces are some subgroups of the monomial matrix group. We provide empirical evidences to demonstrate the advantages of our model over existing baselines, achieving competitive performance and efficiency. The code is publicly available at https://github.com/MathematicalAI-NUS/Monomial-NFN.",
      "authors": [
        "Hoang V. Tran",
        "Thieu Vo",
        "Tho Tran Huu",
        "An Nguyen The",
        "Tan Minh Nguyen"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rQYyWGYuzK",
      "cdate": 1715774706672,
      "mdate": 1730873981619,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.446883"
    },
    {
      "id": "mxMvWwyBWe",
      "title": "Crafting Interpretable Embeddings for Language Neuroscience by Asking LLMs Questions",
      "abstract": "Large language models (LLMs) have rapidly improved text embeddings for a growing array of natural-language processing tasks. However, their opaqueness and proliferation into scientific domains such as neuroscience have created a growing need for interpretability. Here, we ask whether we can obtain interpretable embeddings through LLM prompting. We introduce question-answering embeddings (QA-Emb), embeddings where each feature represents an answer to a yes/no question asked to an LLM. Training QA-Emb reduces to selecting a set of underlying questions rather than learning model weights.\n\nWe use QA-Emb to flexibly generate interpretable models for predicting fMRI voxel responses to language stimuli. QA-Emb significantly outperforms an established interpretable baseline, and does so while requiring very few questions. This paves the way towards building flexible feature spaces that can concretize and evaluate our understanding of semantic brain representations. We additionally find that QA-Emb can be effectively approximated with an efficient model, and we explore broader applications in simple NLP tasks.",
      "authors": [
        "Vinamra Benara",
        "Chandan Singh",
        "John Xavier Morris",
        "Richard Antonello",
        "Ion Stoica",
        "Alexander Huth",
        "Jianfeng Gao"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=mxMvWwyBWe",
      "cdate": 1715774612890,
      "mdate": 1737828157062,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.446888"
    },
    {
      "id": "FVgCwcwpJw",
      "title": "Policy Improvement using Language Feedback Models",
      "abstract": "We introduce Language Feedback Models (LFMs) that identify desirable behaviour --- actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFMs can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.",
      "authors": [
        "Victor Zhong",
        "Dipendra Misra",
        "Xingdi Yuan",
        "Marc-Alexandre Côté"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FVgCwcwpJw",
      "cdate": 1715774576337,
      "mdate": 1730873981481,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.446892"
    },
    {
      "id": "uatPOPWzzU",
      "title": "Unifying Homophily and Heterophily for Spectral Graph Neural Networks via Triple Filter Ensembles",
      "abstract": "Polynomial-based learnable spectral graph neural networks (GNNs) utilize polynomial to approximate graph convolutions and have achieved impressive performance on graphs. Nevertheless, there are three progressive problems to be solved. Some models use polynomials with better approximation for approximating filters, yet perform worse on real-world graphs. Carefully crafted graph learning methods, sophisticated polynomial approximations, and refined coefficient constraints leaded to overfitting, which diminishes the generalization of the models. How to design a model that retains the ability of polynomial-based spectral GNNs to approximate filters while it possesses higher generalization and performance? In this paper, we propose a spectral GNN with triple filter ensemble (TFE-GNN), which extracts homophily and heterophily from graphs with different levels of homophily adaptively while utilizing the initial features. Specifically, the first and second ensembles are combinations of a set of base low-pass and high-pass filters, respectively, after which the third ensemble combines them with two learnable coefficients and yield a graph convolution (TFE-Conv). Theoretical analysis shows that the approximation ability of TFE-GNN is consistent with that of ChebNet under certain conditions, namely it can learn arbitrary filters. TFE-GNN can be viewed as a reasonable combination of two unfolded and integrated excellent spectral GNNs, which motivates it to perform well. Experiments show that TFE-GNN achieves high generalization and new state-of-the-art performance on various real-world datasets.",
      "authors": [
        "Rui Duan",
        "Mingjian Guang",
        "Junli Wang",
        "Chungang Yan",
        "Hongda Qi",
        "Wenkang Su",
        "Can Tian",
        "Haoran Yang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=uatPOPWzzU",
      "cdate": 1715774474219,
      "mdate": 1730873981374,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.446897"
    },
    {
      "id": "Xo1Yqyw7Yx",
      "title": "Enabling Adaptive Agent Training in Open-Ended Simulators by Targeting Diversity",
      "abstract": "The wider application of end-to-end learning methods to embodied decision-making domains remains bottlenecked by their reliance on a superabundance of training data representative of the target domain.\nMeta-reinforcement learning (meta-RL) approaches abandon the aim of zero-shot *generalization*—the goal of standard reinforcement learning (RL)—in favor of few-shot *adaptation*, and thus hold promise for bridging larger generalization gaps.\nWhile learning this meta-level adaptive behavior still requires substantial data, efficient environment simulators approaching real-world complexity are growing in prevalence.\nEven so, hand-designing sufficiently diverse and numerous simulated training tasks for these complex domains is prohibitively labor-intensive.\nDomain randomization (DR) and procedural generation (PG), offered as solutions to this problem, require simulators to possess carefully-defined parameters which directly translate to meaningful task diversity—a similarly prohibitive assumption.\nIn this work, we present **DIVA**, an evolutionary approach for generating diverse training tasks in such complex, open-ended simulators.\nLike unsupervised environment design (UED) methods, DIVA can be applied to arbitrary parameterizations, but can additionally incorporate realistically-available domain knowledge—thus inheriting the *flexibility* and *generality* of UED, and the supervised *structure* embedded in well-designed simulators exploited by DR and PG.\nOur empirical results showcase DIVA's unique ability to overcome complex parameterizations and successfully train adaptive agent behavior, far outperforming competitive baselines from prior literature.\nThese findings highlight the potential of such *semi-supervised environment design* (SSED) approaches, of which DIVA is the first humble constituent, to enable training in realistic simulated domains, and produce more robust and capable adaptive agents.\nOur code is available at [https://github.com/robbycostales/diva](https://github.com/robbycostales/diva).",
      "authors": [
        "Robby Costales",
        "Stefanos Nikolaidis"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Xo1Yqyw7Yx",
      "cdate": 1715774416298,
      "mdate": 1730873981347,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446902"
    },
    {
      "id": "YPqHSTSoFs",
      "title": "Cross-model Control: Improving Multiple Large Language Models in One-time Training",
      "abstract": "The number of large language models (LLMs) with varying parameter scales and vocabularies is increasing. While they deliver powerful performance, they also face a set of common optimization needs to meet specific requirements or standards, such as instruction following or avoiding the output of sensitive information from the real world. However, how to reuse the fine-tuning outcomes of one model to other models to reduce training costs remains a challenge. To bridge this gap, we introduce Cross-model Control (CMC), a method that improves multiple LLMs in one-time training with a portable tiny language model. Specifically, we have observed that the logit shift before and after fine-tuning is remarkably similar across different models. Based on this insight, we incorporate a tiny language model with a minimal number of parameters. By training alongside a frozen template LLM, the tiny model gains the capability to alter the logits output by the LLMs. To make this tiny language model applicable to models with different vocabularies, we propose a novel token mapping strategy named PM-MinED. We have conducted extensive experiments on instruction tuning and unlearning tasks, demonstrating the effectiveness of CMC. Our code is available at https://github.com/wujwyi/CMC",
      "authors": [
        "Jiayi Wu",
        "Hao Sun",
        "Hengyi Cai",
        "Lixin Su",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Xiang Li",
        "Ming Gao"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=YPqHSTSoFs",
      "cdate": 1715774376499,
      "mdate": 1730873981318,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.446906"
    },
    {
      "id": "FY6vPtITtE",
      "title": "The Challenges of the Nonlinear Regime for Physics-Informed Neural Networks",
      "abstract": "The Neural Tangent Kernel (NTK) viewpoint is widely employed to analyze the training dynamics of overparameterized Physics-Informed Neural Networks (PINNs). However, unlike the case of linear Partial Differential Equations (PDEs), we show how the NTK perspective falls short in the nonlinear scenario. Specifically, we establish that the NTK yields a random matrix at initialization that is not constant during training, contrary to conventional belief. Another significant difference from the linear regime is that, even in the idealistic infinite-width limit, the Hessian does not vanish and hence it cannot be disregarded during training. This motivates the adoption of second-order optimization methods. We explore the convergence guarantees of such methods in both linear and nonlinear cases, addressing challenges such as spectral bias and slow convergence. Every theoretical result is supported by numerical examples with both linear and nonlinear PDEs, and we highlight the benefits of second-order methods in benchmark test cases.",
      "authors": [
        "Andrea Bonfanti",
        "Giuseppe Bruno",
        "Cristina Cipriani"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FY6vPtITtE",
      "cdate": 1715774320911,
      "mdate": 1730873981231,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.446911"
    },
    {
      "id": "N2wYPMpifA",
      "title": "Understanding Scaling Laws with Statistical and Approximation Theory for Transformer Neural Networks on Intrinsically Low-dimensional Data",
      "abstract": "When training deep neural networks, a model's generalization error is often observed to follow a power scaling law dependent both on the model size and the data size. Perhaps the best known example of such scaling laws are for transformer-based large language models (**LLMs**), where networks with billions of parameters are trained on trillions of tokens of text. Yet, despite sustained widespread interest, a rigorous understanding of why transformer scaling laws exist is still missing. To answer this question, we establish novel statistical estimation and mathematical approximation theories for transformers when the input data are concentrated on a low-dimensional manifold. Our theory predicts a power law between the generalization error and both the training data size and the network size for transformers, where the power depends on the intrinsic dimension $d$ of the training data. Notably, the constructed model architecture is shallow, requiring only logarithmic depth in $d$. By leveraging low-dimensional data structures under a manifold hypothesis, we are able to explain transformer scaling laws in a way which respects the data geometry.  Moreover, we test our theory with empirical observation by training LLMs on natural language datasets. We find the observed empirical scaling laws closely agree with our theoretical predictions. Taken together, these results rigorously show the intrinsic dimension of data to be a crucial quantity affecting transformer scaling laws in both theory and practice.",
      "authors": [
        "Alexander Havrilla",
        "Wenjing Liao"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=N2wYPMpifA",
      "cdate": 1715774189141,
      "mdate": 1736187710766,
      "matched_keywords": [
        "large language model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.446915"
    },
    {
      "id": "a6HzEu4Kpo",
      "title": "Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series OOD Generalization",
      "abstract": "Out-of-Distribution (OOD) generalization in machine learning is a burgeoning area of study. Its primary goal is to enhance the adaptability and resilience of machine learning models when faced with new, unseen, and potentially adversarial data that significantly diverges from their original training datasets. In this paper, we investigate time series OOD generalization via pre-trained Large Language Models (LLMs). We first propose a novel \\textbf{T}ri-level learning framework for \\textbf{T}ime \\textbf{S}eries \\textbf{O}OD generalization, termed TTSO, which considers both sample-level and group-level uncertainties. This formula offers a fresh theoretic perspective for formulating and analyzing OOD generalization problem. In addition, we provide a theoretical analysis to justify this method is well motivated. We then develop a stratified localization algorithm tailored for this tri-level optimization problem, theoretically demonstrating the guaranteed convergence of the proposed algorithm. Our analysis also reveals that the iteration complexity to obtain an $\\epsilon$-stationary point is bounded by O($\\frac{1}{\\epsilon^{2}}$). Extensive experiments on real-world datasets have been conducted to elucidate the effectiveness of the proposed method.",
      "authors": [
        "Chengtao Jian",
        "Kai Yang",
        "Yang Jiao"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=a6HzEu4Kpo",
      "cdate": 1715774148914,
      "mdate": 1734579672348,
      "matched_keywords": [
        "large language model",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446920"
    },
    {
      "id": "QoWf3lo6m7",
      "title": "Towards a Theoretical Understanding of the 'Reversal Curse' via Training Dynamics",
      "abstract": "Auto-regressive large language models (LLMs) show impressive capacities to solve many complex reasoning tasks while struggling with some simple logical reasoning tasks such as inverse search: when trained on ''$A \\to B$'' (e.g., *Tom is the parent of John*), LLM fails to directly conclude ''$B \\gets A$'' (e.g., *John is the child of Tom*) during inference even if the two sentences are semantically identical, which is known as the ''reversal curse''. In this paper, we theoretically analyze the reversal curse via the training dynamics of (stochastic) gradient descent for two auto-regressive models: (1) a bilinear model that can be viewed as a simplification of a one-layer transformer; (2) one-layer transformers under certain assumptions. Our analysis reveals that for both models, the reversal curse is a consequence of the (effective) model weights *asymmetry*, i.e., the increase of weights from a token $A$ to token $B$ during training does not necessarily cause the increase of the weights from $B$ to $A$, which is caused by the training dynamics under certain choice of loss function and the optimization space of model parameters. Moreover, our analysis can be naturally applied to other logical reasoning tasks such as chain-of-thought (COT), which provides a new perspective different from previous work that focuses on expressivity. Finally, we conduct experiments to validate our theory on multi-layer transformers under different settings. Our code is available at [https://github.com/marlo-z/reversal_curse_analysis/](https://github.com/marlo-z/reversal_curse_analysis/).",
      "authors": [
        "Hanlin Zhu",
        "Baihe Huang",
        "Shaolun Zhang",
        "Michael Jordan",
        "Jiantao Jiao",
        "Yuandong Tian",
        "Stuart Russell"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=QoWf3lo6m7",
      "cdate": 1715774099636,
      "mdate": 1730873981122,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.446925"
    },
    {
      "id": "hWRVbdAWiS",
      "title": "Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning",
      "abstract": "Diffusion policy has shown a strong ability to express complex action distributions in offline reinforcement learning (RL). However, it suffers from overestimating Q-value functions on out-of-distribution (OOD) data points due to the offline dataset limitation. To address it, this paper proposes a novel entropy-regularized diffusion policy and takes into account the confidence of the Q-value prediction with Q-ensembles. At the core of our diffusion policy is a mean-reverting stochastic differential equation (SDE) that transfers the action distribution into a standard Gaussian form and then samples actions conditioned on the environment state with a corresponding reverse-time process. We show that the entropy of such a policy is tractable and that can be used to increase the exploration of OOD samples in offline RL training. Moreover, we propose using the lower confidence bound of Q-ensembles for pessimistic Q-value function estimation. The proposed approach demonstrates state-of-the-art performance across a range of tasks in the D4RL benchmarks, significantly improving upon existing diffusion-based policies. The code is available at https://github.com/ruoqizzz/entropy-offlineRL.",
      "authors": [
        "Ruoqi Zhang",
        "Ziwei Luo",
        "Jens Sjölund",
        "Thomas B. Schön",
        "Per Mattsson"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hWRVbdAWiS",
      "cdate": 1715774046443,
      "mdate": 1730873981097,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446930"
    },
    {
      "id": "K3h2kZFz8h",
      "title": "An Analytical Study of Utility Functions in Multi-Objective Reinforcement Learning",
      "abstract": "Multi-objective reinforcement learning (MORL) is an excellent framework for multi-objective sequential decision-making. MORL employs a utility function to aggregate multiple objectives into one that expresses a user's preferences. However, MORL still misses two crucial theoretical analyses of the properties of utility functions: (1) a characterisation of the utility functions for which an associated optimal policy exists, and (2) a characterisation of the types of preferences that can be expressed as utility functions. As a result, we formally characterise the families of preferences and utility functions that MORL should focus on: those for which an optimal policy is guaranteed to exist. We expect our theoretical results to promote the development of novel MORL algorithms that exploit our theoretical findings.",
      "authors": [
        "Manel Rodriguez-Soto",
        "Juan Antonio Rodriguez Aguilar",
        "Maite López-Sánchez"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=K3h2kZFz8h",
      "cdate": 1715773871882,
      "mdate": 1730873980970,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446935"
    },
    {
      "id": "LPkcoml66W",
      "title": "Visual Pinwheel Centers Act as Geometric Saliency Detectors",
      "abstract": "During natural evolution, the primary visual cortex (V1) of lower mammals typically forms salt-and-pepper organizations, while higher mammals and primates develop pinwheel structures with distinct topological properties. Despite the general belief that V1 neurons primarily serve as edge detectors, the functional advantages of pinwheel structures over salt-and-peppers are not well recognized. To this end, we propose a two-dimensional self-evolving spiking neural network that integrates Hebbian-like plasticity and empirical morphological data. Through extensive exposure to image data, our network evolves from salt-and-peppers to pinwheel structures, with neurons becoming localized bandpass filters responsive to various orientations. This transformation is accompanied by an increase in visual field overlap. Our findings indicate that neurons in pinwheel centers (PCs) respond more effectively to complex spatial textures in natural images, exhibiting quicker responses than those in salt-and-pepper organizations. PCs act as first-order stage processors with heightened sensitivity and reduced latency to intricate contours, while adjacent iso-orientation domains serve as second-order stage processors that refine edge representations for clearer perception. This study presents the first theoretical evidence that pinwheel structures function as crucial detectors of spatial contour saliency in the visual cortex.",
      "authors": [
        "Haixin Zhong",
        "Mingyi Huang",
        "Wei P Dai",
        "Haoyu Wang",
        "Anna Wang Roe",
        "yuguo yu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LPkcoml66W",
      "cdate": 1715773803750,
      "mdate": 1736934616023,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.446940"
    },
    {
      "id": "qf2uZAdy1N",
      "title": "Reinforcement Learning Under Latent Dynamics: Toward Statistical and Algorithmic Modularity",
      "abstract": "Real-world applications of reinforcement learning often involve  environments where agents operate on complex, high-dimensional observations, but the underlying (``latent'')  dynamics are comparatively simple. However, beyond restrictive settings\n  such as tabular latent dynamics,  the fundamental statistical requirements and algorithmic principles for *reinforcement learning under latent dynamics* are poorly\n  understood.\n\n  This paper addresses the question of reinforcement learning under *general latent dynamics* from a\n  statistical and algorithmic perspective.  On the statistical side, our main negative\nresult shows that *most* well-studied settings for reinforcement learning with function approximation become intractable when composed with rich observations; we complement this with a positive result, identifying *latent pushforward coverability* as a\ngeneral condition that enables statistical tractability. Algorithmically, we develop provably efficient *observable-to-latent* reductions ---that is, reductions that transform an arbitrary algorithm for the\n  latent MDP into an algorithm that can operate on rich observations--- in two settings: one where the agent has access to hindsight\nobservations of the latent dynamics (Lee et al., 2023) and one\nwhere the agent can estimate *self-predictive* latent models (Schwarzer et al., 2020). Together, our results serve as a\n  first step toward a unified statistical and algorithmic theory for\nreinforcement learning under latent dynamics.",
      "authors": [
        "Philip Amortila",
        "Dylan J Foster",
        "Nan Jiang",
        "Akshay Krishnamurthy",
        "Zakaria Mhammedi"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qf2uZAdy1N",
      "cdate": 1715773704689,
      "mdate": 1730873980831,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446945"
    },
    {
      "id": "W5U3XB1C11",
      "title": "Relational Verification Leaps Forward with RABBit",
      "abstract": "We propose RABBit, a Branch-and-Bound-based verifier for verifying relational properties defined over Deep Neural Networks, such as robustness against universal adversarial perturbations (UAP). Existing SOTA complete $L_{\\infty}$-robustness verifiers can not reason about dependencies between multiple executions and, as a result, are imprecise for relational verification. In contrast, existing SOTA relational verifiers only apply a single bounding step and do not utilize any branching strategies to refine the obtained bounds, thus producing imprecise results. We develop the first scalable Branch-and-Bound-based relational verifier, RABBit, which efficiently combines branching over multiple executions with cross-executional bound refinement to utilize relational constraints, gaining substantial precision over SOTA baselines on a wide range of datasets and networks. Our code is at https://github.com/uiuc-focal-lab/RABBit.",
      "authors": [
        "Tarun Suresh",
        "Debangshu Banerjee",
        "Gagandeep Singh"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=W5U3XB1C11",
      "cdate": 1715773103945,
      "mdate": 1730873980564,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.446949"
    },
    {
      "id": "nRdST1qifJ",
      "title": "Fight Back Against Jailbreaking via Prompt Adversarial Tuning",
      "abstract": "While Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to jailbreaking attacks. Several primary defense strategies have been proposed to protect LLMs from producing harmful information, mostly focusing on model fine-tuning or heuristical defense designs. However, how to achieve intrinsic robustness through prompt optimization remains an open problem. In this paper, motivated by adversarial training paradigms for achieving reliable robustness, we propose an approach named **Prompt Adversarial Tuning (PAT)** that trains a prompt control attached to the user prompt as a guard prefix. To achieve our defense goal whilst maintaining natural performance, we optimize the control prompt with both adversarial and benign prompts. Comprehensive experiments show that our method is effective against both grey-box and black-box attacks, reducing the success rate of advanced attacks to nearly 0, while maintaining the model's utility on the benign task and incurring only negligible computational overhead, charting a new perspective for future explorations in LLM security. Our code is available at https://github.com/PKU-ML/PAT.",
      "authors": [
        "Yichuan Mo",
        "Yuji Wang",
        "Zeming Wei",
        "Yisen Wang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nRdST1qifJ",
      "cdate": 1715773008594,
      "mdate": 1730873980534,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.446955"
    },
    {
      "id": "RnQdRY1h5v",
      "title": "B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory",
      "abstract": "We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span ('context' in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access short-term eidetic memory 'in-context,' permanent structural memory 'in-weights,' fading memory 'in-state,' and long-term eidetic memory 'in-storage' by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B'MOJO and describe a basic implementation that can be stacked and scaled efficiently in hardware. We test B'MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B'MOJO achieves perplexity comparable to similarly-sized Transformers and SSMs up to 1.4B parameters, while being up to 10% faster to train. Finally, we test whether models trained inductively on a-priori bounded sequences (up to 8K tokens) can still perform transductive inference on sequences many-fold longer. B'MOJO's ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training.",
      "authors": [
        "Luca Zancato",
        "Arjun Seshadri",
        "Yonatan Dukler",
        "Aditya Golatkar",
        "Yantao Shen",
        "Benjamin Bowman",
        "Matthew Trager",
        "Alessandro Achille",
        "Stefano Soatto"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=RnQdRY1h5v",
      "cdate": 1715772892742,
      "mdate": 1736907986379,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:09.446959"
    },
    {
      "id": "55zLbH7dE1",
      "title": "Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution",
      "abstract": "Embedding plays a key role in modern recommender systems because they are virtual representations of real-world entities and the foundation for subsequent decision-making models.  In this paper, we propose a novel embedding update mechanism, Structure-aware Embedding Evolution (SEvo for short), to encourage related nodes to evolve similarly at each step. Unlike GNN (Graph Neural Network) that typically serves as an intermediate module, SEvo is able to directly inject graph structural information into embedding with minimal computational overhead during training.  The convergence properties of SEvo along with its potential variants are theoretically analyzed to justify the validity of the designs.  Moreover, SEvo can be seamlessly integrated into existing optimizers for state-of-the-art performance. Particularly SEvo-enhanced AdamW with moment estimate correction demonstrates consistent improvements across a spectrum of models and datasets, suggesting a novel technical route to effectively utilize graph structural information beyond explicit GNN modules.",
      "authors": [
        "Cong Xu",
        "Jun Wang",
        "Jianyong Wang",
        "Wei Zhang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=55zLbH7dE1",
      "cdate": 1715772653702,
      "mdate": 1730873980321,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.446965"
    },
    {
      "id": "3f8i9GlBzu",
      "title": "Can Transformers Smell Like Humans?",
      "abstract": "The human brain encodes stimuli from the environment into representations that form a sensory perception of the world. Despite recent advances in understanding visual and auditory perception, olfactory perception remains an under-explored topic in the machine learning community due to the lack of large-scale datasets annotated with labels of human olfactory perception. In this work, we ask the question of whether pre-trained transformer models of chemical structures encode representations that are aligned with human olfactory perception, i.e., can transformers smell like humans? We demonstrate that representations encoded from transformers pre-trained on general chemical structures are highly aligned with human olfactory perception.  We use multiple datasets and different types of perceptual representations to show that the representations encoded by transformer models are able to predict: (i) labels associated with odorants‌‌ provided by experts; (ii) continuous ratings provided by human participants with respect to pre-defined descriptors; and (iii) similarity ratings between odorants provided by human participants. Finally, we evaluate the extent to which this alignment is associated with physicochemical features of odorants known to be relevant for olfactory decoding.",
      "authors": [
        "Farzaneh Taleb",
        "Miguel Vasco",
        "Antonio H. Ribeiro",
        "Mårten Björkman",
        "Danica Kragic"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3f8i9GlBzu",
      "cdate": 1715772629939,
      "mdate": 1730873980272,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446969"
    },
    {
      "id": "qTypwXvNJa",
      "title": "Geodesic Optimization for Predictive Shift Adaptation on EEG data",
      "abstract": "Electroencephalography (EEG) data is often collected from diverse contexts involving different populations and EEG devices. This variability can induce distribution shifts in the data $X$ and in the biomedical variables of interest $y$, thus limiting the application of supervised machine learning (ML) algorithms. While domain adaptation (DA) methods have been developed to mitigate the impact of these shifts, such methods struggle when distribution shifts occur simultaneously in $X$ and $y$. As state-of-the-art ML models for EEG represent the data by spatial covariance matrices, which lie on the Riemannian manifold of Symmetric Positive Definite (SPD) matrices, it is appealing to study DA techniques operating on the SPD manifold. This paper proposes a novel method termed Geodesic Optimization for Predictive Shift Adaptation (GOPSA) to address test-time multi-source DA for situations in which source domains have distinct $y$ distributions. GOPSA exploits the geodesic structure of the Riemannian manifold to jointly learn a domain-specific re-centering operator representing site-specific intercepts and the regression model. We performed empirical benchmarks on the cross-site generalization of age-prediction models with resting-state EEG data from a large multi-national dataset (HarMNqEEG), which included $14$ recording sites and more than $1500$ human participants. Compared to state-of-the-art methods, our results showed that GOPSA achieved significantly higher performance on three regression metrics ($R^2$, MAE, and Spearman's $\\rho$) for several source-target site combinations, highlighting its effectiveness in tackling multi-source DA with predictive shifts in EEG data analysis. Our method has the potential to combine the advantages of mixed-effects modeling with machine learning for biomedical applications of EEG, such as multicenter clinical trials.",
      "authors": [
        "Apolline Mellot",
        "Antoine Collas",
        "Sylvain Chevallier",
        "Alexandre Gramfort",
        "Denis Alexander Engemann"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qTypwXvNJa",
      "cdate": 1715772615309,
      "mdate": 1730873980237,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446974"
    },
    {
      "id": "08GbdALmEs",
      "title": "Learning Versatile Skills with Curriculum Masking",
      "abstract": "Masked prediction has emerged as a promising pretraining paradigm in offline reinforcement learning (RL) due to its versatile masking schemes, enabling flexible inference across various downstream tasks with a unified model. Despite the versatility of masked prediction, it remains unclear how to balance the learning of skills at different levels of complexity. To address this, we propose CurrMask, a curriculum masking pretraining paradigm for sequential decision making. Motivated by how humans learn by organizing knowledge in a curriculum, CurrMask adjusts its masking scheme during pretraining for learning versatile skills.  Through extensive experiments, we show that CurrMask exhibits superior zero-shot performance on skill prompting tasks, goal-conditioned planning tasks, and competitive finetuning performance on offline RL tasks. Additionally, our analysis of training dynamics reveals that CurrMask gradually acquires skills of varying complexity by dynamically adjusting its masking scheme.",
      "authors": [
        "Yao Tang",
        "Zhihui Xie",
        "Zichuan Lin",
        "Deheng Ye",
        "Shuai Li"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=08GbdALmEs",
      "cdate": 1715772201814,
      "mdate": 1734722252075,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446979"
    },
    {
      "id": "VVd3iOKPMJ",
      "title": "Learning from Offline Foundation Features with Tensor Augmentations",
      "abstract": "We introduce Learning from Offline Foundation Features with Tensor Augmentations (LOFF-TA), an efficient training scheme designed to harness the capabilities of foundation models in limited resource settings where their direct development is not feasible. LOFF-TA involves training a compact classifier on cached feature embeddings  from a frozen foundation model, resulting in up to  $37\\times$ faster training and up to $26\\times$ reduced GPU memory usage. Because the embeddings of augmented images would be too numerous to store, yet the augmentation process is essential for training, we propose to apply tensor augmentations to the cached embeddings of the original non-augmented images. LOFF-TA makes it possible to leverage the power of foundation models, regardless of their size, in settings with limited computational capacity. Moreover, LOFF-TA can be used to apply foundation models to high-resolution images without increasing compute.    In certain scenarios, we find that training with LOFF-TA yields better results than directly fine-tuning the foundation model.",
      "authors": [
        "Emir Konuk",
        "Christos Matsoukas",
        "Moein Sorkhei",
        "Phitchapha Lertsiravarameth",
        "Kevin Smith"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=VVd3iOKPMJ",
      "cdate": 1715772168365,
      "mdate": 1730873980082,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:09.446984"
    },
    {
      "id": "EIl9qmMmvy",
      "title": "Diffusion-DICE: In-Sample Diffusion Guidance for Offline Reinforcement Learning",
      "abstract": "One important property of DIstribution Correction Estimation (DICE) methods is that the solution is the optimal stationary distribution ratio between the optimized and data collection policy. In this work, we show that DICE-based methods can be viewed as a transformation from the behavior distribution to the optimal policy distribution. Based on this, we propose a novel approach, Diffusion-DICE, that directly performs this transformation using diffusion models. We find that the optimal policy's score function can be decomposed into two terms: the behavior policy's score function and the gradient of a guidance term which depends on the optimal distribution ratio. The first term can be obtained from a diffusion model trained on the dataset and we propose an in-sample learning objective to learn the second term. Due to the multi-modality contained in the optimal policy distribution, the transformation in Diffusion-DICE may guide towards those local-optimal modes. We thus generate a few candidate actions and carefully select from them to achieve global-optimum. Different from all other diffusion-based offline RL methods, the \\textit{guide-then-select} paradigm in Diffusion-DICE only uses in-sample actions for training and brings minimal error exploitation in the value function. We use a didatic toycase example to show how previous diffusion-based methods fail to generate optimal actions due to leveraging these errors and how Diffusion-DICE successfully avoid that. We then conduct extensive experiments on benchmark datasets to show the strong performance of Diffusion-DICE.",
      "authors": [
        "Liyuan Mao",
        "Haoran Xu",
        "Xianyuan Zhan",
        "Weinan Zhang",
        "Amy Zhang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EIl9qmMmvy",
      "cdate": 1715771983949,
      "mdate": 1730873979915,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446988"
    },
    {
      "id": "Pox8jNQOo5",
      "title": "Second-order forward-mode optimization of recurrent neural networks for neuroscience",
      "abstract": "A common source of anxiety for the computational neuroscience student is the question “will my recurrent neural network (RNN) model finally learn that task?”. Unlike in machine learning where any architectural modification of an RNN (e.g. GRU or LSTM) is acceptable if it speeds up training, the RNN models trained as _models of brain dynamics_ are subject to plausibility constraints that fundamentally exclude the usual machine learning hacks. The “vanilla” RNNs commonly used in computational neuroscience find themselves plagued by ill-conditioned loss surfaces that complicate training and significantly hinder our capacity to investigate the brain dynamics underlying complex tasks. Moreover, some tasks may require very long time horizons which backpropagation cannot handle given typical GPU memory limits. Here, we develop SOFO, a second-order optimizer that efficiently navigates loss surfaces whilst _not_ requiring backpropagation. By relying instead on easily parallelized batched forward-mode differentiation, SOFO enjoys constant memory cost in time. Morever, unlike most second-order optimizers which involve inherently sequential operations, SOFO's effective use of GPU parallelism yields a per-iteration wallclock time essentially on par with first-order gradient-based optimizers. We show vastly superior performance compared to Adam on a number of RNN tasks, including a difficult double-reaching motor task and the learning of an adaptive Kalman filter algorithm trained over a long horizon.",
      "authors": [
        "Youjing Yu",
        "Rui Xia",
        "Qingxi Ma",
        "Máté Lengyel",
        "Guillaume Hennequin"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Pox8jNQOo5",
      "cdate": 1715771567924,
      "mdate": 1730873979695,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446993"
    },
    {
      "id": "UuiZEOVtHx",
      "title": "Safe and Efficient: A Primal-Dual Method for Offline Convex CMDPs under Partial Data Coverage",
      "abstract": "Offline safe reinforcement learning (RL) aims to find an optimal policy using a pre-collected dataset when data collection is impractical or risky. We propose a novel linear programming (LP) based primal-dual algorithm for convex MDPs that incorporates ``uncertainty'' parameters to improve data efficiency while requiring only partial data coverage assumption. Our theoretical results achieve a sample complexity of $\\mathcal{O}(1/(1-\\gamma)\\sqrt{n})$ under general function approximation, improving the current state-of-the-art by a factor of $1/(1-\\gamma)$, where $n$ is the number of data samples in an offline dataset, and $\\gamma$ is the discount factor. The numerical experiments validate our theoretical findings, demonstrating the practical efficacy of our approach in achieving improved safety and learning efficiency in safe offline settings.",
      "authors": [
        "Haobo Zhang",
        "Xiyue Peng",
        "Honghao Wei",
        "Xin Liu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=UuiZEOVtHx",
      "cdate": 1715771444656,
      "mdate": 1730873979722,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.446998"
    },
    {
      "id": "woRFmNJiLp",
      "title": "Alignment at Pre-training! Towards Native Alignment for Arabic LLMs",
      "abstract": "The alignment of large language models (LLMs) is critical for developing effective and safe language models. Traditional approaches focus on aligning models during the instruction tuning or reinforcement learning stages, referred to in this paper as `\\textit{post alignment}'. We argue that alignment during the pre-training phase, which we term 'native alignment', warrants investigation. Native alignment aims to prevent unaligned content from the beginning, rather than relying on post-hoc processing. This approach leverages extensively aligned pre-training data to enhance the effectiveness and usability of pre-trained models. Our study specifically explores the application of native alignment in the context of Arabic LLMs. We conduct comprehensive experiments and ablation studies to evaluate the impact of native alignment on model performance and alignment stability. Additionally, we release open-source Arabic LLMs that demonstrate state-of-the-art performance on various benchmarks, providing significant benefits to the Arabic LLM community.",
      "authors": [
        "Juhao Liang",
        "Zhenyang Cai",
        "Jianqing Zhu",
        "Huang Huang",
        "Kewei Zong",
        "Bang An",
        "Mosen Alharthi",
        "Juncai He",
        "Lian Zhang",
        "Haizhou Li",
        "Benyou Wang",
        "Jinchao Xu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=woRFmNJiLp",
      "cdate": 1715771409719,
      "mdate": 1730873979638,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447002"
    },
    {
      "id": "fs28jccJj5",
      "title": "SpikedAttention: Training-Free and Fully Spike-Driven Transformer-to-SNN Conversion with Winner-Oriented Spike Shift for Softmax Operation",
      "abstract": "Event-driven spiking neural networks(SNNs) are promising neural networks that reduce the energy consumption of continuously growing AI models. Recently, keeping pace with the development of transformers, transformer-based SNNs were presented. Due to the incompatibility of self-attention with spikes, however, existing transformer-based SNNs limit themselves by either restructuring self-attention architecture or conforming to non-spike computations. In this work, we propose a novel transformer-to-SNN conversion method that outputs an end-to-end spike-based transformer, named SpikedAttention. Our method directly converts the well-trained transformer without modifying its attention architecture. For the vision task, the proposed method converts Swin Transformer into an SNN without post-training or conversion-aware training, achieving state-of-the-art SNN accuracy on ImageNet dataset, i.e., 80.0\\% with 28.7M parameters. Considering weight accumulation, neuron potential update, and on-chip data movement, SpikedAttention reduces energy consumption by 42\\% compared to the baseline ANN, i.e., Swin-T. Furthermore, for the first time, we demonstrate that SpikedAttention successfully converts a BERT model to an SNN with only 0.3\\% accuracy loss on average consuming 58\\% less energy on GLUE benchmark. Our code is available at Github ( https://github.com/sangwoohwang/SpikedAttention ).",
      "authors": [
        "Sangwoo Hwang",
        "Seunghyun Lee",
        "Dahoon Park",
        "Donghun Lee",
        "Jaeha Kung"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fs28jccJj5",
      "cdate": 1715771334740,
      "mdate": 1730873979581,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447007"
    },
    {
      "id": "aNHEqFMS0N",
      "title": "An Efficient Recipe for Long Context Extension via Middle-Focused Positional Encoding",
      "abstract": "Recently, many methods have been developed to extend the context length of pre-trained large language models (LLMs), but they often require fine-tuning at the target length ($\\gg4K$) and struggle to effectively utilize information from the middle part of the context. To address these issues, we propose $\\textbf{C}$ontinuity-$\\textbf{R}$elativity ind$\\textbf{E}$xing with g$\\textbf{A}$ussian $\\textbf{M}$iddle ($\\texttt{CREAM}$), which interpolates positional encodings by manipulating position indices. Apart from being simple, $\\texttt{CREAM}$ is training-efficient: it only requires fine-tuning at the pre-trained context window (e.g., Llama 2-4K) and can extend LLMs to a much longer target context length (e.g., 256K). To ensure that the model focuses more on the information in the middle, we introduce a truncated Gaussian to encourage sampling from the middle part of the context during fine-tuning, thus alleviating the ''Lost-in-the-Middle'' problem faced by long-context LLMs. Experimental results show that $\\texttt{CREAM}$ successfully extends LLMs to the target length for both Base and Chat versions of $\\texttt{Llama2-7B}$ with ``Never Miss A Beat''. Our code is publicly available at https://github.com/bigai-nlco/cream.",
      "authors": [
        "Tong Wu",
        "Yanpeng Zhao",
        "Zilong Zheng"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=aNHEqFMS0N",
      "cdate": 1715770949249,
      "mdate": 1730873979456,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447012"
    },
    {
      "id": "fkuseU0nJs",
      "title": "Active Sequential Posterior Estimation for Sample-Efficient Simulation-Based Inference",
      "abstract": "Computer simulations have long presented the exciting possibility of scientific insight into complex real-world processes. Despite the power of modern computing, however, it remains challenging to systematically perform inference under simulation models. This has led to the rise of simulation-based inference (SBI), a class of machine learning-enabled techniques for approaching inverse problems with stochastic simulators. Many such methods, however, require large numbers of simulation samples and face difficulty scaling to high-dimensional settings, often making inference prohibitive under resource-intensive simulators. To mitigate these drawbacks, we introduce active sequential neural posterior estimation (ASNPE). ASNPE brings an active learning scheme into the inference loop to estimate the utility of simulation parameter candidates to the underlying probabilistic model. The proposed acquisition scheme is easily integrated into existing posterior estimation pipelines, allowing for improved sample efficiency with low computational overhead. We further demonstrate the effectiveness of the proposed method in the travel demand calibration setting, a high-dimensional inverse problem commonly requiring computationally expensive traffic simulators. Our method outperforms well-tuned benchmarks and state-of-the-art posterior estimation methods on a large-scale real-world traffic network, as well as demonstrates a performance advantage over non-active counterparts on a suite of SBI benchmark environments.",
      "authors": [
        "Sam Griesemer",
        "Defu Cao",
        "Zijun Cui",
        "Carolina Osorio",
        "Yan Liu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fkuseU0nJs",
      "cdate": 1715770799963,
      "mdate": 1737243198654,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447017"
    },
    {
      "id": "BGOGknwHbi",
      "title": "Self-Guiding Exploration for Combinatorial Problems",
      "abstract": "Large Language Models (LLMs) have become pivotal in addressing reasoning tasks across diverse domains, including arithmetic, commonsense, and symbolic reasoning. They utilize prompting techniques such as Exploration-of-Thought, Decomposition, and Refinement to effectively navigate and solve intricate tasks. Despite these advancements, the application of LLMs to Combinatorial Problems (CPs), known for their NP-hardness and critical roles in logistics and resource management remains underexplored. To address this gap, we introduce a novel prompting strategy: Self-Guiding Exploration (SGE), designed to enhance the performance of solving CPs. SGE operates autonomously, generating multiple thought trajectories for each CP task. It then breaks these trajectories down into actionable subtasks, executes them sequentially, and refines the results to ensure optimal outcomes. We present our research as the first to apply LLMs to a broad range of CPs and demonstrate that SGE outperforms existing prompting strategies by over 27.84% in CP optimization performance. Additionally, SGE achieves a 2.46% higher accuracy over the best existing results in other reasoning tasks (arithmetic, commonsense, and symbolic).",
      "authors": [
        "Zangir Iklassov",
        "Yali Du",
        "Farkhad Akimov",
        "Martin Takáč"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BGOGknwHbi",
      "cdate": 1715770690831,
      "mdate": 1730873979349,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447021"
    },
    {
      "id": "eUcyIe1AzY",
      "title": "Generating Origin-Destination Matrices in Neural Spatial Interaction Models",
      "abstract": "Agent-based models (ABMs) are proliferating as decision-making tools across policy areas in transportation, economics, and epidemiology. In these models, a central object of interest is the discrete origin-destination matrix which captures spatial interactions and agent trip counts between locations. Existing approaches resort to continuous approximations of this matrix and subsequent ad-hoc discretisations in order to perform ABM simulation and calibration. This impedes conditioning on partially observed summary statistics, fails to explore the multimodal matrix distribution over a discrete combinatorial support, and incurs discretisation errors. To address these challenges, we introduce a computationally efficient framework that scales linearly with the number of origin-destination pairs, operates directly on the discrete combinatorial space, and learns the agents' trip intensity through a neural differential equation that embeds spatial interactions. Our approach outperforms the prior art in terms of reconstruction error and ground truth matrix coverage, at a fraction of the computational cost. We demonstrate these benefits in two large-scale spatial mobility ABMs in Washington, DC and Cambridge, UK.",
      "authors": [
        "Ioannis Zachos",
        "Mark Girolami",
        "Theodoros Damoulas"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=eUcyIe1AzY",
      "cdate": 1715770419822,
      "mdate": 1737133483867,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.447026"
    },
    {
      "id": "tSWoT8ttkO",
      "title": "Efficient Recurrent Off-Policy RL Requires a Context-Encoder-Specific Learning Rate",
      "abstract": "Real-world decision-making tasks are usually partially observable Markov decision processes (POMDPs), where the state is not fully observable. Recent progress has demonstrated that recurrent reinforcement learning (RL), which consists of a context encoder based on recurrent neural networks (RNNs) for unobservable state prediction and a multilayer perceptron (MLP) policy for decision making, can mitigate partial observability and serve as a robust baseline for POMDP tasks. However, prior recurrent RL algorithms have faced issues with training instability. In this paper, we find that this instability stems from the autoregressive nature of RNNs, which causes even small changes in RNN parameters to produce large output variations over long trajectories. Therefore, we propose **R**ecurrent Off-policy RL with Context-**E**ncoder-**S**p**e**cific **L**earning Rate (RESeL) to tackle this issue. Specifically, RESeL uses a lower learning rate for context encoder than other MLP layers to ensure the stability of the former while maintaining the training efficiency of the latter. We integrate this technique into existing off-policy RL methods, resulting in the RESeL algorithm. We evaluated RESeL in 18 POMDP tasks, including classic, meta-RL, and credit assignment scenarios, as well as five MDP locomotion tasks. The experiments demonstrate significant improvements in training stability with RESeL. Comparative results show that RESeL achieves notable performance improvements over previous recurrent RL baselines in POMDP tasks, and is competitive with or even surpasses state-of-the-art methods in MDP tasks. Further ablation studies highlight the necessity of applying a distinct learning rate for the context encoder. Code is available at https://github.com/FanmingL/Recurrent-Offpolicy-RL.",
      "authors": [
        "Fan-Ming Luo",
        "Zuolin Tu",
        "Zefang Huang",
        "Yang Yu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=tSWoT8ttkO",
      "cdate": 1715769911325,
      "mdate": 1736842295651,
      "matched_keywords": [
        "reinforcement learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447031"
    },
    {
      "id": "isZ8XRe3De",
      "title": "Customizing Language Models with Instance-wise LoRA for Sequential Recommendation",
      "abstract": "Sequential recommendation systems predict the next interaction item based on users' past interactions, aligning recommendations with individual preferences. Leveraging the strengths of Large Language Models (LLMs) in knowledge comprehension and reasoning, recent approaches  are eager to apply LLMs to sequential recommendation. A common  paradigm is converting user behavior sequences into instruction data, and fine-tuning the LLM with parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA). However, the uniform application of LoRA across diverse user behaviors is insufficient to capture individual variability, resulting in negative transfer between disparate sequences.\nTo address these challenges, we propose Instance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation task as a form of multi-task learning, integrating LoRA with the Mixture of Experts (MoE) framework. This approach encourages different experts to capture various aspects of user behavior. Additionally, we introduce a sequence representation guided gate function that generates customized expert participation weights for each user sequence, which allows dynamic parameter adjustment for instance-wise recommendations. \nIn sequential recommendation, iLoRA achieves an average relative improvement of 11.4\\% over basic LoRA in the hit ratio metric, with less than a 1\\% relative increase in trainable parameters.\nExtensive experiments on three benchmark datasets demonstrate the effectiveness of iLoRA, highlighting its superior performance compared to existing methods in mitigating negative transfer and improving recommendation accuracy.\nOur data and code are available at https://github.com/AkaliKong/iLoRA.",
      "authors": [
        "Xiaoyu Kong",
        "Jiancan Wu",
        "An Zhang",
        "Leheng Sheng",
        "Hui Lin",
        "Xiang Wang",
        "Xiangnan He"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=isZ8XRe3De",
      "cdate": 1715769847090,
      "mdate": 1730873979004,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447036"
    },
    {
      "id": "w6vbfSC1y0",
      "title": "Self-Calibrated Tuning of Vision-Language Models for Out-of-Distribution Detection",
      "abstract": "Out-of-distribution (OOD) detection is crucial for deploying reliable machine learning models in open-world applications. Recent advances in CLIP-based OOD detection have shown promising results via regularizing prompt tuning with OOD features extracted from ID data. However, the irrelevant context mined from ID data can be spurious due to the inaccurate foreground-background decomposition, thus limiting the OOD detection performance. In this work, we propose a novel framework, namely, \\textit{Self-Calibrated Tuning (SCT)}, to mitigate this problem for effective OOD detection with only the given few-shot ID data. Specifically, SCT introduces modulating factors respectively on the two components of the original learning objective. It adaptively directs the optimization process between the two tasks during training on data with different prediction uncertainty to calibrate the influence of OOD regularization, which is compatible with many prompt tuning based OOD detection methods. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed SCT. The code is publicly available at: https://github.com/tmlr-group/SCT.",
      "authors": [
        "Geng Yu",
        "Jianing Zhu",
        "Jiangchao Yao",
        "Bo Han"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=w6vbfSC1y0",
      "cdate": 1715769531841,
      "mdate": 1730873978742,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447041"
    },
    {
      "id": "zaXuMqOAF4",
      "title": "Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs",
      "abstract": "Large language models (LLMs), although having revolutionized many fields, still suffer from the challenging extrapolation problem, where the inference ability of LLMs sharply declines beyond their max training lengths. In this work, we conduct a theoretical analysis to better understand why No Position Encoding (NoPE) fails outside its effective range, as well as examining the power of Position Encoding (PE) in this context. Our findings reveal that with meticulous weave position, PE can indeed be extended beyond effective range. Our theorems establish that LLMs equipped with weave PE can achieve improved extrapolation performance without additional cost. Furthermore, we introduce a novel weave PE method, Mesa-Extrapolation, which utilizes a chunk-based triangular attention matrix and applies Stair PE to manage the final chunk. This method not only retains competitive performance but also offers substantial benefits such as significantly reduced memory demand and faster inference speed. Extensive experiments validate the effectiveness of Mesa-Extrapolation, demonstrating its potential as a scalable solution to enhancing LLMs’ applicative reach.",
      "authors": [
        "Xin Ma",
        "Yang Liu",
        "Jingjing Liu",
        "Xiaoxu Ma"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=zaXuMqOAF4",
      "cdate": 1715769361913,
      "mdate": 1730873978618,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447046"
    },
    {
      "id": "qdV1vp1AtL",
      "title": "Universal Sample Coding",
      "abstract": "In this work, we study the problem of communicating multiple samples from an unknown probability distribution using as few bits as possible. This is a generalization of the channel simulation problem, which has recently found applications and achieved state of the art results in realistic image compression, neural network compression, and communication-efficient federated learning. In this problem, the transmitter wants the receiver to generate multiple independent and identically distributed (i.i.d.) samples from a target distribution $P$, while the transmitter and the receiver have access to independent samples from a reference distribution $Q$. The core idea is to employ channel simulation in multiple rounds while updating the reference distribution $Q$ after each round in order to reduce the KL-divergence between $P$ and $Q$, thereby reducing the communication cost in subsequent rounds. We derive a lower bound on the expected communication cost and construct a practical algorithm that achieves the lower bound up to a multiplicative constant. We then employ this algorithm in communication-efficient federated learning, in which model updates correspond to samples from a distribution, and achieve a 37% reduction in the communication load. To further highlight the potential of sample communication for generative models, we show that the number of bits needed to communicate samples from a large language model can be reduced by up to 16 times, compared to entropy-based data compression.",
      "authors": [
        "Szymon Kobus",
        "Tze-Yang Tung",
        "Deniz Gunduz"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qdV1vp1AtL",
      "cdate": 1715769233591,
      "mdate": 1735933383996,
      "matched_keywords": [
        "large language model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447051"
    },
    {
      "id": "i8LoWBJf7j",
      "title": "Interpretable Lightweight Transformer via Unrolling of Learned Graph Smoothness Priors",
      "abstract": "We build interpretable and lightweight transformer-like neural networks by unrolling iterative optimization algorithms that minimize graph smoothness priors---the quadratic graph Laplacian regularizer (GLR) and the $\\ell_1$-norm graph total variation (GTV)---subject to an interpolation constraint. The crucial insight is that a normalized signal-dependent graph learning module amounts to a variant of the basic self-attention mechanism in conventional transformers. Unlike \"black-box\" transformers that require learning of large key, query and value matrices to compute scaled dot products as affinities and subsequent output embeddings, resulting in huge parameter sets, our unrolled networks employ shallow CNNs to learn low-dimensional features per node to establish pairwise Mahalanobis distances and construct sparse similarity graphs. At each layer, given a learned graph, the target interpolated signal is simply a low-pass filtered output derived from the minimization of an assumed graph smoothness prior, leading to a dramatic reduction in parameter count. Experiments for two image interpolation applications verify the restoration performance, parameter efficiency and robustness to covariate shift of our graph-based unrolled networks compared to conventional transformers.",
      "authors": [
        "VIET HO TAM THUC DO",
        "Parham Eftekhar",
        "Seyed Alireza Hosseini",
        "Gene Cheung",
        "Philip Chou"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=i8LoWBJf7j",
      "cdate": 1715769083194,
      "mdate": 1730873978496,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447055"
    },
    {
      "id": "rTxCIWsfsD",
      "title": "Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions",
      "abstract": "Real-world offline datasets are often subject to data corruptions (such as noise or adversarial attacks) due to sensor failures or malicious attacks. Despite advances in robust offline reinforcement learning (RL), existing methods struggle to learn robust agents under high uncertainty caused by the diverse corrupted data (i.e., corrupted states, actions, rewards, and dynamics), leading to performance degradation in clean environments. To tackle this problem, we propose a novel robust variational Bayesian inference for offline RL (TRACER). It introduces Bayesian inference for the first time to capture the uncertainty via offline data for robustness against all types of data corruptions. Specifically, TRACER first models all corruptions as the uncertainty in the action-value function. Then, to capture such uncertainty, it uses all offline data as the observations to approximate the posterior distribution of the action-value function under a Bayesian inference framework. An appealing feature of TRACER is that it can distinguish corrupted data from clean data using an entropy-based uncertainty measure, since corrupted data often induces higher uncertainty and entropy. Based on the aforementioned measure, TRACER can regulate the loss associated with corrupted data to reduce its influence, thereby enhancing robustness and performance in clean environments. Experiments demonstrate that TRACER significantly outperforms several state-of-the-art approaches across both individual and simultaneous data corruptions.",
      "authors": [
        "Rui Yang",
        "Jie Wang",
        "Guoping Wu",
        "Bin Li"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=rTxCIWsfsD",
      "cdate": 1715769021547,
      "mdate": 1730873978411,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447060"
    },
    {
      "id": "0VeSCjRDBy",
      "title": "Adversarial Moment-Matching Distillation of Large Language Models",
      "abstract": "Knowledge distillation (KD) has been shown to be highly effective in guiding a student model with a larger teacher model and achieving practical benefits in improving the computational and memory efficiency for large language models (LLMs). State-of-the-art KD methods for LLMs mostly rely on minimizing explicit metrics measuring the divergence between teacher and student probability predictions. Instead of optimizing these mandatory cloning objectives, we explore an imitation learning strategy for KD of LLMs. In particular, we minimize the imitation gap by matching the action-value moments of the teacher's behavior from both on- and off-policy perspectives. To achieve this moment-matching goal, we propose an adversarial training algorithm to jointly estimate the moment-matching distance and optimize the student policy to minimize it. Results from both task-agnostic instruction-following experiments and task-specific experiments demonstrate the effectiveness of our method and achieve new state-of-the-art performance.",
      "authors": [
        "Chen Jia"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0VeSCjRDBy",
      "cdate": 1715768635458,
      "mdate": 1737022922731,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447065"
    },
    {
      "id": "ALISPmDPCq",
      "title": "ConStat: Performance-Based Contamination Detection in Large Language Models",
      "abstract": "Public benchmarks play an essential role in the evaluation of large language models.  However, data contamination can lead to inflated performance, rendering them unreliable for model comparison. It is therefore crucial to detect contamination and estimate its impact on measured performance. Unfortunately, existing detection methods can be easily evaded and fail to quantify contamination. To overcome these limitations, we propose a novel definition of *contamination as artificially inflated and non-generalizing benchmark performance* instead of the inclusion of benchmark samples in the training data. This perspective enables us to detect *any* model with inflated performance, i.e., performance that does not generalize to rephrased samples, synthetic samples from the same distribution, or different benchmarks for the same task. Based on this insight, we develop ConStat, a statistical method that reliably detects and quantifies contamination by comparing performance between a primary and reference benchmark relative to a set of reference models. We demonstrate the effectiveness of ConStat in an extensive evaluation of diverse model architectures, benchmarks, and contamination scenarios and find high levels of contamination in multiple popular models including Mistral, Llama, Yi, and the top-3 Open LLM Leaderboard models.",
      "authors": [
        "Jasper Dekoninck",
        "Mark Niklas Mueller",
        "Martin Vechev"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ALISPmDPCq",
      "cdate": 1715768454586,
      "mdate": 1730873978129,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447071"
    },
    {
      "id": "aQv5AbN1wF",
      "title": "On Feature Learning in Structured State Space Models",
      "abstract": "This paper studies the scaling behavior of state-space models (SSMs) and their structured variants, such as Mamba, that have recently arisen in popularity as alternatives to transformer-based neural network architectures. Specifically, we focus on the capability of SSMs to learn features as their network width approaches infinity. Our findings reveal that established scaling rules, such as the Maximal Update Parameterization, fail to support feature learning as these models cannot be represented in the form of Tensor Programs. Additionally, we demonstrate that spectral scaling conditions, shown to be effective for feature learning in a host of other architectures, do not hold the same implications for SSMs. Through a detailed signal propagation analysis in SSMs, both forward and backward, we identify the appropriate scaling necessary for non-trivial feature evolution in the infinite-width limit. Our proposed scaling shows behavior akin to the Maximal Update Parameterization, such as improved stability, better generalization, and transferability of optimal hyper-parameters from small to large scale SSMs.",
      "authors": [
        "Leena Chennuru Vankadara",
        "Jin Xu",
        "Moritz Haas",
        "Volkan Cevher"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=aQv5AbN1wF",
      "cdate": 1715768411544,
      "mdate": 1730873978051,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447076"
    },
    {
      "id": "d5cKDHCrFJ",
      "title": "EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular Data Classification via Large Language Models",
      "abstract": "Large language models (LLMs) have demonstrated remarkable in-context learning capabilities across diverse applications. In this work, we explore the effectiveness of LLMs for generating realistic synthetic tabular data, identifying key prompt design elements to optimize performance. We introduce EPIC, a novel approach that leverages balanced, grouped data samples and consistent formatting with unique variable mapping to guide LLMs in generating accurate synthetic data across all classes, even for imbalanced datasets. Evaluations on real-world datasets show that EPIC achieves state-of-the-art machine learning classification performance, significantly improving generation efficiency. These findings highlight the effectiveness of EPIC for synthetic tabular data generation, particularly in addressing class imbalance.",
      "authors": [
        "Jinhee Kim",
        "Taesung Kim",
        "Jaegul Choo"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=d5cKDHCrFJ",
      "cdate": 1715768046990,
      "mdate": 1736758355943,
      "matched_keywords": [
        "large language model",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447081"
    },
    {
      "id": "GYqs5Z4joA",
      "title": "SpGesture: Source-Free Domain-adaptive sEMG-based Gesture Recognition with Jaccard Attentive Spiking Neural Network",
      "abstract": "Surface electromyography (sEMG) based gesture recognition offers a natural and intuitive interaction modality for wearable devices. Despite significant advancements in sEMG-based gesture recognition models, existing methods often suffer from high computational latency and increased energy consumption. Additionally, the inherent instability of sEMG signals, combined with their sensitivity to distribution shifts in real-world settings, compromises model robustness. \nTo tackle these challenges, we propose a novel SpGesture framework based on Spiking Neural Networks, which possesses several unique merits compared with existing methods: (1) Robustness: By utilizing membrane potential as a memory list, we pioneer the introduction of Source-Free Domain Adaptation into SNN for the first time. This enables SpGesture to mitigate the accuracy degradation caused by distribution shifts. (2) High Accuracy: With a novel Spiking Jaccard Attention, SpGesture enhances the SNNs' ability to represent sEMG features, leading to a notable rise in system accuracy. To validate SpGesture's performance, we collected a new sEMG gesture dataset which has different forearm postures, where SpGesture achieved the highest accuracy among the baselines ($89.26\\%$). Moreover, the actual deployment on the CPU demonstrated a latency below 100ms, well within real-time requirements. This impressive performance showcases SpGesture's potential to enhance the applicability of sEMG in real-world scenarios. The code is available at https://github.com/guoweiyu/SpGesture/.",
      "authors": [
        "Weiyu Guo",
        "Ying Sun",
        "Yijie Xu",
        "Ziyue Qiao",
        "Yongkui Yang",
        "Hui Xiong"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GYqs5Z4joA",
      "cdate": 1715767857887,
      "mdate": 1735122090890,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447086"
    },
    {
      "id": "kq166jACVP",
      "title": "Aligner: Efficient Alignment by Learning to Correct",
      "abstract": "With the rapid development of large language models (LLMs) and ever-evolving practical requirements, finding an efficient and effective alignment method has never been more critical. However, the tension between the complexity of current alignment methods and the need for rapid iteration in deployment scenarios necessitates the development of a model-agnostic alignment approach that can operate under these constraints. In this paper, we introduce Aligner, a novel and simple alignment paradigm that learns the correctional residuals between preferred and dispreferred answers using a small model. Designed as a model-agnostic, plug-and-play module, Aligner can be directly applied to various open-source and API-based models with only one-off training, making it suitable for rapid iteration. Notably, Aligner can be applied to any powerful, large-scale upstream models. Moreover, it can even iteratively bootstrap the upstream models using corrected responses as synthetic human preference data, breaking through the model's performance ceiling. Our experiments demonstrate performance improvements by deploying the same Aligner model across 11 different LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and honesty). Specifically, Aligner-7B has achieved an average improvement of 68.9% in helpfulness and 22.8% in harmlessness across the tested LLMs while also effectively reducing hallucination. In the Alpaca-Eval leaderboard, stacking Aligner-2B on GPT-4 Turbo improved its LC Win Rate from 55.0% to 58.3%, surpassing GPT-4 Omni's 57.5% Win Rate (community report).",
      "authors": [
        "Jiaming Ji",
        "Boyuan Chen",
        "Hantao Lou",
        "Donghai Hong",
        "Borong Zhang",
        "Xuehai Pan",
        "Tianyi Qiu",
        "Juntao Dai",
        "Yaodong Yang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=kq166jACVP",
      "cdate": 1715767606194,
      "mdate": 1737126458522,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447090"
    },
    {
      "id": "v4dXL3LsGX",
      "title": "Learning to Cooperate with Humans using Generative Agents",
      "abstract": "Training agents that can coordinate zero-shot with humans is a key mission in multi-agent reinforcement learning (MARL). Current algorithms focus on training simulated human partner policies which are then used to train a Cooperator agent. The simulated human is produced either through behavior cloning over a dataset of human cooperation behavior, or by using MARL to create a population of simulated agents. However, these approaches often struggle to produce a Cooperator that can coordinate well with real humans, since the simulated humans fail to cover the diverse strategies and styles employed by people in the real world.  We show \\emph{learning a generative model of human partners} can effectively address this issue. Our model learns a latent variable representation of the human that can be regarded as encoding the human's unique strategy, intention, experience, or style. This generative model can be flexibly trained from any (human or neural policy) agent interaction data. By sampling from the latent space, we can use the generative model to produce different partners to train Cooperator agents. We evaluate our method---Generative Agent Modeling for Multi-agent Adaptation (GAMMA)---on Overcooked, a challenging cooperative cooking game that has become a standard benchmark for zero-shot coordination. We conduct an evaluation with real human teammates, and the results show that GAMMA consistently improves performance, whether the generative model is trained on simulated populations or human datasets. Further, we propose a method for posterior sampling from the generative model that is biased towards the human data, enabling us to efficiently improve performance with only a small amount of expensive human interaction data.",
      "authors": [
        "Yancheng Liang",
        "Daphne Chen",
        "Abhishek Gupta",
        "Simon Shaolei Du",
        "Natasha Jaques"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=v4dXL3LsGX",
      "cdate": 1715767543540,
      "mdate": 1730873977591,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447096"
    },
    {
      "id": "MLhZ8ZNOEk",
      "title": "Swift Sampler: Efficient Learning of Sampler by 10 Parameters",
      "abstract": "Data selection is essential for training deep learning models. An effective data sampler assigns proper sampling probability for training data and helps the model converge to a good local minimum with high performance. Previous studies in data sampling are mainly based on heuristic rules or learning through a huge amount of time-consuming trials. In this paper, we propose an automatic swift sampler search algorithm, SS, to explore automatically learning effective samplers efficiently. In particular, SS utilizes a novel formulation to map a sampler to a low dimension of hyper-parameters and uses an approximated local minimum to quickly examine the quality of a sampler. Benefiting from its low computational expense, SS can be applied on large-scale data sets with high efficiency. Comprehensive experiments on various tasks demonstrate that SS powered sampling can achieve obvious improvements (e.g., 1.5% on ImageNet) and transfer among different neural networks. Project page: https://github.com/Alexander-Yao/Swift-Sampler.",
      "authors": [
        "Jiawei Yao",
        "Chuming Li",
        "Canran Xiao"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MLhZ8ZNOEk",
      "cdate": 1715767326823,
      "mdate": 1730873977566,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447100"
    },
    {
      "id": "7gf6oGdKPU",
      "title": "Retrieval-Retro: Retrieval-based Inorganic Retrosynthesis with Expert Knowledge",
      "abstract": "While inorganic retrosynthesis planning is essential in the field of chemical science, the application of machine learning in this area has been notably less explored compared to organic retrosynthesis planning. In this paper, we propose Retrieval-Retro for inorganic retrosynthesis planning, which implicitly extracts the precursor information of reference materials that are retrieved from the knowledge base regarding domain expertise in the field. Specifically, instead of directly employing the precursor information of reference materials, we propose implicitly extracting it with various attention layers, which enables the model to learn novel synthesis recipes more effectively.\nMoreover, during retrieval, we consider the thermodynamic relationship between target material and precursors, which is essential domain expertise in identifying the most probable precursor set among various options. Extensive experiments demonstrate the superiority of Retrieval-Retro in retrosynthesis planning, especially in discovering novel synthesis recipes, which is crucial for materials discovery.\nThe source code for Retrieval-Retro is available at https://github.com/HeewoongNoh/Retrieval-Retro.",
      "authors": [
        "Heewoong Noh",
        "Namkyeong Lee",
        "Gyoung S. Na",
        "Chanyoung Park"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7gf6oGdKPU",
      "cdate": 1715766665664,
      "mdate": 1730873977365,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447105"
    },
    {
      "id": "dhFHO90INk",
      "title": "Implicitly Guided Design with PropEn: Match your Data to Follow the Gradient",
      "abstract": "Across scientific domains, generating new models or optimizing existing ones while meeting specific criteria is crucial. Traditional machine learning frameworks for guided design use a generative model and a surrogate model (discriminator), requiring large datasets. However, real-world scientific applications often have limited data and complex landscapes, making data-hungry models inefficient or impractical. We propose a new framework, PropEn, inspired by ``matching'', which enables implicit guidance without training a discriminator. By matching each sample with a similar one that has a better property value, we create a larger training dataset that inherently indicates the direction of improvement. Matching, combined with an encoder-decoder architecture, forms a domain-agnostic generative framework for property enhancement. We show that training with a matched dataset approximates the gradient of the property of interest while remaining within the data distribution, allowing efficient design optimization. Extensive evaluations in toy problems and scientific applications, such as therapeutic protein design and airfoil optimization, demonstrate PropEn's advantages over common baselines. Notably, the protein design results are validated with wet lab experiments, confirming the competitiveness and effectiveness of our approach. Our code is available at https://github.com/prescient-design/propen.",
      "authors": [
        "Natasa Tagasovska",
        "Vladimir Gligorijevic",
        "Kyunghyun Cho",
        "Andreas Loukas"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=dhFHO90INk",
      "cdate": 1715766544352,
      "mdate": 1730873977295,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447110"
    },
    {
      "id": "Oo7dlLgqQX",
      "title": "Questioning the Survey Responses of Large Language Models",
      "abstract": "Surveys have recently gained popularity as a tool to study large language models. By comparing models’ survey responses to those of different human reference populations, researchers aim to infer the demographics, political opinions, or values best represented by current language models. In this work, we critically examine language models' survey responses on the basis of the well-established American Community Survey by the U.S. Census Bureau. Evaluating 43 different language models using de-facto standard prompting methodologies, we establish two dominant patterns. First, models' responses are governed by ordering and labeling biases, for example, towards survey responses labeled with the letter “A”. Second, when adjusting for these systematic biases through randomized answer ordering, models across the board trend towards uniformly random survey responses, irrespective of model size or training data. As a result, models consistently appear to better represent subgroups whose aggregate statistics are closest to uniform for the survey under consideration, leading to potentially misguided conclusions about model alignment.",
      "authors": [
        "Ricardo Dominguez-Olmedo",
        "Moritz Hardt",
        "Celestine Mendler-Dünner"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Oo7dlLgqQX",
      "cdate": 1715766505527,
      "mdate": 1730873977257,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447115"
    },
    {
      "id": "gJbZyKGfd6",
      "title": "HyperLogic: Enhancing Diversity and Accuracy in Rule Learning with HyperNets",
      "abstract": "Exploring the integration of if-then logic rules within neural network architectures  presents an intriguing area. This integration seamlessly transforms the rule learning task into neural network training using backpropagation and stochastic gradient descent. From a well-trained sparse and shallow neural network, one can interpret each layer and neuron through the language of logic rules, and a global explanatory rule set can be directly extracted. However, ensuring interpretability may impose constraints on the flexibility, depth, and width of neural networks. In this paper, we propose HyperLogic: a novel framework leveraging hypernetworks to generate weights of the main network. HyperLogic can unveil multiple diverse rule sets, each capable of capturing heterogeneous patterns in data. This provides a simple yet effective method to increase model flexibility and preserve interpretability. We theoretically analyzed the benefits of the HyperLogic by examining the approximation error and generalization capabilities under two types of regularization terms: sparsity and diversity regularizations. Experiments on real data demonstrate that our method can learn more diverse, accurate, and concise rules.",
      "authors": [
        "Yang Yang",
        "Wendi Ren",
        "Shuang Li"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=gJbZyKGfd6",
      "cdate": 1715766347886,
      "mdate": 1730873977156,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447120"
    },
    {
      "id": "eezCLKwx6T",
      "title": "Adversarial Environment Design via Regret-Guided Diffusion Models",
      "abstract": "Training agents that are robust to environmental changes remains a significant challenge in deep reinforcement learning (RL). Unsupervised environment design (UED) has recently emerged to address this issue by generating a set of training environments tailored to the agent's capabilities. While prior works demonstrate that UED has the potential to learn a robust policy, their performance is constrained by the capabilities of the environment generation. To this end, we propose a novel UED algorithm, adversarial environment design via regret-guided diffusion models (ADD). The proposed method guides the diffusion-based environment generator with the regret of the agent to produce environments that the agent finds challenging but conducive to further improvement. By exploiting the representation power of diffusion models, ADD can directly generate adversarial environments while maintaining the diversity of training environments, enabling the agent to effectively learn a robust policy. Our experimental results demonstrate that the proposed method successfully generates an instructive curriculum of environments, outperforming UED baselines in zero-shot generalization across novel, out-of-distribution environments.",
      "authors": [
        "Hojun Chung",
        "Junseo Lee",
        "Minsoo Kim",
        "Dohyeong Kim",
        "Songhwai Oh"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=eezCLKwx6T",
      "cdate": 1715766099516,
      "mdate": 1735175696497,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447125"
    },
    {
      "id": "hS1jvV3Dk3",
      "title": "Localized Zeroth-Order Prompt Optimization",
      "abstract": "The efficacy of large language models (LLMs) in understanding and generating natural language has aroused a wide interest in developing prompt-based methods to harness the power of black-box LLMs. Existing methodologies usually prioritize a global optimization for finding the global optimum, which however will perform poorly in certain tasks. This thus motivates us to re-think the necessity of finding a global optimum in prompt optimization. To answer this, we conduct a thorough empirical study on prompt optimization and draw two major insights. Contrasting with the rarity of global optimum, local optima are usually prevalent and well-performed, which can be more worthwhile for efficient prompt optimization (**Insight I**). The choice of the input domain, covering both the generation and the representation of prompts, affects the identification of well-performing local optima (**Insight II**). Inspired by these insights, we propose a novel algorithm, namely localized zeroth-order prompt optimization (ZOPO), which incorporates a Neural Tangent Kernel-based derived Gaussian process into standard zeroth-order optimization for an efficient search of well-performing local optima in prompt optimization. Remarkably, ZOPO outperforms existing baselines in terms of both the optimization performance and the query efficiency, which we demonstrate through extensive experiments.",
      "authors": [
        "Wenyang Hu",
        "Yao Shu",
        "Zongmin Yu",
        "Zhaoxuan Wu",
        "Xiaoqiang Lin",
        "Zhongxiang Dai",
        "See-Kiong Ng",
        "Bryan Kian Hsiang Low"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=hS1jvV3Dk3",
      "cdate": 1715765971721,
      "mdate": 1730873976910,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447130"
    },
    {
      "id": "YbxFwaSA9Z",
      "title": "Can Learned Optimization Make Reinforcement Learning Less Difficult?",
      "abstract": "While reinforcement learning (RL) holds great potential for decision making in the real world, it suffers from a number of unique difficulties which often need specific consideration. In particular: it is highly non-stationary; suffers from high degrees of plasticity loss; and requires exploration to prevent premature convergence to local optima and maximize return. In this paper, we consider whether learned optimization can help overcome these problems. Our method, Learned **O**ptimization for **P**lasticity, **E**xploration and **N**on-stationarity (*OPEN*), meta-learns an update rule whose input features and output structure are informed by previously proposed solutions to these difficulties. We show that our parameterization is flexible enough to enable meta-learning in diverse learning contexts, including the ability to use stochasticity for exploration. Our experiments demonstrate that when meta-trained on single and small sets of environments, *OPEN* outperforms or equals traditionally used optimizers. Furthermore, *OPEN* shows strong generalization characteristics across a range of environments and agent architectures.",
      "authors": [
        "Alexander D. Goldie",
        "Chris Lu",
        "Matthew Thomas Jackson",
        "Shimon Whiteson",
        "Jakob Nicolaus Foerster"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=YbxFwaSA9Z",
      "cdate": 1715765822887,
      "mdate": 1730873976760,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447135"
    },
    {
      "id": "Y1fPxGevQj",
      "title": "xMIL: Insightful Explanations for Multiple Instance Learning in Histopathology",
      "abstract": "Multiple instance learning (MIL) is an effective and widely used approach for weakly supervised machine learning. In histopathology, MIL models have achieved remarkable success in tasks like tumor detection, biomarker prediction, and outcome prognostication. However, MIL explanation methods are still lagging behind, as they are limited to small bag sizes or disregard instance interactions. We revisit MIL through the lens of explainable AI (XAI) and introduce xMIL, a refined framework with more general assumptions. We demonstrate how to obtain improved MIL explanations using layer-wise relevance propagation (LRP) and conduct extensive evaluation experiments on three toy settings and four real-world histopathology datasets. Our approach consistently outperforms previous explanation attempts with particularly improved faithfulness scores on challenging biomarker prediction tasks. Finally, we showcase how xMIL explanations enable pathologists to extract insights from MIL models, representing a significant advance for knowledge discovery and model debugging in digital histopathology.",
      "authors": [
        "Julius Hense",
        "Mina Jamshidi Idaji",
        "Oliver Eberle",
        "Thomas Schnake",
        "Jonas Dippel",
        "Laure Ciernik",
        "Oliver Buchstab",
        "Andreas Mock",
        "Frederick Klauschen",
        "Klaus Robert Muller"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Y1fPxGevQj",
      "cdate": 1715765784962,
      "mdate": 1736256487246,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447139"
    },
    {
      "id": "7txPaUpUnc",
      "title": "Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning",
      "abstract": "Identifying the features learned by neural networks is a core challenge in mechanistic interpretability. Sparse autoencoders (SAEs), which learn a sparse, overcomplete dictionary that reconstructs a network's internal activations, have been used to identify these features. However, SAEs may learn more about the structure of the datatset than the computational structure of the network. There is therefore only indirect reason to believe that the directions found in these dictionaries are functionally important to the network. We propose end-to-end (e2e) sparse dictionary learning, a method for training SAEs that ensures the features learned are functionally important by minimizing the KL divergence between the output distributions of the original model and the model with SAE activations inserted. Compared to standard SAEs, e2e SAEs offer a Pareto improvement: They explain more network performance, require fewer total features, and require fewer simultaneously active features per datapoint, all with no cost to interpretability. We explore geometric and qualitative differences between e2e SAE features and standard SAE features. E2e dictionary learning brings us closer to methods that can explain network behavior concisely and accurately. We release our library for training e2e SAEs and reproducing our analysis at\nhttps://github.com/ApolloResearch/e2e_sae.",
      "authors": [
        "Dan Braun",
        "Jordan Taylor",
        "Nicholas Goldowsky-Dill",
        "Lee Sharkey"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7txPaUpUnc",
      "cdate": 1715765733449,
      "mdate": 1730873976646,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447144"
    },
    {
      "id": "6uRrwWhZlM",
      "title": "Prompt Optimization with EASE? Efficient Ordering-aware Automated Selection of Exemplars",
      "abstract": "Large language models (LLMs) have shown impressive capabilities in real-world applications. The capability of *in-context learning* (ICL) allows us to adapt an LLM to downstream tasks by including input-label exemplars in the prompt without model fine-tuning. However, the quality of these exemplars in the prompt greatly impacts performance, highlighting the need for an effective automated exemplar selection method. Recent studies have explored retrieval-based approaches to select exemplars tailored to individual test queries, which can be undesirable due to extra test-time computation and an increased risk of data exposure. Moreover, existing methods fail to adequately account for the impact of exemplar ordering on the performance. On the other hand, the impact of the *instruction*, another essential component in the prompt given to the LLM, is often overlooked in existing exemplar selection methods. To address these challenges, we propose a novel method named $\\texttt{EASE}$, which leverages the hidden embedding from a pre-trained language model to represent ordered sets of exemplars and uses a neural bandit algorithm to optimize the sets of exemplars *while accounting for exemplar ordering*. Our  $\\texttt{EASE}$ can efficiently find an ordered set of exemplars that *performs well for all test queries* from a given task, thereby eliminating test-time computation. Importantly,  $\\texttt{EASE}$ can be readily extended to *jointly optimize both the exemplars and the instruction*. Through extensive empirical evaluations (including novel tasks), we demonstrate the superiority of  $\\texttt{EASE}$ over existing methods, and reveal practical insights about the impact of exemplar selection on ICL, which may be of independent interest. Our code is available at https://github.com/ZhaoxuanWu/EASE-Prompt-Optimization.",
      "authors": [
        "Zhaoxuan Wu",
        "Xiaoqiang Lin",
        "Zhongxiang Dai",
        "Wenyang Hu",
        "Yao Shu",
        "See-Kiong Ng",
        "Patrick Jaillet",
        "Bryan Kian Hsiang Low"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=6uRrwWhZlM",
      "cdate": 1715765731810,
      "mdate": 1730873976562,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447149"
    },
    {
      "id": "U4BC0GrFAz",
      "title": "Do causal predictors generalize better to new domains?",
      "abstract": "We study how well machine learning models trained on causal features generalize across domains. We consider 16 prediction tasks on tabular datasets covering applications in health, employment, education, social benefits, and politics. Each dataset comes with multiple domains, allowing us to test how well a model trained in one domain performs in another. For each prediction task, we select features that have a causal influence on the target of prediction. Our goal is to test the hypothesis that models trained on causal features generalize better across domains. Without exception, we find that predictors using all available features, regardless of causality, have better in-domain and out-of-domain accuracy than predictors using causal features. Moreover, even the absolute drop in accuracy from one domain to the other is no better for causal predictors than for models that use all features.  In addition, we show that recent causal machine learning methods for domain generalization do not perform better in our evaluation than standard predictors trained on the set of causal features. Likewise, causal discovery algorithms either fail to run or select causal variables that perform no better than our selection. Extensive robustness checks confirm that our findings are stable under variable misclassification.",
      "authors": [
        "Vivian Yvonne Nastl",
        "Moritz Hardt"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=U4BC0GrFAz",
      "cdate": 1715765687890,
      "mdate": 1730873976485,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447154"
    },
    {
      "id": "D6nlm2AYHi",
      "title": "Learning Distinguishable Trajectory Representation with Contrastive Loss",
      "abstract": "Policy network parameter sharing is a commonly used technique in advanced deep multi-agent reinforcement learning (MARL) algorithms to improve learning efficiency by reducing the number of policy parameters and sharing experiences among agents. Nevertheless, agents that share the policy parameters tend to learn similar behaviors. To encourage multi-agent diversity, prior works typically maximize the mutual information between trajectories and agent identities using variational inference. However, this category of methods easily leads to inefficient exploration due to limited trajectory visitations. To resolve this limitation, inspired by the learning of pre-trained models, in this paper, we propose a novel Contrastive Trajectory Representation (CTR) method based on learning distinguishable trajectory representations to encourage multi-agent diversity. Specifically, CTR maps the trajectory of an agent into a latent trajectory representation space by an encoder and an autoregressive model. To achieve the distinguishability among trajectory representations of different agents, we introduce contrastive learning to maximize the mutual information between the trajectory representations and learnable identity representations of different agents. We implement CTR on top of QMIX and evaluate its performance in various cooperative multi-agent tasks. The empirical results demonstrate that our proposed CTR yields significant performance improvement over the state-of-the-art methods.",
      "authors": [
        "Tianxu Li",
        "Kun Zhu",
        "Juan Li",
        "Yang Zhang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=D6nlm2AYHi",
      "cdate": 1715765512582,
      "mdate": 1730873976330,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447158"
    },
    {
      "id": "nxumYwxJPB",
      "title": "If You Want to Be Robust, Be Wary of Initialization",
      "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable performance across a spectrum of graph-related tasks, however concerns persist regarding their vulnerability to adversarial perturbations. While prevailing defense strategies focus primarily on pre-processing techniques and adaptive message-passing schemes, this study delves into an under-explored dimension: the impact of weight initialization and associated hyper-parameters, such as training epochs, on a model’s robustness.\nWe introduce a theoretical framework bridging the connection between initialization strategies and a network's resilience to adversarial perturbations. Our analysis reveals a direct relationship between initial weights, number of training epochs and the model’s vulnerability, offering new insights into adversarial robustness beyond conventional defense mechanisms. While our primary focus is on GNNs, we extend our theoretical framework, providing a general upper-bound applicable to Deep Neural Networks.\nExtensive experiments, spanning diverse models and real-world datasets subjected to various adversarial attacks, validate our findings. We illustrate that selecting appropriate initialization not only ensures performance on clean datasets but also enhances model robustness against adversarial perturbations, with observed gaps of up to 50\\% compared to alternative initialization approaches.",
      "authors": [
        "Sofiane ENNADIR",
        "Johannes F. Lutzeyer",
        "Michalis Vazirgiannis",
        "El houcine Bergou"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nxumYwxJPB",
      "cdate": 1715765231235,
      "mdate": 1730873976146,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447164"
    },
    {
      "id": "oTv6Qa12G0",
      "title": "A theoretical design of concept sets: improving the predictability of concept bottleneck models",
      "abstract": "Concept-based learning, a promising approach in machine learning, emphasizes the value of high-level representations called concepts. However, despite growing interest in concept-bottleneck models (CBMs), there is a lack of clear understanding regarding the properties of concept sets and their impact on model performance. In this work, we define concepts within the machine learning context, highlighting their core properties: 'expressiveness' and 'model-aware inductive bias', and we make explicit the underlying assumption of CBMs. We establish theoretical results for concept-bottleneck models (CBMs), revealing how these properties guide the design of concept sets that optimize model performance. Specifically, we demonstrate that well-chosen concept sets can improve sample efficiency and out-of-distribution robustness in the appropriate regimes. Based on these insights, we propose a method to effectively identify informative and non-redundant concepts. We validate our approach with experiments on CIFAR-10 and MetaShift, showing that concept-bottleneck models outperform the foundational embedding counterpart, particularly in low-data regimes and under distribution shifts. We also examine failure modes and discuss how they can be tackled.",
      "authors": [
        "Max Ruiz Luyten",
        "Mihaela van der Schaar"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=oTv6Qa12G0",
      "cdate": 1715765134825,
      "mdate": 1730873976119,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447168"
    },
    {
      "id": "MRO2QhydPF",
      "title": "Reinforcement Learning with Adaptive Regularization for Safe Control of Critical Systems",
      "abstract": "Reinforcement Learning (RL) is a powerful method for controlling dynamic systems, but its learning mechanism can lead to unpredictable actions that undermine the safety of critical systems. Here, we propose RL with Adaptive Regularization (RL-AR), an algorithm that enables safe RL exploration by combining the RL policy with a policy regularizer that hard-codes the safety constraints. RL-AR performs policy combination via a \"focus module,\" which determines the appropriate combination depending on the state—relying more on the safe policy regularizer for less-exploited states while allowing unbiased convergence for well-exploited states. In a series of critical control applications, we demonstrate that RL-AR not only ensures safety during training but also achieves a return competitive with the standards of model-free RL that disregards safety.",
      "authors": [
        "Haozhe Tian",
        "Homayoun Hamedmoghadam",
        "Robert Noel Shorten",
        "Pietro Ferraro"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=MRO2QhydPF",
      "cdate": 1715765110944,
      "mdate": 1736810604441,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447173"
    },
    {
      "id": "06JRFVK88O",
      "title": "Mimicking To Dominate: Imitation Learning Strategies for Success in Multiagent Games",
      "abstract": "Training agents in multi-agent games presents significant challenges due to their intricate nature. These challenges are exacerbated by dynamics influenced not only by the environment but also by strategies of opponents. Existing methods often struggle with slow convergence and instability.\nTo address these challenges, we harness the potential of imitation learning (IL) to comprehend and anticipate actions of the opponents, aiming to mitigate uncertainties with respect to the game dynamics.\nOur key contributions include:\n(i) a new multi-agent IL model for predicting next moves of the opponents - our model works with hidden actions of opponents and local observations;\n(ii) a new multi-agent reinforcement learning (MARL) algorithm that combines our IL model and policy training into one single training process;\nand (iii) extensive experiments in three challenging game environments, including an advanced version of the Star-Craft multi-agent challenge (i.e., SMACv2).\nExperimental results show that our approach achieves superior performance compared to state-of-the-art MARL algorithms.",
      "authors": [
        "The Viet Bui",
        "Tien Anh Mai",
        "Thanh Hong Nguyen"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=06JRFVK88O",
      "cdate": 1715765005910,
      "mdate": 1730873975935,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447178"
    },
    {
      "id": "DylSyAfmWs",
      "title": "Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs",
      "abstract": "Large language models can memorize and repeat their training data, causing privacy and copyright risks. To mitigate memorization, we introduce a subtle modification to the next-token training objective that we call the goldfish loss. During training, a randomly sampled subsets of tokens are excluded from the loss computation. These dropped tokens are not memorized by the model, which prevents verbatim reproduction of a complete chain of tokens from the training set. We run extensive experiments training billion-scale LLaMA-2 models, both pre-trained and trained from scratch, and demonstrate significant reductions in extractable memorization with little to no impact on downstream benchmarks.\n\n_Code and checkpoints: https://github.com/ahans30/goldfish-loss_",
      "authors": [
        "Abhimanyu Hans",
        "John Kirchenbauer",
        "Yuxin Wen",
        "Neel Jain",
        "Hamid Kazemi",
        "Prajwal Singhania",
        "Siddharth Singh",
        "Gowthami Somepalli",
        "Jonas Geiping",
        "Abhinav Bhatele",
        "Tom Goldstein"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=DylSyAfmWs",
      "cdate": 1715764766381,
      "mdate": 1730873975727,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447183"
    },
    {
      "id": "H3at5y8VFW",
      "title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language Model",
      "abstract": "The rise of large language models (LLMs) has significantly transformed both the construction and application of information retrieval (IR) systems. \nHowever, current interactions between IR systems and LLMs remain limited, with LLMs merely serving as part of components within IR systems, and IR systems being constructed independently of LLMs. This separated architecture restricts knowledge sharing and deep collaboration between them.\nIn this paper, we introduce Self-Retrieval, a novel end-to-end LLM-driven information retrieval architecture.\nSelf-Retrieval unifies all essential IR functions within a single LLM, leveraging the inherent capabilities of LLMs throughout the IR process.\nSpecifically, Self-Retrieval internalizes the retrieval corpus through self-supervised learning,  transforms the retrieval process into sequential passage generation, and performs relevance assessment for reranking.\nExperimental results demonstrate that Self-Retrieval not only outperforms existing retrieval approaches by a significant margin, but also substantially enhances the performance of LLM-driven downstream applications like retrieval-augmented generation.",
      "authors": [
        "Qiaoyu Tang",
        "Jiawei Chen",
        "Zhuoqun Li",
        "Bowen Yu",
        "Yaojie Lu",
        "ChengFu",
        "Haiyang Yu",
        "Hongyu Lin",
        "Fei Huang",
        "Ben He",
        "Xianpei Han",
        "Le Sun",
        "Yongbin Li"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=H3at5y8VFW",
      "cdate": 1715764762393,
      "mdate": 1730873975644,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447188"
    },
    {
      "id": "biAqUbAuG7",
      "title": "Adam on Local Time: Addressing Nonstationarity in RL with Relative Adam Timesteps",
      "abstract": "In reinforcement learning (RL), it is common to apply techniques used broadly in \nmachine learning such as neural network function approximators and momentum-based optimizers. However, such tools were largely developed for supervised learning rather than nonstationary RL, leading practitioners to adopt target networks, clipped policy updates, and other RL-specific implementation tricks to combat this mismatch, rather than directly adapting this toolchain for use in RL. \nIn this paper, we take a different approach and instead address the effect of nonstationarity by adapting the widely used Adam optimiser. \nWe first analyse the impact of nonstationary gradient magnitude --- such as that caused by a change in target network --- on Adam's update size, demonstrating that such a change can lead to large updates and hence sub-optimal performance.\nTo address this, we introduce Adam-Rel.\nRather than using the global timestep in the Adam update, Adam-Rel uses the *local* timestep within an epoch, essentially resetting Adam's timestep to 0 after target changes.\nWe demonstrate that this avoids large updates and reduces to learning rate annealing in the absence of such increases in gradient magnitude. Evaluating Adam-Rel in both on-policy and off-policy RL, we demonstrate improved performance in both Atari and Craftax.\nWe then show that increases in gradient norm occur in RL in practice, and examine the differences between our \ntheoretical model and the observed data.",
      "authors": [
        "Benjamin Ellis",
        "Matthew Thomas Jackson",
        "Andrei Lupu",
        "Alexander D. Goldie",
        "Mattie Fellows",
        "Shimon Whiteson",
        "Jakob Nicolaus Foerster"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=biAqUbAuG7",
      "cdate": 1715764736521,
      "mdate": 1730873975616,
      "matched_keywords": [
        "reinforcement learning",
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447192"
    },
    {
      "id": "0ZeONp33f0",
      "title": "Graph Neural Networks and Arithmetic Circuits",
      "abstract": "We characterize the computational power of neural networks that follow the graph neural network (GNN) architecture, not restricted to aggregate-combine GNNs or other particular types. We establish an exact correspondence between the expressivity of GNNs using diverse activation functions and arithmetic circuits over real numbers. In our results the activation function of the network becomes a gate type in the circuit. Our result holds for families of constant depth circuits and networks, both uniformly and non-uniformly, for all common activation functions.",
      "authors": [
        "Timon Barlag",
        "Vivian Holzapfel",
        "Laura Strieker",
        "Jonni Virtema",
        "Heribert Vollmer"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0ZeONp33f0",
      "cdate": 1715764500825,
      "mdate": 1734602131120,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447197"
    },
    {
      "id": "qCpCy0EQAJ",
      "title": "Dynamic Neural Regeneration: Enhancing Deep Learning Generalization on Small Datasets",
      "abstract": "The efficacy of deep learning techniques is contingent upon access to large volumes of data (labeled or unlabeled). However, in practical domains such as medical applications, data availability is often limited. This presents a significant challenge: How can we effectively train deep neural networks on relatively small datasets while improving generalization? Recent works have explored evolutionary or iterative training paradigms, which reinitialize a subset of parameters to enhance generalization performance for small datasets. However, these methods typically rely on randomly selected parameter subsets and maintain fixed masks throughout training, potentially leading to suboptimal outcomes. Inspired by neurogenesis in the brain, we propose a novel iterative training framework, Dynamic Neural Regeneration (DNR), that employs a data-aware dynamic masking scheme to eliminate redundant connections by estimating their significance. This approach increases the model's capacity for further learning through random weight reinitialization. Experimental results demonstrate that our approach outperforms existing methods in accuracy and robustness, highlighting its potential for real-world applications where data collection is challenging.",
      "authors": [
        "Vijaya Raghavan T Ramkumar",
        "Elahe Arani",
        "Bahram Zonooz"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qCpCy0EQAJ",
      "cdate": 1715764481318,
      "mdate": 1730873975401,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447202"
    },
    {
      "id": "6Kg26g1quR",
      "title": "ROIDICE: Offline Return on Investment Maximization for Efficient Decision Making",
      "abstract": "In this paper, we propose a novel policy optimization framework that maximizes Return on Investment (ROI) of a policy using a fixed dataset within a Markov Decision Process (MDP) equipped with a cost function. ROI, defined as the ratio between the return and the accumulated cost of a policy, serves as a measure of efficiency of the policy. Despite the importance of maximizing ROI in various applications, it remains a challenging problem due to its nature as a ratio of two long-term values: return and accumulated cost. To address this, we formulate the ROI maximizing reinforcement learning problem as a linear fractional programming. We then incorporate the stationary distribution correction (DICE) framework to develop a practical offline ROI maximization algorithm.\nOur proposed algorithm, ROIDICE, yields an efficient policy that offers a superior trade-off between return and accumulated cost compared to policies trained using existing frameworks.",
      "authors": [
        "Woosung Kim",
        "Hayeong Lee",
        "Jongmin Lee",
        "Byung-Jun Lee"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=6Kg26g1quR",
      "cdate": 1715764474043,
      "mdate": 1730873975367,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447207"
    },
    {
      "id": "VpuOuZOVhP",
      "title": "LLM-AutoDA: Large Language Model-Driven Automatic Data Augmentation for Long-tailed Problems",
      "abstract": "The long-tailed distribution is the underlying nature of real-world data, and it presents unprecedented challenges for training deep learning models. Existing long-tailed learning paradigms based on re-balancing or data augmentation have partially alleviated the long-tailed problem. However, they still have limitations, such as relying on manually designed augmentation strategies, having a limited search space, and using fixed augmentation strategies. To address these limitations, this paper proposes a novel LLM-based long-tailed data augmentation framework called LLM-AutoDA, which leverages large-scale pretrained models to automatically search for the optimal augmentation strategies suitable for long-tailed data distributions. In addition, it applies this strategy to the original imbalanced data to create an augmented dataset and fine-tune the underlying long-tailed learning model. The performance improvement on the validation set serves as a reward signal to update the generation model, enabling the generation of more effective augmentation strategies in the next iteration. We conducted extensive experiments on multiple mainstream long-tailed learning benchmarks. The results show that LLM-AutoDA outperforms state-of-the-art data augmentation methods and other re-balancing methods significantly.",
      "authors": [
        "Pengkun Wang",
        "Zhe Zhao",
        "HaiBin Wen",
        "Fanfu Wang",
        "Binwu Wang",
        "Qingfu Zhang",
        "Yang Wang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=VpuOuZOVhP",
      "cdate": 1715764304499,
      "mdate": 1730873975230,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447212"
    },
    {
      "id": "oTzydUKWpq",
      "title": "Intruding with Words: Towards Understanding Graph Injection Attacks at the Text Level",
      "abstract": "Graph Neural Networks (GNNs) excel across various applications but remain vulnerable to adversarial attacks, particularly Graph Injection Attacks (GIAs), which inject malicious nodes into the original graph and pose realistic threats.\nText-attributed graphs (TAGs), where nodes are associated with textual features, are crucial due to their prevalence in real-world applications and are commonly used to evaluate these vulnerabilities.\nHowever, existing research only focuses on embedding-level GIAs, which inject node embeddings rather than actual textual content, limiting their applicability and simplifying detection.\nIn this paper, we pioneer the exploration of GIAs at the text level, presenting three novel attack designs that inject textual content into the graph.\nThrough theoretical and empirical analysis, we demonstrate that text interpretability, a factor previously overlooked at the embedding level, plays a crucial role in attack strength. \nAmong the designs we investigate, the Word-frequency-based Text-level GIA (WTGIA) is particularly notable for its balance between performance and interpretability. \nDespite the success of WTGIA, we discover that defenders can easily enhance their defenses with customized text embedding methods or large language model (LLM)--based predictors. \nThese insights underscore the necessity for further research into the potential and practical significance of text-level GIAs.",
      "authors": [
        "Runlin Lei",
        "Yuwei Hu",
        "Yuchen Ren",
        "Zhewei Wei"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=oTzydUKWpq",
      "cdate": 1715764303095,
      "mdate": 1730873975169,
      "matched_keywords": [
        "large language model",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447217"
    },
    {
      "id": "00uVk06eVK",
      "title": "On the Noise Robustness of In-Context Learning for Text Generation",
      "abstract": "Large language models (LLMs) have shown impressive performance on downstream tasks by in-context learning (ICL), which heavily relies on the quality of demonstrations selected from a large set of annotated examples. Recent works claim that in-context learning is robust to noisy demonstrations in text classification. In this work, we show that, on text generation tasks, noisy annotations significantly hurt the performance of in-context learning. To circumvent the issue, we propose a simple and effective approach called Local Perplexity Ranking (LPR), which replaces the \"noisy\" candidates with their nearest neighbors that are more likely to be clean. Our method is motivated by analyzing the perplexity deviation caused by noisy labels and decomposing perplexity into inherent perplexity and matching perplexity. Our key idea behind LPR is thus to decouple the matching perplexity by performing the ranking among the neighbors in semantic space. Our approach can prevent the selected demonstrations from including mismatched input-label pairs while preserving the effectiveness of the original selection methods. Extensive experiments demonstrate the effectiveness of LPR, improving the EM score by up to 18.75 on common benchmarks with noisy annotations.",
      "authors": [
        "Hongfu Gao",
        "Feipeng Zhang",
        "Wenyu Jiang",
        "Jun Shu",
        "Feng Zheng",
        "Hongxin Wei"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=00uVk06eVK",
      "cdate": 1715764116515,
      "mdate": 1730873975103,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447221"
    },
    {
      "id": "8abNCVJs2j",
      "title": "S-STE: Continuous Pruning Function for Efficient 2:4 Sparse Pre-training",
      "abstract": "Training deep neural networks (DNNs) is costly. Fortunately, Nvidia Ampere and Hopper GPUs can accelerate matrix multiplications twice as fast as a dense equivalent by implementing 2:4 sparsity. However, previous STE-based 2:4 pre-training methods (\\eg~STE with hard-thresholding, SR-STE) suffer from optimization difficulties because of discontinuous pruning function.\nIn this study, we comprehensively analyse the bottleneck of traditional N:M sparse training and recognize three drawbacks with discontinuity: incorrect descending direction, inability to predict the amount of descent and sparse mask oscillation. In the light of this statement, we propose S-STE, a simple yet powerful 2:4 training method that contains two parts: to continuously project weights to be 2:4 sparse, and to rescale sparse weights with a per-tensor fixed scaling factor. Besides, we adopt minimum-variance unbiased estimation for activation gradient and FP8 quantization for whole process. Results show that our method surpass previous 2:4 pre-training recipes and is comparable even with full parameter models.",
      "authors": [
        "Yuezhou Hu",
        "Jun Zhu",
        "Jianfei Chen"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=8abNCVJs2j",
      "cdate": 1715764086353,
      "mdate": 1734574986210,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447226"
    },
    {
      "id": "lzfzjYuWgY",
      "title": "Do LLMs Build World Representations? Probing Through the Lens of State Abstraction",
      "abstract": "How do large language models (LLMs) encode the state of the world, including the status of entities and their relations, as described by a text? While existing work directly probes for a complete state of the world, our research explores whether and how LLMs abstract this world state in their internal representations. We propose a new framework for probing for world representations through the lens of state abstraction theory from reinforcement learning, which emphasizes different levels of abstraction, distinguishing between general abstractions that facilitate predicting future states and goal-oriented abstractions that guide the subsequent actions to accomplish tasks. To instantiate this framework, we design a text-based planning task, where an LLM acts as an agent in an environment and interacts with objects in containers to achieve a specified goal state. Our experiments reveal that fine-tuning as well as advanced pre-training strengthens LLM-built representations' tendency of maintaining goal-oriented abstractions during decoding, prioritizing task completion over recovery of the world's state and dynamics.",
      "authors": [
        "Zichao Li",
        "Yanshuai Cao",
        "Jackie CK Cheung"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lzfzjYuWgY",
      "cdate": 1715763917595,
      "mdate": 1730873974958,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447231"
    },
    {
      "id": "cV4fcjcwmz",
      "title": "Extracting Training Data from Molecular Pre-trained Models",
      "abstract": "Graph Neural Networks (GNNs) have significantly advanced the field of drug discovery, enhancing the speed and efficiency of molecular identification. However, training these GNNs demands vast amounts of molecular data, which has spurred the emergence of collaborative model-sharing initiatives. These initiatives facilitate the sharing of molecular pre-trained models among organizations without exposing proprietary training data. Despite the benefits, these molecular pre-trained models may still pose privacy risks. For example, malicious adversaries could perform data extraction attack to recover private training data, thereby threatening commercial secrets and collaborative trust. This work, for the first time, explores the risks of extracting private training molecular data from molecular pre-trained models. This task is nontrivial as the molecular pre-trained models are non-generative and exhibit a diversity of model architectures, which differs significantly from language and image models. To address these issues, we introduce a molecule generation approach and propose a novel, model-independent scoring function for selecting promising molecules. To efficiently reduce the search space of potential molecules, we further introduce a Molecule Extraction Policy Network for molecule extraction. Our experiments demonstrate that even with only query access to molecular pre-trained models, there is a considerable risk of extracting training data, challenging the assumption that model sharing alone provides adequate protection against data extraction attacks. Our codes are publicly available at: \\url{https://github.com/renH2/Molextract}.",
      "authors": [
        "Renhong Huang",
        "Jiarong Xu",
        "Zhiming Yang",
        "Xiang Si",
        "Xin Jiang",
        "Hanyang Yuan",
        "Chunping Wang",
        "Yang Yang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=cV4fcjcwmz",
      "cdate": 1715763882701,
      "mdate": 1736759402393,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447236"
    },
    {
      "id": "FbXQrfkvtY",
      "title": "Probing the Decision Boundaries of In-context Learning in Large Language Models",
      "abstract": "In-context learning is an emergent paradigm in large language models (LLMs) that enables them to generalize to new tasks and domains by simply prompting these models with a few exemplars without explicit parameter updates. Many attempts have been made to understand in-context learning in LLMs as a function of model scale, pretraining data, and other factors. In this work, we propose a new mechanism to probe and understand in-context learning from the lens of decision boundaries for in-context binary classification. Decision boundaries are straightforward to visualize and provide important information about the qualitative behavior of the inductive biases of standard classifiers. To our surprise, we find that the decision boundaries learned by current LLMs in simple binary classification tasks are often irregularly non-smooth, regardless of task linearity. This paper investigates the factors influencing these decision boundaries and explores methods to enhance their generalizability. We assess various approaches, including training-free and fine-tuning methods for LLMs, the impact of model architecture, and the effectiveness of active prompting techniques for smoothing decision boundaries in a data-efficient manner. Our findings provide a deeper understanding of in-context learning dynamics and offer practical improvements for enhancing robustness and generalizability of in-context learning.",
      "authors": [
        "Siyan Zhao",
        "Tung Nguyen",
        "Aditya Grover"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=FbXQrfkvtY",
      "cdate": 1715763846659,
      "mdate": 1731568443808,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447240"
    },
    {
      "id": "7WvwzuYkUq",
      "title": "Progressive Entropic Optimal Transport Solvers",
      "abstract": "Optimal transport (OT) has profoundly impacted machine learning by providing theoretical and computational tools to realign datasets.\nIn this context, given two large point clouds of sizes $n$ and $m$ in $\\mathbb{R}^d$, entropic OT (EOT) solvers have emerged as the most reliable tool to either solve the Kantorovich problem and output a $n\\times m$ coupling matrix, or to solve the Monge problem and learn a vector-valued push-forward map. \nWhile the robustness of EOT couplings/maps makes them a go-to choice in practical applications, EOT solvers remain difficult to tune because of a small but influential set of hyperparameters, notably the omnipresent entropic regularization strength $\\varepsilon$. Setting $\\varepsilon$ can be difficult, as it simultaneously impacts various performance metrics, such as compute speed, statistical performance, generalization, and bias. In this work, we propose a new class of EOT solvers (ProgOT), that can estimate both plans and transport maps.\nWe take advantage of several opportunities to optimize the computation of EOT solutions by *dividing* mass displacement using a time discretization, borrowing inspiration from dynamic OT formulations, and *conquering* each of these steps using EOT with properly scheduled parameters. We provide experimental evidence demonstrating that ProgOT is a faster and more robust alternative to *standard solvers* when computing couplings at large scales, even outperforming neural network-based approaches. We also prove statistical consistency of our approach for estimating OT maps.",
      "authors": [
        "Parnian Kassraie",
        "Aram-Alexandre Pooladian",
        "Michal Klein",
        "James Thornton",
        "Jonathan Niles-Weed",
        "marco cuturi"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7WvwzuYkUq",
      "cdate": 1715763777393,
      "mdate": 1730873974856,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447244"
    },
    {
      "id": "X64IJvdftR",
      "title": "Certified Robustness for Deep Equilibrium Models via Serialized Random Smoothing",
      "abstract": "Implicit models such as Deep Equilibrium Models (DEQs) have emerged as promising alternative approaches for building deep neural networks. Their certified robustness has gained increasing research attention due to security concerns. Existing certified defenses for DEQs employing interval bound propagation and Lipschitz-bounds not only offer conservative certification bounds but also are restricted to specific forms of DEQs. In this paper, we provide the first randomized smoothing certified defense for DEQs to solve these limitations. Our study reveals that simply applying randomized smoothing to certify DEQs provides certified robustness generalized to large-scale datasets but incurs extremely expensive computation costs. To reduce computational redundancy, we propose a novel Serialized Randomized Smoothing (SRS) approach that leverages historical information. Additionally, we derive a new certified radius estimation for SRS to theoretically ensure the correctness of our algorithm. Extensive experiments and ablation studies on image recognition demonstrate that our algorithm can significantly accelerate the certification of DEQs by up to 7x almost without sacrificing the certified accuracy. The implementation will be publicly available upon the acceptance of this work. Our code is available at https://github.com/WeizhiGao/Serialized-Randomized-Smoothing.",
      "authors": [
        "Weizhi Gao",
        "Zhichao Hou",
        "Han Xu",
        "Xiaorui Liu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=X64IJvdftR",
      "cdate": 1715763717358,
      "mdate": 1730873974829,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447249"
    },
    {
      "id": "CKgNgKmHYp",
      "title": "HYDRA: Model Factorization Framework for Black-Box LLM Personalization",
      "abstract": "Personalization has emerged as a critical research area in modern intelligent systems, focusing on mining users' behavioral history and adapting to their preferences for delivering tailored experiences. Despite the remarkable few-shot capabilities exhibited by black-box large language models (LLMs), the inherent opacity of their model parameters presents significant challenges in aligning the generated output with individual expectations. Existing solutions have primarily focused on prompt design to incorporate user-specific profiles and behaviors; however, such approaches often struggle to generalize effectively due to their inability to capture shared knowledge among all users. To address these challenges, we propose HYDRA, a model factorization framework that captures both user-specific behavior patterns from historical data and shared general knowledge among all users to deliver personalized generation. In order to capture user-specific behavior patterns, we first train a reranker to prioritize the most useful information from top-retrieved relevant historical records.\nBy combining the prioritized history with the corresponding query, we train an adapter to align the output with individual user-specific preferences, eliminating the reliance on access to inherent model parameters of black-box LLMs. Both the reranker and the adapter can be decomposed into a base model with multiple user-specific heads, resembling a hydra. The base model maintains shared knowledge across users, while the multiple personal heads capture user-specific preferences. Experimental results demonstrate that \\method outperforms existing state-of-the-art prompt-based methods by an average relative improvement of 9.01% across five diverse personalization tasks in the LaMP benchmark.",
      "authors": [
        "Yuchen Zhuang",
        "Haotian Sun",
        "Yue Yu",
        "Rushi Qiang",
        "Qifan Wang",
        "Chao Zhang",
        "Bo Dai"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=CKgNgKmHYp",
      "cdate": 1715763699076,
      "mdate": 1730873974784,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447253"
    },
    {
      "id": "LUIXdWn6Z5",
      "title": "Risk-sensitive control as inference with Rényi divergence",
      "abstract": "This paper introduces the risk-sensitive control as inference (RCaI) that extends CaI by using Rényi divergence variational inference. RCaI is shown to be equivalent to log-probability regularized risk-sensitive control, which is an extension of the maximum entropy (MaxEnt) control. We also prove that the risk-sensitive optimal policy can be obtained by solving a soft Bellman equation, which reveals several equivalences between RCaI, MaxEnt control, the optimal posterior for CaI, and linearly-solvable control. Moreover, based on RCaI, we derive the risk-sensitive reinforcement learning (RL) methods: the policy gradient and the soft actor-critic. As the risk-sensitivity parameter vanishes, we recover the risk-neutral CaI and RL, which means that RCaI is a unifying framework. Furthermore, we give another risk-sensitive generalization of the MaxEnt control using Rényi entropy regularization. We show that in both of our extensions, the optimal policies have the same structure even though the derivations are very different.",
      "authors": [
        "Kaito Ito",
        "Kenji Kashima"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=LUIXdWn6Z5",
      "cdate": 1715763485653,
      "mdate": 1730873974499,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447258"
    },
    {
      "id": "9JFSJitKC0",
      "title": "Spectral-Risk Safe Reinforcement Learning with Convergence Guarantees",
      "abstract": "The field of risk-constrained reinforcement learning (RCRL) has been developed to effectively reduce the likelihood of worst-case scenarios by explicitly handling risk-measure-based constraints.\nHowever, the nonlinearity of risk measures makes it challenging to achieve convergence and optimality.\nTo overcome the difficulties posed by the nonlinearity, we propose a spectral risk measure-constrained RL algorithm, spectral-risk-constrained policy optimization (SRCPO), a bilevel optimization approach that utilizes the duality of spectral risk measures.\nIn the bilevel optimization structure, the outer problem involves optimizing dual variables derived from the risk measures, while the inner problem involves finding an optimal policy given these dual variables.\nThe proposed method, to the best of our knowledge, is the first to guarantee convergence to an optimum in the tabular setting.\nFurthermore, the proposed method has been evaluated on continuous control tasks and showed the best performance among other RCRL algorithms satisfying the constraints.\nOur code is available at https://github.com/rllab-snu/Spectral-Risk-Constrained-RL.",
      "authors": [
        "Dohyeong Kim",
        "Taehyun Cho",
        "Seungyub Han",
        "Hojun Chung",
        "Kyungjae Lee",
        "Songhwai Oh"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9JFSJitKC0",
      "cdate": 1715763314924,
      "mdate": 1730873974471,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447263"
    },
    {
      "id": "fPBACAbqSN",
      "title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention",
      "abstract": "The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparse-that can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse\nindices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of longcontext LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By\nevaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM-4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.",
      "authors": [
        "Huiqiang Jiang",
        "YUCHENG LI",
        "Chengruidong Zhang",
        "Qianhui Wu",
        "Xufang Luo",
        "Surin Ahn",
        "Zhenhua Han",
        "Amir H. Abdi",
        "Dongsheng Li",
        "Chin-Yew Lin",
        "Yuqing Yang",
        "Lili Qiu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=fPBACAbqSN",
      "cdate": 1715763002814,
      "mdate": 1736912130182,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447269"
    },
    {
      "id": "CAC74VuMWX",
      "title": "An In-depth Investigation of Sparse Rate Reduction in Transformer-like Models",
      "abstract": "Deep neural networks have long been criticized for being black-box. To unveil the inner workings of modern neural architectures, a recent work proposed an information-theoretic objective function called Sparse Rate Reduction (SRR) and interpreted its unrolled optimization as a Transformer-like model called Coding Rate Reduction Transformer (CRATE). However, the focus of the study was primarily on the basic implementation, and whether this objective is optimized in practice and its causal relationship to generalization remain elusive. Going beyond this study, we derive different implementations by analyzing layer-wise behaviors of CRATE, both theoretically and empirically. To reveal the predictive power of SRR on generalization, we collect a set of model variants induced by varied implementations and hyperparameters and evaluate SRR as a complexity measure based on its correlation with generalization. Surprisingly, we find out that SRR has a positive correlation coefficient and outperforms other baseline measures, such as path-norm and sharpness-based ones. Furthermore, we show that generalization can be improved using SRR as regularization on benchmark image classification datasets. We hope this paper can shed light on leveraging SRR to design principled models and study their generalization ability.",
      "authors": [
        "Yunzhe Hu",
        "Difan Zou",
        "Dong Xu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=CAC74VuMWX",
      "cdate": 1715762914361,
      "mdate": 1730873974275,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447274"
    },
    {
      "id": "OoOCoZFVK3",
      "title": "Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) has emerged as a pivotal technique for fine-tuning large language models (LLMs) on specific tasks. However, prevailing RL fine-tuning methods predominantly rely on PPO and its variants. Though these algorithms are effective in general RL settings, they often exhibit suboptimal performance and vulnerability to distribution collapse when applied to the fine-tuning of LLMs. In this paper, we propose CORY, extending the RL fine-tuning of LLMs to a sequential cooperative multi-agent reinforcement learning framework, to leverage the inherent coevolution and emergent capabilities of multi-agent systems. In CORY, the LLM to be fine-tuned is initially duplicated into two autonomous agents: a pioneer and an observer. The pioneer generates responses based on queries, while the observer generates responses using both the queries and the pioneer’s responses. The two agents are trained together. During training, the agents exchange roles periodically, fostering cooperation and coevolution between them. Experiments evaluate CORY's performance by fine-tuning GPT-2 and Llama-2 under subjective and objective reward functions on the IMDB Review and GSM8K datasets, respectively. Results show that CORY outperforms PPO in terms of policy optimality, resistance to distribution collapse, and training robustness, thereby underscoring its potential as a superior methodology for refining LLMs in real-world applications.",
      "authors": [
        "Hao Ma",
        "Tianyi Hu",
        "Zhiqiang Pu",
        "Boyin Liu",
        "Xiaolin Ai",
        "Yanyan Liang",
        "Min Chen"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OoOCoZFVK3",
      "cdate": 1715762899283,
      "mdate": 1730873974241,
      "matched_keywords": [
        "large language model",
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447279"
    },
    {
      "id": "SF2GlFhVsS",
      "title": "Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization",
      "abstract": "Although Large Visual Language Models (LVLMs) have demonstrated exceptional abilities in understanding multimodal data, they invariably suffer from hallucinations, leading to a disconnection between the generated text and the corresponding images. Almost all  current visual contrastive decoding methods attempt to mitigate these hallucinations by introducing visual uncertainty information that appropriately widens the contrastive logits gap between hallucinatory and targeted ones.\n    However, due to uncontrollable nature of the global visual uncertainty, they struggle to precisely induce the hallucinatory tokens, which severely limits their effectiveness in mitigating hallucinations and may even lead to the generation of undesired hallucinations.\n    To tackle this issue, we conducted the theoretical analysis to promote the effectiveness of contrast decoding. Building on this insight, we introduce a novel optimization strategy named Hallucination-Induced Optimization (HIO). This strategy seeks to amplify the contrast between hallucinatory and targeted tokens relying on a fine-tuned theoretical preference model (i.e., Contrary Bradley-Terry Model), thereby facilitating efficient contrast decoding to alleviate hallucinations in LVLMs.\n    Extensive experimental research demonstrates that our HIO strategy can effectively reduce hallucinations in LVLMs, outperforming state-of-the-art methods across various benchmarks.",
      "authors": [
        "Xinyu Lyu",
        "Beitao Chen",
        "Lianli Gao",
        "Heng Tao Shen",
        "Jingkuan Song"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=SF2GlFhVsS",
      "cdate": 1715762847146,
      "mdate": 1736316826111,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.447284"
    },
    {
      "id": "bescO94wog",
      "title": "Active Learning with LLMs for Partially Observed and Cost-Aware Scenarios",
      "abstract": "Conducting experiments and gathering data for machine learning models is a complex and expensive endeavor, particularly when confronted with limited information. Typically, extensive _experiments_ to obtain features and labels come with a significant acquisition cost, making it impractical to carry out all of them. Therefore, it becomes crucial to strategically determine what to acquire to maximize the predictive performance while minimizing costs. To perform this task, existing data acquisition methods assume the availability of an initial dataset that is both fully-observed and labeled, crucially overlooking the **partial observability** of features characteristic of many real-world scenarios. In response to this challenge, we present Partially Observable Cost-Aware Active-Learning (POCA), a new learning approach aimed at improving model generalization in data-scarce and data-costly scenarios through label and/or feature acquisition. Introducing $\\mu$POCA as an instantiation, we maximise the uncertainty reduction in the predictive model when obtaining labels and features, considering associated costs. $\\mu$POCA enhance traditional Active Learning metrics based solely on the observed features by generating the unobserved features through Generative Surrogate Models, particularly Large Language Models (LLMs). We empirically validate $\\mu$POCA across diverse tabular datasets, varying data availability, acquisition costs, and LLMs.",
      "authors": [
        "Nicolás Astorga",
        "Tennison Liu",
        "Nabeel Seedat",
        "Mihaela van der Schaar"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=bescO94wog",
      "cdate": 1715762667837,
      "mdate": 1737028625518,
      "matched_keywords": [
        "large language model",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447289"
    },
    {
      "id": "57C9mszjj3",
      "title": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient In-Context Learning",
      "abstract": "Transformer-based large language models (LLMs) have displayed remarkable creative prowess and emergence capabilities. Existing empirical studies have revealed a strong connection between these LLMs' impressive emergence abilities and their in-context learning (ICL) capacity, allowing them to solve new tasks using only task-specific prompts without further fine-tuning. On the other hand, existing empirical and theoretical studies also show that there is a linear regularity of the multi-concept encoded semantic representation behind transformer-based LLMs. However, existing theoretical work fail to build up an understanding of the connection between this regularity and the innovative power of ICL. Additionally, prior work often focuses on simplified, unrealistic scenarios involving linear transformers or unrealistic loss functions, and they achieve only linear or sub-linear convergence rates. In contrast, this work provides a fine-grained mathematical analysis to show how transformers leverage the multi-concept semantics of words to enable powerful ICL and excellent out-of-distribution ICL abilities, offering insights into how transformers innovate solutions for certain unseen tasks encoded with multiple cross-concept semantics. Inspired by empirical studies on the linear latent geometry of LLMs, the analysis is based on a concept-based low-noise sparse coding prompt model. Leveraging advanced techniques, this work showcases the exponential 0-1 loss convergence over the highly non-convex training dynamics, which pioneeringly incorporates the challenges of softmax self-attention, ReLU-activated MLPs, and cross-entropy loss. Empirical simulations corroborate the theoretical findings.",
      "authors": [
        "Dake Bu",
        "Wei Huang",
        "Andi Han",
        "Atsushi Nitanda",
        "Taiji Suzuki",
        "Qingfu Zhang",
        "Hau-San Wong"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=57C9mszjj3",
      "cdate": 1715762459784,
      "mdate": 1736050887820,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447293"
    },
    {
      "id": "yUckuDjAE0",
      "title": "Learning Bregman Divergences with Application to Robustness",
      "abstract": "We propose a novel and general method to learn Bregman divergences from raw high-dimensional data that measure similarity between images in pixel space. As a prototypical application, we learn divergences that consider real-world corruptions of images (e.g., blur) as close to the original and noisy perturbations as far, even if in $L^p$-distance the opposite holds. We also show that the learned Bregman divergence excels on datasets of human perceptual similarity judgment, suggesting its utility in a range of applications. We then define adversarial attacks by replacing the projected gradient descent (PGD) with the mirror descent associated with the learned Bregman divergence, and use them to improve the state-of-the-art in robustness through adversarial training for common image corruptions. In particular, for the contrast corruption that was found problematic in prior work we achieve an accuracy that exceeds the $L^p$- and the LPIPS-based adversarially trained neural networks by a margin of 27.16\\% on the CIFAR-10-C corruption data set.",
      "authors": [
        "Mohamed-Hicham LEGHETTAS",
        "Markus Püschel"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=yUckuDjAE0",
      "cdate": 1715762398170,
      "mdate": 1730873974042,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447298"
    },
    {
      "id": "JXKbf1d4ib",
      "title": "Near-Minimax-Optimal Distributional Reinforcement Learning with a Generative Model",
      "abstract": "We propose a new algorithm for model-based distributional reinforcement learning (RL), and prove that it is minimax-optimal for approximating return distributions in the generative model regime (up to logarithmic factors), the first result of this kind for any distributional RL algorithm. Our analysis also provides new theoretical perspectives on categorical approaches to distributional RL, as well as introducing a new distributional Bellman equation, the stochastic categorical CDF Bellman equation, which we expect to be of independent interest. Finally, we provide an experimental study comparing a variety of model-based distributional RL algorithms, with several key takeaways for practitioners.",
      "authors": [
        "Mark Rowland",
        "Li Kevin Wenliang",
        "Remi Munos",
        "Clare Lyle",
        "Yunhao Tang",
        "Will Dabney"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=JXKbf1d4ib",
      "cdate": 1715762238398,
      "mdate": 1736967052684,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447303"
    },
    {
      "id": "jXsxGt80sv",
      "title": "Star-Agents: Automatic Data Optimization with LLM Agents for Instruction Tuning",
      "abstract": "The efficacy of large language models (LLMs) on downstream tasks usually hinges on instruction tuning, which relies critically on the quality of training data. Unfortunately, collecting high-quality and diverse data is both expensive and time-consuming. To mitigate this issue, we propose  a novel Star-Agents framework, which automates the enhancement of data quality across datasets through multi-agent collaboration and assessment. The framework adopts a three-pronged strategy. It  initially generates diverse instruction data with multiple LLM agents through a bespoke sampling method. Subsequently, the generated data undergo a rigorous evaluation using a dual-model method that assesses both difficulty and quality. Finaly, the above process evolves in a dynamic refinement phase, where more effective LLMs are prioritized, enhancing the overall data quality. Our empirical studies, including instruction tuning experiments with models such as Pythia and LLaMA, demonstrate the effectiveness of the proposed framework. Optimized datasets have achieved substantial improvements, with an average increase of 12\\% and notable gains in specific metrics, such as a 40\\% improvement in Fermi, as evidenced by benchmarks like MT-bench, Vicuna bench, and WizardLM testset. Codes will be released soon.",
      "authors": [
        "Hang Zhou",
        "Yehui Tang",
        "Haochen Qin",
        "Yujie Yang",
        "Renren Jin",
        "Deyi Xiong",
        "Kai Han",
        "Yunhe Wang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jXsxGt80sv",
      "cdate": 1715762207878,
      "mdate": 1730873973874,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447308"
    },
    {
      "id": "qaC4sSztlF",
      "title": "Towards Safe Concept Transfer of Multi-Modal Diffusion via Causal Representation Editing",
      "abstract": "Recent advancements in vision-language-to-image (VL2I) diffusion generation have made significant progress. While generating images from broad vision-language inputs holds promise, it also raises concerns about potential misuse, such as copying artistic styles without permission, which could have legal and social consequences. Therefore, it's crucial to establish governance frameworks to ensure ethical and copyright integrity, especially with widely used diffusion models. To address these issues, researchers have explored various approaches, such as dataset filtering, adversarial perturbations, machine unlearning, and inference-time refusals. However, these methods often lack either scalability or effectiveness. In response, we propose a new framework called causal representation editing (CRE), which extends representation editing from large language models (LLMs) to diffusion-based models. CRE enhances the efficiency and flexibility of safe content generation by intervening at diffusion timesteps causally linked to unsafe concepts. This allows for precise removal of harmful content while preserving acceptable content quality, demonstrating superior effectiveness, precision and scalability compared to existing methods. CRE can handle complex scenarios, including incomplete or blurred representations of unsafe concepts, offering a promising solution to challenges in managing harmful content generation in diffusion-based models.",
      "authors": [
        "Peiran Dong",
        "Bingjie WANG",
        "Song Guo",
        "Junxiao Wang",
        "Jie ZHANG",
        "Zicong Hong"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qaC4sSztlF",
      "cdate": 1715762135577,
      "mdate": 1730873973861,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447312"
    },
    {
      "id": "7arAADUK6D",
      "title": "Ensemble Learning for Heterogeneous Large Language Models with Deep Parallel Collaboration",
      "abstract": "Large language models (LLMs) exhibit complementary strengths in various tasks, motivating the research of LLM ensembling.\nHowever, existing work focuses on training an extra reward model or fusion model to select or combine all candidate answers, posing a great challenge to the generalization on unseen data distributions.\nBesides, prior methods use textual responses as communication media, ignoring the valuable information in the internal representations.\nIn this work, we propose a training-free ensemble framework \\textsc{DeePEn}, fusing the informative probability distributions yielded by different LLMs at each decoding step.\nUnfortunately, the vocabulary discrepancy between heterogeneous LLMs directly makes averaging the distributions unfeasible due to the token misalignment.\nTo address this challenge, \\textsc{DeePEn} maps the probability distribution of each model from its own probability space to a universal \\textit{relative space} based on the relative representation theory, and performs aggregation.\nNext, we devise a search-based inverse transformation to transform the aggregated result back to the probability space of one of the ensembling LLMs (main model), in order to determine the next token.\nWe conduct extensive experiments on ensembles of different number of LLMs, ensembles of LLMs with different architectures, and ensembles between the LLM and the specialist model.\nExperimental results show that (i) \\textsc{DeePEn} achieves consistent improvements across six benchmarks covering subject examination, reasoning, and knowledge, (ii) a well-performing specialist model can benefit from a less effective LLM through distribution fusion, and (iii) \\textsc{DeePEn} has complementary strengths with other ensemble methods such as voting.",
      "authors": [
        "Yichong Huang",
        "Xiaocheng Feng",
        "Baohang Li",
        "Yang Xiang",
        "Hui Wang",
        "Ting Liu",
        "Bing Qin"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7arAADUK6D",
      "cdate": 1715762124285,
      "mdate": 1730873973776,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447317"
    },
    {
      "id": "c8HOQIMwKP",
      "title": "UnSeg: One Universal Unlearnable Example Generator is Enough against All Image Segmentation",
      "abstract": "Image segmentation is a crucial vision task that groups pixels within an image into semantically meaningful segments, which is pivotal in obtaining a fine-grained understanding of real-world scenes. However, an increasing privacy concern exists regarding training large-scale image segmentation models on unauthorized private data. In this work, we exploit the concept of unlearnable examples to make images unusable to model training by generating and adding unlearnable noise into the original images. Particularly, we propose a novel Unlearnable Segmentation (UnSeg) framework to train a universal unlearnable noise generator that is capable of transforming any downstream images into their unlearnable version. The unlearnable noise generator is finetuned from the Segment Anything Model (SAM) via bilevel optimization on an interactive segmentation dataset towards minimizing the training error of a surrogate model that shares the same architecture with SAM (but trains from scratch). We empirically verify the effectiveness of UnSeg across 6 mainstream image segmentation tasks, 10 widely used datasets, and 7 different network architectures, and show that the unlearnable images can reduce the segmentation performance by a large margin. Our work provides useful insights into how to leverage foundation models in a data-efficient and computationally affordable manner to protect images against image segmentation models.",
      "authors": [
        "Ye Sun",
        "Hao Zhang",
        "Tiehua Zhang",
        "Xingjun Ma",
        "Yu-Gang Jiang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=c8HOQIMwKP",
      "cdate": 1715762101019,
      "mdate": 1730873973749,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447322"
    },
    {
      "id": "ucXUtMPWhv",
      "title": "ElasTST: Towards Robust Varied-Horizon Forecasting with Elastic Time-Series Transformer",
      "abstract": "Numerous industrial sectors necessitate models capable of providing robust forecasts across various horizons. Despite the recent strides in crafting specific architectures for time-series forecasting and developing pre-trained universal models, a comprehensive examination of their capability in accommodating varied-horizon forecasting during inference is still lacking. This paper bridges this gap through the design and evaluation of the Elastic Time-Series Transformer (ElasTST). The ElasTST model incorporates a non-autoregressive design with placeholders and structured self-attention masks, warranting future outputs that are invariant to adjustments in inference horizons. A tunable version of rotary position embedding is also integrated into ElasTST to capture time-series-specific periods and enhance adaptability to different horizons. Additionally, ElasTST employs a multi-scale patch design, effectively integrating both fine-grained and coarse-grained information.  During the training phase, ElasTST uses a horizon reweighting strategy that approximates the effect of random sampling across multiple horizons with a single fixed horizon setting. Through comprehensive experiments and comparisons with state-of-the-art time-series architectures and contemporary foundation models, we demonstrate the efficacy of ElasTST's unique design elements. Our findings position ElasTST as a robust solution for the practical necessity of varied-horizon forecasting.",
      "authors": [
        "Jiawen Zhang",
        "Shun Zheng",
        "Xumeng Wen",
        "Xiaofang Zhou",
        "Jiang Bian",
        "Jia Li"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ucXUtMPWhv",
      "cdate": 1715762099576,
      "mdate": 1730873973705,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447326"
    },
    {
      "id": "3ADBiWNUBb",
      "title": "Graph Structure Inference with BAM: Neural Dependency Processing via Bilinear Attention",
      "abstract": "Detecting dependencies among variables is a fundamental task across scientific disciplines. We propose a novel neural network model for graph structure inference, which aims to learn a mapping from observational data to the corresponding underlying dependence structures. The model is trained with variably shaped and coupled simulated input data and requires only a single forward pass through the trained network for inference. Central to our approach is a novel bilinear attention mechanism (BAM) operating on covariance matrices of transformed data while respecting the geometry of the manifold of symmetric positive definite (SPD) matrices. Inspired by graphical lasso methods, our model optimizes over continuous graph representations in the SPD space, where inverse covariance matrices encode conditional independence relations. Empirical evaluations demonstrate the robustness of our method in detecting diverse dependencies, excelling in undirected graph estimation and showing competitive performance in completed partially directed acyclic graph estimation via a novel two-step approach. The trained model effectively detects causal relationships and generalizes well across different functional forms of nonlinear dependencies.",
      "authors": [
        "Philipp Froehlich",
        "Heinz Koeppl"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3ADBiWNUBb",
      "cdate": 1715762045553,
      "mdate": 1736883475092,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447331"
    },
    {
      "id": "GrMczQGTlA",
      "title": "Humanoid Locomotion as Next Token Prediction",
      "abstract": "We cast real-world humanoid control as a next token prediction problem, akin to predicting the next word in language. Our model is a causal transformer trained via autoregressive prediction of sensorimotor sequences. To account for the multi-modal nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality. This general formulation enables us to leverage data with missing modalities, such as videos without actions. We train our model on a dataset of sequences from a prior neural network policy, a model-based controller, motion capture, and YouTube videos of humans. We show that our model enables a real humanoid robot to walk in San Francisco zero-shot. Our model can transfer to the real world even when trained on only 27 hours of walking data, and can generalize to commands not seen during training. These findings suggest a promising path toward learning challenging real-world control tasks by generative modeling of sensorimotor sequences.",
      "authors": [
        "Ilija Radosavovic",
        "Bike Zhang",
        "Baifeng Shi",
        "Jathushan Rajasegaran",
        "Sarthak Kamat",
        "Trevor Darrell",
        "Koushil Sreenath",
        "Jitendra Malik"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GrMczQGTlA",
      "cdate": 1715761984139,
      "mdate": 1736992007091,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447336"
    },
    {
      "id": "ISa7mMe7Vg",
      "title": "Exploiting LLM Quantization",
      "abstract": "Quantization leverages lower-precision weights to reduce the memory usage of large language models (LLMs) and is a key technique for enabling their deployment on commodity hardware. While LLM quantization's impact on utility has been extensively explored, this work for the first time studies its adverse effects from a security perspective. We reveal that widely used quantization methods can be exploited to produce a harmful quantized LLM, even though the full-precision counterpart appears benign, potentially tricking users into deploying the malicious quantized model. \nWe demonstrate this threat using a three-staged attack framework: (i) first, we obtain a malicious LLM through fine-tuning on an adversarial task; (ii) next, we quantize the malicious model and calculate constraints that characterize all full-precision models that map to the same quantized model; (iii) finally, using projected gradient descent, we tune out the poisoned behavior from the full-precision model while ensuring that its weights satisfy the constraints computed in step (ii). This procedure results in an LLM that exhibits benign behavior in full precision but when quantized, it follows the adversarial behavior injected in step (i). We experimentally demonstrate the feasibility and severity of such an attack across three diverse scenarios: vulnerable code generation, content injection, and over-refusal attack. In practice, the adversary could host the resulting full-precision model on an LLM community hub such as Hugging Face, exposing millions of users to the threat of deploying its malicious quantized version on their devices.",
      "authors": [
        "Kazuki Egashira",
        "Mark Vero",
        "Robin Staab",
        "Jingxuan He",
        "Martin Vechev"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ISa7mMe7Vg",
      "cdate": 1715761646679,
      "mdate": 1730873973457,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447341"
    },
    {
      "id": "GgV6UczIWM",
      "title": "A distributional simplicity bias in the learning dynamics of transformers",
      "abstract": "The remarkable capability of over-parameterised neural networks to generalise effectively has been explained by invoking a ``simplicity bias'': neural networks prevent overfitting by initially learning simple classifiers before progressing to more complex, non-linear functions. While simplicity biases have been described theoretically and experimentally in feed-forward networks for supervised learning, the extent to which they also explain the remarkable success of transformers trained with self-supervised techniques remains unclear. In our study, we demonstrate that transformers, trained on natural language data, also display a simplicity bias. Specifically, they sequentially learn many-body interactions among input tokens, reaching a saturation point in the prediction error for low-degree interactions while continuing to learn high-degree interactions.  To conduct this analysis, we develop a procedure to generate \\textit{clones} of a given natural language data set, which rigorously capture the interactions between tokens up to a specified order. This approach opens up the possibilities of studying how interactions of different orders in the data affect learning, in natural language processing and beyond.",
      "authors": [
        "Riccardo Rende",
        "Federica Gerace",
        "Alessandro Laio",
        "Sebastian Goldt"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GgV6UczIWM",
      "cdate": 1715760964011,
      "mdate": 1730873973285,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447345"
    },
    {
      "id": "oe5ZEqTOaz",
      "title": "Classifier-guided Gradient Modulation for Enhanced Multimodal Learning",
      "abstract": "Multimodal learning has developed very fast in recent years. However, during the multimodal training process, the model tends to rely on only one modality based on which it could learn faster, thus leading to inadequate use of other modalities. Existing methods to balance the training process always have some limitations on the loss functions, optimizers and the number of modalities and only consider modulating the magnitude of the gradients while ignoring the directions of the gradients. To solve these problems, in this paper, we present a novel method to balance multimodal learning with **C**lassifier-**G**uided **G**radient **M**odulation (CGGM), considering both the magnitude and directions of the gradients. We conduct extensive experiments on four multimodal datasets: UPMC-Food 101, CMU-MOSI, IEMOCAP and BraTS 2021, covering classification, regression and segmentation tasks. The results show that CGGM outperforms all the baselines and other state-of-the-art methods consistently, demonstrating its effectiveness and versatility. Our code is available at https://github.com/zrguo/CGGM.",
      "authors": [
        "Zirun Guo",
        "Tao Jin",
        "Jingyuan Chen",
        "Zhou Zhao"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=oe5ZEqTOaz",
      "cdate": 1715760544175,
      "mdate": 1730873973158,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.447350"
    },
    {
      "id": "pW9Jwim918",
      "title": "ReMoDetect: Reward Models Recognize Aligned LLM's Generations",
      "abstract": "The remarkable capabilities and easy accessibility of large language models (LLMs) have significantly increased societal risks (e.g., fake news generation), necessitating the development of LLM-generated text (LGT) detection methods for safe usage. However, detecting LGTs is challenging due to the vast number of LLMs, making it impractical to account for each LLM individually; hence, it is crucial to identify the common characteristics shared by these models. In this paper, we draw attention to a common feature of recent powerful LLMs, namely the alignment training, i.e., training LLMs to generate human-preferable texts. Our key finding is that as these aligned LLMs are trained to maximize the human preferences, they generate texts with higher estimated preferences even than human-written texts; thus, such texts are easily detected by using the reward model (i.e., an LLM trained to model human preference distribution). Based on this finding, we propose two training schemes to further improve the detection ability of the reward model, namely (i) continual preference fine-tuning to make reward model prefer aligned LGTs even further and (ii) reward modeling of Human/LLM mixed texts (a rephrased texts from human-written texts using aligned LLMs), which serves as a median preference text corpus between LGTs and human-written texts to learn the decision boundary better. We provide an extensive evaluation by considering six text domains across twelve aligned LLMs, where our method demonstrates state-of-the-art results.",
      "authors": [
        "Hyunseok Lee",
        "Jihoon Tack",
        "Jinwoo Shin"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pW9Jwim918",
      "cdate": 1715760532912,
      "mdate": 1730873973170,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447354"
    },
    {
      "id": "EFrgBP9au6",
      "title": "Emergence of heavy tails in homogenized stochastic gradient descent",
      "abstract": "It has repeatedly been observed that loss minimization by stochastic gradient descent (SGD) leads to heavy-tailed distributions of neural network parameters. Here, we analyze a continuous diffusion approximation of SGD, called homogenized stochastic gradient descent (hSGD), and show in a regularized linear regression framework that it leads to an asymptotically heavy-tailed parameter distribution, even though local gradient noise is Gaussian. We give explicit upper and lower bounds on the tail-index of the resulting parameter distribution and validate these bounds in numerical experiments. Moreover, the explicit form of these bounds enables us to quantify the interplay between optimization hyperparameters and the tail-index. Doing so, we contribute to the ongoing discussion on links between heavy tails and the generalization performance of neural networks as well as the ability of SGD to avoid suboptimal local minima.",
      "authors": [
        "Zhe Jiao",
        "Martin Keller-Ressel"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EFrgBP9au6",
      "cdate": 1715760467630,
      "mdate": 1735880263980,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447359"
    },
    {
      "id": "2WQjNXZbhR",
      "title": "Dendritic Integration Inspired Artificial Neural Networks Capture Data Correlation",
      "abstract": "Incorporating biological neuronal properties into Artificial Neural Networks (ANNs) to enhance computational capabilities is under active investigation in the field of deep learning. Inspired by recent findings indicating that dendrites adhere to quadratic integration rule for synaptic inputs, this study explores the computational benefits of quadratic neurons. We theoretically demonstrate that quadratic neurons inherently capture correlation within structured data, a feature that grants them superior generalization abilities over traditional neurons. This is substantiated by few-shot learning experiments. Furthermore, we integrate the quadratic rule into Convolutional Neural Networks (CNNs) using a biologically plausible approach, resulting in innovative architectures—Dendritic integration inspired CNNs (Dit-CNNs). Our Dit-CNNs compete favorably with state-of-the-art models across multiple classification benchmarks, e.g., ImageNet-1K, while retaining the simplicity and efficiency of traditional CNNs. All source code are available at https://github.com/liuchongming1999/Dendritic-integration-inspired-CNN-NeurIPS-2024.",
      "authors": [
        "Chongming Liu",
        "Jingyang Ma",
        "Songting Li",
        "Douglas Zhou"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=2WQjNXZbhR",
      "cdate": 1715760141270,
      "mdate": 1730873972884,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447364"
    },
    {
      "id": "chLoLUHnai",
      "title": "Large Stepsize Gradient Descent for Non-Homogeneous Two-Layer Networks: Margin Improvement and Fast Optimization",
      "abstract": "The typical training of neural networks using large stepsize gradient descent (GD) under the logistic loss often involves two distinct phases, where the empirical risk oscillates in the first phase but decreases monotonically in the second phase. We investigate this phenomenon in two-layer networks that satisfy a near-homogeneity condition. We show that the second phase begins once the empirical risk falls below a certain threshold, dependent on the stepsize. Additionally, we show that the normalized margin grows nearly monotonically in the second phase, demonstrating an implicit bias of GD in training non-homogeneous predictors. If the dataset is linearly separable and the derivative of the activation function is bounded away from zero, we show that the average empirical risk decreases, implying that the first phase must stop in finite steps. Finally, we demonstrate that by choosing a suitably large stepsize, GD that undergoes this phase transition is more efficient than GD that monotonically decreases the risk. Our analysis applies to networks of any width, beyond the well-known neural tangent kernel and mean-field regimes.",
      "authors": [
        "Yuhang Cai",
        "Jingfeng Wu",
        "Song Mei",
        "Michael Lindsey",
        "Peter Bartlett"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=chLoLUHnai",
      "cdate": 1715759960836,
      "mdate": 1730873972814,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447369"
    },
    {
      "id": "sEpSxteEKJ",
      "title": "Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction",
      "abstract": "Dynamical systems theory (DST) is fundamental for many areas of science and engineering. It can provide deep insights into the behavior of systems evolving in time, as typically described by differential or recursive equations. A common approach to facilitate mathematical tractability and interpretability of DS models involves decomposing nonlinear DS into multiple linear DS combined by switching manifolds, i.e. piecewise linear (PWL) systems. PWL models are popular in engineering and a frequent choice in mathematics for analyzing the topological properties of DS. However, hand-crafting such models is tedious and only possible for very low-dimensional scenarios, while inferring them from data usually gives rise to unnecessarily complex representations with very many linear subregions. Here we introduce Almost-Linear Recurrent Neural Networks (AL-RNNs) which automatically and robustly produce most parsimonious PWL representations of DS from time series data, using as few PWL nonlinearities as possible. AL-RNNs can be efficiently trained with any SOTA algorithm for dynamical systems reconstruction (DSR), and naturally give rise to a symbolic encoding of the underlying DS that provably preserves important topological properties. We show that for the Lorenz and Rössler systems, AL-RNNs derive, in a purely data-driven way, the known topologically minimal PWL representations of the corresponding chaotic attractors. We further illustrate on two challenging empirical datasets that interpretable symbolic encodings of the dynamics can be achieved, tremendously facilitating mathematical and computational analysis of the underlying systems.",
      "authors": [
        "Manuel Brenner",
        "Christoph Jürgen Hemmer",
        "Zahra Monfared",
        "Daniel Durstewitz"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=sEpSxteEKJ",
      "cdate": 1715759920993,
      "mdate": 1736935666860,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447373"
    },
    {
      "id": "ZWNdgc13aw",
      "title": "NeoRL: Efficient Exploration for Nonepisodic RL",
      "abstract": "We study the problem of nonepisodic reinforcement learning (RL) for nonlinear dynamical systems, where the system dynamics are unknown and the RL agent has to learn from a single trajectory, i.e., without resets. We propose **N**on**e**pisodic **O**ptistmic **RL** (NeoRL), an approach based on the principle of optimism in the face of uncertainty. NeoRL uses well-calibrated probabilistic models and plans optimistically w.r.t. the epistemic uncertainty about the unknown dynamics. Under continuity and bounded energy assumptions on the system, we\nprovide a first-of-its-kind regret bound of  $\\mathcal{O}(\\beta_T \\sqrt{T \\Gamma_T})$ for general nonlinear systems with Gaussian process dynamics. We compare NeoRL to other baselines on several deep RL environments and empirically demonstrate that NeoRL achieves the optimal average cost while incurring the least regret.",
      "authors": [
        "Bhavya Sukhija",
        "Lenart Treven",
        "Florian Dorfler",
        "Stelian Coros",
        "Andreas Krause"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ZWNdgc13aw",
      "cdate": 1715759562763,
      "mdate": 1730873972492,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447378"
    },
    {
      "id": "vjAORqq71s",
      "title": "Newton Losses: Using Curvature Information for Learning with Differentiable Algorithms",
      "abstract": "When training neural networks with custom objectives, such as ranking losses and shortest-path losses, a common problem is that they are, per se, non-differentiable. A popular approach is to continuously relax the objectives to provide gradients, enabling learning. However, such differentiable relaxations are often non-convex and can exhibit vanishing and exploding gradients, making them (already in isolation) hard to optimize. Here, the loss function poses the bottleneck when training a deep neural network. We present Newton Losses, a method for improving the performance of existing hard to optimize losses by exploiting their second-order information via their empirical Fisher and Hessian matrices. Instead of training the neural network with second-order techniques, we only utilize the loss function's second-order information to replace it by a Newton Loss, while training the network with gradient descent.  This makes our method computationally efficient. We apply Newton Losses to eight differentiable algorithms for sorting and shortest-paths, achieving significant improvements for less-optimized differentiable algorithms, and consistent improvements, even for well-optimized differentiable algorithms.",
      "authors": [
        "Felix Petersen",
        "Christian Borgelt",
        "Tobias Sutter",
        "Hilde Kuehne",
        "Oliver Deussen",
        "Stefano Ermon"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vjAORqq71s",
      "cdate": 1715759541459,
      "mdate": 1730873972449,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447383"
    },
    {
      "id": "UGlDVc0GTU",
      "title": "LLM-based Skill Diffusion for Zero-shot Policy Adaptation",
      "abstract": "Recent advances in data-driven imitation learning and offline reinforcement learning have highlighted the use of expert data for skill acquisition and the development of hierarchical policies based on these skills. However, these approaches have not significantly advanced in adapting these skills to unseen contexts, which may involve changing environmental conditions or different user requirements. In this paper, we present a novel LLM-based policy adaptation framework LDuS which leverages an LLM to guide the generation process of a skill diffusion model upon contexts specified in language, facilitating zero-shot skill-based policy adaptation to different contexts. To implement the skill diffusion model, we adapt the loss-guided diffusion with a sequential in-painting technique, where target trajectories are conditioned by masking them with past state-action sequences, thereby enabling the robust and controlled generation of skill trajectories in test-time. To have a loss function for a given context, we employ the LLM-based code generation with iterative refinement, by which the code and controlled trajectory are validated to align with the context in a closed-loop manner. Through experiments, we demonstrate the zero-shot adaptability of LDuS to various context types including different specification levels, multi-modality, and varied temporal conditions for several robotic manipulation tasks, outperforming other language-conditioned imitation and planning methods.",
      "authors": [
        "Woo Kyung Kim",
        "Youngseok Lee",
        "Jooyoung Kim",
        "Honguk Woo"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=UGlDVc0GTU",
      "cdate": 1715759439314,
      "mdate": 1734693464480,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447388"
    },
    {
      "id": "nw6ANsC66G",
      "title": "Probabilistic Federated Prompt-Tuning with Non-IID and Imbalanced Data",
      "abstract": "Fine-tuning pre-trained models is a popular approach in machine learning for solving complex tasks with moderate data. However, fine-tuning the entire pre-trained model is ineffective in federated data scenarios where local data distributions are diversely skewed. To address this, we explore integrating federated learning with a more effective prompt-tuning method, optimizing for a small set of input prefixes to reprogram the pre-trained model's behavior. Our approach transforms federated learning into a distributed set modeling task, aggregating diverse sets of prompts to globally fine-tune the pre-trained model. We benchmark various baselines based on direct adaptations of existing federated model aggregation techniques and introduce a new probabilistic prompt aggregation method that substantially outperforms these baselines. Our reported results on a variety of computer vision datasets confirm that the proposed method is most effective to combat extreme data heterogeneity in federated learning.",
      "authors": [
        "Pei-Yau Weng",
        "Minh Hoang",
        "Lam M. Nguyen",
        "My T. Thai",
        "Tsui-Wei Weng",
        "Trong Nghia Hoang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nw6ANsC66G",
      "cdate": 1715759338772,
      "mdate": 1730873972369,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447392"
    },
    {
      "id": "ctXYOoAgRy",
      "title": "How do Large Language Models Handle Multilingualism?",
      "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across diverse languages. This study explores how LLMs handle multilingualism. Based on observed language ratio shifts among layers and the relationships between network structures and certain capabilities, we hypothesize the LLM's multilingual workflow ($\\texttt{MWork}$): LLMs initially understand the query, converting multilingual inputs into English for task-solving. In the intermediate layers, they employ English for thinking and incorporate multilingual knowledge with self-attention and feed-forward structures, respectively. In the final layers, LLMs generate responses aligned with the original language of the query. \nTo verify $\\texttt{MWork}$, we introduce Parallel Language-specific Neuron Detection ($\\texttt{PLND}$) to identify activated neurons for inputs in different languages without any labeled data. Using $\\texttt{PLND}$, we validate $\\texttt{MWork}$ through extensive experiments involving the deactivation of language-specific neurons across various layers and structures. \nMoreover, $\\texttt{MWork}$ allows fine-tuning of language-specific neurons with a small dataset, enhancing multilingual abilities in a specific language without compromising others. This approach results in an average improvement of $3.6\\%$ for high-resource languages and $2.3\\%$ for low-resource languages across all tasks with just $400$ documents.",
      "authors": [
        "Yiran Zhao",
        "Wenxuan Zhang",
        "Guizhen Chen",
        "Kenji Kawaguchi",
        "Lidong Bing"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ctXYOoAgRy",
      "cdate": 1715759311291,
      "mdate": 1730873972283,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447397"
    },
    {
      "id": "iykao97YXf",
      "title": "Reinforcement Learning with LTL and $\\omega$-Regular Objectives via Optimality-Preserving Translation to Average Rewards",
      "abstract": "Linear temporal logic (LTL) and, more generally, $\\omega$-regular objectives are alternatives to the traditional discount sum and average reward objectives in reinforcement learning (RL), offering the advantage of greater comprehensibility and hence explainability. In this work, we study the relationship between these objectives. Our main result is that each RL problem for $\\omega$-regular objectives can be reduced to a limit-average reward problem in an optimality-preserving fashion, via (finite-memory) reward machines. Furthermore, we demonstrate the efficacy of this approach by showing that optimal policies for limit-average problems can be found asymptotically by solving a sequence of discount-sum problems approximately. Consequently, we resolve an open problem: optimal policies for LTL and $\\omega$-regular objectives can be learned asymptotically.",
      "authors": [
        "Xuan-Bach Le",
        "Dominik Wagner",
        "Leon Witzman",
        "Alexander Rabinovich",
        "Luke Ong"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=iykao97YXf",
      "cdate": 1715759295681,
      "mdate": 1730873972259,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447402"
    },
    {
      "id": "9Y8zUO11EQ",
      "title": "SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents",
      "abstract": "Rigorous software testing is crucial for developing and maintaining high-quality code, making automated test generation a promising avenue for both improving software quality and boosting the effectiveness of code generation methods. However, while code generation with Large Language Models (LLMs) is an extraordinarily active research area, test generation remains relatively unexplored. We address this gap and investigate the capability of LLM-based Code Agents to formalize user issues into test cases. To this end, we propose a novel benchmark based on popular GitHub repositories, containing real-world issues, ground-truth bug-fixes, and golden tests.  We find that LLMs generally perform surprisingly well at generating relevant test cases, with Code Agents designed for code repair exceeding the performance of systems designed specifically for test generation. Further, as test generation is a similar but more structured task than code generation, it allows for a more fine-grained analysis using issue reproduction rate and coverage changes, providing a dual metric for analyzing systems designed for code repair. Finally, we find that generated tests are an effective filter for proposed code fixes, doubling the precision of SWE-Agent. We release all data and code at https://github.com/logic-star-ai/SWT-Bench.",
      "authors": [
        "Niels Mündler",
        "Mark Niklas Mueller",
        "Jingxuan He",
        "Martin Vechev"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9Y8zUO11EQ",
      "cdate": 1715759243288,
      "mdate": 1737117632913,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447407"
    },
    {
      "id": "CbHz30KeA4",
      "title": "Taming \"data-hungry\" reinforcement learning? Stability in continuous state-action spaces",
      "abstract": "We introduce a novel framework for analyzing reinforcement learning (RL) in continuous state-action spaces, and use it to prove fast rates of convergence in both off-line and on-line settings. Our analysis highlights two key stability properties, relating to how changes in value functions and/or policies affect the Bellman operator and occupation measures. We argue that these properties are satisfied in many continuous state-action Markov decision processes. Our analysis also offers fresh perspectives on the roles of pessimism and optimism in off-line and on-line RL.",
      "authors": [
        "Yaqi Duan",
        "Martin J Wainwright"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=CbHz30KeA4",
      "cdate": 1715759187305,
      "mdate": 1730873972147,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447412"
    },
    {
      "id": "nmUkwoOHFO",
      "title": "The Representation Landscape of Few-Shot Learning and Fine-Tuning in Large Language Models",
      "abstract": "In-context learning (ICL) and supervised fine-tuning (SFT) are two common strategies for improving the performance of modern large language models (LLMs) on specific tasks. Despite their different natures, these strategies often lead to comparable performance gains. \nHowever, little is known about whether they induce similar representations inside LLMs.  We approach this problem by analyzing the probability landscape of their hidden representations in the two cases. More specifically, we compare how LLMs solve the same question-answering task, finding that ICL and SFT create very different internal structures, in both cases undergoing a sharp transition in the middle of the network. In the first half of the network, ICL shapes interpretable representations hierarchically organized according to their semantic content. In contrast, the probability landscape obtained with SFT is fuzzier and semantically mixed. In the second half of the model, the fine-tuned representations develop probability modes that better encode the identity of answers, while less-defined peaks characterize the landscape of ICL representations. Our approach reveals the diverse computational strategies developed inside LLMs to solve the same task across different conditions, allowing us to make a step towards designing optimal methods to extract information from language models.",
      "authors": [
        "Diego Doimo",
        "Alessandro Pietro Serra",
        "Alessio ansuini",
        "Alberto Cazzaniga"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=nmUkwoOHFO",
      "cdate": 1715759065935,
      "mdate": 1730873972101,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447417"
    },
    {
      "id": "7HFQfRjdcn",
      "title": "Neural Characteristic Activation Analysis and Geometric Parameterization for ReLU Networks",
      "abstract": "We introduce a novel approach for analyzing the training dynamics of ReLU networks by examining the characteristic activation boundaries of individual ReLU neurons. Our proposed analysis reveals a critical instability in common neural network parameterizations and normalizations during stochastic optimization, which impedes fast convergence and hurts generalization performance. Addressing this, we propose Geometric Parameterization (GmP), a novel neural network parameterization technique that effectively separates the radial and angular components of weights in the hyperspherical coordinate system. We show theoretically that GmP resolves the aforementioned instability issue. We report empirical results on various models and benchmarks to verify GmP's advantages of optimization stability, convergence speed and generalization performance.",
      "authors": [
        "Wenlin Chen",
        "Hong Ge"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7HFQfRjdcn",
      "cdate": 1715759054152,
      "mdate": 1730873972050,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447422"
    },
    {
      "id": "0JSKjdePGq",
      "title": "When to Sense and Control? A Time-adaptive Approach for Continuous-Time RL",
      "abstract": "Reinforcement learning (RL) excels in optimizing policies for discrete-time Markov decision processes (MDP). However, various systems are inherently continuous in time, making discrete-time MDPs an inexact modeling choice. \nIn many applications, such as greenhouse control or medical treatments, each interaction (measurement or switching of action) involves manual intervention and thus is inherently costly. Therefore, \nwe generally prefer a time-adaptive approach with fewer interactions with the system.\nIn this work, we formalize an RL framework, \n**T**ime-**a**daptive **Co**ntrol \\& **S**ensing (**TaCoS**), that tackles this challenge by optimizing over policies that besides control predict the duration of its application. Our formulation results in an extended MDP that any standard RL algorithm can solve.\nWe demonstrate that state-of-the-art RL algorithms trained on TaCoS drastically reduce the interaction amount over their discrete-time counterpart while retaining the same or improved performance, and exhibiting robustness over discretization frequency.\nFinally, we propose OTaCoS, an efficient model-based algorithm for our setting. We show that OTaCoS enjoys sublinear regret for systems with sufficiently smooth dynamics and empirically results in further sample-efficiency gains.",
      "authors": [
        "Lenart Treven",
        "Bhavya Sukhija",
        "Yarden As",
        "Florian Dorfler",
        "Andreas Krause"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=0JSKjdePGq",
      "cdate": 1715759047735,
      "mdate": 1730873971981,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447426"
    },
    {
      "id": "3HpgVs22UJ",
      "title": "Adaptive $Q$-Aid for Conditional Supervised Learning in Offline Reinforcement Learning",
      "abstract": "Offline reinforcement learning (RL) has progressed with return-conditioned supervised learning (RCSL), but its lack of stitching ability remains a limitation. We introduce $Q$-Aided Conditional Supervised Learning (QCS), which effectively combines the stability of RCSL with the stitching capability of $Q$-functions. By analyzing $Q$-function over-generalization, which impairs stable stitching, QCS adaptively integrates $Q$-aid into RCSL's loss function based on trajectory return. Empirical results show that QCS significantly outperforms RCSL and value-based methods, consistently achieving or exceeding the highest trajectory returns across diverse offline RL benchmarks. QCS represents a breakthrough in offline RL, pushing the limits of what can be achieved and fostering further innovations.",
      "authors": [
        "Jeonghye Kim",
        "Suyoung Lee",
        "Woojun Kim",
        "Youngchul Sung"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=3HpgVs22UJ",
      "cdate": 1715759006727,
      "mdate": 1730873972006,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447431"
    },
    {
      "id": "BUpxPo80QP",
      "title": "InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction",
      "abstract": "Text-conditioned human motion generation has experienced significant advancements with diffusion models trained on extensive motion capture data and corresponding textual annotations. However, extending such success to 3D dynamic human-object interaction (HOI) generation faces notable challenges, primarily due to the lack of large-scale interaction data and comprehensive descriptions that align with these interactions. This paper takes the initiative and showcases the potential of generating human-object interactions without direct training on text-interaction pair data. Our key insight in achieving this is that interaction semantics and dynamics can be decoupled. Being unable to learn interaction semantics through supervised training, we instead leverage pre-trained large models, synergizing knowledge from a large language model and a text-to-motion model. While such knowledge offers high-level control over interaction semantics, it cannot grasp the intricacies of low-level interaction dynamics. To overcome this issue, we introduce a world model designed to comprehend simple physics, modeling how human actions influence object motion. By integrating these components, our novel framework, InterDreamer, is able to generate text-aligned 3D HOI sequences without relying on paired text-interaction data. We apply InterDreamer to the BEHAVE, OMOMO, and CHAIRS datasets, and our comprehensive experimental analysis demonstrates its capability to generate realistic and coherent interaction sequences that seamlessly align with the text directives.",
      "authors": [
        "Sirui Xu",
        "Ziyin Wang",
        "Yu-Xiong Wang",
        "Liangyan Gui"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=BUpxPo80QP",
      "cdate": 1715758818103,
      "mdate": 1730873971788,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447436"
    },
    {
      "id": "GnAfyR8AhC",
      "title": "Towards Calibrated Robust Fine-Tuning of Vision-Language Models",
      "abstract": "Improving out-of-distribution (OOD) generalization during in-distribution (ID) adaptation is a primary goal of robust fine-tuning of zero-shot models beyond naive fine-tuning. However, despite decent OOD generalization performance from recent robust fine-tuning methods, confidence calibration for reliable model output has not been fully addressed. This work proposes a robust fine-tuning method that improves both OOD accuracy and confidence calibration simultaneously in vision language models. Firstly, we show that both OOD classification and OOD calibration errors have a shared upper bound consisting of two terms of ID data: 1) ID calibration error and 2) the smallest singular value of the ID input covariance matrix. Based on this insight, we design a novel framework that conducts fine-tuning with a constrained multimodal contrastive loss enforcing a larger smallest singular value, which is further guided by the self-distillation of a moving-averaged model to achieve calibrated prediction as well. Starting from empirical evidence supporting our theoretical statements, we provide extensive experimental results on ImageNet distribution shift benchmarks that demonstrate the effectiveness of our theorem and its practical implementation.",
      "authors": [
        "Changdae Oh",
        "Hyesu Lim",
        "Mijoo Kim",
        "Dongyoon Han",
        "Sangdoo Yun",
        "Jaegul Choo",
        "Alexander G Hauptmann",
        "Zhi-Qi Cheng",
        "Kyungwoo Song"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=GnAfyR8AhC",
      "cdate": 1715758745380,
      "mdate": 1735729991391,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.447441"
    },
    {
      "id": "APSBwuMopO",
      "title": "Optimized Feature Generation for Tabular Data via LLMs with Decision Tree Reasoning",
      "abstract": "In tabular prediction tasks, tree-based models combined with automated feature engineering methods often outperform deep learning approaches that rely on learned representations. While these feature engineering techniques are effective, they typically depend on a pre-defined search space and primarily use validation scores for feature selection, thereby missing valuable insights from previous experiments.\nTo address these limitations, we propose a novel tabular learning framework that utilizes large language models (LLMs), termed Optimizing Column feature generator with decision Tree reasoning (OCTree). Our key idea is to leverage the reasoning capabilities of LLMs to identify effective feature generation rules without manually specifying the search space and provide language-based reasoning information highlighting past experiments as feedback for iterative rule improvements. We use decision trees to convey this reasoning information, as they can be easily represented in natural language, effectively providing knowledge from prior experiments (i.e., the impact of the generated features on performance) to the LLMs. Our empirical results demonstrate that OCTree consistently enhances the performance of various prediction models across diverse benchmarks, outperforming competing automated feature engineering methods. Code is available at https://github.com/jaehyun513/OCTree.",
      "authors": [
        "Jaehyun Nam",
        "Kyuyoung Kim",
        "Seunghyuk Oh",
        "Jihoon Tack",
        "Jaehyung Kim",
        "Jinwoo Shin"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=APSBwuMopO",
      "cdate": 1715758531858,
      "mdate": 1730873971586,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447446"
    },
    {
      "id": "faBXeVBNqz",
      "title": "Higher-Rank Irreducible Cartesian Tensors for Equivariant Message Passing",
      "abstract": "The ability to perform fast and accurate atomistic simulations is crucial for advancing the chemical sciences. By learning from high-quality data, machine-learned interatomic potentials achieve accuracy on par with ab initio and first-principles methods at a fraction of their computational cost. The success of machine-learned interatomic potentials arises from integrating inductive biases such as equivariance to group actions on an atomic system, e.g., equivariance to rotations and reflections. In particular, the field has notably advanced with the emergence of equivariant message passing. Most of these models represent an atomic system using spherical tensors, tensor products of which require complicated numerical coefficients and can be computationally demanding. Cartesian tensors offer a promising alternative, though state-of-the-art methods lack flexibility in message-passing mechanisms, restricting their architectures and expressive power. This work explores higher-rank irreducible Cartesian tensors to address these limitations. We integrate irreducible Cartesian tensor products into message-passing neural networks and prove the equivariance and traceless property of the resulting layers. Through empirical evaluations on various benchmark data sets, we consistently observe on-par or better performance than that of state-of-the-art spherical and Cartesian models.",
      "authors": [
        "Viktor Zaverkin",
        "Francesco Alesiani",
        "Takashi Maruyama",
        "Federico Errica",
        "Henrik Christiansen",
        "Makoto Takamoto",
        "Nicolas Weber",
        "Mathias Niepert"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=faBXeVBNqz",
      "cdate": 1715758475526,
      "mdate": 1730873971553,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447451"
    },
    {
      "id": "y6JotynERr",
      "title": "Towards Diverse Device Heterogeneous Federated Learning via Task Arithmetic Knowledge Integration",
      "abstract": "Federated Learning (FL) has emerged as a promising paradigm for collaborative machine learning, while preserving user data privacy. Despite its potential, standard FL algorithms lack support for diverse heterogeneous device prototypes, which vary significantly in model and dataset sizes---from small IoT devices to large workstations. This limitation is only partially addressed by existing knowledge distillation (KD) techniques, which often fail to transfer knowledge effectively across a broad spectrum of device prototypes with varied capabilities. This failure primarily stems from two issues: the dilution of informative logits from more capable devices by those from less capable ones, and the use of a single integrated logits as the distillation target across all devices, which neglects their individual learning capacities and and the unique contributions of each device. To address these challenges, we introduce TAKFL, a novel KD-based framework that treats the knowledge transfer from each device prototype's ensemble as a separate task, independently distilling each to preserve its unique contributions and avoid dilution. TAKFL also incorporates a KD-based self-regularization technique to mitigate the issues related to the noisy and unsupervised ensemble distillation process. To integrate the separately distilled knowledge, we introduce an adaptive task arithmetic knowledge integration process, allowing each student model to customize the knowledge integration for optimal performance. Additionally, we present theoretical results demonstrating the effectiveness of task arithmetic in transferring knowledge across heterogeneous device prototypes with varying capacities. Comprehensive evaluations of our method across both computer vision (CV) and natural language processing (NLP) tasks demonstrate that TAKFL achieves state-of-the-art results in a variety of datasets and settings, significantly outperforming existing KD-based methods. Our code is released at https://github.com/MMorafah/TAKFL and the project website is available at https://mmorafah.github.io/takflpage .",
      "authors": [
        "Mahdi Morafah",
        "Vyacheslav Kungurtsev",
        "Hojin Matthew Chang",
        "Chen Chen",
        "Bill Lin"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=y6JotynERr",
      "cdate": 1715758137322,
      "mdate": 1730873971419,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447456"
    },
    {
      "id": "RcPAJAnpnm",
      "title": "Incremental Learning of Retrievable Skills For Efficient Continual Task Adaptation",
      "abstract": "Continual Imitation Learning (CiL) involves extracting and accumulating task knowledge from demonstrations across multiple stages and tasks to achieve a multi-task policy. With recent advancements in foundation models, there has been a growing interest in adapter-based CiL approaches, where adapters are established parameter-efficiently for tasks newly demonstrated. While these approaches isolate parameters for specific tasks and tend to mitigate catastrophic forgetting, they limit knowledge sharing among different demonstrations. We introduce IsCiL, an adapter-based CiL framework that addresses this limitation of knowledge sharing by incrementally learning shareable skills from different demonstrations, thus enabling sample-efficient task adaptation using the skills particularly in non-stationary CiL environments. In IsCiL, demonstrations are mapped into the state embedding space, where proper skills can be retrieved upon input states through prototype-based memory. These retrievable skills are incrementally learned on their corresponding adapters. Our CiL experiments with complex tasks in the Franka-Kitchen and Meta-World demonstrate the robust performance of IsCiL in both task adaptation and sample-efficiency. We also show a simple extension of IsCiL for task unlearning scenarios.",
      "authors": [
        "Daehee Lee",
        "Minjong Yoo",
        "Woo Kyung Kim",
        "Wonje Choi",
        "Honguk Woo"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=RcPAJAnpnm",
      "cdate": 1715758057569,
      "mdate": 1736070369609,
      "matched_keywords": [
        "foundation model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447461"
    },
    {
      "id": "gnnmB7y0Xx",
      "title": "In-Context Learning State Vector with Inner and Momentum Optimization",
      "abstract": "Large Language Models (LLMs) have exhibited an impressive ability to perform In-Context Learning (ICL) from only a few examples. Recent works have indicated that the functions learned by ICL can be represented through compressed vectors derived from the transformer. However, the working mechanisms and optimization of these vectors are yet to be thoroughly explored. In this paper, we address this gap by presenting a comprehensive analysis of these compressed vectors, drawing parallels to the parameters trained with gradient descent, and introducing the concept of state vector. Inspired by the works on model soup and momentum-based gradient descent, we propose inner and momentum optimization methods that are applied to refine the state vector progressively as test-time adaptation. Moreover, we simulate state vector aggregation in the multiple example setting, where demonstrations comprising numerous examples are usually too lengthy for regular ICL, and further propose a divide-and-conquer aggregation method to address this challenge. We conduct extensive experiments using Llama-2 and GPT-J in both zero-shot setting and few-shot setting. The experimental results show that our optimization method effectively enhances the state vector and achieves the state-of-the-art performance on diverse tasks.",
      "authors": [
        "Dongfang Li",
        "zhenyu liu",
        "Xinshuo Hu",
        "Zetian Sun",
        "Baotian Hu",
        "Min Zhang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=gnnmB7y0Xx",
      "cdate": 1715758025083,
      "mdate": 1730873971322,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447466"
    },
    {
      "id": "9utMGIbHBt",
      "title": "UDPM: Upsampling Diffusion Probabilistic Models",
      "abstract": "Denoising Diffusion Probabilistic Models (DDPM) have recently gained significant attention. DDPMs compose a Markovian process that begins in the data domain and gradually adds noise until reaching pure white noise. DDPMs generate high-quality samples from complex data distributions by defining an inverse process and training a deep neural network to learn this mapping. However, these models are inefficient because they require many diffusion steps to produce aesthetically pleasing samples. Additionally, unlike generative adversarial networks (GANs), the latent space of diffusion models is less interpretable. In this work, we propose to generalize the denoising diffusion process into an Upsampling Diffusion Probabilistic Model (UDPM). In the forward process, we reduce the latent variable dimension through downsampling, followed by the traditional noise perturbation. As a result, the reverse process gradually denoises and upsamples the latent variable to produce a sample from the data distribution. We formalize the Markovian diffusion processes of UDPM and demonstrate its generation capabilities on the popular FFHQ, AFHQv2, and CIFAR10 datasets. UDPM generates images with as few as three network evaluations, whose overall computational cost is less than a single DDPM or EDM step while achieving an FID score of 6.86. This surpasses current state-of-the-art efficient diffusion models that use a single denoising step for sampling. Additionally, UDPM offers an interpretable and interpolable latent space, which gives it an advantage over traditional DDPMs. Our code is available online: \\url{https://github.com/shadyabh/UDPM/}",
      "authors": [
        "Shady Abu-Hussein",
        "Raja Giryes"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9utMGIbHBt",
      "cdate": 1715758016792,
      "mdate": 1730873971268,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447470"
    },
    {
      "id": "ntV5xZfzEk",
      "title": "Constrained Binary Decision Making",
      "abstract": "Binary statistical decision making involves choosing between two states based on statistical evidence. The optimal decision strategy is typically formulated through a constrained optimization problem, where both the objective and constraints are expressed as integrals involving two Lebesgue measurable functions, one of which represents the strategy being optimized. In this work, we present a comprehensive formulation of the binary decision making problem and provide a detailed characterization of the optimal solution. Our framework encompasses a wide range of well-known and recently proposed decision making problems as specific cases. We demonstrate how our generic approach can be used to derive the optimal decision strategies for these diverse instances. Our results offer a robust mathematical tool that simplifies the process of solving both existing and novel formulations of binary decision making problems which are in the core of many Machine Learning algorithms.",
      "authors": [
        "Daniel Průša",
        "Vojtech Franc"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=ntV5xZfzEk",
      "cdate": 1715757964255,
      "mdate": 1736894614473,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447475"
    },
    {
      "id": "O23XfTnhWR",
      "title": "Graphcode: Learning from multiparameter persistent homology using graph neural networks",
      "abstract": "We introduce graphcodes, a novel multi-scale summary of the topological properties of a dataset that is based on the well-established theory of persistent homology. Graphcodes handle datasets that are filtered along two real-valued scale parameters. Such multi-parameter topological summaries are usually based on complicated theoretical foundations and difficult to compute; in contrast, graphcodes yield an informative and interpretable summary and can be computed as efficient as one-parameter summaries. Moreover, a graphcode is simply an embedded graph and can therefore be readily integrated in machine learning pipelines using graph neural networks. We describe such a pipeline and demonstrate that graphcodes achieve better classification accuracy than state-of-the-art approaches on various datasets.",
      "authors": [
        "Florian Russold",
        "Michael Kerber"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=O23XfTnhWR",
      "cdate": 1715757958506,
      "mdate": 1730873971205,
      "matched_keywords": [
        "neural network",
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447479"
    },
    {
      "id": "exATQD4HSv",
      "title": "A scalable generative model for dynamical system reconstruction from neuroimaging data",
      "abstract": "Data-driven inference of the generative dynamics underlying a set of observed time series is of growing interest in machine learning and the natural sciences. In neuroscience, such methods promise to alleviate the need to handcraft models based on biophysical principles and allow to automatize the inference of inter-individual differences in brain dynamics. \nRecent breakthroughs in training techniques for state space models (SSMs) specifically geared toward dynamical systems (DS) reconstruction (DSR) enable to recover the underlying system including its geometrical (attractor) and long-term statistical invariants from even short time series. These techniques are based on control-theoretic ideas, like modern variants of teacher forcing (TF), to ensure stable loss gradient propagation while training. \nHowever, as it currently stands, these techniques are not directly applicable to data modalities where current observations depend on an entire history of previous states due to a signal’s filtering properties, as common in neuroscience (and physiology more generally). \nProminent examples are the blood oxygenation level dependent (BOLD) signal in functional magnetic resonance imaging (fMRI) or Ca$^{2+}$ imaging data. \nSuch types of signals render the SSM's decoder model non-invertible, a requirement for previous TF-based methods.\nHere, exploiting the recent success of control techniques for training SSMs, we propose a novel algorithm that solves this problem and scales exceptionally well with model dimensionality and filter length. We demonstrate its efficiency in reconstructing dynamical systems, including their state space geometry and long-term temporal properties, from just short BOLD time series.",
      "authors": [
        "Eric Volkmann",
        "Alena Brändle",
        "Daniel Durstewitz",
        "Georgia Koppe"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=exATQD4HSv",
      "cdate": 1715757952980,
      "mdate": 1730873971142,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447484"
    },
    {
      "id": "pGOBEYcXzs",
      "title": "Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models",
      "abstract": "Binarization, which converts weight parameters to binary values, has emerged as an effective strategy to reduce the size of large language models (LLMs). However, typical binarization techniques significantly diminish linguistic effectiveness of LLMs.\nTo address this issue, we introduce a novel binarization technique called Mixture of Scales (BinaryMoS). Unlike conventional methods, BinaryMoS employs multiple scaling experts for binary weights, dynamically merging these experts for each token to adaptively generate scaling factors. This token-adaptive approach boosts the representational power of binarized LLMs by enabling contextual adjustments to the values of binary weights. Moreover, because this adaptive process only involves the scaling factors rather than the entire weight matrix, BinaryMoS maintains compression efficiency similar to traditional static binarization methods. Our experimental results reveal that BinaryMoS surpasses conventional binarization techniques in various natural language processing tasks and even outperforms 2-bit quantization methods, all while maintaining similar model size to static binarization techniques.",
      "authors": [
        "Dongwon Jo",
        "Taesu Kim",
        "Yulhwa Kim",
        "Jae-Joon Kim"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pGOBEYcXzs",
      "cdate": 1715757833355,
      "mdate": 1730873971031,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447488"
    },
    {
      "id": "VqkAKQibpq",
      "title": "SGLang: Efficient Execution of Structured Language Model Programs",
      "abstract": "Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to $6.4\\times$ higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang.",
      "authors": [
        "Lianmin Zheng",
        "Liangsheng Yin",
        "Zhiqiang Xie",
        "Chuyue Sun",
        "Jeff Huang",
        "Cody Hao Yu",
        "Shiyi Cao",
        "Christos Kozyrakis",
        "Ion Stoica",
        "Joseph E. Gonzalez",
        "Clark Barrett",
        "Ying Sheng"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=VqkAKQibpq",
      "cdate": 1715757712664,
      "mdate": 1730873970967,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447493"
    },
    {
      "id": "qwl3EiDi9r",
      "title": "Integrating GNN and Neural ODEs for Estimating Non-Reciprocal Two-Body Interactions in Mixed-Species Collective Motion",
      "abstract": "Analyzing the motion of multiple biological agents, be it cells or individual animals, is pivotal for the understanding of complex collective behaviors. \nWith the advent of advanced microscopy, detailed images of complex tissue formations involving multiple cell types have become more accessible in recent years. However, deciphering the underlying rules that govern cell movements is far from trivial. Here, we present a novel deep learning framework for estimating the underlying equations of motion from observed trajectories, a pivotal step in decoding such complex dynamics. \nOur framework integrates graph neural networks with neural differential equations, enabling effective prediction of two-body interactions based on the states of the interacting entities. We demonstrate the efficacy of our approach through two numerical experiments. First, we used simulated data from a toy model to tune the hyperparameters. Based on the obtained hyperparameters, we then applied this approach to a more complex model with non-reciprocal forces that mimic the collective dynamics of the cells of slime molds. Our results show that the proposed method can accurately estimate the functional forms of two-body interactions -- even when they are nonreciprocal -- thereby precisely replicating both individual and collective behaviors within these systems.",
      "authors": [
        "Masahito Uwamichi",
        "Simon K. Schnyder",
        "Tetsuya J. Kobayashi",
        "Satoshi Sawai"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=qwl3EiDi9r",
      "cdate": 1715757680611,
      "mdate": 1736822117375,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447498"
    },
    {
      "id": "xCUXJqQySD",
      "title": "Med-Real2Sim: Non-Invasive Medical Digital Twins using Physics-Informed Self-Supervised Learning",
      "abstract": "A digital twin is a virtual replica of a real-world physical phenomena that uses mathematical modeling to characterize and simulate its defining features. By constructing digital twins for disease processes, we can perform in-silico simulations that mimic patients' health conditions and counterfactual outcomes under hypothetical interventions in a virtual setting. This eliminates the need for invasive procedures or uncertain treatment decisions. In this paper, we propose a method to identify digital twin model parameters using only noninvasive patient health data. We approach the digital twin modeling as a composite inverse problem, and observe that its structure resembles pretraining and finetuning in self-supervised learning (SSL). Leveraging this, we introduce a physics-informed SSL algorithm that initially pretrains a neural network on the pretext task of learning a differentiable simulator of a physiological process. Subsequently, the model is trained to reconstruct physiological measurements from noninvasive modalities while being constrained by the physical equations learned in pretraining. We apply our method to identify digital twins of cardiac hemodynamics using noninvasive echocardiogram videos, and demonstrate its utility in unsupervised disease detection and in-silico clinical trials.",
      "authors": [
        "Keying Kuang",
        "Frances Dean",
        "Jack B. Jedlicki",
        "David Ouyang",
        "Anthony Philippakis",
        "David Sontag",
        "Ahmed Alaa"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=xCUXJqQySD",
      "cdate": 1715757388928,
      "mdate": 1730873970834,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447503"
    },
    {
      "id": "Mi853QaJx6",
      "title": "On the Worst Prompt Performance of Large Language Models",
      "abstract": "The performance of large language models (LLMs) is acutely sensitive to the phrasing of prompts, which raises significant concerns about their reliability in real-world scenarios. Existing studies often divide prompts into task-level instructions and case-level inputs and primarily focus on evaluating and improving robustness against variations in tasks-level instructions. However, this setup fails to fully address the diversity of real-world user queries and assumes the existence of task-specific datasets. To address these limitations, we introduce RobustAlpacaEval, a new benchmark that consists of semantically equivalent case-level queries and emphasizes the importance of using the worst prompt performance to gauge the lower bound of model performance. Extensive experiments on RobustAlpacaEval with ChatGPT and six open-source LLMs from the Llama, Mistral, and Gemma families uncover substantial variability in model performance; for instance, a difference of 45.48% between the worst and best performance for the Llama-2-70B-chat model, with its worst performance dipping as low as 9.38%. We further illustrate the difficulty in identifying the worst prompt from both model-agnostic and model-dependent perspectives, emphasizing the absence of a shortcut to characterize the worst prompt. We also attempt to enhance the worst prompt performance using existing prompt engineering and prompt consistency methods, but find that their impact is limited. These findings underscore the need to create more resilient LLMs that can maintain high performance across diverse prompts.",
      "authors": [
        "Bowen Cao",
        "Deng Cai",
        "Zhisong Zhang",
        "Yuexian Zou",
        "Wai Lam"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Mi853QaJx6",
      "cdate": 1715757346345,
      "mdate": 1730873970761,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447508"
    },
    {
      "id": "R0bnWrpIeN",
      "title": "CoSy: Evaluating Textual Explanations of Neurons",
      "abstract": "A crucial aspect of understanding the complex nature of Deep Neural Networks (DNNs) is the ability to explain learned concepts within their latent representations. While methods exist to connect neurons to human-understandable textual descriptions, evaluating the quality of these explanations is challenging due to the lack of a unified quantitative approach. We introduce CoSy (Concept Synthesis), a novel, architecture-agnostic framework for evaluating textual explanations of latent neurons. Given textual explanations, our proposed framework uses a generative model conditioned on textual input to create data points representing the explanations. By comparing the neuron's response to these generated data points and control data points, we can estimate the quality of the explanation. We validate our framework through sanity checks and benchmark various neuron description methods for Computer Vision tasks, revealing significant differences in quality.",
      "authors": [
        "Laura Kopf",
        "Philine Lou Bommer",
        "Anna Hedström",
        "Sebastian Lapuschkin",
        "Marina MC Höhne",
        "Kirill Bykov"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=R0bnWrpIeN",
      "cdate": 1715757314300,
      "mdate": 1730873970745,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447513"
    },
    {
      "id": "WEoOreP0n5",
      "title": "Efficient Reinforcement Learning by Discovering Neural Pathways",
      "abstract": "Reinforcement learning (RL) algorithms have been very successful at tackling complex control problems, such as AlphaGo or fusion control. However, current research mainly emphasizes solution quality, often achieved by using large models trained on large amounts of data, and does not account for the financial, environmental, and societal costs associated with developing and deploying such models. Modern neural networks are often overparameterized and a significant number of parameters can be pruned without meaningful loss in performance, resulting in more efficient use of the model's capacity lottery ticket. We present a methodology for identifying sub-networks within a larger network in reinforcement learning (RL). We call such sub-networks, neural pathways. We show empirically that even very small learned sub-networks, using less than 5%  of the large network's parameters, can provide very good quality solutions. We also demonstrate the training of multiple pathways within the same networks in a multitask setup, where each pathway is encouraged to tackle a separate task. We evaluate empirically our approach on several continuous control tasks, in both online and offline training",
      "authors": [
        "Samin Yeasar Arnob",
        "Riyasat Ohib",
        "Sergey M. Plis",
        "Amy Zhang",
        "Alessandro Sordoni",
        "Doina Precup"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=WEoOreP0n5",
      "cdate": 1715756950312,
      "mdate": 1730873970560,
      "matched_keywords": [
        "reinforcement learning",
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447517"
    },
    {
      "id": "8ihVBYpMV4",
      "title": "Autoformalize Mathematical Statements by Symbolic Equivalence and Semantic Consistency",
      "abstract": "Autoformalization, the task of automatically translating natural language descriptions into a formal language, poses a significant challenge across various domains, especially in mathematics. Recent advancements in large language models (LLMs) have unveiled their promising capabilities to formalize even competition-level math problems. However, we observe a considerable discrepancy between pass@1 and pass@k accuracies in LLM-generated formalizations. To address this gap, we introduce a novel framework that scores and selects the best result from k autoformalization candidates based on two complementary self-consistency methods: symbolic equivalence and semantic consistency. Elaborately, symbolic equivalence identifies the logical homogeneity among autoformalization candidates using automated theorem provers, and semantic consistency evaluates the preservation of the original meaning by informalizing the candidates and computing the similarity between the embeddings of the original and informalized texts. \nOur extensive experiments on the MATH and miniF2F datasets demonstrate that our approach significantly enhances autoformalization accuracy, achieving up to 0.22-1.35x relative improvements across various LLMs and baseline methods.",
      "authors": [
        "Zenan Li",
        "Yifan Wu",
        "Zhaoyu Li",
        "Xinming Wei",
        "Xian Zhang",
        "Fan Yang",
        "Xiaoxing Ma"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=8ihVBYpMV4",
      "cdate": 1715756935327,
      "mdate": 1734574740047,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447522"
    },
    {
      "id": "9U0nLnNMJ7",
      "title": "Compact Language Models via Pruning and Knowledge Distillation",
      "abstract": "Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. In this paper, we investigate if pruning an existing LLM and then re-training it with a fraction <3% of the original training data can be a suitable alternative to repeated, full retraining. To this end, we develop a set of practical and effective **compression best practices** for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; we arrive at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. We use this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4x, and compare their performance to similarly-sized models on a variety of language modeling tasks. On these tasks, we perform better than Nemotron-3 8B and LLaMa2 7B using **up to 40x** fewer training tokens}, on par with Mistral 7B and Gemma 7B using **up to 85x fewer tokens** and slightly worse than LLaMa3 8B using **up to 159x fewer tokens**. Our models also compare favorably to state-of-the-art compression techniques from the literature.",
      "authors": [
        "Saurav Muralidharan",
        "Sharath Turuvekere Sreenivas",
        "Raviraj Bhuminand Joshi",
        "Marcin Chochowski",
        "Mostofa Patwary",
        "Mohammad Shoeybi",
        "Bryan Catanzaro",
        "Jan Kautz",
        "Pavlo Molchanov"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9U0nLnNMJ7",
      "cdate": 1715756906562,
      "mdate": 1737437494847,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447526"
    },
    {
      "id": "9m87e9Keq1",
      "title": "RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold",
      "abstract": "Training on model-generated synthetic data is a promising approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this question for math reasoning via an empirical study, followed by building a conceptual understanding of our observations. First, we find that while the typical approach of finetuning a model on synthetic correct or positive problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner itself followed by subsequent fine-tuning on this self-generated data doubles the efficiency of the same synthetic problems. At the same time, training on model-generated positives can amplify various spurious  correlations, resulting in flat or even inverse scaling trends as the amount of data increases. Surprisingly, we find that several of these issues can be addressed if we also utilize negative responses, i.e., model-generated responses that are deemed incorrect by a final answer verifier. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or advantage of each intermediate step in the negative response. With this per-step scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by $\\mathbf{8 \\times}$. We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits robustness benefits of RL over imitating positive data alone.",
      "authors": [
        "Amrith Setlur",
        "Saurabh Garg",
        "Xinyang Geng",
        "Naman Garg",
        "Virginia Smith",
        "Aviral Kumar"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=9m87e9Keq1",
      "cdate": 1715756797928,
      "mdate": 1730873970366,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447530"
    },
    {
      "id": "OFmclNhp0y",
      "title": "Deterministic Uncertainty Propagation for Improved Model-Based Offline Reinforcement Learning",
      "abstract": "Current approaches to model-based offline reinforcement learning often incorporate uncertainty-based reward penalization to address the distributional shift problem. These approaches, commonly known as pessimistic value iteration, use Monte Carlo sampling to estimate the Bellman target to perform temporal difference-based policy evaluation. We find out that the randomness caused by this sampling step significantly delays convergence. We present a theoretical result demonstrating the strong dependency of suboptimality on the number of Monte Carlo samples taken per Bellman target calculation. Our main contribution is a deterministic approximation to the Bellman target that uses progressive moment matching, a method developed originally for deterministic variational inference. The resulting algorithm, which we call Moment Matching Offline Model-Based Policy Optimization (MOMBO), propagates the uncertainty of the next state through a nonlinear Q-network in a deterministic fashion by approximating the distributions of hidden layer activations by a normal distribution. We show that it is possible to provide tighter guarantees for the suboptimality of MOMBO than the existing Monte Carlo sampling approaches. We also observe MOMBO to converge faster than these approaches in a large set of benchmark tasks.",
      "authors": [
        "Abdullah Akgül",
        "Manuel Haussmann",
        "Melih Kandemir"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=OFmclNhp0y",
      "cdate": 1715756572129,
      "mdate": 1737018072558,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447535"
    },
    {
      "id": "SNmuKbU0am",
      "title": "Mixtures of Experts for Audio-Visual Learning",
      "abstract": "With the rapid development of multimedia technology, audio-visual learning has emerged as a promising research topic within the field of multimodal analysis. In this paper, we explore parameter-efficient transfer learning for audio-visual learning and propose the Audio-Visual Mixture of Experts (\\ourmethodname) to inject adapters into pre-trained models flexibly. Specifically, we introduce unimodal and cross-modal adapters as multiple experts to specialize in intra-modal and inter-modal information, respectively, and employ a lightweight router to dynamically allocate the weights of each expert according to the specific demands of each task. Extensive experiments demonstrate that our proposed approach \\ourmethodname achieves superior performance across multiple audio-visual tasks, \nincluding AVE, AVVP, AVS, and AVQA. Furthermore, visual-only experimental results also indicate that our approach can tackle challenging scenes where modality information is missing.\nThe source code is available at \\url{https://github.com/yingchengy/AVMOE}.",
      "authors": [
        "Ying Cheng",
        "Yang Li",
        "Junjie He",
        "Rui Feng"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=SNmuKbU0am",
      "cdate": 1715756473310,
      "mdate": 1730873970206,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.447540"
    },
    {
      "id": "gkOzoHBXUw",
      "title": "Federated Fine-tuning of Large Language Models under Heterogeneous Tasks and Client Resources",
      "abstract": "Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs). While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients.This study introduces FlexLoRA, a simple yet effective  aggregation scheme for LLM fine-tuning, which mitigates the \"buckets effect\" in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants. FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge. By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources. Involving thousands of clients performing heterogeneous NLP tasks and client resources, our experiments validate the efficacy of FlexLoRA, with the federated global model achieving consistently better improvement over SOTA FL methods in downstream NLP task performance across various heterogeneous distributions. FlexLoRA's practicality is further underscored by our theoretical analysis and its seamless integration with existing LoRA-based FL methods, offering a path toward cross-device, privacy-preserving federated tuning for LLMs.",
      "authors": [
        "Jiamu Bai",
        "Daoyuan Chen",
        "Bingchen Qian",
        "Liuyi Yao",
        "Yaliang Li"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=gkOzoHBXUw",
      "cdate": 1715756304623,
      "mdate": 1730873970107,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447545"
    },
    {
      "id": "YRemB4naKK",
      "title": "Deterministic Policies for Constrained Reinforcement Learning in Polynomial Time",
      "abstract": "We present a novel algorithm that efficiently computes near-optimal deterministic policies for constrained reinforcement learning (CRL) problems. Our approach combines three key ideas: (1) value-demand augmentation, (2) action-space approximate dynamic programming, and (3) time-space rounding. Our algorithm constitutes a fully polynomial-time approximation scheme (FPTAS) for any time-space recursive (TSR) cost criteria. A TSR criteria requires the cost of a policy to be computable recursively over both time and (state) space, which includes classical expectation, almost sure, and anytime constraints. Our work answers three open questions spanning two long-standing lines of research: polynomial-time approximability is possible for 1) anytime-constrained policies, 2) almost-sure-constrained policies, and 3) deterministic expectation-constrained policies.",
      "authors": [
        "Jeremy McMahan"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=YRemB4naKK",
      "cdate": 1715756153815,
      "mdate": 1730873970088,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447550"
    },
    {
      "id": "jCMYIUwprx",
      "title": "INDICT: Code Generation with Internal Dialogues of Critiques for Both Security and Helpfulness",
      "abstract": "Large language models (LLMs) for code are typically trained to align with natural language instructions to closely follow their intentions and requirements. However, in many practical scenarios, it becomes increasingly challenging for these models to navigate the intricate boundary between helpfulness and safety, especially against highly complex yet potentially malicious instructions. In this work, we introduce INDICT: a new framework that empowers LLMs with Internal Dialogues of Critiques for both safety and helpfulness guidance. The internal dialogue is a dual cooperative system between a safety-driven critic and a helpfulness-driven critic. Each critic provides analysis against the given task and corresponding generated response, equipped with external knowledge queried through relevant code snippets and tools like web search and code interpreter. We engage the dual critic system in both code generation stage as well as code execution stage, providing preemptive and post-hoc guidance respectively to LLMs. We evaluated INDICT on 8 diverse tasks across 8 programming languages from 5 benchmarks, using LLMs from 7B to 70B parameters. We observed that our approach can provide an advanced level of critiques of both safety and helpfulness analysis, significantly improving the quality of output codes (+10% absolute improvements in all models).",
      "authors": [
        "Hung Le",
        "Doyen Sahoo",
        "Yingbo Zhou",
        "Caiming Xiong",
        "Silvio Savarese"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jCMYIUwprx",
      "cdate": 1715756105408,
      "mdate": 1730873969940,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447555"
    },
    {
      "id": "EbSSBvwUWw",
      "title": "Matching the Statistical Query Lower Bound for $k$-Sparse Parity Problems with Sign Stochastic Gradient Descent",
      "abstract": "The $k$-sparse parity problem is a classical problem in computational complexity and algorithmic theory, serving as a key benchmark for understanding computational classes. In this paper, we solve the $k$-sparse parity problem with sign stochastic gradient descent, a variant of stochastic gradient descent (SGD) on two-layer fully-connected neural networks. We demonstrate that this approach can efficiently solve the $k$-sparse parity problem on a $d$-dimensional hypercube ($k\\le O(\\sqrt{d})$) with a sample complexity of $\\tilde{O}(d^{k-1})$ using $2^{\\Theta(k)}$ neurons, matching the established $\\Omega(d^{k})$ lower bounds of Statistical Query (SQ) models. Our theoretical analysis begins by constructing a good neural network capable of correctly solving the $k$-parity problem. We then demonstrate how a trained neural network with sign SGD can effectively approximate this good network, solving the $k$-parity problem with small statistical errors. To the best of our knowledge, this is the first result that matches the SQ lower bound for solving $k$-sparse parity problem using gradient-based methods.",
      "authors": [
        "Yiwen Kou",
        "Zixiang Chen",
        "Quanquan Gu",
        "Sham M. Kakade"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=EbSSBvwUWw",
      "cdate": 1715756083519,
      "mdate": 1730873969937,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447560"
    },
    {
      "id": "RzlCqnncQv",
      "title": "Leveraging Environment Interaction for Automated PDDL Translation and Planning with Large Language Models",
      "abstract": "Large Language Models (LLMs) have shown remarkable performance in various natural language tasks, but they often struggle with planning problems that require structured reasoning. To address this limitation, the conversion of planning problems into the Planning Domain Definition Language (PDDL) has been proposed as a potential solution, enabling the use of automated planners. However, generating accurate PDDL files typically demands human inputs or correction, which can be time-consuming and costly. In this paper, we propose a novel approach that leverages LLMs and environment feedback to automatically generate PDDL domain and problem description files without the need for human intervention. Our method introduces an iterative refinement process that generates multiple problem PDDL candidates and progressively refines the domain PDDL based on feedback obtained from interacting with the environment. To guide the refinement process, we develop an Exploration Walk (EW) metric, which provides rich feedback signals for LLMs to update the PDDL file. We evaluate our approach on $10$ PDDL environments. We achieve an average task solve rate of 66\\% compared to a 29\\% solve rate by GPT-4's intrinsic planning with chain-of-thought prompting. Our work enables the automated modeling of planning environments using LLMs and environment feedback, eliminating the need for human intervention in the PDDL translation process and paving the way for more reliable LLM agents in challenging problems. Our code is available at https://github.com/BorealisAI/llm-pddl-planning",
      "authors": [
        "Sadegh Mahdavi",
        "Raquel Aoki",
        "Keyi Tang",
        "Yanshuai Cao"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=RzlCqnncQv",
      "cdate": 1715756049997,
      "mdate": 1730873969825,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447565"
    },
    {
      "id": "z2739hYuR3",
      "title": "Provably Efficient Reinforcement Learning with Multinomial Logit Function Approximation",
      "abstract": "We study a new class of MDPs that employs multinomial logit (MNL) function approximation to ensure valid probability distributions over the state space. Despite its significant benefits, incorporating the non-linear function raises substantial challenges in both *statistical* and *computational* efficiency. The best-known result of Hwang and Oh [2023] has achieved an $\\widetilde{\\mathcal{O}}(\\kappa^{-1}dH^2\\sqrt{K})$ regret upper bound, where $\\kappa$ is a problem-dependent quantity, $d$ is the feature dimension, $H$ is the episode length, and $K$ is the number of episodes. However, we observe that $\\kappa^{-1}$ exhibits polynomial dependence on the number of reachable states, which can be as large as the state space size in the worst case and thus undermines the motivation for function approximation. Additionally, their method requires storing all historical data and the time complexity scales linearly with the episode count, which is computationally expensive. In this work, we propose a statistically efficient algorithm that achieves a regret of $\\widetilde{\\mathcal{O}}(dH^2\\sqrt{K} + \\kappa^{-1}d^2H^2)$, eliminating the dependence on $\\kappa^{-1}$ in the dominant term for the first time. We then address the computational challenges by introducing an enhanced algorithm that achieves the same regret guarantee but with only constant cost. Finally, we establish the first lower bound for this problem, justifying the optimality of our results in $d$ and $K$.",
      "authors": [
        "Long-Fei Li",
        "Yu-Jie Zhang",
        "Peng Zhao",
        "Zhi-Hua Zhou"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=z2739hYuR3",
      "cdate": 1715755839916,
      "mdate": 1736994725672,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447569"
    },
    {
      "id": "pwLdvYIMrF",
      "title": "Train-Attention: Meta-Learning Where to Focus in Continual Knowledge Learning",
      "abstract": "Previous studies on continual knowledge learning (CKL) in large language models (LLMs) have predominantly focused on approaches such as regularization, architectural modifications, and rehearsal techniques to mitigate catastrophic forgetting. However, these methods naively inherit the inefficiencies of standard training procedures, indiscriminately applying uniform weight across all tokens, which can lead to unnecessary parameter updates and increased forgetting. To address these shortcomings, we propose a novel CKL approach termed Train-Attention-Augmented Language Model (TAALM), which enhances learning efficiency by dynamically predicting and applying weights to tokens based on their usefulness. This method employs a meta-learning framework that optimizes token importance predictions, facilitating targeted knowledge updates and minimizing forgetting. Also, we observe that existing benchmarks do not clearly exhibit the trade-off between learning and retaining, therefore we propose a new benchmark, LAMA-ckl, to address this issue. Through experiments conducted on both newly introduced and established CKL benchmarks, TAALM proves the state-of-the-art performance upon the baselines, and also shows synergistic compatibility when integrated with previous CKL approaches. The code and the dataset are available online.",
      "authors": [
        "Yeongbin Seo",
        "Dongha Lee",
        "Jinyoung Yeo"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pwLdvYIMrF",
      "cdate": 1715755694612,
      "mdate": 1737783769595,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447574"
    },
    {
      "id": "opt72TYzwZ",
      "title": "Optimal ablation for interpretability",
      "abstract": "Interpretability studies often involve tracing the flow of information through machine learning models to identify specific model components that perform relevant computations for tasks of interest. Prior work quantifies the importance of a model component on a particular task by measuring the impact of performing ablation on that component, or simulating model inference with the component disabled.\n We propose a new method, optimal ablation (OA), and show that OA-based component importance has theoretical and empirical advantages over measuring importance via other ablation methods. We also show that OA-based component importance can benefit several downstream interpretability tasks, including circuit discovery, localization of factual recall, and latent prediction.",
      "authors": [
        "Maximilian Li",
        "Lucas Janson"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=opt72TYzwZ",
      "cdate": 1715755637253,
      "mdate": 1730873969558,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447579"
    },
    {
      "id": "AVrGtVrx10",
      "title": "Probabilistic Conformal Distillation for Enhancing Missing Modality Robustness",
      "abstract": "Multimodal models trained on modality-complete data are plagued with severe performance degradation when encountering modality-missing data. Prevalent cross-modal knowledge distillation-based methods precisely align the representation of modality-missing data and that of its modality-complete counterpart to enhance robustness. However, due to the irreparable information asymmetry, this determinate alignment is too stringent, easily inducing modality-missing features to capture spurious factors erroneously. In this paper, a novel multimodal Probabilistic Conformal Distillation (PCD) method is proposed, which considers the inherent indeterminacy in this alignment. Given a modality-missing input, our goal is to learn the unknown Probability Density Function (PDF) of the mapped variables in the modality-complete space, rather than relying on the brute-force point alignment. Specifically, PCD models the modality-missing feature as a probabilistic distribution, enabling it to satisfy two characteristics of the PDF. One is the extremes of probabilities of modality-complete feature points on the PDF, and the other is the geometric consistency between the modeled distributions and the peak points of different PDFs. Extensive experiments on a range of benchmark datasets demonstrate the superiority of PCD over state-of-the-art methods. Code is available at: https://github.com/mxchen-mc/PCD.",
      "authors": [
        "mengxi Chen",
        "Fei Zhang",
        "Zihua Zhao",
        "Jiangchao Yao",
        "Ya Zhang",
        "Yanfeng Wang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=AVrGtVrx10",
      "cdate": 1715755633456,
      "mdate": 1730873969512,
      "matched_keywords": [
        "multimodal"
      ],
      "fetched_at": "2025-08-10T23:47:09.447583"
    },
    {
      "id": "jzngdJQ2lY",
      "title": "Solving Minimum-Cost Reach Avoid using Reinforcement Learning",
      "abstract": "Current reinforcement-learning methods are unable to directly learn policies that solve the minimum cost reach-avoid problem to minimize cumulative costs subject to the constraints of reaching the goal and avoiding unsafe states, as the structure of this new optimization problem is incompatible with current methods. Instead, a surrogate problem is solved where all objectives are combined with a weighted sum. However, this surrogate objective results in suboptimal policies that do not directly minimize the cumulative cost. In this work, we propose RC-PPO, a reinforcement-learning-based method for solving the minimum-cost reach-avoid problem by using connections to Hamilton-Jacobi reachability. Empirical results demonstrate that RC-PPO learns policies with comparable goal-reaching rates to while achieving up to 57% lower cumulative costs compared to existing methods on a suite of minimum-cost reach-avoid benchmarks on the Mujoco simulator. The project page can be found at https://oswinso.xyz/rcppo.",
      "authors": [
        "Oswin So",
        "Cheng Ge",
        "Chuchu Fan"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=jzngdJQ2lY",
      "cdate": 1715755495252,
      "mdate": 1730873969450,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447587"
    },
    {
      "id": "5MIk4VFn1c",
      "title": "Private Attribute Inference from Images with Vision-Language Models",
      "abstract": "As large language models (LLMs) become ubiquitous in our daily tasks and digital interactions, associated privacy risks are increasingly in focus. While LLM privacy research has primarily focused on the leakage of model training data, it has recently been shown that LLMs can make accurate privacy-infringing inferences from previously unseen texts. With the rise of vision-language models (VLMs), capable of understanding both images and text, a key question is whether this concern transfers to the previously unexplored domain of benign images posted online. To answer this question, we compile an image dataset with human-annotated labels of the image owner's personal attributes. In order to understand the privacy risks posed by VLMs beyond traditional human attribute recognition, our dataset consists of images where the inferable private attributes do not stem from direct depictions of humans. On this dataset, we evaluate 7 state-of-the-art VLMs, finding that they can infer various personal attributes at up to 77.6% accuracy. Concerningly, we observe that accuracy scales with the general capabilities of the models, implying that future models can be misused as stronger inferential adversaries, establishing an imperative for the development of adequate defenses.",
      "authors": [
        "Batuhan Tömekçe",
        "Mark Vero",
        "Robin Staab",
        "Martin Vechev"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=5MIk4VFn1c",
      "cdate": 1715755473350,
      "mdate": 1730873969412,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447592"
    },
    {
      "id": "glGeXu1zG4",
      "title": "Learning to Understand: Identifying Interactions via the Möbius Transform",
      "abstract": "One of the key challenges in machine learning is to find interpretable representations of learned functions. The Möbius transform is essential for this purpose, as its coefficients correspond to unique *importance scores* for *sets of input variables*. This transform is closely related to widely used game-theoretic notions of importance like the *Shapley* and *Bhanzaf value*, but it also captures crucial higher-order interactions. Although computing the Möbius Transform of a function with $n$ inputs involves $2^n$ coefficients, it becomes tractable when the function is *sparse* and of *low-degree* as we show is the case for many real-world functions. Under these conditions, the complexity of the transform computation is significantly reduced. When there are $K$ non-zero coefficients, our algorithm recovers the Möbius transform in $O(Kn)$ samples and $O(Kn^2)$ time asymptotically under certain assumptions, the first non-adaptive algorithm to do so. We also uncover a surprising connection between group testing and the Möbius transform. For functions where all interactions involve at most $t$ inputs, we use group testing results to compute the Möbius transform with $O(Kt\\log n)$ sample complexity and $O(K\\mathrm{poly}(n))$ time.  A robust version of this algorithm withstands noise and maintains this complexity. This marks the first $n$ sub-linear query complexity, noise-tolerant algorithm for the Möbius transform. While our algorithms are conceptualized in an idealized setting, they indicate that the Möbius transform is a potent tool for interpreting deep learning models.",
      "authors": [
        "Justin Singh Kang",
        "Yigit Efe Erginbas",
        "Landon Butler",
        "Ramtin Pedarsani",
        "Kannan Ramchandran"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=glGeXu1zG4",
      "cdate": 1715755159012,
      "mdate": 1736895003631,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447597"
    },
    {
      "id": "Q8yfhrBBD8",
      "title": "Bridge-IF: Learning Inverse Protein Folding with Markov Bridges",
      "abstract": "Inverse protein folding is a fundamental task in computational protein design, which aims to design protein sequences that fold into the desired backbone structures. While the development of machine learning algorithms for this task has seen significant success, the prevailing approaches, which predominantly employ a discriminative formulation, frequently encounter the error accumulation issue and often fail to capture the extensive variety of plausible sequences. To fill these gaps, we propose Bridge-IF, a generative diffusion bridge model for inverse folding, which is designed to learn the probabilistic dependency between the distributions of backbone structures and protein sequences. Specifically, we harness an expressive structure encoder to propose a discrete, informative prior derived from structures, and establish a Markov bridge to connect this prior with native sequences. During the inference stage, Bridge-IF progressively refines the prior sequence, culminating in a more plausible design. Moreover, we introduce a reparameterization perspective on Markov bridge models, from which we derive a simplified loss function that facilitates more effective training. We also modulate protein language models (PLMs) with structural conditions to precisely approximate the Markov bridge process, thereby significantly enhancing generation performance while maintaining parameter-efficient training. Extensive experiments on well-established benchmarks demonstrate that Bridge-IF predominantly surpasses existing baselines in sequence recovery and excels in the design of plausible proteins with high foldability. The code is available at https://github.com/violet-sto/Bridge-IF.",
      "authors": [
        "Yiheng Zhu",
        "Jialu Wu",
        "Qiuyi Li",
        "Jiahuan Yan",
        "Mingze Yin",
        "Wei Wu",
        "Mingyang Li",
        "Jieping Ye",
        "Zheng Wang",
        "Jian Wu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=Q8yfhrBBD8",
      "cdate": 1715755139793,
      "mdate": 1730873969273,
      "matched_keywords": [
        "machine learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447602"
    },
    {
      "id": "cIXETwTkhK",
      "title": "Training Binary Neural Networks via Gaussian Variational Inference and Low-Rank Semidefinite Programming",
      "abstract": "Current methods for training Binarized Neural Networks (BNNs) heavily rely on the heuristic straight-through estimator (STE), which crucially enables the application of SGD-based optimizers to the combinatorial training problem. Although the STE heuristics and their variants have led to significant improvements in BNN performance, their theoretical underpinnings remain unclear and relatively understudied. In this paper, we propose a theoretically motivated optimization framework for BNN training based on Gaussian variational inference. In its simplest form, our approach yields a non-convex linear programming formulation whose variables and associated gradients motivate the use of latent weights and STE gradients. More importantly, our framework allows us to  formulate  semidefinite programming (SDP) relaxations to the BNN training task. Such formulations are able to explicitly models pairwise correlations between weights during training, leading to a more accurate optimization characterization of the training problem. As the size of such formulations grows quadratically in the number of weights, quickly becoming intractable for large networks, we apply the Burer-Monteiro approach and only optimize over linear-size low-rank SDP solutions. Our empirical evaluation on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet datasets shows our method consistently outperforming all state-of-the-art algorithms for training BNNs.",
      "authors": [
        "Lorenzo Orecchia",
        "Jiawei Hu",
        "Xue He",
        "Wang Zhe Mark",
        "Xulei Yang",
        "Min Wu",
        "Xue Geng"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=cIXETwTkhK",
      "cdate": 1715755111928,
      "mdate": 1736667691080,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447606"
    },
    {
      "id": "lsd27JUJ8v",
      "title": "Exploiting the Replay Memory Before Exploring the Environment: Enhancing Reinforcement Learning Through Empirical MDP Iteration",
      "abstract": "Reinforcement learning (RL) algorithms are typically based on optimizing a Markov Decision Process (MDP) using the optimal Bellman equation. Recent studies have revealed that focusing the optimization of Bellman equations solely on in-sample actions tends to result in more stable optimization, especially in the presence of function approximation. Upon on these findings, in this paper, we propose an Empirical MDP Iteration (EMIT) framework. EMIT constructs a sequence of empirical MDPs using data from the growing replay memory. For each of these empirical MDPs, it learns an estimated Q-function denoted as $\\widehat{Q}$. The key strength is that by restricting the Bellman update to in-sample bootstrapping, each empirical MDP converges to a unique optimal $\\widehat{Q}$ function. Furthermore, gradually expanding from the empirical MDPs to the original MDP induces a monotonic policy improvement. Instead of creating entirely new algorithms, we demonstrate that EMIT can be seamlessly integrated with existing online RL algorithms, effectively acting as a regularizer for contemporary Q-learning methods. We show this by implementing EMIT for two representative RL algorithms, DQN and TD3. Experimental results on Atari and MuJoCo benchmarks show that EMIT significantly reduces estimation errors and substantially improves the performance of both algorithms.",
      "authors": [
        "Hongming Zhang",
        "Chenjun Xiao",
        "Chao Gao",
        "Han Wang",
        "bo xu",
        "Martin Müller"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=lsd27JUJ8v",
      "cdate": 1715755061999,
      "mdate": 1734627612704,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447611"
    },
    {
      "id": "YawXY6mWiK",
      "title": "A Full-duplex Speech Dialogue Scheme Based On Large Language Model",
      "abstract": "We present a generative dialogue system capable of operating in a full-duplex manner, allowing for seamless interaction. It is based on a large language model (LLM) carefully aligned to be aware of a perception module, a motor function module, and the concept of a simple finite state machine (called neural FSM) with two states. The perception and motor function modules operate in tandem, allowing the system to speak and listen to the user simultaneously. The LLM generates textual tokens for inquiry responses and makes autonomous decisions to start responding to, wait for, or interrupt the user by emitting control tokens to the neural FSM. All these tasks of the LLM are carried out as next token prediction on a serialized view of the dialogue in real-time. In automatic quality evaluations simulating real-life interaction, the proposed system reduces the average conversation response latency by more than threefold compared with LLM-based half-duplex dialogue systems while responding within less than 500 milliseconds in more than 50% of evaluated interactions. Running an LLM with only 8 billion parameters, our system exhibits an 8% higher interruption precision rate than the best available commercial LLM for voice-based dialogue.",
      "authors": [
        "Peng Wang",
        "Songshuo Lu",
        "Yaohua Tang",
        "Sijie Yan",
        "Wei Xia",
        "Yuanjun Xiong"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=YawXY6mWiK",
      "cdate": 1715755041470,
      "mdate": 1730873968988,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447616"
    },
    {
      "id": "pPeXYByHNd",
      "title": "MSAGPT: Neural Prompting Protein Structure Prediction via MSA Generative Pre-Training",
      "abstract": "Multiple Sequence Alignment (MSA) plays a pivotal role in unveiling the evolutionary trajectories of protein families. The accuracy of protein structure predictions is often compromised for protein sequences that lack sufficient homologous information to construct high-quality MSA. Although various methods have been proposed to generate high-quality MSA under these conditions, they fall short in comprehensively capturing the intricate co-evolutionary patterns within MSA or require guidance from external oracle models. Here we introduce MSAGPT, a novel approach to prompt protein structure predictions via MSA generative pre-training in a low-MSA regime. MSAGPT employs a simple yet effective 2D evolutionary positional encoding scheme to model the complex evolutionary patterns. Endowed by this, the flexible 1D MSA decoding framework facilitates zero- or few-shot learning. Moreover, we demonstrate leveraging the feedback from AlphaFold2 (AF2) can further enhance the model’s capacity via Rejective Fine-tuning (RFT) and Reinforcement Learning from AF2 Feedback (RLAF). Extensive experiments confirm the efficacy of MSAGPT in generating faithful and informative MSA (up to +8.5% TM-Score on few-shot scenarios). The transfer learning also demonstrates its great potential for the wide range of tasks resorting to the quality of MSA.",
      "authors": [
        "Bo Chen",
        "Zhilei Bei",
        "Xingyi Cheng",
        "Pan Li",
        "Jie Tang",
        "Le Song"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=pPeXYByHNd",
      "cdate": 1715755022054,
      "mdate": 1730873968897,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447620"
    },
    {
      "id": "IbIB8SBKFV",
      "title": "Improving Alignment and Robustness with Circuit Breakers",
      "abstract": "AI systems can take harmful actions and are highly vulnerable to adversarial attacks. We present an approach, inspired by recent advances in representation engineering, that interrupts the models as they respond with harmful outputs with \"circuit breakers.\" Existing techniques aimed at improving alignment, such as refusal training, are often bypassed. Techniques such as adversarial training try to plug these holes by countering specific attacks. As an alternative to refusal training and adversarial training, circuit-breaking directly controls the representations that are responsible for harmful outputs in the first place. Our technique can be applied to both text-only and multimodal language models to prevent the generation of harmful outputs without sacrificing utility -- even in the presence of powerful unseen attacks. Notably, while adversarial robustness in standalone image recognition remains an open challenge, circuit breakers allow the larger multimodal system to reliably withstand image \"hijacks\" that aim to produce harmful content. Finally, we extend our approach to AI agents, demonstrating considerable reductions in the rate of harmful actions when they are under attack. Our approach represents a significant step forward in the development of reliable safeguards to harmful behavior and adversarial attacks.",
      "authors": [
        "Andy Zou",
        "Long Phan",
        "Justin Wang",
        "Derek Duenas",
        "Maxwell Lin",
        "Maksym Andriushchenko",
        "J Zico Kolter",
        "Matt Fredrikson",
        "Dan Hendrycks"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=IbIB8SBKFV",
      "cdate": 1715754772080,
      "mdate": 1730873968707,
      "matched_keywords": [
        "multimodal",
        "ai agent"
      ],
      "fetched_at": "2025-08-10T23:47:09.447625"
    },
    {
      "id": "J2wI2rCG2u",
      "title": "Stochastic Taylor Derivative Estimator: Efficient amortization for arbitrary differential operators",
      "abstract": "Optimizing neural networks with loss that contain high-dimensional and high-order differential operators\n  is expensive to evaluate with back-propagation due to $\\mathcal{O}(d^{k})$ scaling of the derivative tensor size and the $\\mathcal{O}(2^{k-1}L)$ scaling in the computation graph, where $d$ is the dimension of the domain, $L$ is the number of ops in the forward computation graph, and $k$ is the derivative order. In previous works, the polynomial scaling in $d$ was addressed by amortizing the computation over the optimization process via randomization. Separately, the exponential scaling in $k$ for univariate functions ($d=1$) was addressed with high-order auto-differentiation (AD). In this work, we show how to efficiently perform arbitrary contraction of the derivative tensor of arbitrary order for multivariate functions, by properly constructing the input tangents to univariate high-order AD, which can be used to efficiently randomize any differential operator.\n  When applied to Physics-Informed Neural Networks (PINNs), our method provides >1000$\\times$ speed-up and >30$\\times$ memory reduction over randomization with first-order AD, and we can now solve 1-million-dimensional PDEs in 8 minutes on a single NVIDIA A100 GPU. This work opens the possibility of using high-order differential operators in large-scale problems.",
      "authors": [
        "Zekun Shi",
        "Zheyuan Hu",
        "Min Lin",
        "Kenji Kawaguchi"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=J2wI2rCG2u",
      "cdate": 1715754586923,
      "mdate": 1737120965617,
      "matched_keywords": [
        "neural network"
      ],
      "fetched_at": "2025-08-10T23:47:09.447630"
    },
    {
      "id": "vCIc9BXzze",
      "title": "Unveiling the Bias Impact on Symmetric Moral Consistency of Large Language Models",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities, surpassing human experts in various benchmark tests and playing a vital role in various industry sectors. Despite their effectiveness, a notable drawback of LLMs is their inconsistent moral behavior, which raises ethical concerns. This work delves into symmetric moral consistency in large language models and demonstrates that modern LLMs lack sufficient consistency ability in moral scenarios. Our extensive investigation of twelve popular LLMs reveals that their assessed consistency scores are influenced by position bias and selection bias rather than their intrinsic abilities. We propose a new framework tSMC, which gauges the effects of these biases and effectively mitigates the bias impact based on the Kullback–Leibler divergence to pinpoint LLMs' mitigated Symmetric Moral Consistency. We find that the ability of LLMs to maintain consistency varies across different moral scenarios. Specifically, LLMs show more consistency in scenarios with clear moral answers compared to those where no choice is morally perfect. The average consistency score of 12 LLMs ranges from $60.7\\%$ in high-ambiguity moral scenarios to $84.8\\%$ in low-ambiguity moral scenarios.",
      "authors": [
        "Ziyi Zhou",
        "Xinwei Guo",
        "Jiashi Gao",
        "Xiangyu Zhao",
        "Shiyao Zhang",
        "Xin Yao",
        "Xuetao Wei"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=vCIc9BXzze",
      "cdate": 1715754421817,
      "mdate": 1730873968658,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447634"
    },
    {
      "id": "7RQvjayHrM",
      "title": "RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling Large Language Models",
      "abstract": "Recent works show that assembling multiple off-the-shelf large language models (LLMs) can harness their complementary abilities. To achieve this, routing is a promising method, which learns a router to select the most suitable LLM for each query. However, existing routing models are ineffective when multiple LLMs perform well for a query. To address this problem, in this paper, we propose a method called query-based Router by Dual Contrastive learning (RouterDC). The RouterDC model, which consists of an encoder and LLM embeddings, is trained by two proposed contrastive losses (sample-LLM and sample-sample losses). Experimental results show that RouterDC is effective in assembling LLMs and largely outperforms individual top-performing LLMs as well as existing routing methods on both in-distribution (+2.76\\%) and out-of-distribution (+1.90\\%) tasks. The source code is available at https://github.com/shuhao02/RouterDC.",
      "authors": [
        "Shuhao Chen",
        "Weisen Jiang",
        "Baijiong Lin",
        "James Kwok",
        "Yu Zhang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=7RQvjayHrM",
      "cdate": 1715754378398,
      "mdate": 1730873968596,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447639"
    },
    {
      "id": "haVPmN8UGi",
      "title": "GraphVis: Boosting LLMs with Visual Knowledge Graph Integration",
      "abstract": "The rapid evolution of large language models (LLMs) has expanded their capabilities across various data modalities, extending from well-established image data to increasingly popular graph data. Given the limitation of LLMs in hallucinations and inaccuracies in recalling factual knowledge, Knowledge Graph (KG) has emerged as a crucial data modality to support more accurate reasoning by LLMs. However, integrating structured knowledge from KGs into LLMs remains challenging, as most current KG-enhanced LLM methods directly convert the KG into linearized text triples, which is not as expressive as the original structured data. To address this, we introduce GraphVis, which conserves the intricate graph structure through the visual modality to enhance the comprehension of KGs with the aid of Large Vision Language Models (LVLMs). Our approach incorporates a unique curriculum fine-tuning scheme which first instructs LVLMs to recognize basic graphical features from the images, and subsequently incorporates reasoning on QA tasks with the visual graphs. This cross-modal methodology not only markedly enhances performance on standard textual QA  but also shows improved zero-shot VQA performance by utilizing synthetic graph images to augment the data for VQA tasks. We present comprehensive evaluations across commonsense reasoning QA benchmarks, where GraphVis provides an average improvement of 11.1% over its base model and outperforms existing KG-enhanced LLM approaches. Across VQA benchmarks such as ScienceQA that share similar scientific diagram images, GraphVis provides a notable gain of 4.32%.",
      "authors": [
        "Yihe Deng",
        "Chenchen Ye",
        "Zijie Huang",
        "Mingyu Derek Ma",
        "Yiwen Kou",
        "Wei Wang"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=haVPmN8UGi",
      "cdate": 1715754232690,
      "mdate": 1730873968563,
      "matched_keywords": [
        "large language model"
      ],
      "fetched_at": "2025-08-10T23:47:09.447644"
    },
    {
      "id": "82Ndsr4OS6",
      "title": "Adversarially Trained Weighted Actor-Critic for Safe Offline Reinforcement Learning",
      "abstract": "We propose WSAC (Weighted Safe Actor-Critic), a novel algorithm for Safe Offline Reinforcement Learning (RL) under functional approximation, which can robustly optimize policies to improve upon an arbitrary reference policy with limited data coverage. WSAC is designed as a two-player Stackelberg game to optimize a refined objective function. The actor optimizes the policy against two adversarially trained value critics with small importance-weighted Bellman errors, which focus on scenarios where the actor's performance is inferior to the reference policy. In theory, we demonstrate that when the actor employs a no-regret optimization oracle, WSAC achieves a number of guarantees: $(i)$ For the first time in the safe offline  RL setting, we establish that WSAC can produce a policy that outperforms {\\bf any} reference policy while maintaining the same level of safety, which is critical to designing a safe algorithm for offline RL. $(ii)$ WSAC achieves the optimal statistical convergence rate of $1/\\sqrt{N}$ to the reference policy, where $N$ is the size of the offline dataset. $(iii)$ We theoretically show that WSAC guarantees a safe policy improvement across a broad range of hyperparameters that control the degree of pessimism, indicating its practical robustness. Additionally, we offer a practical version of WSAC and compare it with existing state-of-the-art safe offline RL algorithms in several continuous control environments. WSAC outperforms all baselines across a range of tasks, supporting the theoretical results.",
      "authors": [
        "Honghao Wei",
        "Xiyue Peng",
        "Arnob Ghosh",
        "Xin Liu"
      ],
      "conference": "NeurIPS 2024",
      "venue_id": "NeurIPS.cc/2024/Conference",
      "url": "https://openreview.net/forum?id=82Ndsr4OS6",
      "cdate": 1715753894125,
      "mdate": 1736645647625,
      "matched_keywords": [
        "reinforcement learning"
      ],
      "fetched_at": "2025-08-10T23:47:09.447649"
    }
  ]
}